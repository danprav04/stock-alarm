--- START OF FILE project_structure_backend.txt ---

.gitignore
api_clients.py
app_analysis.log
config.py
database.py
email_generator.py
error_handler.py
ipo_analyzer.py
main.py
models.py
news_analyzer.py
project_structure.py
requirements.txt
stock_analyzer.py


---------- .gitignore ----------
venv
/__pycache__

---------- END .gitignore ----------


---------- api_clients.py ----------
# api_clients.py
import requests
import time
import json
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup  # For news scraping
import re  # For S-1/10-K parsing

from config import (
    GOOGLE_API_KEYS, FINNHUB_API_KEY, FINANCIAL_MODELING_PREP_API_KEY,
    EODHD_API_KEY, RAPIDAPI_UPCOMING_IPO_KEY, API_REQUEST_TIMEOUT,
    API_RETRY_ATTEMPTS, API_RETRY_DELAY, CACHE_EXPIRY_SECONDS, EDGAR_USER_AGENT,
    ALPHA_VANTAGE_API_KEY  # Added Alpha Vantage Key
)
from error_handler import logger
from database import SessionLocal  # Direct import for SessionLocal
from models import CachedAPIData


# Removed unused global variable: current_google_api_key_index

class APIClient:
    def __init__(self, base_url, api_key_name=None, api_key_value=None, headers=None):
        self.base_url = base_url
        self.api_key_name = api_key_name
        self.api_key_value = api_key_value
        self.headers = headers or {}
        if api_key_name and api_key_value:
            self.params = {api_key_name: api_key_value}
        else:
            self.params = {}

    def _get_cached_response(self, request_url_or_params_str):
        session = SessionLocal()
        try:
            current_time_utc = datetime.now(timezone.utc)
            cache_entry = session.query(CachedAPIData).filter(
                CachedAPIData.request_url_or_params == request_url_or_params_str,
                CachedAPIData.expires_at > current_time_utc
            ).first()
            if cache_entry:
                logger.info(f"Cache hit for: {request_url_or_params_str[:100]}...")
                return cache_entry.response_data
        except Exception as e:
            logger.error(f"Error reading from cache for '{request_url_or_params_str[:100]}...': {e}", exc_info=True)
        finally:
            session.close()
        return None

    def _cache_response(self, request_url_or_params_str, response_data, api_source):
        session = SessionLocal()
        try:
            now_utc = datetime.now(timezone.utc)
            expires_at_utc = now_utc + timedelta(seconds=CACHE_EXPIRY_SECONDS)

            session.query(CachedAPIData).filter(
                CachedAPIData.request_url_or_params == request_url_or_params_str).delete(synchronize_session=False)

            new_cache_entry = CachedAPIData(
                api_source=api_source,
                request_url_or_params=request_url_or_params_str,
                response_data=response_data,
                timestamp=now_utc,
                expires_at=expires_at_utc
            )
            session.add(new_cache_entry)
            session.commit()
            logger.info(f"Cached response for: {request_url_or_params_str[:100]}...")
        except Exception as e:
            logger.error(f"Error writing to cache for '{request_url_or_params_str[:100]}...': {e}", exc_info=True)
            session.rollback()
        finally:
            session.close()

    def request(self, method, endpoint, params=None, data=None, json_data=None, use_cache=True,
                api_source_name="unknown", is_json_response=True):
        url = f"{self.base_url}{endpoint}"
        current_call_params = params.copy() if params else {}
        full_query_params = self.params.copy()
        full_query_params.update(current_call_params)

        # Create a string representation for caching, ensuring order for params
        # and including JSON body if present.
        sorted_params = sorted(full_query_params.items()) if full_query_params else []
        param_string = "&".join([f"{k}={v}" for k, v in sorted_params])
        cache_key_str = f"{method.upper()}:{url}?{param_string}"
        if json_data:
            # Sort keys in json_data to ensure consistent cache key for same data
            sorted_json_data_str = json.dumps(json_data, sort_keys=True)
            cache_key_str += f"|BODY:{sorted_json_data_str}"

        if use_cache:
            cached_data = self._get_cached_response(cache_key_str)
            if cached_data is not None:
                return cached_data

        for attempt in range(API_RETRY_ATTEMPTS):
            try:
                response = requests.request(
                    method, url, params=full_query_params, data=data, json=json_data,
                    headers=self.headers, timeout=API_REQUEST_TIMEOUT
                )
                response.raise_for_status()  # Raises HTTPError for bad responses (4XX or 5XX)

                # If response is not expected to be JSON (e.g., raw text, HTML)
                if not is_json_response:
                    response_content = response.text
                    if use_cache:
                        self._cache_response(cache_key_str, response_content, api_source_name)
                    return response_content

                # Process as JSON
                response_json = response.json()
                if use_cache:
                    self._cache_response(cache_key_str, response_json, api_source_name)
                return response_json

            except requests.exceptions.HTTPError as e:
                # Log params, masking API key if it's in params
                log_params_for_error = {k: (
                    str(v)[:-6] + '******' if k == self.api_key_name and isinstance(v, str) and len(str(v)) > 6 else v)
                    for k, v in full_query_params.items()}
                # Log headers, masking sensitive headers
                log_headers_for_error = self.headers.copy()
                sensitive_header_keys = ["X-RapidAPI-Key", "Authorization", "Token"]  # Add more if needed
                for h_key in sensitive_header_keys:
                    if h_key in log_headers_for_error and isinstance(log_headers_for_error[h_key], str) and len(
                            log_headers_for_error[h_key]) > 6:
                        log_headers_for_error[h_key] = log_headers_for_error[h_key][:4] + "******" + \
                                                       log_headers_for_error[h_key][-4:]

                status_code = e.response.status_code
                logger.warning(
                    f"HTTP error on attempt {attempt + 1}/{API_RETRY_ATTEMPTS} for {url} "
                    f"(Params: {log_params_for_error}, Headers: {log_headers_for_error}): "
                    f"{status_code} - {e.response.text[:200]}..."
                )
                # Specific handling for Alpha Vantage rate limit note
                if api_source_name.startswith(
                        "alphavantage") and "Our standard API call frequency is 25 requests per day." in e.response.text:
                    logger.error(
                        f"Alpha Vantage API daily limit likely reached for key. Params: {log_params_for_error}")
                    return None  # Do not retry if daily limit message is present

                if status_code == 429:  # Too Many Requests
                    delay = API_RETRY_DELAY * (2 ** attempt)  # Exponential backoff
                    logger.info(f"Rate limit hit. Waiting for {delay} seconds.")
                    time.sleep(delay)
                elif 500 <= status_code < 600:  # Server-side errors
                    delay = API_RETRY_DELAY * (2 ** attempt)
                    logger.info(f"Server error. Waiting for {delay} seconds.")
                    time.sleep(delay)
                else:  # Other client errors (400, 401, 403, 404 etc.) - typically not retryable
                    logger.error(f"Non-retryable client error for {url}: {status_code} {e.response.reason}",
                                 exc_info=False)  # No exc_info for cleaner log for non-retryable
                    return None
            except requests.exceptions.RequestException as e:  # Other request issues (timeout, connection error)
                logger.warning(f"Request error on attempt {attempt + 1}/{API_RETRY_ATTEMPTS} for {url}: {e}")
                if attempt < API_RETRY_ATTEMPTS - 1:
                    delay = API_RETRY_DELAY * (2 ** attempt)
                    time.sleep(delay)
            except json.JSONDecodeError as e_json:  # If response.json() fails
                logger.error(
                    f"JSON decode error for {url} on attempt {attempt + 1}. Response text: {response.text[:500]}... Error: {e_json}")
                if attempt < API_RETRY_ATTEMPTS - 1:
                    delay = API_RETRY_DELAY * (2 ** attempt)  # Basic retry for transient decode issues
                    time.sleep(delay)
                else:
                    return None  # Failed all retries

        logger.error(f"All {API_RETRY_ATTEMPTS} attempts failed for {url}. Last query params: {full_query_params}")
        return None


class FinnhubClient(APIClient):
    def __init__(self):
        super().__init__("https://finnhub.io/api/v1", api_key_name="token", api_key_value=FINNHUB_API_KEY)

    def get_market_news(self, category="general", min_id=0):
        params = {"category": category}
        if min_id > 0: params["minId"] = min_id
        return self.request("GET", "/news", params=params, api_source_name="finnhub_news")

    def get_company_profile2(self, ticker):
        return self.request("GET", "/stock/profile2", params={"symbol": ticker}, api_source_name="finnhub_profile")

    def get_financials_reported(self, ticker, freq="quarterly"):
        # freq can be 'annual', 'quarterly', or 'ttm'
        params = {"symbol": ticker, "freq": freq}
        return self.request("GET", "/stock/financials-reported", params=params,
                            api_source_name="finnhub_financials_reported")

    def get_basic_financials(self, ticker, metric_type="all"):
        return self.request("GET", "/stock/metric", params={"symbol": ticker, "metric": metric_type},
                            api_source_name="finnhub_metrics")

    def get_ipo_calendar(self, from_date=None, to_date=None):
        if from_date is None: from_date = (datetime.now(timezone.utc) - timedelta(days=30)).strftime('%Y-%m-%d')
        if to_date is None: to_date = (datetime.now(timezone.utc) + timedelta(days=90)).strftime('%Y-%m-%d')
        params = {"from": from_date, "to": to_date}
        return self.request("GET", "/calendar/ipo", params=params, api_source_name="finnhub_ipo_calendar")

    def get_sec_filings(self, ticker, from_date=None, to_date=None):
        if from_date is None: from_date = (datetime.now(timezone.utc) - timedelta(days=365 * 2)).strftime('%Y-%m-%d')
        if to_date is None: to_date = datetime.now(timezone.utc).strftime('%Y-%m-%d')
        params = {"symbol": ticker, "from": from_date, "to": to_date}
        return self.request("GET", "/stock/filings", params=params, api_source_name="finnhub_filings")


class FinancialModelingPrepClient(APIClient):
    def __init__(self):
        super().__init__("https://financialmodelingprep.com/api/v3", api_key_name="apikey",
                         api_key_value=FINANCIAL_MODELING_PREP_API_KEY)

    def get_ipo_calendar(self, from_date=None, to_date=None):  # Note: FMP IPO Calendar is often premium
        params = {}
        if from_date: params["from"] = from_date
        if to_date: params["to"] = to_date
        logger.warning("FinancialModelingPrepClient.get_ipo_calendar called, but may be restricted by subscription.")
        return self.request("GET", "/ipo_calendar", params=params, api_source_name="fmp_ipo_calendar")

    def get_financial_statements(self, ticker, statement_type="income-statement", period="quarter", limit=40):
        actual_limit = limit
        # FMP has different historical data limits for free/paid plans and by period
        if period == "annual" and limit > 15:  # Max for annual usually around 5-10 for free, more for paid
            actual_limit = 15  # Adjust based on typical observed limits if known
            logger.debug(f"FMP {statement_type} for {ticker} (annual): limit adjusted to {actual_limit}.")
        elif period == "quarter" and limit > 60:  # Max for quarterly typically more
            actual_limit = 60
            logger.debug(f"FMP {statement_type} for {ticker} (quarterly): limit adjusted to {actual_limit}.")
        return self.request("GET", f"/{statement_type}/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_{statement_type.replace('-', '_')}_{period}")

    def get_key_metrics(self, ticker, period="quarter", limit=40):
        actual_limit = limit
        if period == "annual" and limit > 15:
            actual_limit = 15
        elif period == "quarter" and limit > 60:
            actual_limit = 60
        return self.request("GET", f"/key-metrics/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_key_metrics_{period}")

    def get_ratios(self, ticker, period="quarter", limit=40):
        actual_limit = limit
        if period == "annual" and limit > 15:
            actual_limit = 15
        elif period == "quarter" and limit > 60:
            actual_limit = 60
        return self.request("GET", f"/ratios/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_ratios_{period}")

    def get_company_profile(self, ticker):
        return self.request("GET", f"/profile/{ticker}", params={}, api_source_name="fmp_profile")

    def get_analyst_estimates(self, ticker, period="annual"):
        return self.request("GET", f"/analyst-estimates/{ticker}", params={"period": period},
                            api_source_name="fmp_analyst_estimates")


class AlphaVantageClient(APIClient):
    def __init__(self):
        super().__init__("https://www.alphavantage.co", api_key_name="apikey", api_key_value=ALPHA_VANTAGE_API_KEY)
        # Alpha Vantage has a specific call pattern, usually /query?function=FUNCTION_NAME&symbol=TICKER&apikey=KEY

    def get_company_overview(self, ticker):
        params = {"function": "OVERVIEW", "symbol": ticker}
        return self.request("GET", "/query", params=params, api_source_name="alphavantage_overview")

    def get_income_statement_quarterly(self, ticker):
        params = {"function": "INCOME_STATEMENT", "symbol": ticker}
        # Data contains "annualReports" and "quarterlyReports". We are interested in quarterlyReports.
        # Each report has fiscalDateEnding, reportedCurrency, totalRevenue, netIncome etc.
        # Reports are typically sorted oldest to newest.
        return self.request("GET", "/query", params=params, api_source_name="alphavantage_income_quarterly")

    def get_balance_sheet_quarterly(self, ticker):
        params = {"function": "BALANCE_SHEET", "symbol": ticker}
        return self.request("GET", "/query", params=params, api_source_name="alphavantage_balance_quarterly")

    def get_cash_flow_quarterly(self, ticker):
        params = {"function": "CASH_FLOW", "symbol": ticker}
        return self.request("GET", "/query", params=params, api_source_name="alphavantage_cashflow_quarterly")


class EODHDClient(APIClient):
    def __init__(self):
        super().__init__("https://eodhistoricaldata.com/api", api_key_name="api_token", api_key_value=EODHD_API_KEY)
        self.params["fmt"] = "json"  # Common param for EODHD

    def get_fundamental_data(self, ticker_with_exchange):  # e.g., AAPL.US
        return self.request("GET", f"/fundamentals/{ticker_with_exchange}", api_source_name="eodhd_fundamentals")

    def get_ipo_calendar(self, from_date=None, to_date=None):  # EODHD IPO data might be limited
        params = {}
        if from_date: params["from"] = from_date
        if to_date: params["to"] = to_date
        logger.warning("EODHDClient.get_ipo_calendar called, but may be restricted or have limited data.")
        return self.request("GET", "/calendar/ipos", params=params, api_source_name="eodhd_ipo_calendar")


class SECEDGARClient(APIClient):
    def __init__(self):
        self.company_tickers_url = "https://www.sec.gov/files/company_tickers.json"
        super().__init__("https://data.sec.gov/submissions/")  # Base for submissions API
        self.headers = {"User-Agent": EDGAR_USER_AGENT, "Accept-Encoding": "gzip, deflate"}
        self._cik_map = None
        self._archives_base = "https://www.sec.gov/Archives/edgar/data/"

    def _load_cik_map(self):
        if self._cik_map is None:
            logger.info("Fetching CIK map from SEC...")
            try:
                # Direct request, not using self.request to avoid base_url prepending for this specific URL
                response = requests.get(self.company_tickers_url, headers=self.headers, timeout=API_REQUEST_TIMEOUT)
                response.raise_for_status()
                data = response.json()
                self._cik_map = {item['ticker']: str(item['cik_str']).zfill(10)
                                 for item in data.values() if 'ticker' in item and 'cik_str' in item}
                logger.info(f"CIK map loaded with {len(self._cik_map)} entries.")
            except requests.exceptions.RequestException as e:
                logger.error(f"Error fetching CIK map from SEC: {e}", exc_info=True);
                self._cik_map = {}
            except json.JSONDecodeError as e_json:
                logger.error(f"Error decoding CIK map JSON from SEC: {e_json}", exc_info=True);
                self._cik_map = {}
        return self._cik_map

    def get_cik_by_ticker(self, ticker):
        ticker = ticker.upper()
        try:
            cik_map = self._load_cik_map()
            return cik_map.get(ticker)
        except Exception as e:
            logger.error(f"Unexpected error in get_cik_by_ticker for {ticker}: {e}", exc_info=True)
            return None

    def get_company_filings_summary(self, cik):
        if not cik: return None
        formatted_cik_for_api = str(cik).zfill(10)
        # Uses self.request from base APIClient class
        return self.request("GET", f"CIK{formatted_cik_for_api}.json", api_source_name="edgar_filings_summary")

    def get_filing_document_url(self, cik, form_type="10-K", priordate_str=None, count=1):
        # Fetches summary, then constructs URL. Summary uses self.request.
        if not cik: return None if count == 1 else []
        company_summary = self.get_company_filings_summary(cik)  # Uses self.request
        if not company_summary or "filings" not in company_summary or "recent" not in company_summary["filings"]:
            logger.warning(f"No recent filings data for CIK {cik} in company summary.")
            return None if count == 1 else []

        recent_filings = company_summary["filings"]["recent"]
        target_filings_info = []
        forms = recent_filings.get("form", [])
        accession_numbers = recent_filings.get("accessionNumber", [])
        primary_documents = recent_filings.get("primaryDocument", [])
        filing_dates = recent_filings.get("filingDate", [])

        priordate_dt = None
        if priordate_str:
            try:
                priordate_dt = datetime.strptime(priordate_str, '%Y-%m-%d').date()
            except ValueError:
                logger.warning(f"Invalid priordate_str: {priordate_str}. Ignoring.")

        for i, form in enumerate(forms):
            if form.upper() == form_type.upper():
                current_filing_date = datetime.strptime(filing_dates[i], '%Y-%m-%d').date()
                if priordate_dt and current_filing_date > priordate_dt: continue

                acc_num_no_hyphens = accession_numbers[i].replace('-', '')
                # Ensure CIK is int for URL construction if it was fetched as string and padded
                try:
                    cik_int_for_url = int(cik)
                except ValueError:
                    logger.error(f"CIK '{cik}' for URL construction is not a valid integer.")
                    continue  # Skip this filing if CIK format is bad for URL

                doc_url = f"{self._archives_base}{cik_int_for_url}/{acc_num_no_hyphens}/{primary_documents[i]}"
                target_filings_info.append({"url": doc_url, "date": current_filing_date, "form": form})

        if not target_filings_info:
            logger.info(f"No '{form_type}' filings for CIK {cik} matching criteria.")
            return None if count == 1 else []

        target_filings_info.sort(key=lambda x: x["date"], reverse=True)  # Get most recent first
        return target_filings_info[0]["url"] if count == 1 else [f_info["url"] for f_info in
                                                                 target_filings_info[:count]]

    def get_filing_text(self, filing_url):
        if not filing_url: return None
        logger.info(f"Fetching filing text from: {filing_url}")
        cache_key_str = f"GET_SEC_DOC:{filing_url}"  # Specific cache key for raw doc fetches
        cached_text = self._get_cached_response(cache_key_str)
        if cached_text is not None: return cached_text

        try:
            # Direct request, not using self.request's base_url logic for SEC Archives
            response = requests.get(filing_url, headers=self.headers,
                                    timeout=API_REQUEST_TIMEOUT + 30)  # Longer timeout for large docs
            response.raise_for_status()
            try:
                text_content = response.content.decode('utf-8')
            except UnicodeDecodeError:
                logger.warning(f"UTF-8 decode failed for {filing_url}, trying latin-1.")
                text_content = response.content.decode('latin-1', errors='replace')  # Replace errors to avoid crash

            self._cache_response(cache_key_str, text_content, "edgar_filing_text_content")
            return text_content
        except requests.exceptions.RequestException as e:
            logger.error(f"Error fetching SEC filing text from {filing_url}: {e}")
            return None


class GeminiAPIClient:
    def __init__(self):
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models"
        # self.current_api_key_index = 0 # Managed by _get_next_api_key_for_attempt

    def _get_next_api_key_for_attempt(self, overall_attempt_num, max_attempts_per_key, total_keys):
        # Determine which key to use based on overall_attempt_num
        # This rotates through keys if previous ones fail or hit limits within their retry window.
        key_group_index = (overall_attempt_num // max_attempts_per_key) % total_keys
        api_key = GOOGLE_API_KEYS[key_group_index]
        current_retry_for_this_key = (overall_attempt_num % max_attempts_per_key) + 1  # 1-based for logging
        logger.debug(
            f"Gemini: Using key ...{api_key[-4:]} (Index {key_group_index}), Attempt {current_retry_for_this_key}/{max_attempts_per_key}")
        return api_key, current_retry_for_this_key

    def generate_text(self, prompt, model="gemini-1.5-flash-latest"):
        max_attempts_per_key = API_RETRY_ATTEMPTS  # How many times to try each key
        total_keys = len(GOOGLE_API_KEYS)
        if total_keys == 0:
            logger.error("Gemini: No API keys configured.");
            return "Error: No Google API keys."

        # Prompt length check (crude character count, Gemini has token limits)
        # This is a safeguard, actual token limits are more complex.
        if len(prompt) > 150000:  # Using a general large char limit
            logger.warning(f"Gemini prompt length {len(prompt)} very long. Truncating to 30000 chars for safety.")
            # Truncate in a way that keeps the structure if possible, or just head + tail
            prompt = prompt[:150000] + "\n...[PROMPT TRUNCATED DUE TO EXCESSIVE LENGTH]..."

        # Total attempts will be total_keys * max_attempts_per_key
        for overall_attempt_num in range(total_keys * max_attempts_per_key):
            api_key, current_retry_for_this_key = self._get_next_api_key_for_attempt(
                overall_attempt_num, max_attempts_per_key, total_keys
            )

            url = f"{self.base_url}/{model}:generateContent?key={api_key}"
            # Standard payload for Gemini API
            payload = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {  # Adjust these as needed
                    "temperature": 0.5,  # Controls randomness, lower is more deterministic
                    "maxOutputTokens": 8192,  # Max tokens in the response
                    "topP": 0.9,  # Nucleus sampling: considers tokens with cumulative probability >= topP
                    "topK": 35  # Considers top K tokens
                },
                "safetySettings": [  # Standard safety settings
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                ]
            }
            try:
                response = requests.post(url, json=payload,
                                         timeout=API_REQUEST_TIMEOUT + 120)  # Longer timeout for GenAI
                response.raise_for_status()
                response_json = response.json()

                # Check for prompt feedback (blocked prompt)
                if response_json.get("promptFeedback", {}).get("blockReason"):
                    reason = response_json["promptFeedback"]["blockReason"]
                    logger.error(
                        f"Gemini prompt blocked for key ...{api_key[-4:]}. Reason: {reason}. Prompt: '{prompt[:100]}...'")
                    time.sleep(API_RETRY_DELAY);  # Wait before trying next key/attempt
                    continue  # Move to next attempt (potentially next key)

                # Check for candidates and content
                if "candidates" in response_json and response_json["candidates"]:
                    candidate = response_json["candidates"][0]
                    # Check for finishReason (e.g., "SAFETY", "MAX_TOKENS")
                    finish_reason = candidate.get("finishReason")
                    if finish_reason not in [None, "STOP", "MAX_TOKENS", "MODEL_LENGTH", "OK"]:  # OK is sometimes seen
                        logger.warning(
                            f"Gemini unexpected finish: {finish_reason} for key ...{api_key[-4:]}. Prompt: '{prompt[:100]}...'")
                        if finish_reason == "SAFETY":  # Candidate content blocked by safety
                            logger.error(f"Gemini candidate blocked by safety settings for key ...{api_key[-4:]}.")
                            time.sleep(API_RETRY_DELAY);  # Wait
                            continue  # Move to next attempt

                    content_part = candidate.get("content", {}).get("parts", [{}])[0]
                    if "text" in content_part:
                        return content_part["text"]
                    else:  # Should not happen if finishReason is STOP
                        logger.error(f"Gemini response missing text for key ...{api_key[-4:]}: {response_json}")
                else:  # No candidates, malformed response
                    logger.error(f"Gemini response malformed for key ...{api_key[-4:]}: {response_json}")

            except requests.exceptions.HTTPError as e:
                logger.warning(
                    f"Gemini API HTTP error key ...{api_key[-4:]} attempt {current_retry_for_this_key}: {e.response.status_code} - {e.response.text[:200]}. Prompt: '{prompt[:100]}...'")
                if e.response.status_code == 400:  # Bad Request (e.g. malformed prompt for API)
                    logger.error(
                        f"Gemini API Bad Request (400). Aborting for this prompt. Response: {e.response.text[:500]}")
                    return f"Error: Gemini API bad request (400). {e.response.text[:200]}"
                # Other errors like 429 (rate limit), 500 (server error) will cause a retry with delay (handled below)
            except requests.exceptions.RequestException as e:  # Timeout, connection error
                logger.warning(
                    f"Gemini API request error key ...{api_key[-4:]} attempt {current_retry_for_this_key}: {e}. Prompt: '{prompt[:100]}...'")
            except json.JSONDecodeError as e_json_gemini:
                logger.error(
                    f"Gemini API JSON decode error key ...{api_key[-4:]} attempt {current_retry_for_this_key}. Resp: {response.text[:500]}. Err: {e_json_gemini}")

            # If not the absolute last attempt, sleep before retrying (with current or next key)
            if overall_attempt_num < (total_keys * max_attempts_per_key) - 1:
                # Exponential backoff based on retries for THIS KEY, reset for new key
                time.sleep(API_RETRY_DELAY * (
                    current_retry_for_this_key))  # (overall_attempt_num % max_attempts_per_key) is retry for current key group

        logger.error(
            f"All attempts ({total_keys * max_attempts_per_key}) for Gemini API failed for prompt: {prompt[:100]}...")
        return "Error: Could not get response from Gemini API after multiple attempts."

    def summarize_text_with_context(self, text_to_summarize, context_summary, max_length=None):
        if max_length and len(text_to_summarize) > max_length:
            text_to_summarize = text_to_summarize[:max_length] + "\n... [TRUNCATED FOR BREVITY] ..."
            logger.info(f"Truncated text for Gemini summary to length: {max_length}")
        prompt = f"Context: {context_summary}\n\nPlease provide a concise and factual summary of the following text, focusing on key information relevant to the context:\n\nText:\n\"\"\"\n{text_to_summarize}\n\"\"\"\n\nSummary:"
        return self.generate_text(prompt)

    def analyze_sentiment_with_reasoning(self, text_to_analyze, context=""):
        # Updated prompt for clearer, structured sentiment output
        prompt = (
            f"Analyze the sentiment of the following text. "
            f"Context for analysis (if any): '{context}'.\n\n"
            f"Text to Analyze:\n\"\"\"\n{text_to_analyze}\n\"\"\"\n\n"
            f"Instructions: Respond with the sentiment classification and reasoning, structured as follows:\n"
            f"Sentiment: [Choose one: Positive, Negative, Neutral]\n"
            f"Reasoning: [Provide a brief 1-2 sentence explanation, citing specific phrases from the text if possible to justify the sentiment.]"
        )
        return self.generate_text(prompt)


# --- Helper functions for scraping and parsing ---
def scrape_article_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            # Common user agent
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Connection': 'keep-alive'
        }
        response = requests.get(url, headers=headers, timeout=API_REQUEST_TIMEOUT - 10,
                                allow_redirects=True)  # Slightly less timeout for scrape
        response.raise_for_status()

        # Check content type - if not HTML, probably not worth parsing with BeautifulSoup
        if 'html' not in response.headers.get('content-type', '').lower():
            logger.warning(
                f"Content type for {url} is not HTML ('{response.headers.get('content-type')}'). Skipping scrape.");
            return None

        soup = BeautifulSoup(response.content, 'lxml')  # lxml is generally faster

        # Remove common unwanted tags
        for tag_name in ['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'iframe', 'noscript', 'link',
                         'meta']:  # Consider 'figure', 'figcaption' if they become noisy
            for tag in soup.find_all(tag_name): tag.decompose()

        # Try to find main content using common semantic tags or class/id patterns
        main_content_html = None
        selectors = ['article', 'main', 'div[role="main"]', 'div[class*="article-content"]', 'div[id="content"]',
                     'div[class*="post-content"]', 'div[class*="entry-content"]', 'div[class*="story-body"]']
        for selector in selectors:
            tag = soup.select_one(selector)
            if tag:
                # Further clean common noisy elements within the selected main content
                for unwanted_pattern in ['ad', 'social', 'related', 'share', 'comment', 'promo', 'sidebar', 'popup',
                                         'banner', 'meta-info', 'byline-container']:
                    # Find by class, id, or aria attributes
                    for sub_tag in tag.find_all(lambda t: any(unwanted_pattern in c for c in t.get('class', [])) or \
                                                          any(unwanted_pattern in i for i in t.get('id', [])) or \
                                                          t.get('aria-label', '').lower().count(unwanted_pattern) > 0):
                        sub_tag.decompose()
                main_content_html = tag;
                break

        article_text = ""
        if main_content_html:
            # Extract text from relevant tags within the main content
            # Favor 'p', 'h1-h6', 'li'. Include 'div' and 'span' if they are direct text containers.
            text_parts = [p.get_text(separator=' ', strip=True) for p in
                          main_content_html.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'span', 'div'],
                                                     recursive=True)  # Recursive needed for nested structures
                          if p.get_text(strip=True) and not p.find_all(['p', 'div'])
                          # Avoid double counting from parent div/span if it also has 'p's
                          ]
            article_text = '\n'.join(filter(None, text_parts))  # Join non-empty parts
            article_text = re.sub(r'\s+\n\s*', '\n', article_text)  # Consolidate multiple newlines around spaces
            article_text = re.sub(r'\n{3,}', '\n\n', article_text)  # Max 2 consecutive newlines
        elif soup.body:  # Fallback: try to get text from the whole body if main content selectors fail
            logger.info(f"Main content selectors failed for {url}, trying body text. This might be noisy.")
            article_text = soup.body.get_text(separator='\n', strip=True)
            article_text = re.sub(r'\n{3,}', '\n\n', article_text)  # Max 2 consecutive newlines
        else:
            logger.warning(f"Could not extract main content or body text from {url}.");
            return None

        if len(article_text) < 150:  # Arbitrary threshold for "very short"
            logger.info(
                f"Extracted text from {url} is very short ({len(article_text)} chars). Might be a stub or paywall.")

        logger.info(f"Successfully scraped ~{len(article_text)} chars from {url}")
        return article_text.strip()

    except requests.exceptions.RequestException as e:
        logger.error(f"Request error scraping {url}: {e}");
        return None
    except Exception as e:  # Catch any other BeautifulSoup or general error
        logger.error(f"General error scraping {url}: {e}", exc_info=True);
        return None


def extract_S1_text_sections(filing_text, sections_map):
    # sections_map: e.g., {"business": ["Item 1.", "Business"], "risk_factors": ["Item 1A.", "Risk Factors"]}
    if not filing_text or not sections_map: return {}
    extracted_sections = {}

    # Attempt to parse as HTML first, then get text. This helps with HTML entities.
    # Use 'lxml' for robustness if available, otherwise 'html.parser'.
    try:
        soup_text = BeautifulSoup(filing_text, 'lxml').get_text(separator='\n')
    except Exception:  # Fallback if lxml fails or not installed
        try:
            soup_text = BeautifulSoup(filing_text, 'html.parser').get_text(separator='\n')
        except Exception as e_bs_parse:
            logger.error(f"BeautifulSoup failed to parse filing text: {e_bs_parse}. Using raw text.")
            soup_text = filing_text  # Use raw text if parsing fails badly

    # Normalize text: replace multiple newlines, strip leading/trailing whitespace
    # Remove null characters and other problematic control characters
    normalized_text = re.sub(r'\n\s*\n', '\n\n', soup_text.strip())
    normalized_text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\xff]', '', normalized_text)  # Keep \t, \n, \r

    section_patterns = []
    for key, patterns_list in sections_map.items():
        # ITEM 1. or Item 1A. (optional period after number/letter)
        item_num_pattern_str = patterns_list[0].replace('.', r'\.?')  # Make period optional in "Item X."
        # Regex for "ITEM X." or "Item X. Business"
        # Using \s+ for spaces, re.IGNORECASE
        # Ensure word boundaries around ITEM to avoid matching "ITEMS"
        # Allow for optional colon after item number/letter
        start_regex_str = r"(?:\bITEM\b|\bItem\b)\s*" + item_num_pattern_str.split()[-1] + r"\.?\s*:?\s*"
        if len(patterns_list) > 1:  # If descriptive name is also provided
            start_regex_str += r"\s*" + re.escape(patterns_list[1])
        section_patterns.append({"key": key, "start_regex": re.compile(start_regex_str, re.IGNORECASE)})

    found_sections = []
    # First pass: find sections by "ITEM X." patterns
    for pattern_info in section_patterns:
        for match in pattern_info["start_regex"].finditer(normalized_text):
            found_sections.append(
                {"key": pattern_info["key"], "start": match.start(), "header_text": match.group(0).strip()})

    # Fallback: If no "ITEM X." found, try matching by descriptive names only (more carefully)
    if not found_sections:
        logger.warning(f"No primary ITEM X. headers found in SEC filing. Trying descriptive names as section headers.")
        for key, patterns_list in sections_map.items():
            if len(patterns_list) > 1:  # Must have a descriptive name
                # Match descriptive name at the start of a line, possibly with some leading whitespace.
                # This is less precise and might match sub-headers if not careful.
                # Look for lines that *primarily* consist of the descriptive name.
                desc_name_pattern = re.compile(r"^\s*" + re.escape(patterns_list[1]) + r"\s*$",
                                               re.IGNORECASE | re.MULTILINE)
                for match in desc_name_pattern.finditer(normalized_text):
                    # Heuristic: check if line before is empty or very short, or if line after starts itemization.
                    # This is to avoid picking up mentions of the phrase deep within another section.
                    # For now, accept if found, but this could be refined.
                    found_sections.append({"key": key, "start": match.start(), "header_text": match.group(0).strip()})

    if not found_sections:
        logger.warning("No sections extracted from SEC filing based on any patterns.");
        return {}

    # Sort found sections by their start index
    found_sections.sort(key=lambda x: x["start"])

    # Extract text between sections
    for i, current_sec_info in enumerate(found_sections):
        start_index = current_sec_info["start"] + len(current_sec_info["header_text"])
        end_index = found_sections[i + 1]["start"] if i + 1 < len(found_sections) else None

        section_text = normalized_text[start_index:end_index].strip()

        if section_text:
            # If a section (e.g. "Business") is found multiple times, take the longest one,
            # or the first one. Current logic takes the one that results in longest section_text.
            if current_sec_info["key"] not in extracted_sections or len(section_text) > len(
                    extracted_sections.get(current_sec_info["key"], "")):
                extracted_sections[current_sec_info["key"]] = section_text
                logger.debug(
                    f"Extracted section '{current_sec_info['key']}' (header: '{current_sec_info['header_text']}') len {len(section_text)}")

    if not extracted_sections:
        logger.warning("No text content could be extracted for any identified section headers.")

    return extracted_sections
---------- END api_clients.py ----------


---------- app_analysis.log ----------
2025-05-25 15:16:42,103 - root - INFO - main:167 - ===================================================================
2025-05-25 15:16:42,103 - root - INFO - main:168 - Starting Financial Analysis Script at 2025-05-25 12:16:42 UTC
2025-05-25 15:16:42,104 - root - INFO - main:169 - ===================================================================
2025-05-25 15:16:42,105 - root - INFO - main:59 - --- Generating Today's Email Summary ---
2025-05-25 15:16:43,420 - root - INFO - main:72 - Found 0 stock analyses, 0 IPO analyses, 0 news analyses since 2025-05-25 00:00:00 UTC for email.
2025-05-25 15:16:43,420 - root - INFO - main:78 - No new analyses performed recently to include in the email summary.
2025-05-25 15:16:43,482 - root - INFO - main:157 - --- Main script execution finished. ---
2025-05-25 15:16:43,482 - root - INFO - main:174 - Financial Analysis Script finished at 2025-05-25 12:16:43 UTC
2025-05-25 15:16:43,483 - root - INFO - main:175 - Total execution time: 0:00:01.379016
2025-05-25 15:16:43,483 - root - INFO - main:176 - ===================================================================
2025-05-25 15:18:02,652 - root - INFO - main:167 - ===================================================================
2025-05-25 15:18:02,653 - root - INFO - main:168 - Starting Financial Analysis Script at 2025-05-25 12:18:02 UTC
2025-05-25 15:18:02,653 - root - INFO - main:169 - ===================================================================
2025-05-25 15:18:02,655 - root - INFO - main:18 - --- Starting Individual Stock Analysis for: ['NKE'] ---
2025-05-25 15:18:05,017 - root - INFO - api_clients:68 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-25 15:18:06,019 - root - INFO - stock_analyzer:139 - Fetched profile from FMP for NKE.
2025-05-25 15:18:06,019 - root - INFO - stock_analyzer:173 - Stock NKE not found in DB, creating new entry.
2025-05-25 15:18:06,489 - root - INFO - stock_analyzer:195 - Stock entry for NKE (ID: 1, CIK: 0000320187) ready.
2025-05-25 15:18:06,489 - root - INFO - stock_analyzer:687 - Full analysis pipeline started for NKE...
2025-05-25 15:18:06,490 - root - INFO - stock_analyzer:199 - Fetching financial statements for NKE...
2025-05-25 15:18:07,565 - root - INFO - api_clients:68 - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91TVzf...
2025-05-25 15:18:10,262 - root - INFO - api_clients:68 - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/NKE?apikey=62ERGmJoqQgGD0nSGxRZ...
2025-05-25 15:18:12,968 - root - INFO - api_clients:68 - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91T...
2025-05-25 15:18:14,471 - root - INFO - stock_analyzer:221 - FMP Annuals: Income(5), Balance(5), Cashflow(5).
2025-05-25 15:18:15,879 - root - INFO - api_clients:68 - Cached response for: GET:https://finnhub.io/api/v1/stock/financials-reported?freq=quarterly&symbol=NKE&token=d0o7hphr01qq...
2025-05-25 15:18:17,383 - root - INFO - stock_analyzer:228 - Fetched 45 quarterly reports from Finnhub.
2025-05-25 15:18:18,922 - root - INFO - api_clients:68 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=INCOME_STATEMENT&symbol=NKE...
2025-05-25 15:18:33,924 - root - INFO - stock_analyzer:234 - Fetched 81 quarterly income reports from Alpha Vantage.
2025-05-25 15:18:35,318 - root - INFO - api_clients:68 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=BALANCE_SHEET&symbol=NKE...
2025-05-25 15:18:50,321 - root - INFO - stock_analyzer:241 - Fetched 81 quarterly balance reports from Alpha Vantage.
2025-05-25 15:18:52,288 - root - INFO - api_clients:68 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=CASH_FLOW&symbol=NKE...
2025-05-25 15:19:07,290 - root - INFO - stock_analyzer:248 - Fetched 81 quarterly cash flow reports from Alpha Vantage.
2025-05-25 15:19:07,290 - root - INFO - stock_analyzer:256 - Fetching key metrics and profile for NKE.
2025-05-25 15:19:08,601 - root - INFO - api_clients:68 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-25 15:19:10,961 - root - WARNING - api_clients:122 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/key-metrics/NKE (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-25 15:19:10,963 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/key-metrics/NKE: 403 Forbidden
2025-05-25 15:19:12,466 - root - WARNING - stock_analyzer:262 - FMP quarterly key metrics failed for NKE.
2025-05-25 15:19:13,719 - root - INFO - api_clients:68 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=NKE&token=d0o7hphr01qqr9alj38gd0o7hphr0...
2025-05-25 15:19:15,221 - root - INFO - stock_analyzer:272 - FMP KM Annual: 5, FMP KM Quarterly: 0. Finnhub Basic Financials: OK.
2025-05-25 15:19:15,223 - root - INFO - stock_analyzer:276 - Calculating derived metrics for NKE...
2025-05-25 15:19:15,224 - root - INFO - stock_analyzer:452 - Calculated metrics for NKE: {'pe_ratio': 25.306645614035087, 'pb_ratio': 9.996388080388082, 'ps_ratio': 1.8526, 'ev_to_sales': None, 'ev_to_ebitda': None, 'dividend_yield': 0.015036616136056905, 'eps': 3.76, 'net_profit_margin': None, 'gross_profit_margin': None, 'operating_profit_margin': 0.1302519372, 'roe': 0.39501039501039503, 'roa': 0.14956704277092628, 'debt_to_equity': 0.8282744282744283, 'current_ratio': 2.3961106391012934, 'quick_ratio': 1.5112810346455205, 'debt_to_ebitda': 1.6704402515723271, 'revenue_growth_yoy': 0.0028310912392369722, 'eps_growth_yoy': 0.14984709480122316, 'revenue_growth_cagr_3yr': 0.04861491256483208, 'eps_growth_cagr_3yr': -0.009180522194930263, 'revenue_growth_cagr_5yr': 0.08251487385466705, 'eps_growth_cagr_5yr': 0.23239537592729897, 'revenue_growth_qoq': -0.1110334820228946, 'free_cash_flow_per_share': 4.475599946941684, 'free_cash_flow_yield': 0.07456847629026464, 'free_cash_flow_trend': 'Growing', 'retained_earnings_trend': 'Declining', 'roic': 0.3444796354747266}
2025-05-25 15:19:15,227 - root - INFO - stock_analyzer:457 - Performing simplified DCF analysis for NKE...
2025-05-25 15:19:15,228 - root - INFO - stock_analyzer:519 - DCF for NKE: Intrinsic Value/Share: 72.57462873131917, Upside: 0.20917408749282176
2025-05-25 15:19:15,229 - root - INFO - stock_analyzer:525 - Fetching and attempting to summarize latest 10-K for NKE
2025-05-25 15:19:16,594 - root - INFO - api_clients:68 - Cached response for: GET:https://data.sec.gov/submissions/CIK0000320187.json?...
2025-05-25 15:19:17,097 - root - INFO - api_clients:368 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/320187/000032018724000044/nke-20240531.htm
2025-05-25 15:19:18,417 - root - INFO - api_clients:68 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/320187/000032018724000044/nke-20240531.htm...
2025-05-25 15:19:18,419 - root - INFO - stock_analyzer:539 - Fetched 10-K text (length: 2651098) for NKE. Extracting sections.
2025-05-25 15:20:02,563 - root - INFO - stock_analyzer:605 - 10-K qualitative summaries generated for NKE.
2025-05-25 15:20:02,564 - root - INFO - stock_analyzer:610 - Synthesizing investment thesis for NKE...
2025-05-25 15:20:09,361 - root - INFO - stock_analyzer:683 - Generated thesis for NKE. Decision: Review AI Output
2025-05-25 15:20:09,925 - root - INFO - stock_analyzer:719 - Successfully analyzed and saved stock: NKE (Analysis ID: 1)
2025-05-25 15:20:14,993 - root - INFO - main:157 - --- Main script execution finished. ---
2025-05-25 15:20:14,995 - root - INFO - main:174 - Financial Analysis Script finished at 2025-05-25 12:20:14 UTC
2025-05-25 15:20:14,996 - root - INFO - main:175 - Total execution time: 0:02:12.342937
2025-05-25 15:20:14,997 - root - INFO - main:176 - ===================================================================
2025-05-25 15:21:09,056 - root - INFO - main:167 - ===================================================================
2025-05-25 15:21:09,057 - root - INFO - main:168 - Starting Financial Analysis Script at 2025-05-25 12:21:09 UTC
2025-05-25 15:21:09,057 - root - INFO - main:169 - ===================================================================
2025-05-25 15:21:09,060 - root - INFO - main:59 - --- Generating Today's Email Summary ---
2025-05-25 15:21:10,460 - root - INFO - main:72 - Found 1 stock analyses, 0 IPO analyses, 0 news analyses since 2025-05-25 00:00:00 UTC for email.
2025-05-25 15:21:12,444 - root - INFO - email_generator:340 - Email sent successfully to daniprav@gmail.com
2025-05-25 15:21:12,506 - root - INFO - main:157 - --- Main script execution finished. ---
2025-05-25 15:21:12,507 - root - INFO - main:174 - Financial Analysis Script finished at 2025-05-25 12:21:12 UTC
2025-05-25 15:21:12,508 - root - INFO - main:175 - Total execution time: 0:00:03.450668
2025-05-25 15:21:12,508 - root - INFO - main:176 - ===================================================================
2025-05-25 15:33:28,302 - root - INFO - main:167 - ===================================================================
2025-05-25 15:33:28,302 - root - INFO - main:168 - Starting Financial Analysis Script at 2025-05-25 12:33:28 UTC
2025-05-25 15:33:28,302 - root - INFO - main:169 - ===================================================================
2025-05-25 15:33:28,304 - root - INFO - main:118 - Initializing database as per command line argument...
2025-05-25 15:33:28,305 - root - INFO - database:21 - Initializing database and creating tables...
2025-05-25 15:33:31,545 - root - INFO - database:26 - Database tables created successfully (if they didn't exist).
2025-05-25 15:33:31,545 - root - INFO - main:121 - Database initialization complete.
2025-05-25 15:33:31,546 - root - INFO - main:157 - --- Main script execution finished. ---
2025-05-25 15:33:31,546 - root - INFO - main:174 - Financial Analysis Script finished at 2025-05-25 12:33:31 UTC
2025-05-25 15:33:31,546 - root - INFO - main:175 - Total execution time: 0:00:03.243735
2025-05-25 15:33:31,547 - root - INFO - main:176 - ===================================================================
2025-05-25 15:33:38,385 - root - INFO - main:167 - ===================================================================
2025-05-25 15:33:38,385 - root - INFO - main:168 - Starting Financial Analysis Script at 2025-05-25 12:33:38 UTC
2025-05-25 15:33:38,386 - root - INFO - main:169 - ===================================================================
2025-05-25 15:33:38,388 - root - INFO - main:18 - --- Starting Individual Stock Analysis for: ['NKE'] ---
2025-05-25 15:33:40,755 - root - INFO - api_clients:68 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-25 15:33:41,757 - root - INFO - stock_analyzer:158 - Fetched profile from FMP for NKE.
2025-05-25 15:33:41,758 - root - INFO - stock_analyzer:195 - Stock NKE not found in DB, creating new entry.
2025-05-25 15:33:42,220 - root - INFO - stock_analyzer:226 - Stock entry for NKE (ID: 1, CIK: 0000320187) ready.
2025-05-25 15:33:42,221 - root - INFO - stock_analyzer:976 - Full analysis pipeline started for NKE...
2025-05-25 15:33:42,223 - root - INFO - stock_analyzer:230 - Fetching financial statements for NKE...
2025-05-25 15:33:43,480 - root - INFO - api_clients:68 - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91TVzf...
2025-05-25 15:33:46,191 - root - INFO - api_clients:68 - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/NKE?apikey=62ERGmJoqQgGD0nSGxRZ...
2025-05-25 15:33:48,887 - root - INFO - api_clients:68 - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91T...
2025-05-25 15:33:50,389 - root - INFO - stock_analyzer:253 - FMP Annuals for NKE: Income(5 records), Balance(5 records), Cashflow(5 records).
2025-05-25 15:33:51,941 - root - INFO - api_clients:68 - Cached response for: GET:https://finnhub.io/api/v1/stock/financials-reported?freq=quarterly&symbol=NKE&token=d0o7hphr01qq...
2025-05-25 15:33:53,444 - root - INFO - stock_analyzer:261 - Fetched 45 quarterly reports from Finnhub for NKE.
2025-05-25 15:33:54,887 - root - INFO - api_clients:68 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=INCOME_STATEMENT&symbol=NKE...
2025-05-25 15:34:09,890 - root - INFO - stock_analyzer:270 - Fetched 81 quarterly income reports from Alpha Vantage for NKE.
2025-05-25 15:34:11,392 - root - INFO - api_clients:68 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=BALANCE_SHEET&symbol=NKE...
2025-05-25 15:34:26,395 - root - INFO - stock_analyzer:279 - Fetched 81 quarterly balance reports from Alpha Vantage for NKE.
2025-05-25 15:34:27,793 - root - INFO - api_clients:68 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=CASH_FLOW&symbol=NKE...
2025-05-25 15:34:42,796 - root - INFO - stock_analyzer:288 - Fetched 81 quarterly cash flow reports from Alpha Vantage for NKE.
2025-05-25 15:34:42,797 - root - INFO - stock_analyzer:299 - Fetching key metrics and profile for NKE.
2025-05-25 15:34:44,112 - root - INFO - api_clients:68 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-25 15:34:46,461 - root - WARNING - api_clients:133 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/key-metrics/NKE (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: {}): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-25 15:34:46,462 - root - ERROR - api_clients:154 - Non-retryable client error for https://financialmodelingprep.com/api/v3/key-metrics/NKE: 403 Forbidden
2025-05-25 15:34:47,964 - root - WARNING - stock_analyzer:310 - FMP quarterly key metrics API call failed or returned None for NKE. Data will be empty.
2025-05-25 15:34:49,204 - root - INFO - api_clients:68 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=NKE&token=d0o7hphr01qqr9alj38gd0o7hphr0...
2025-05-25 15:34:50,706 - root - INFO - stock_analyzer:328 - FMP KM Annual for NKE: 5 records. FMP KM Quarterly for NKE: 0 records. Finnhub Basic Financials for NKE: OK.
2025-05-25 15:34:50,707 - root - INFO - stock_analyzer:334 - Calculating derived metrics for NKE...
2025-05-25 15:34:50,709 - root - INFO - stock_analyzer:589 - Calculated metrics for NKE: {'pe_ratio': 25.306645614035087, 'pb_ratio': 9.996388080388082, 'ps_ratio': 1.8526, 'ev_to_sales': None, 'ev_to_ebitda': None, 'dividend_yield': 0.015036616136056905, 'eps': 3.76, 'net_profit_margin': None, 'gross_profit_margin': None, 'operating_profit_margin': 0.1302519372, 'roe': 0.39501039501039503, 'roa': 0.14956704277092628, 'debt_to_equity': 0.8282744282744283, 'current_ratio': 2.3961106391012934, 'quick_ratio': 1.5112810346455205, 'debt_to_ebitda': 1.6704402515723271, 'revenue_growth_yoy': 0.0028310912392369722, 'eps_growth_yoy': 0.14984709480122316, 'revenue_growth_cagr_3yr': 0.04861491256483208, 'eps_growth_cagr_3yr': -0.009180522194930263, 'revenue_growth_cagr_5yr': 0.08251487385466705, 'eps_growth_cagr_5yr': 0.23239537592729897, 'revenue_growth_qoq': -0.1110334820228946, 'free_cash_flow_per_share': 4.475599946941684, 'free_cash_flow_yield': 0.07456847629026464, 'free_cash_flow_trend': 'Growing', 'retained_earnings_trend': 'Declining', 'roic': 0.3444796354747266}
2025-05-25 15:34:50,712 - root - INFO - stock_analyzer:594 - Performing simplified DCF analysis for NKE...
2025-05-25 15:34:50,713 - root - INFO - stock_analyzer:680 - DCF for NKE: Intrinsic Value/Share: 72.57462873131917, Upside: 20.917408749282178%
2025-05-25 15:34:50,714 - root - INFO - stock_analyzer:687 - Fetching and attempting to summarize latest 10-K for NKE
2025-05-25 15:34:51,823 - root - INFO - api_clients:68 - Cached response for: GET:https://data.sec.gov/submissions/CIK0000320187.json?...
2025-05-25 15:34:52,327 - root - INFO - api_clients:391 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/320187/000032018724000044/nke-20240531.htm
2025-05-25 15:34:53,644 - root - INFO - api_clients:68 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/320187/000032018724000044/nke-20240531.htm...
2025-05-25 15:34:53,646 - root - INFO - stock_analyzer:710 - Fetched 10-K text (length: 2651098) for NKE. Extracting sections.
2025-05-25 15:34:59,103 - root - INFO - api_clients:529 - Truncated text for Gemini summary to length: 200000
2025-05-25 15:34:59,104 - root - WARNING - api_clients:439 - Gemini prompt length 200359 very long. Truncating to 30000 chars for safety.
2025-05-25 15:35:34,062 - root - INFO - stock_analyzer:798 - 10-K qualitative summaries generated for NKE.
2025-05-25 15:35:34,064 - root - INFO - stock_analyzer:884 - Synthesizing investment thesis for NKE...
2025-05-25 15:35:37,653 - root - INFO - stock_analyzer:971 - Generated thesis for NKE. Parsed Decision: Buy, Strategy: GARP (Growth at a Reasonable Price), Confidence: Medium
2025-05-25 15:35:38,203 - root - INFO - stock_analyzer:1035 - Successfully analyzed and saved stock data: NKE (Analysis ID: 1)
2025-05-25 15:35:43,271 - root - INFO - main:157 - --- Main script execution finished. ---
2025-05-25 15:35:43,271 - root - INFO - main:174 - Financial Analysis Script finished at 2025-05-25 12:35:43 UTC
2025-05-25 15:35:43,272 - root - INFO - main:175 - Total execution time: 0:02:04.886489
2025-05-25 15:35:43,272 - root - INFO - main:176 - ===================================================================
2025-05-25 15:35:48,381 - root - INFO - main:167 - ===================================================================
2025-05-25 15:35:48,382 - root - INFO - main:168 - Starting Financial Analysis Script at 2025-05-25 12:35:48 UTC
2025-05-25 15:35:48,382 - root - INFO - main:169 - ===================================================================
2025-05-25 15:35:48,384 - root - INFO - main:59 - --- Generating Today's Email Summary ---
2025-05-25 15:35:49,844 - root - INFO - main:72 - Found 1 stock analyses, 0 IPO analyses, 0 news analyses since 2025-05-25 00:00:00 UTC for email.
2025-05-25 15:35:51,801 - root - INFO - email_generator:340 - Email sent successfully to daniprav@gmail.com
2025-05-25 15:35:51,866 - root - INFO - main:157 - --- Main script execution finished. ---
2025-05-25 15:35:51,867 - root - INFO - main:174 - Financial Analysis Script finished at 2025-05-25 12:35:51 UTC
2025-05-25 15:35:51,867 - root - INFO - main:175 - Total execution time: 0:00:03.485198
2025-05-25 15:35:51,868 - root - INFO - main:176 - ===================================================================

---------- END app_analysis.log ----------


---------- config.py ----------
# config.py

# API Keys
# It's recommended to load sensitive keys from environment variables or a secure vault in production.

GOOGLE_API_KEYS = [
    "AIzaSyDLkwkVYBTUjabShS7VfdLkQTe7vZkxcjY",
    "AIzaSyAjECAJZVZz6PzDaUVaAkgfcOeLXCPFA6Y",
    "AIzaSyBRDIgN7ffBvoqAgaizQfuWRQExKc_oVig",
    "AIzaSyC4XLSmSX4U2iuAqW_pvQ87eNyPaJwQpDo",
]

FINNHUB_API_KEY = "d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390"  # Replace with your actual key
FINANCIAL_MODELING_PREP_API_KEY = "62ERGmJoqQgGD0nSGxRZS91TVzfz61uB"  # Replace with your actual key
EODHD_API_KEY = "683079df749c42.21476005"  # Replace with your actual key or "demo"
RAPIDAPI_UPCOMING_IPO_KEY = "0bd9b5144cmsh50c0e6d95c0b662p1cbdefjsn2d1cb0104cde"  # Replace with your actual key
ALPHA_VANTAGE_API_KEY = "HB6N4X55UTFGN2FP" # User provided Alpha Vantage Key

# SEC EDGAR Configuration
EDGAR_USER_AGENT = "YourAppName YourContactEmail@example.com"  # SEC requests a user agent

# Database Configuration
DATABASE_URL = "postgresql://avnadmin:AVNS_IeMYS-rv46Au9xqkza2@pg-4d810ff-daxiake-7258.d.aivencloud.com:26922/stock-alarm?sslmode=require"

# Email Configuration
EMAIL_HOST = "smtp-relay.brevo.com"
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_HOST_USER = "8dca1d001@smtp-brevo.com" # Replace with your actual email user
EMAIL_HOST_PASSWORD = "VrNUkDdcR5G9AL8P"    # Replace with your actual email password
EMAIL_SENDER = "testypesty54@gmail.com" # Replace with your sender email
EMAIL_RECIPIENT = "daniprav@gmail.com"      # The user who receives the summary

# Logging Configuration
LOG_FILE_PATH = "app_analysis.log"
LOG_LEVEL = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# API Client Settings
API_REQUEST_TIMEOUT = 45  # seconds, increased for potentially larger data
API_RETRY_ATTEMPTS = 3
API_RETRY_DELAY = 10  # seconds, increased
# Alpha Vantage Free Tier Limit: 25 requests per day.
# Using a longer delay for AV specifically in its client or being mindful of call frequency.
# For now, the general API_RETRY_DELAY will apply if AV hits HTTP errors like 429.
# Consider a specific rate limiter or call counter for Alpha Vantage if usage becomes high.
MAX_GEMINI_TEXT_LENGTH = 150000 # Max characters to send to Gemini for summaries to avoid hitting limits


# Analysis Settings
MAX_NEWS_ARTICLES_PER_QUERY = 10
MAX_NEWS_TO_ANALYZE_PER_RUN = 5 # Control how many new news items are analyzed in one script run
MIN_MARKET_CAP = 1000000000  # 1 Billion (example, not currently used but good for future filters)
STOCK_FINANCIAL_YEARS = 7 # Number of years for financial statement analysis
IPO_ANALYSIS_REANALYZE_DAYS = 7 # Re-analyze IPO if last analysis is older than this

# Path to store cached API responses
CACHE_DIR = "api_cache"  # Currently using DB cache, this is for potential file cache fallback
CACHE_EXPIRY_SECONDS = 3600 * 6  # 6 hours for general data

# DCF Analysis Defaults (Stock Analyzer)
DEFAULT_DISCOUNT_RATE = 0.09  # WACC estimate
DEFAULT_PERPETUAL_GROWTH_RATE = 0.025
DEFAULT_FCF_PROJECTION_YEARS = 5

# News Analysis
NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI = 250000  # Max characters of full news article to send to Gemini

# IPO Analysis
# Define keywords to identify sections in S-1/F-1 filings (very basic)
S1_KEY_SECTIONS = {
    "business": ["Item 1.", "Business"],
    "risk_factors": ["Item 1A.", "Risk Factors"],
    "mda": ["Item 7.", "Management's Discussion and Analysis"],
    "financial_statements": ["Item 8.", "Financial Statements and Supplementary Data"]
}
MAX_S1_SECTION_LENGTH_FOR_GEMINI = 200000  # Max characters per S-1 section to send to Gemini

# Stock Analysis (10-K sections, similar to S-1)
TEN_K_KEY_SECTIONS = S1_KEY_SECTIONS
MAX_10K_SECTION_LENGTH_FOR_GEMINI = MAX_S1_SECTION_LENGTH_FOR_GEMINI

---------- END config.py ----------


---------- database.py ----------
# database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, scoped_session # scoped_session helps manage sessions in web apps or threads
from config import DATABASE_URL
from error_handler import logger

try:
    engine = create_engine(DATABASE_URL, pool_pre_ping=True) # pool_pre_ping can help with stale connections
    # Use scoped_session for thread safety if different parts of app might access DB concurrently.
    # For a sequential script, simple sessionmaker might be enough, but scoped_session is more robust.
    SessionFactory = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    SessionLocal = scoped_session(SessionFactory) # Creates a thread-local session

    Base = declarative_base()
    Base.query = SessionLocal.query_property() # For convenience if using Flask-SQLAlchemy style queries, less common now

    def init_db():
        """Initializes the database and creates tables if they don't exist."""
        try:
            logger.info("Initializing database and creating tables...")
            # Import all modules here that define models so that
            # they are registered with the Base meta-data when Base.metadata.create_all is called.
            import models # noqa F401 (flake8 ignore 'imported but unused')
            Base.metadata.create_all(bind=engine)
            logger.info("Database tables created successfully (if they didn't exist).")
        except Exception as e:
            logger.critical(f"CRITICAL Error initializing database: {e}", exc_info=True)
            raise # Re-raise to halt execution if DB init fails

    def get_db_session():
        """Provides a database session. Caller is responsible for closing."""
        db = SessionLocal()
        try:
            yield db
        finally:
            # The session is closed by the context manager (StockAnalyzer, IPOAnalyzer, etc.)
            # or here if used in a `with get_db_session() as session:` block in main.
            # However, analyzers now manage their own sessions.
            # For main.py's email summary, it will close its own session.
            # If this function is directly used elsewhere, ensure closure.
            # Scoped session handles removal, but explicit close is good practice for non-scoped use.
            # SessionLocal.remove() is called automatically when the scope (e.g. thread) ends.
            # For a script, direct SessionLocal() then .close() is fine.
            # This generator pattern is more common for web request scopes.
            # For the analyzers, they call SessionLocal() directly.
            # For main.py's direct usage:
            if db.is_active:
                db.close()


except Exception as e:
    logger.critical(f"CRITICAL Failed to connect to database or setup SQLAlchemy: {e}", exc_info=True)
    raise # Re-raise as DB is critical
---------- END database.py ----------


---------- email_generator.py ----------
# email_generator.py
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from config import (
    EMAIL_HOST, EMAIL_PORT, EMAIL_USE_TLS, EMAIL_HOST_USER,
    EMAIL_HOST_PASSWORD, EMAIL_SENDER, EMAIL_RECIPIENT
)
from error_handler import logger
from models import StockAnalysis, IPOAnalysis, NewsEventAnalysis  # Ensure these are imported
from datetime import datetime, timezone  # Added timezone
import json
from markdown2 import Markdown
import math


class EmailGenerator:
    def __init__(self):
        self.markdowner = Markdown(extras=["tables", "fenced-code-blocks", "break-on-newline"])

    def _md_to_html(self, md_text):
        if md_text is None: return "<p>N/A</p>"
        if isinstance(md_text, (dict, list)):  # Handle JSON data directly if passed
            return f"<pre>{json.dumps(md_text, indent=2)}</pre>"
        if not isinstance(md_text, str): md_text = str(md_text)

        # Basic check for existing HTML tags
        if "<" in md_text and ">" in md_text and ("<p>" in md_text.lower() or "<div>" in md_text.lower()):
            return md_text  # Assume it's already HTMLish
        return self.markdowner.convert(md_text)

    def _format_stock_analysis_html(self, analysis: StockAnalysis):
        if not analysis: return ""
        stock = analysis.stock

        # Helper for formatting numbers (percentage, decimal)
        def fmt_num(val, type="decimal", na_val="N/A"):
            if val is None or (isinstance(val, float) and (math.isnan(val) or math.isinf(val))): return na_val
            if type == "percent": return f"{val * 100:.2f}%"
            if type == "decimal": return f"{val:.2f}"
            return str(val)

        # Qualitative summaries from potentially Markdown content
        business_summary_html = self._md_to_html(analysis.business_summary)
        economic_moat_html = self._md_to_html(analysis.economic_moat_summary)
        industry_trends_html = self._md_to_html(analysis.industry_trends_summary)
        competitive_landscape_html = self._md_to_html(analysis.competitive_landscape_summary)
        management_assessment_html = self._md_to_html(analysis.management_assessment_summary)
        risk_factors_html = self._md_to_html(analysis.risk_factors_summary)
        investment_thesis_html = self._md_to_html(analysis.investment_thesis_full)  # Full thesis
        reasoning_points_html = self._md_to_html(analysis.reasoning)  # Key reasoning points

        html = f"""
        <div class="analysis-block stock-analysis">
            <h2>Stock Analysis: {stock.company_name} ({stock.ticker})</h2>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Industry:</strong> {stock.industry or 'N/A'}, <strong>Sector:</strong> {stock.sector or 'N/A'}</p>
            <p><strong>Investment Decision:</strong> {analysis.investment_decision or 'N/A'}</p>
            <p><strong>Strategy Type:</strong> {analysis.strategy_type or 'N/A'}</p>
            <p><strong>Confidence Level:</strong> {analysis.confidence_level or 'N/A'}</p>

            <details>
                <summary><strong>Investment Thesis & Reasoning (Click to expand)</strong></summary>
                <h4>Full Thesis:</h4>
                <div class="markdown-content">{investment_thesis_html}</div>
                <h4>Key Reasoning Points:</h4>
                <div class="markdown-content">{reasoning_points_html}</div>
            </details>

            <details>
                <summary><strong>Key Financial Metrics (Click to expand)</strong></summary>
                <ul>
                    <li>P/E Ratio: {fmt_num(analysis.pe_ratio)}</li>
                    <li>P/B Ratio: {fmt_num(analysis.pb_ratio)}</li>
                    <li>P/S Ratio: {fmt_num(analysis.ps_ratio)}</li>
                    <li>EV/Sales: {fmt_num(analysis.ev_to_sales)}</li>
                    <li>EV/EBITDA: {fmt_num(analysis.ev_to_ebitda)}</li>
                    <li>EPS: {fmt_num(analysis.eps)}</li>
                    <li>ROE: {fmt_num(analysis.roe, 'percent')}</li>
                    <li>ROA: {fmt_num(analysis.roa, 'percent')}</li>
                    <li>ROIC: {fmt_num(analysis.roic, 'percent')}</li>
                    <li>Dividend Yield: {fmt_num(analysis.dividend_yield, 'percent')}</li>
                    <li>Debt-to-Equity: {fmt_num(analysis.debt_to_equity)}</li>
                    <li>Debt-to-EBITDA: {fmt_num(analysis.debt_to_ebitda)}</li>
                    <li>Interest Coverage: {fmt_num(analysis.interest_coverage_ratio)}x</li>
                    <li>Current Ratio: {fmt_num(analysis.current_ratio)}</li>
                    <li>Quick Ratio: {fmt_num(analysis.quick_ratio)}</li>
                    <li>Gross Profit Margin: {fmt_num(analysis.gross_profit_margin, 'percent')}</li>
                    <li>Operating Profit Margin: {fmt_num(analysis.operating_profit_margin, 'percent')}</li>
                    <li>Net Profit Margin: {fmt_num(analysis.net_profit_margin, 'percent')}</li>
                    <li>Revenue Growth YoY: {fmt_num(analysis.revenue_growth_yoy, 'percent')} (QoQ: {fmt_num(analysis.revenue_growth_qoq, 'percent')})</li>
                    <li>Revenue Growth CAGR (3yr/5yr): {fmt_num(analysis.revenue_growth_cagr_3yr, 'percent')} / {fmt_num(analysis.revenue_growth_cagr_5yr, 'percent')}</li>
                    <li>EPS Growth YoY: {fmt_num(analysis.eps_growth_yoy, 'percent')}</li>
                    <li>EPS Growth CAGR (3yr/5yr): {fmt_num(analysis.eps_growth_cagr_3yr, 'percent')} / {fmt_num(analysis.eps_growth_cagr_5yr, 'percent')}</li>
                    <li>FCF per Share: {fmt_num(analysis.free_cash_flow_per_share)}</li>
                    <li>FCF Yield: {fmt_num(analysis.free_cash_flow_yield, 'percent')}</li>
                    <li>FCF Trend: {analysis.free_cash_flow_trend or 'N/A'}</li>
                    <li>Retained Earnings Trend: {analysis.retained_earnings_trend or 'N/A'}</li>
                </ul>
            </details>

            <details>
                <summary><strong>DCF Analysis (Simplified) (Click to expand)</strong></summary>
                <ul>
                    <li>Intrinsic Value per Share: {fmt_num(analysis.dcf_intrinsic_value)}</li>
                    <li>Upside/Downside: {fmt_num(analysis.dcf_upside_percentage, 'percent')}</li>
                </ul>
                <p><em>Assumptions:</em></p>
                <div class="markdown-content">{self._md_to_html(analysis.dcf_assumptions)}</div>
            </details>

            <details>
                <summary><strong>Qualitative Analysis (from 10-K/Profile & AI) (Click to expand)</strong></summary>
                <p><strong>Business Summary:</strong></p><div class="markdown-content">{business_summary_html}</div>
                <p><strong>Economic Moat:</strong></p><div class="markdown-content">{economic_moat_html}</div>
                <p><strong>Industry Trends & Position:</strong></p><div class="markdown-content">{industry_trends_html}</div>
                <p><strong>Competitive Landscape:</strong></p><div class="markdown-content">{competitive_landscape_html}</div>
                <p><strong>Management Discussion Highlights (MD&A/Assessment):</strong></p><div class="markdown-content">{management_assessment_html}</div>
                <p><strong>Key Risk Factors:</strong></p><div class="markdown-content">{risk_factors_html}</div>
            </details>

            <details>
                <summary><strong>Supporting Data Snapshots (Click to expand)</strong></summary>
                <p><em>Key Metrics Data Points Used:</em></p>
                <div class="markdown-content">{self._md_to_html(analysis.key_metrics_snapshot)}</div>
                <p><em>Qualitative Analysis Sources Summary:</em></p>
                <div class="markdown-content">{self._md_to_html(analysis.qualitative_sources_summary)}</div>
            </details>
        </div>
        """
        return html

    def _format_ipo_analysis_html(self, analysis: IPOAnalysis):
        if not analysis: return ""
        ipo = analysis.ipo

        # Helper for formatting numbers/price ranges
        def fmt_price(val_low, val_high, currency="USD"):
            if val_low is None and val_high is None: return "N/A"
            if val_low is not None and val_high is not None:
                if val_low == val_high: return f"{val_low:.2f} {currency}"
                return f"{val_low:.2f} - {val_high:.2f} {currency}"
            if val_low is not None: return f"{val_low:.2f} {currency}"
            if val_high is not None: return f"{val_high:.2f} {currency}"  # Should ideally have low if high exists
            return "N/A"

        reasoning_html = self._md_to_html(analysis.reasoning)
        s1_business_summary_html = self._md_to_html(
            analysis.s1_business_summary or analysis.business_model_summary)  # Fallback to old field
        s1_risk_factors_summary_html = self._md_to_html(
            analysis.s1_risk_factors_summary or analysis.risk_factors_summary)
        s1_mda_summary_html = self._md_to_html(analysis.s1_mda_summary)
        s1_financial_health_summary_html = self._md_to_html(
            analysis.s1_financial_health_summary or analysis.pre_ipo_financials_summary)
        competitive_landscape_html = self._md_to_html(analysis.competitive_landscape_summary)
        industry_outlook_html = self._md_to_html(analysis.industry_outlook_summary)  # was industry_health_summary
        use_of_proceeds_html = self._md_to_html(analysis.use_of_proceeds_summary)
        management_team_html = self._md_to_html(analysis.management_team_assessment)
        underwriter_html = self._md_to_html(analysis.underwriter_quality_assessment)
        valuation_html = self._md_to_html(analysis.valuation_comparison_summary)

        html = f"""
        <div class="analysis-block ipo-analysis">
            <h2>IPO Analysis: {ipo.company_name} ({ipo.symbol or 'N/A'})</h2>
            <p><strong>Expected IPO Date:</strong> {ipo.ipo_date.strftime('%Y-%m-%d') if ipo.ipo_date else ipo.ipo_date_str or 'N/A'}</p>
            <p><strong>Expected Price Range:</strong> {fmt_price(ipo.expected_price_range_low, ipo.expected_price_range_high, ipo.expected_price_currency)}</p>
            <p><strong>Exchange:</strong> {ipo.exchange or 'N/A'}, <strong>Status:</strong> {ipo.status or 'N/A'}</p>
            <p><strong>S-1 Filing URL:</strong> {f'<a href="{ipo.s1_filing_url}">{ipo.s1_filing_url}</a>' if ipo.s1_filing_url else 'Not Found'}</p>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Preliminary Stance:</strong> {analysis.investment_decision or 'N/A'}</p>

            <details>
                <summary><strong>AI Synthesized Reasoning & Critical Verification Points (Click to expand)</strong></summary>
                <div class="markdown-content">{reasoning_html}</div>
            </details>

            <details>
                <summary><strong>S-1 Based Summaries (if available) & AI Analysis (Click to expand)</strong></summary>
                <p><strong>Business Summary (from S-1 or inferred):</strong></p><div class="markdown-content">{s1_business_summary_html}</div>
                <p><strong>Competitive Landscape:</strong></p><div class="markdown-content">{competitive_landscape_html}</div>
                <p><strong>Industry Outlook:</strong></p><div class="markdown-content">{industry_outlook_html}</div>
                <p><strong>Risk Factors Summary (from S-1 or inferred):</strong></p><div class="markdown-content">{s1_risk_factors_summary_html}</div>
                <p><strong>Use of Proceeds (from S-1 or inferred):</strong></p><div class="markdown-content">{use_of_proceeds_html}</div>
                <p><strong>MD&A / Financial Health Summary (from S-1 or inferred):</strong></p><div class="markdown-content">{s1_mda_summary_html if s1_mda_summary_html and not s1_mda_summary_html.startswith("Section not found") else s1_financial_health_summary_html}</div>
                <p><strong>Management Team Assessment (Placeholder):</strong></p><div class="markdown-content">{management_team_html}</div>
                <p><strong>Underwriter Quality Assessment (Placeholder):</strong></p><div class="markdown-content">{underwriter_html}</div>
                <p><strong>Valuation Comparison Guidance (Generic):</strong></p><div class="markdown-content">{valuation_html}</div>
            </details>

            <details>
                <summary><strong>Supporting Data (Click to expand)</strong></summary>
                <p><em>Raw data from IPO calendar API:</em></p>
                <div class="markdown-content">{self._md_to_html(analysis.key_data_snapshot)}</div>
                <p><em>S-1 Sections Used for Analysis (True if found & used):</em></p>
                <div class="markdown-content">{self._md_to_html(analysis.s1_sections_used)}</div>
            </details>
        </div>
        """
        return html

    def _format_news_event_analysis_html(self, analysis: NewsEventAnalysis):
        if not analysis: return ""
        news_event = analysis.news_event

        sentiment_html = self._md_to_html(
            f"**Sentiment:** {analysis.sentiment or 'N/A'}\n**Reasoning:** {analysis.sentiment_reasoning or 'N/A'}")
        news_summary_detailed_html = self._md_to_html(analysis.news_summary_detailed)
        impact_companies_html = self._md_to_html(analysis.potential_impact_on_companies)
        impact_sectors_html = self._md_to_html(analysis.potential_impact_on_sectors)
        mechanism_html = self._md_to_html(analysis.mechanism_of_impact)
        timing_duration_html = self._md_to_html(analysis.estimated_timing_duration)
        magnitude_direction_html = self._md_to_html(analysis.estimated_magnitude_direction)
        confidence_html = self._md_to_html(analysis.confidence_of_assessment)
        investor_summary_html = self._md_to_html(analysis.summary_for_email)

        html = f"""
        <div class="analysis-block news-analysis">
            <h2>News/Event Analysis: {news_event.event_title}</h2>
            <p><strong>Event Date:</strong> {news_event.event_date.strftime('%Y-%m-%d %H:%M %Z') if news_event.event_date else 'N/A'}</p>
            <p><strong>Source:</strong> <a href="{news_event.source_url}">{news_event.source_name or news_event.source_url}</a></p>
            <p><strong>Full Article Scraped:</strong> {'Yes' if news_event.full_article_text else 'No (Analysis based on headline/summary if available)'}</p>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>

            <p><strong>Investor Summary:</strong></p>
            <div class="markdown-content">{investor_summary_html}</div>

            <details>
                <summary><strong>Detailed AI Analysis (Click to expand)</strong></summary>
                <p><strong>Sentiment Analysis:</strong></p>
                <div class="markdown-content">{sentiment_html}</div>
                <p><strong>Detailed News Summary:</strong></p>
                <div class="markdown-content">{news_summary_detailed_html}</div>
                <p><strong>Potentially Affected Companies/Stocks:</strong></p>
                <div class="markdown-content">{impact_companies_html}</div>
                <p><strong>Potentially Affected Sectors:</strong></p>
                <div class="markdown-content">{impact_sectors_html}</div>
                <p><strong>Mechanism of Impact:</strong></p>
                <div class="markdown-content">{mechanism_html}</div>
                <p><strong>Estimated Timing & Duration:</strong></p>
                <div class="markdown-content">{timing_duration_html}</div>
                <p><strong>Estimated Magnitude & Direction:</strong></p>
                <div class="markdown-content">{magnitude_direction_html}</div>
                <p><strong>Confidence of Assessment:</strong></p>
                <div class="markdown-content">{confidence_html}</div>
            </details>
             <details>
                <summary><strong>Key Snippets Used for Analysis (Click to expand)</strong></summary>
                <div class="markdown-content">{self._md_to_html(analysis.key_news_snippets)}</div>
            </details>
        </div>
        """
        return html

    def create_summary_email(self, stock_analyses=None, ipo_analyses=None, news_analyses=None):
        if not any([stock_analyses, ipo_analyses, news_analyses]):
            logger.info("No analyses provided to create an email.")
            return None

        subject_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        subject = f"Financial Analysis Summary - {subject_date}"

        html_body = f"""
        <html>
            <head>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; background-color: #f4f4f4; line-height: 1.6; color: #333; }}
                    .container {{ background-color: #ffffff; padding: 20px; border-radius: 8px; box-shadow: 0 0 15px rgba(0,0,0,0.1); max-width: 900px; margin: auto; }}
                    .analysis-block {{ border: 1px solid #ddd; padding: 15px; margin-bottom: 25px; border-radius: 5px; background-color: #fdfdfd; box-shadow: 0 2px 4px rgba(0,0,0,0.05);}}
                    .stock-analysis {{ border-left: 5px solid #4CAF50; }} /* Green for stocks */
                    .ipo-analysis {{ border-left: 5px solid #2196F3; }}   /* Blue for IPOs */
                    .news-analysis {{ border-left: 5px solid #FFC107; }} /* Yellow for News */
                    h1 {{ color: #2c3e50; text-align: center; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
                    h2 {{ color: #34495e; border-bottom: 1px solid #eee; padding-bottom: 5px; margin-top: 0; }}
                    h4 {{ color: #555; margin-top: 15px; margin-bottom: 5px; }}
                    details > summary {{ cursor: pointer; font-weight: bold; margin-bottom: 10px; color: #2980b9; padding: 5px; background-color: #ecf0f1; border-radius:3px; }}
                    details[open] > summary {{ background-color: #dde5e8; }}
                    pre {{ background-color: #eee; padding: 10px; border-radius: 4px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; font-size: 0.85em; border: 1px solid #ccc; }}
                    ul {{ list-style-type: disc; margin-left: 20px; padding-left: 5px; }}
                    li {{ margin-bottom: 8px; }}
                    .markdown-content {{ padding: 5px 0; }}
                    .markdown-content p {{ margin: 0.5em 0; }}
                    .markdown-content ul, .markdown-content ol {{ margin-left: 20px; }}
                    .markdown-content table {{ border-collapse: collapse; width: 100%; margin-bottom: 1em;}}
                    .markdown-content th, .markdown-content td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                    .markdown-content th {{ background-color: #f2f2f2; }}
                    .report-footer {{ text-align: center; font-size: 0.9em; color: #777; margin-top: 30px; }}
                </style>
            </head>
            <body>
                <div class="container">
                    <h1>Financial Analysis Report</h1>
                    <p style="text-align:center; font-style:italic; color:#555;">Generated: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z')}</p>
                    <p style="text-align:center; font-style:italic; color:#7f8c8d;"><em>This email contains automated analysis. Always do your own research before making investment decisions.</em></p>
        """

        if stock_analyses:
            html_body += "<h2>Individual Stock Analyses</h2>"
            for sa in stock_analyses:
                html_body += self._format_stock_analysis_html(sa)

        if ipo_analyses:
            html_body += "<h2>Upcoming IPO Analyses</h2>"
            for ia in ipo_analyses:
                html_body += self._format_ipo_analysis_html(ia)

        if news_analyses:
            html_body += "<h2>Recent News & Event Analyses</h2>"
            for na in news_analyses:
                html_body += self._format_news_event_analysis_html(na)

        html_body += """
                    <div class="report-footer">
                        <p>© Automated Financial Analysis System</p>
                    </div>
                </div>
            </body>
        </html>
        """

        msg = MIMEMultipart('alternative')
        msg['Subject'] = subject
        msg['From'] = EMAIL_SENDER
        msg['To'] = EMAIL_RECIPIENT

        msg.attach(MIMEText(html_body, 'html', 'utf-8'))  # Ensure UTF-8
        return msg

    def send_email(self, message: MIMEMultipart):
        if not message:
            logger.error("No message object provided to send_email.")
            return False
        try:
            # Ensure SMTP settings are correctly fetched from config
            smtp_server = smtplib.SMTP(EMAIL_HOST, EMAIL_PORT, timeout=20)  # Added timeout
            if EMAIL_USE_TLS:
                smtp_server.starttls()
            smtp_server.login(EMAIL_HOST_USER, EMAIL_HOST_PASSWORD)
            smtp_server.sendmail(EMAIL_SENDER, EMAIL_RECIPIENT, message.as_string())
            smtp_server.quit()
            logger.info(f"Email sent successfully to {EMAIL_RECIPIENT}")
            return True
        except smtplib.SMTPException as e_smtp:
            logger.error(f"SMTP error sending email: {e_smtp}", exc_info=True)
            return False
        except Exception as e:
            logger.error(f"General error sending email: {e}", exc_info=True)
            return False


if __name__ == '__main__':

    logger.info("Starting email generator test with new model fields...")


    # --- Mock Stock Data ---
    class MockStock:
        def __init__(self, ticker, company_name, industry="Tech", sector="Software"):
            self.ticker = ticker
            self.company_name = company_name
            self.industry = industry
            self.sector = sector


    class MockStockAnalysis:
        def __init__(self, stock):
            self.stock = stock
            self.analysis_date = datetime.now(timezone.utc)
            self.investment_decision = "Buy"
            self.strategy_type = "GARP (Growth at a Reasonable Price)"
            self.confidence_level = "Medium"
            self.investment_thesis_full = "This is a **compelling** investment due to *strong growth* and reasonable valuation. AI suggests `monitoring` market conditions."
            self.reasoning = "- Strong revenue growth.\n- Improving margins.\n- Reasonable valuation compared to peers."

            self.pe_ratio = 18.5;
            self.pb_ratio = 3.2;
            self.ps_ratio = 2.5;
            self.ev_to_sales = 2.8;
            self.ev_to_ebitda = 12.0
            self.eps = 2.50;
            self.roe = 0.22;
            self.roa = 0.10;
            self.roic = 0.15;
            self.dividend_yield = 0.015
            self.debt_to_equity = 0.5;
            self.debt_to_ebitda = 2.1;
            self.interest_coverage_ratio = 8.0
            self.current_ratio = 1.8;
            self.quick_ratio = 1.2;
            self.revenue_growth_yoy = 0.15;
            self.revenue_growth_qoq = 0.04;
            self.revenue_growth_cagr_3yr = 0.12;
            self.revenue_growth_cagr_5yr = 0.10
            self.eps_growth_yoy = 0.20;
            self.eps_growth_cagr_3yr = 0.18;
            self.eps_growth_cagr_5yr = 0.15
            self.net_profit_margin = 0.12;
            self.gross_profit_margin = 0.60;
            self.operating_profit_margin = 0.20
            self.free_cash_flow_per_share = 1.80;
            self.free_cash_flow_yield = 0.05;
            self.free_cash_flow_trend = "Growing"
            self.retained_earnings_trend = "Growing"

            self.dcf_intrinsic_value = 120.50;
            self.dcf_upside_percentage = 0.205
            self.dcf_assumptions = {"discount_rate": 0.09, "perpetual_growth_rate": 0.025, "start_fcf": 100000000}

            self.business_summary = "MockCorp is a leading provider of cloud solutions."
            self.economic_moat_summary = "Strong brand recognition and high switching costs."
            self.industry_trends_summary = "Industry is rapidly growing with tailwinds from AI adoption."
            self.competitive_landscape_summary = "Competitive but MockCorp has a differentiated product."
            self.management_assessment_summary = "Experienced management team with a clear vision (from MD&A)."
            self.risk_factors_summary = "Key risks include talent retention and cybersecurity threats."

            self.key_metrics_snapshot = {"price": 100, "marketCap": 1000000000, "latest_revenue_q": 25000000}
            self.qualitative_sources_summary = {"10k_filing_url_used": "http://example.com/10k",
                                                "business_10k_source_length": 5000}


    # --- Mock IPO Data ---
    class MockIPO:
        def __init__(self, company_name, symbol, ipo_date_str="2025-07-15"):
            self.company_name = company_name
            self.symbol = symbol
            self.ipo_date_str = ipo_date_str
            self.ipo_date = datetime.strptime(ipo_date_str, "%Y-%m-%d").date() if ipo_date_str else None
            self.expected_price_range_low = 18.00
            self.expected_price_range_high = 22.00
            self.expected_price_currency = "USD"
            self.exchange = "NASDAQ"
            self.status = "Filed"
            self.s1_filing_url = "http://example.com/s1_filing"


    class MockIPOAnalysis:
        def __init__(self, ipo):
            self.ipo = ipo
            self.analysis_date = datetime.now(timezone.utc)
            self.investment_decision = "Potentially Interesting, S-1 Review Critical"
            self.reasoning = "Promising industry, but financial details from S-1 are key. S-1 summary indicates good initial traction."
            self.s1_business_summary = "NewIPO Inc. is a disruptive player in the fintech space, offering innovative payment solutions."
            self.s1_risk_factors_summary = "Primary risks include regulatory changes and competition from established banks."
            self.s1_mda_summary = "MD&A shows rapid revenue growth but increasing operating losses due to R&D and S&M."
            self.s1_financial_health_summary = "Strong top-line growth, negative FCF, well-funded post-IPO."
            self.competitive_landscape_summary = "Competes with traditional banks and other fintech startups."
            self.industry_outlook_summary = "Fintech industry has strong tailwinds but is becoming crowded."
            self.use_of_proceeds_summary = "Proceeds to be used for product development and market expansion."
            self.management_team_assessment = "Founders have prior startup success. Full team review in S-1 needed."
            self.underwriter_quality_assessment = "Lead underwriters are reputable (e.g., Goldman Sachs, Morgan Stanley - from S-1)."
            self.key_data_snapshot = {"name": ipo.company_name, "symbol": ipo.symbol, "price": "18.00-22.00"}
            self.s1_sections_used = {"business": True, "risk_factors": True, "mda": True}
            # Fallback fields for demonstration
            self.business_model_summary = self.s1_business_summary
            self.risk_factors_summary = self.s1_risk_factors_summary
            self.pre_ipo_financials_summary = self.s1_financial_health_summary
            self.valuation_comparison_summary = "Peer valuation suggests a range of X to Y based on P/S multiples."


    # --- Mock News Data ---
    class MockNewsEvent:
        def __init__(self, title, url, event_date_str="2025-05-25 10:00:00"):
            self.event_title = title
            self.source_url = url
            self.source_name = "Mock News Source"
            self.event_date = datetime.strptime(event_date_str, "%Y-%m-%d %H:%M:%S").replace(
                tzinfo=timezone.utc) if event_date_str else datetime.now(timezone.utc)
            self.full_article_text = "This is the full article text which is much longer and provides more context than just a summary. It discusses market trends and company X's new product."


    class MockNewsEventAnalysis:
        def __init__(self, news_event):
            self.news_event = news_event
            self.analysis_date = datetime.now(timezone.utc)
            self.sentiment = "Positive"
            self.sentiment_reasoning = "The article highlights strong growth and innovation."
            self.news_summary_detailed = "A detailed summary of the news focusing on key impacts."
            self.potential_impact_on_companies = "Company X (TICK) likely to benefit from new product launch. Competitor Y (COMP) may face pressure."
            self.potential_impact_on_sectors = "Technology sector, specifically software services, will see increased activity."
            self.mechanism_of_impact = "New product addresses a key market need, potentially increasing revenue for Company X."
            self.estimated_timing_duration = "Short-term positive sentiment, Medium-term revenue impact."
            self.estimated_magnitude_direction = "Medium Positive for Company X."
            self.confidence_of_assessment = "High"
            self.summary_for_email = "Company X launched a new product, expecting positive revenue impact in the tech sector."
            self.key_news_snippets = {"headline": news_event.event_title, "snippet_used": "Company X's new product..."}


    # Create mock instances
    mock_sa = MockStockAnalysis(MockStock("MOCK", "MockCorp Inc."))
    mock_ipo_a = MockIPOAnalysis(MockIPO("NewIPO Inc.", "NIPO"))
    mock_news_a = MockNewsEventAnalysis(MockNewsEvent("Major Tech Breakthrough Announced", "http://example.com/news1"))

    email_gen = EmailGenerator()
    email_message = email_gen.create_summary_email(
        stock_analyses=[mock_sa],
        ipo_analyses=[mock_ipo_a],
        news_analyses=[mock_news_a]
    )

    if email_message:
        logger.info("Email message created successfully with new fields.")
        # Save to file for inspection
        output_filename = f"test_email_summary_refactored_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(output_filename, "w", encoding="utf-8") as f:
            # MIMEText part is the first payload if it's html only, or might be nested.
            # For 'alternative', payload is a list of MIMEMultipart/MIMEText.
            # We expect the HTML part.
            payload_html = ""
            if email_message.is_multipart():
                for part in email_message.get_payload():
                    if part.get_content_type() == "text/html":
                        payload_html = part.get_payload(decode=True).decode(part.get_content_charset() or 'utf-8')
                        break
            else:  # Not multipart, should be text/html directly
                payload_html = email_message.get_payload(decode=True).decode(
                    email_message.get_content_charset() or 'utf-8')

            if payload_html:
                f.write(payload_html)
                logger.info(f"Test email HTML saved to {output_filename}")
            else:
                logger.error("Could not extract HTML payload from email message.")

        # To actually send, uncomment and ensure config.py is correct:
        # logger.info("Attempting to send test email...")
        # if email_gen.send_email(email_message):
        #     logger.info("Test email sent successfully (check your inbox).")
        # else:
        #     logger.error("Failed to send test email.")

    else:
        logger.error("Failed to create email message.")
---------- END email_generator.py ----------


---------- error_handler.py ----------
# error_handler.py
import logging
import sys
from config import LOG_FILE_PATH, LOG_LEVEL # Ensure these are imported

# Global flag to prevent multiple handler additions if setup_logging is called more than once
# though with import-time setup, this is less of an issue.
_logging_configured = False

def setup_logging():
    """Configures logging for the application."""
    global _logging_configured
    if _logging_configured:
        return logging.getLogger() # Return existing root logger if already configured

    numeric_level = getattr(logging, LOG_LEVEL.upper(), None)
    if not isinstance(numeric_level, int):
        # Fallback to INFO if invalid level is provided in config
        logging.warning(f"Invalid log level: {LOG_LEVEL} in config. Defaulting to INFO.")
        numeric_level = logging.INFO

    # Get the root logger
    logger_obj = logging.getLogger() # Get the root logger
    logger_obj.setLevel(numeric_level) # Set its level

    # Create formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')

    # Console Handler
    if not any(isinstance(h, logging.StreamHandler) for h in logger_obj.handlers):
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        logger_obj.addHandler(console_handler)

    # File Handler
    # Check if a file handler for the specific LOG_FILE_PATH is already attached
    if not any(isinstance(h, logging.FileHandler) and getattr(h, 'baseFilename', None) == LOG_FILE_PATH for h in logger_obj.handlers):
        try:
            file_handler = logging.FileHandler(LOG_FILE_PATH, mode='a') # Append mode
            file_handler.setFormatter(formatter)
            logger_obj.addHandler(file_handler)
        except Exception as e:
            # If file handler fails, log to console and continue
            logging.error(f"Failed to set up file handler for {LOG_FILE_PATH}: {e}", exc_info=True)


    _logging_configured = True
    return logger_obj # Return the configured root logger

# Initialize logger when this module is imported
logger = setup_logging()

def handle_global_exception(exc_type, exc_value, exc_traceback):
    """Custom global exception handler to log unhandled exceptions via the root logger."""
    if issubclass(exc_type, KeyboardInterrupt):
        # Call the default hook for KeyboardInterrupt so it behaves as expected (exit).
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    # Log other unhandled exceptions.
    logger.critical("Unhandled global exception:", exc_info=(exc_type, exc_value, exc_traceback))

# Set the custom global exception hook.
# This will catch any exceptions not caught elsewhere in the application.
# sys.excepthook = handle_global_exception # Uncomment this in main.py or at the top of your script if desired.
---------- END error_handler.py ----------


---------- ipo_analyzer.py ----------
# ipo_analyzer.py
import time
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timedelta, timezone
from dateutil import parser as date_parser  # For flexible date parsing
import concurrent.futures  # For multithreading

from api_clients import FinnhubClient, GeminiAPIClient, SECEDGARClient, extract_S1_text_sections
from database import SessionLocal, get_db_session  # SessionLocal for thread-specific sessions
from models import IPO, IPOAnalysis
from error_handler import logger
from sqlalchemy.exc import SQLAlchemyError
from config import (
    S1_KEY_SECTIONS, MAX_S1_SECTION_LENGTH_FOR_GEMINI,
    IPO_ANALYSIS_REANALYZE_DAYS, MAX_GEMINI_TEXT_LENGTH
)

# Define max workers for ThreadPoolExecutor
MAX_IPO_ANALYSIS_WORKERS = 1  # Adjustable


class IPOAnalyzer:
    def __init__(self):
        # API clients are lightweight and can be instantiated per IPOAnalyzer instance
        # They will be shared by threads if run_ipo_analysis_pipeline is called on one instance.
        self.finnhub = FinnhubClient()
        self.gemini = GeminiAPIClient()
        self.sec_edgar = SECEDGARClient()
        # No self.db_session here; sessions will be managed per-thread or per-task.

    def _parse_ipo_date(self, date_str):
        if not date_str:
            return None
        try:
            return date_parser.parse(date_str).date()
        except (ValueError, TypeError) as e:
            logger.warning(f"Could not parse IPO date string '{date_str}': {e}")
            return None

    def fetch_upcoming_ipos(self):
        logger.info("Fetching upcoming IPOs using Finnhub...")
        ipos_data_to_process = []
        today = datetime.now(timezone.utc)
        from_date = (today - timedelta(days=60)).strftime('%Y-%m-%d')
        to_date = (today + timedelta(days=180)).strftime('%Y-%m-%d')

        finnhub_response = self.finnhub.get_ipo_calendar(from_date=from_date, to_date=to_date)
        actual_ipo_list = []

        if finnhub_response and isinstance(finnhub_response, dict) and "ipoCalendar" in finnhub_response:
            actual_ipo_list = finnhub_response["ipoCalendar"]
            if not isinstance(actual_ipo_list, list):
                logger.warning(f"Finnhub response 'ipoCalendar' field is not a list. Found: {type(actual_ipo_list)}")
                actual_ipo_list = []
            elif not actual_ipo_list:
                logger.info("Finnhub 'ipoCalendar' list is empty for the current period.")
        elif finnhub_response is None:
            logger.error("Failed to fetch IPOs from Finnhub (API call failed or returned None).")
        else:
            logger.info(f"No IPOs found or unexpected format from Finnhub. Response: {str(finnhub_response)[:200]}")

        if actual_ipo_list:
            for ipo_api_data in actual_ipo_list:
                if not isinstance(ipo_api_data, dict):
                    logger.warning(f"Skipping non-dictionary item in Finnhub IPO calendar: {ipo_api_data}")
                    continue
                price_range_raw = ipo_api_data.get("price")
                price_low, price_high = None, None
                if isinstance(price_range_raw, str) and price_range_raw.strip():
                    parts = price_range_raw.split('-', 1)
                    try:
                        price_low = float(parts[0].strip())
                    except:
                        pass
                    try:
                        price_high = float(parts[1].strip()) if len(parts) > 1 and parts[1].strip() else price_low
                    except:
                        price_high = price_low if price_low is not None else None
                elif isinstance(price_range_raw, (float, int)):
                    price_low = float(price_range_raw);
                    price_high = float(price_range_raw)

                parsed_date = self._parse_ipo_date(ipo_api_data.get("date"))
                ipos_data_to_process.append({
                    "company_name": ipo_api_data.get("name"), "symbol": ipo_api_data.get("symbol"),
                    "ipo_date_str": ipo_api_data.get("date"), "ipo_date": parsed_date,
                    "expected_price_range_low": price_low, "expected_price_range_high": price_high,
                    "exchange": ipo_api_data.get("exchange"), "status": ipo_api_data.get("status"),
                    "offered_shares": ipo_api_data.get("numberOfShares"),
                    "total_shares_value": ipo_api_data.get("totalSharesValue"),
                    "source_api": "Finnhub", "raw_data": ipo_api_data
                })
            logger.info(f"Successfully parsed {len(ipos_data_to_process)} IPOs from Finnhub API response.")
        unique_ipos = []
        seen_keys = set()
        for ipo_info in ipos_data_to_process:
            key_name = ipo_info.get("company_name", "").strip().lower() if ipo_info.get(
                "company_name") else "unknown_company"
            key_symbol = ipo_info.get("symbol", "").strip().upper() if ipo_info.get("symbol") else "NO_SYMBOL"
            key_date = ipo_info.get("ipo_date_str", "")
            unique_tuple = (key_name, key_symbol, key_date)
            if unique_tuple not in seen_keys:
                unique_ipos.append(ipo_info);
                seen_keys.add(unique_tuple)
        logger.info(f"Total unique IPOs fetched after deduplication: {len(unique_ipos)}")
        return unique_ipos

    def _get_or_create_ipo_db_entry(self, db_session, ipo_data_from_fetch):
        # Uses passed-in db_session
        ipo_db_entry = None
        if ipo_data_from_fetch.get("symbol"):
            ipo_db_entry = db_session.query(IPO).filter(IPO.symbol == ipo_data_from_fetch["symbol"]).first()
        if not ipo_db_entry and ipo_data_from_fetch.get("company_name") and ipo_data_from_fetch.get("ipo_date_str"):
            ipo_db_entry = db_session.query(IPO).filter(
                IPO.company_name == ipo_data_from_fetch["company_name"],
                IPO.ipo_date_str == ipo_data_from_fetch["ipo_date_str"]
            ).first()

        cik_to_store = ipo_data_from_fetch.get("cik")
        if not cik_to_store and ipo_data_from_fetch.get("symbol"):
            cik_to_store = self.sec_edgar.get_cik_by_ticker(ipo_data_from_fetch["symbol"])  # API Call
            time.sleep(0.5)
        elif not cik_to_store and ipo_db_entry and ipo_db_entry.symbol and not ipo_db_entry.cik:
            cik_to_store = self.sec_edgar.get_cik_by_ticker(ipo_db_entry.symbol)  # API Call
            time.sleep(0.5)

        if not ipo_db_entry:
            logger.info(f"IPO '{ipo_data_from_fetch.get('company_name')}' not found in DB, creating new entry.")
            ipo_db_entry = IPO(
                company_name=ipo_data_from_fetch.get("company_name"), symbol=ipo_data_from_fetch.get("symbol"),
                ipo_date_str=ipo_data_from_fetch.get("ipo_date_str"), ipo_date=ipo_data_from_fetch.get("ipo_date"),
                expected_price_range_low=ipo_data_from_fetch.get("expected_price_range_low"),
                expected_price_range_high=ipo_data_from_fetch.get("expected_price_range_high"),
                offered_shares=ipo_data_from_fetch.get("offered_shares"),
                total_shares_value=ipo_data_from_fetch.get("total_shares_value"),
                exchange=ipo_data_from_fetch.get("exchange"), status=ipo_data_from_fetch.get("status"),
                cik=cik_to_store
            )
            db_session.add(ipo_db_entry)
            try:
                db_session.commit();
                db_session.refresh(ipo_db_entry)
                logger.info(
                    f"Created IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}, CIK: {ipo_db_entry.cik})")
            except SQLAlchemyError as e:
                db_session.rollback();
                logger.error(f"Error creating IPO: {e}", exc_info=True);
                return None
        else:
            updated = False
            fields_to_update = ["company_name", "symbol", "ipo_date_str", "ipo_date", "expected_price_range_low",
                                "expected_price_range_high", "offered_shares", "total_shares_value", "exchange",
                                "status"]
            for field in fields_to_update:
                new_val = ipo_data_from_fetch.get(field)
                if new_val is not None and getattr(ipo_db_entry, field) != new_val:
                    setattr(ipo_db_entry, field, new_val);
                    updated = True
            if cik_to_store and ipo_db_entry.cik != cik_to_store: ipo_db_entry.cik = cik_to_store; updated = True
            if updated:
                try:
                    db_session.commit();
                    db_session.refresh(ipo_db_entry)
                    logger.info(f"Updated IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}).")
                except SQLAlchemyError as e:
                    db_session.rollback();
                    logger.error(f"Error updating IPO: {e}", exc_info=True)
        return ipo_db_entry

    def _fetch_s1_data(self, db_session, ipo_db_entry):
        # Uses passed-in db_session
        if not ipo_db_entry: return None, None
        target_cik = ipo_db_entry.cik
        if not target_cik:
            if ipo_db_entry.symbol:
                target_cik = self.sec_edgar.get_cik_by_ticker(ipo_db_entry.symbol)  # API Call
                time.sleep(0.5)
                if target_cik:
                    ipo_db_entry.cik = target_cik
                    try:
                        db_session.commit()
                    except SQLAlchemyError as e:
                        db_session.rollback(); logger.error(
                            f"Failed to update CIK for {ipo_db_entry.company_name}: {e}")
                else:
                    logger.warning(
                        f"No CIK via symbol {ipo_db_entry.symbol} for '{ipo_db_entry.company_name}'."); return None, None
            else:
                logger.warning(f"No CIK/symbol for '{ipo_db_entry.company_name}'. Cannot fetch S-1."); return None, None

        logger.info(f"Attempting to fetch S-1/F-1 for {ipo_db_entry.company_name} (CIK: {target_cik})")
        filing_types = ["S-1", "S-1/A", "F-1", "F-1/A"]
        s1_url = None
        for form_type in filing_types:
            s1_url = self.sec_edgar.get_filing_document_url(cik=target_cik, form_type=form_type)  # API Call
            time.sleep(0.5)
            if s1_url: logger.info(f"Found {form_type} URL for {ipo_db_entry.company_name}: {s1_url}"); break
        if s1_url:
            if ipo_db_entry.s1_filing_url != s1_url:
                ipo_db_entry.s1_filing_url = s1_url
                try:
                    db_session.commit()
                except SQLAlchemyError as e:
                    db_session.rollback()
            filing_text = self.sec_edgar.get_filing_text(s1_url)  # API Call
            if filing_text:
                logger.info(
                    f"Fetched S-1/F-1 text (len: {len(filing_text)}) for {ipo_db_entry.company_name}"); return filing_text, s1_url
            else:
                logger.warning(f"Failed to fetch S-1/F-1 text from {s1_url}")
        else:
            logger.warning(f"No S-1 or F-1 URL found for {ipo_db_entry.company_name} (CIK: {target_cik}).")
        return None, None

    def _analyze_single_ipo_task(self, db_session, ipo_data_from_fetch):
        # This is the core logic, now designed to be run by a thread worker.
        # It uses the provided db_session.
        ipo_identifier = ipo_data_from_fetch.get("company_name") or ipo_data_from_fetch.get("symbol")
        logger.info(
            f"Task: Starting analysis for IPO: {ipo_identifier} from source {ipo_data_from_fetch.get('source_api')}")

        ipo_db_entry = self._get_or_create_ipo_db_entry(db_session, ipo_data_from_fetch)
        if not ipo_db_entry:
            logger.error(f"Task: Could not get/create DB entry for IPO {ipo_identifier}. Aborting.");
            return None

        # Re-check for recent analysis
        reanalyze_threshold = datetime.now(timezone.utc) - timedelta(days=IPO_ANALYSIS_REANALYZE_DAYS)
        existing_analysis = db_session.query(IPOAnalysis).filter(IPOAnalysis.ipo_id == ipo_db_entry.id) \
            .order_by(IPOAnalysis.analysis_date.desc()).first()

        significant_change = False
        if existing_analysis:
            snap = existing_analysis.key_data_snapshot or {}
            parsed_snap_date = self._parse_ipo_date(snap.get("date"))
            if ipo_db_entry.ipo_date != parsed_snap_date or ipo_db_entry.status != snap.get("status") or \
                    ipo_db_entry.expected_price_range_low != snap.get("price_range_low") or \
                    ipo_db_entry.expected_price_range_high != snap.get("price_range_high"):
                significant_change = True
            if not significant_change and existing_analysis.analysis_date >= reanalyze_threshold:
                logger.info(f"Task: Recent analysis for {ipo_identifier} exists. Skipping.");
                return existing_analysis

        s1_text, _ = self._fetch_s1_data(db_session, ipo_db_entry)
        s1_sections = extract_S1_text_sections(s1_text, S1_KEY_SECTIONS) if s1_text else {}
        analysis_payload = {"key_data_snapshot": ipo_data_from_fetch.get("raw_data", {}),
                            "s1_sections_used": {k: bool(v) for k, v in s1_sections.items()}}

        # Gemini prompts (simplified structure for brevity - original prompts are more detailed)
        company_prompt_id = f"{ipo_db_entry.company_name} ({ipo_db_entry.symbol or 'N/A'})"
        biz_text = s1_sections.get("business", "N/A")[:MAX_S1_SECTION_LENGTH_FOR_GEMINI]
        risk_text = s1_sections.get("risk_factors", "N/A")[:MAX_S1_SECTION_LENGTH_FOR_GEMINI]
        mda_text = s1_sections.get("mda", "N/A")[:MAX_S1_SECTION_LENGTH_FOR_GEMINI]

        prompt_ctx = f"IPO: {company_prompt_id}. Business: {biz_text}. Risks: {risk_text}. MD&A: {mda_text}."
        prompt1 = prompt_ctx + " Summarize: Business Model, Competitive Landscape, Industry Outlook."
        resp1 = self.gemini.generate_text(prompt1[:MAX_GEMINI_TEXT_LENGTH]);
        time.sleep(3)
        analysis_payload["s1_business_summary"] = self._parse_ai_section(resp1, "Business Model")
        analysis_payload["competitive_landscape_summary"] = self._parse_ai_section(resp1, "Competitive Landscape")
        analysis_payload["industry_outlook_summary"] = self._parse_ai_section(resp1, "Industry Outlook")

        prompt2 = prompt_ctx + " Summarize: Risk Factors, Use of Proceeds, Financial Health."
        resp2 = self.gemini.generate_text(prompt2[:MAX_GEMINI_TEXT_LENGTH]);
        time.sleep(3)
        analysis_payload["s1_risk_factors_summary"] = self._parse_ai_section(resp2, "Significant Risk Factors")
        analysis_payload["use_of_proceeds_summary"] = self._parse_ai_section(resp2, "Use of IPO Proceeds")
        analysis_payload["s1_financial_health_summary"] = self._parse_ai_section(resp2, "Financial Health Summary")

        # Fallbacks for older model fields
        if not analysis_payload.get("s1_business_summary") or analysis_payload.get("s1_business_summary",
                                                                                   "").startswith("Section not found"):
            analysis_payload["business_model_summary"] = analysis_payload.get("s1_business_summary")
        if not analysis_payload.get("s1_risk_factors_summary") or analysis_payload.get("s1_risk_factors_summary",
                                                                                       "").startswith(
                "Section not found"):
            analysis_payload["risk_factors_summary"] = analysis_payload.get("s1_risk_factors_summary")

        synth_prompt = f"Synthesize IPO perspective for {company_prompt_id} based on summaries: Business={analysis_payload.get('s1_business_summary', 'N/A')[:100]}, Risks={analysis_payload.get('s1_risk_factors_summary', 'N/A')[:100]}, Financials={analysis_payload.get('s1_financial_health_summary', 'N/A')[:100]}. Provide: Investment Stance, Reasoning, Critical Verification Points."
        gemini_synth = self.gemini.generate_text(synth_prompt[:MAX_GEMINI_TEXT_LENGTH]);
        time.sleep(3)
        parsed_synth = self._parse_ai_synthesis(gemini_synth)
        analysis_payload.update(parsed_synth)

        current_time = datetime.now(timezone.utc)
        if existing_analysis:
            for key, value in analysis_payload.items(): setattr(existing_analysis, key, value)
            existing_analysis.analysis_date = current_time
            entry_to_save = existing_analysis
        else:
            entry_to_save = IPOAnalysis(ipo_id=ipo_db_entry.id, analysis_date=current_time, **analysis_payload)
            db_session.add(entry_to_save)
        ipo_db_entry.last_analysis_date = current_time
        try:
            db_session.commit()
            logger.info(f"Task: Saved IPO analysis for {ipo_identifier} (ID: {entry_to_save.id})")
        except SQLAlchemyError as e:
            db_session.rollback();
            logger.error(f"Task: DB error saving IPO analysis for {ipo_identifier}: {e}", exc_info=True);
            return None
        return entry_to_save

    def _parse_ai_section(self, ai_text, section_header_keywords):
        if not ai_text or ai_text.startswith("Error:"): return "AI Error or No Text"
        keywords = [k.lower() for k in (
            [section_header_keywords] if isinstance(section_header_keywords, str) else section_header_keywords)]
        lines = ai_text.split('\n');
        capture = False;
        content = []
        all_headers = ["business model:", "competitive landscape:", "industry outlook:", "significant risk factors:",
                       "use of ipo proceeds:", "financial health summary:", "investment stance:", "reasoning:",
                       "critical verification points:"]
        for line in lines:
            norm_line = line.strip().lower()
            matched_kw = next((kw for kw in keywords if norm_line.startswith(kw + ":") or norm_line == kw), None)
            if matched_kw:
                capture = True;
                line_content = line.strip()[len(matched_kw):].lstrip(':').strip()
                if line_content: content.append(line_content); continue
            if capture:
                if any(norm_line.startswith(h) for h in all_headers if h not in keywords): break
                content.append(line)
        return "\n".join(content).strip() or "Section not found or empty."

    def _parse_ai_synthesis(self, ai_response):
        parsed = {}
        if ai_response.startswith("Error:") or not ai_response:
            parsed["investment_decision"] = "AI Error";
            parsed["reasoning"] = ai_response;
            return parsed
        parsed["investment_decision"] = self._parse_ai_section(ai_response, "Investment Stance")
        # Combine reasoning and critical points into one field for simplicity here
        parsed["reasoning"] = self._parse_ai_section(ai_response, ["Reasoning", "Critical Verification Points"])
        if parsed["investment_decision"].startswith("Section not found"): parsed[
            "investment_decision"] = "Review AI Output"
        if parsed["reasoning"].startswith("Section not found"): parsed["reasoning"] = ai_response
        return parsed

    def run_ipo_analysis_pipeline(self):
        all_upcoming_ipos = self.fetch_upcoming_ipos()
        analyzed_results = []
        if not all_upcoming_ipos:
            logger.info("No upcoming IPOs found to analyze.");
            return []

        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_IPO_ANALYSIS_WORKERS) as executor:
            future_to_ipo_data = {}
            for ipo_data in all_upcoming_ipos:
                status = ipo_data.get("status", "").lower()
                relevant_statuses = ["expected", "filed", "priced", "upcoming", "active"]
                if status not in relevant_statuses or not ipo_data.get("company_name"):
                    logger.debug(
                        f"Skipping IPO '{ipo_data.get('company_name')}' due to status '{status}' or missing name.")
                    continue
                # Submit to executor
                future = executor.submit(self._thread_worker_analyze_ipo, ipo_data)
                future_to_ipo_data[future] = ipo_data.get("company_name")

            for future in concurrent.futures.as_completed(future_to_ipo_data):
                ipo_name = future_to_ipo_data[future]
                try:
                    result = future.result()
                    if result: analyzed_results.append(result)
                except Exception as exc:
                    logger.error(f"IPO analysis for '{ipo_name}' generated an exception: {exc}", exc_info=True)

        logger.info(f"IPO analysis pipeline completed. Processed {len(analyzed_results)} IPOs.")
        return analyzed_results

    def _thread_worker_analyze_ipo(self, ipo_data_from_fetch):
        """Worker function for ThreadPoolExecutor."""
        db_session = SessionLocal()  # Create a new session for this thread
        try:
            # Pass the new db_session to the analysis logic
            # The API clients (self.finnhub etc.) are shared from the IPOAnalyzer instance
            # Their internal caching methods already use SessionLocal() correctly for DB cache.
            return self._analyze_single_ipo_task(db_session, ipo_data_from_fetch)
        finally:
            SessionLocal.remove()  # Essential for scoped_session


if __name__ == '__main__':
    from database import init_db

    # init_db()

    logger.info("Starting standalone IPO analysis pipeline test...")
    analyzer = IPOAnalyzer()
    results = analyzer.run_ipo_analysis_pipeline()
    if results:
        logger.info(f"Processed {len(results)} IPOs.")
        for res in results:
            if hasattr(res, 'ipo') and res.ipo:
                logger.info(
                    f"IPO: {res.ipo.company_name} ({res.ipo.symbol}), Decision: {res.investment_decision}, Date: {res.ipo.ipo_date}, Status: {res.ipo.status}")
            else:
                logger.warning(f"Result item missing 'ipo' or ipo is None: {res}")
    else:
        logger.info("No IPOs were processed or found by the pipeline.")
---------- END ipo_analyzer.py ----------


---------- main.py ----------
# main.py
import argparse
from datetime import datetime, timezone
from sqlalchemy.orm import joinedload # Ensured import
from database import init_db, get_db_session, SessionLocal
from error_handler import logger
import time

from stock_analyzer import StockAnalyzer
from ipo_analyzer import IPOAnalyzer
from news_analyzer import NewsAnalyzer
from email_generator import EmailGenerator
from models import StockAnalysis, IPOAnalysis, NewsEventAnalysis
from config import MAX_NEWS_TO_ANALYZE_PER_RUN


def run_stock_analysis(tickers):
    logger.info(f"--- Starting Individual Stock Analysis for: {tickers} ---")
    results = []
    for ticker in tickers:
        try:
            analyzer = StockAnalyzer(ticker=ticker) # StockAnalyzer manages its own session lifecycle
            analysis_result = analyzer.analyze()
            if analysis_result:
                results.append(analysis_result)
            else:
                logger.warning(f"Stock analysis for {ticker} did not return a result object.")
        except RuntimeError as rt_err: # Catch init errors from StockAnalyzer
            logger.error(f"Could not run stock analysis for {ticker} due to critical init error: {rt_err}")
        except Exception as e:
            logger.error(f"Error analyzing stock {ticker}: {e}", exc_info=True)
        time.sleep(5) # Delay between analyzing different stocks
    return results


def run_ipo_analysis():
    logger.info("--- Starting IPO Analysis Pipeline ---")
    try:
        analyzer = IPOAnalyzer() # IPOAnalyzer now manages sessions per thread
        results = analyzer.run_ipo_analysis_pipeline()
        return results
    except Exception as e:
        logger.error(f"Error during IPO analysis pipeline: {e}", exc_info=True)
        return []


def run_news_analysis(category="general", count_to_analyze=MAX_NEWS_TO_ANALYZE_PER_RUN):
    logger.info(f"--- Starting News Analysis Pipeline (Category: {category}, Max to Analyze: {count_to_analyze}) ---")
    try:
        analyzer = NewsAnalyzer() # NewsAnalyzer manages its own session lifecycle
        results = analyzer.run_news_analysis_pipeline(category=category, count_to_analyze_this_run=count_to_analyze)
        return results
    except Exception as e:
        logger.error(f"Error during news analysis pipeline: {e}", exc_info=True)
        return []


def generate_and_send_todays_email_summary():
    logger.info("--- Generating Today's Email Summary ---")
    db_session = SessionLocal() # Use scoped_session for a new session
    today_start_utc = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)

    try:
        recent_stock_analyses = db_session.query(StockAnalysis).filter(StockAnalysis.analysis_date >= today_start_utc) \
            .options(joinedload(StockAnalysis.stock)).all()
        recent_ipo_analyses = db_session.query(IPOAnalysis).filter(IPOAnalysis.analysis_date >= today_start_utc) \
            .options(joinedload(IPOAnalysis.ipo)).all()
        recent_news_analyses = db_session.query(NewsEventAnalysis).filter(
            NewsEventAnalysis.analysis_date >= today_start_utc) \
            .options(joinedload(NewsEventAnalysis.news_event)).all()

        logger.info(
            f"Found {len(recent_stock_analyses)} stock analyses, {len(recent_ipo_analyses)} IPO analyses, "
            f"{len(recent_news_analyses)} news analyses since {today_start_utc.strftime('%Y-%m-%d %H:%M:%S %Z')} for email."
        )

        if not any([recent_stock_analyses, recent_ipo_analyses, recent_news_analyses]):
            logger.info("No new analyses performed recently to include in the email summary.")
            return

        email_gen = EmailGenerator()
        email_message = email_gen.create_summary_email(
            stock_analyses=recent_stock_analyses,
            ipo_analyses=recent_ipo_analyses,
            news_analyses=recent_news_analyses
        )

        if email_message:
            email_gen.send_email(email_message)
        else:
            logger.error("Failed to create the email message (returned None).")

    except Exception as e:
        logger.error(f"Error generating or sending email summary: {e}", exc_info=True)
    finally:
        SessionLocal.remove() # Ensure session is closed/removed for scoped_session


def main():
    parser = argparse.ArgumentParser(description="Financial Analysis and Reporting Tool")
    parser.add_argument("--analyze-stocks", nargs="+", metavar="TICKER",
                        help="List of stock tickers to analyze (e.g., AAPL MSFT)")
    parser.add_argument("--analyze-ipos", action="store_true", help="Run IPO analysis pipeline.")
    parser.add_argument("--analyze-news", action="store_true", help="Run news analysis pipeline.")
    parser.add_argument("--news-category", default="general",
                        help="Category for news analysis (e.g., general, forex, crypto, merger).")
    parser.add_argument("--news-count-analyze", type=int, default=MAX_NEWS_TO_ANALYZE_PER_RUN,
                        help=f"Max number of new news items to analyze in this run (default from config: {MAX_NEWS_TO_ANALYZE_PER_RUN}).")
    parser.add_argument("--send-email", action="store_true",
                        help="Generate and send email summary of today's/recent analyses.")
    parser.add_argument("--init-db", action="store_true", help="Initialize the database (create tables).")
    parser.add_argument("--all", action="store_true",
                        help="Run all analyses (stocks from a predefined list, IPOs, News) and send email. Define stock list below.")

    args = parser.parse_args()

    if args.init_db:
        logger.info("Initializing database as per command line argument...")
        try:
            init_db() # This uses the engine defined in database.py
            logger.info("Database initialization complete.")
        except Exception as e:
            logger.critical(f"Database initialization failed: {e}", exc_info=True)
            return # Stop if DB init fails

    if args.all:
        default_stocks_for_all = ["AAPL", "MSFT", "GOOGL", "NVDA", "JPM"] # Example list
        logger.info(
            f"Running all analyses for default stocks: {default_stocks_for_all}, IPOs, and News (max {args.news_count_analyze} items).")
        if default_stocks_for_all: run_stock_analysis(default_stocks_for_all)
        time.sleep(5) # Stagger main analysis types
        run_ipo_analysis()
        time.sleep(5)
        run_news_analysis(category=args.news_category, count_to_analyze=args.news_count_analyze)
        time.sleep(5)
        generate_and_send_todays_email_summary()
        logger.info("--- '--all' tasks finished. ---")
        return

    if args.analyze_stocks:
        run_stock_analysis(args.analyze_stocks)

    if args.analyze_ipos:
        run_ipo_analysis()

    if args.analyze_news:
        run_news_analysis(category=args.news_category, count_to_analyze=args.news_count_analyze)

    if args.send_email:
        generate_and_send_todays_email_summary()

    if not (
            args.analyze_stocks or args.analyze_ipos or args.analyze_news or args.send_email or args.init_db or args.all):
        logger.info("No action specified. Use --help for options.")
        parser.print_help()

    logger.info("--- Main script execution finished. ---")


if __name__ == "__main__":
    # Setup global exception handler (optional, can be useful for uncaught exceptions in threads)
    # import sys
    # from error_handler import handle_global_exception
    # sys.excepthook = handle_global_exception

    script_start_time = datetime.now(timezone.utc)
    logger.info("===================================================================")
    logger.info(f"Starting Financial Analysis Script at {script_start_time.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    logger.info("===================================================================")

    main()

    script_end_time = datetime.now(timezone.utc)
    logger.info(f"Financial Analysis Script finished at {script_end_time.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    logger.info(f"Total execution time: {script_end_time - script_start_time}")
    logger.info("===================================================================")
---------- END main.py ----------


---------- models.py ----------
# models.py
from sqlalchemy import Column, Integer, String, Float, DateTime, Text, JSON, ForeignKey, Boolean, Date, UniqueConstraint
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func  # For server-side SQL functions like func.now() if needed
from database import Base
from datetime import datetime, timezone


class Stock(Base):
    __tablename__ = "stocks"
    id = Column(Integer, primary_key=True, index=True)
    ticker = Column(String, unique=True, index=True, nullable=False)
    company_name = Column(String)
    industry = Column(String, nullable=True)
    sector = Column(String, nullable=True)
    # Use default for Python-side default value generation
    # Use onupdate for Python-side value generation on update
    last_analysis_date = Column(DateTime(timezone=True),
                                default=lambda: datetime.now(timezone.utc),
                                onupdate=lambda: datetime.now(timezone.utc))
    cik = Column(String, nullable=True, index=True)

    analyses = relationship("StockAnalysis", back_populates="stock", cascade="all, delete-orphan")


class StockAnalysis(Base):
    __tablename__ = "stock_analyses"
    id = Column(Integer, primary_key=True, index=True)
    stock_id = Column(Integer, ForeignKey("stocks.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    # Quantitative
    pe_ratio = Column(Float, nullable=True)
    pb_ratio = Column(Float, nullable=True)
    ps_ratio = Column(Float, nullable=True)
    ev_to_sales = Column(Float, nullable=True)
    ev_to_ebitda = Column(Float, nullable=True)
    eps = Column(Float, nullable=True)
    roe = Column(Float, nullable=True)
    roa = Column(Float, nullable=True)
    roic = Column(Float, nullable=True)
    dividend_yield = Column(Float, nullable=True)
    debt_to_equity = Column(Float, nullable=True)
    debt_to_ebitda = Column(Float, nullable=True)
    interest_coverage_ratio = Column(Float, nullable=True)
    current_ratio = Column(Float, nullable=True)
    quick_ratio = Column(Float, nullable=True)

    revenue_growth_yoy = Column(Float, nullable=True)
    revenue_growth_qoq = Column(Float, nullable=True)
    revenue_growth_cagr_3yr = Column(Float, nullable=True)
    revenue_growth_cagr_5yr = Column(Float, nullable=True)
    eps_growth_yoy = Column(Float, nullable=True)
    eps_growth_cagr_3yr = Column(Float, nullable=True)
    eps_growth_cagr_5yr = Column(Float, nullable=True)
    net_profit_margin = Column(Float, nullable=True)
    gross_profit_margin = Column(Float, nullable=True)
    operating_profit_margin = Column(Float, nullable=True)
    free_cash_flow_per_share = Column(Float, nullable=True)
    free_cash_flow_yield = Column(Float, nullable=True)
    free_cash_flow_trend = Column(String, nullable=True)
    retained_earnings_trend = Column(String, nullable=True)

    dcf_intrinsic_value = Column(Float, nullable=True)
    dcf_upside_percentage = Column(Float, nullable=True)
    dcf_assumptions = Column(JSON, nullable=True)

    # Qualitative
    business_summary = Column(Text, nullable=True)
    economic_moat_summary = Column(Text, nullable=True)
    industry_trends_summary = Column(Text, nullable=True)
    competitive_landscape_summary = Column(Text, nullable=True)
    management_assessment_summary = Column(Text, nullable=True)
    risk_factors_summary = Column(Text, nullable=True)

    # Conclusion
    investment_thesis_full = Column(Text, nullable=True)
    investment_decision = Column(String, nullable=True)
    reasoning = Column(Text, nullable=True)
    strategy_type = Column(String, nullable=True)
    confidence_level = Column(String, nullable=True)

    key_metrics_snapshot = Column(JSON, nullable=True)
    qualitative_sources_summary = Column(JSON, nullable=True)

    stock = relationship("Stock", back_populates="analyses")


class IPO(Base):
    __tablename__ = "ipos"
    id = Column(Integer, primary_key=True, index=True)
    company_name = Column(String, index=True, nullable=False)
    symbol = Column(String, index=True, nullable=True)
    ipo_date_str = Column(String, nullable=True)
    ipo_date = Column(Date, nullable=True)
    expected_price_range_low = Column(Float, nullable=True)
    expected_price_range_high = Column(Float, nullable=True)
    expected_price_currency = Column(String, nullable=True, default="USD")
    offered_shares = Column(Float, nullable=True)
    total_shares_value = Column(Float, nullable=True)
    exchange = Column(String, nullable=True)
    status = Column(String, nullable=True)
    cik = Column(String, nullable=True, index=True)
    last_analysis_date = Column(DateTime(timezone=True),
                                default=lambda: datetime.now(timezone.utc),
                                onupdate=lambda: datetime.now(timezone.utc))
    s1_filing_url = Column(String, nullable=True)

    analyses = relationship("IPOAnalysis", back_populates="ipo", cascade="all, delete-orphan")

    __table_args__ = (UniqueConstraint('company_name', 'ipo_date_str', 'symbol', name='uq_ipo_name_date_symbol'),)


class IPOAnalysis(Base):
    __tablename__ = "ipo_analyses"
    id = Column(Integer, primary_key=True, index=True)
    ipo_id = Column(Integer, ForeignKey("ipos.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    s1_business_summary = Column(Text, nullable=True)
    s1_risk_factors_summary = Column(Text, nullable=True)
    s1_mda_summary = Column(Text, nullable=True)
    s1_financial_health_summary = Column(Text, nullable=True)

    competitive_landscape_summary = Column(Text, nullable=True)
    industry_outlook_summary = Column(Text, nullable=True)
    management_team_assessment = Column(Text, nullable=True)
    use_of_proceeds_summary = Column(Text, nullable=True)
    underwriter_quality_assessment = Column(String, nullable=True)

    business_model_summary = Column(Text, nullable=True)
    risk_factors_summary = Column(Text, nullable=True)
    pre_ipo_financials_summary = Column(Text, nullable=True)
    valuation_comparison_summary = Column(Text, nullable=True)

    investment_decision = Column(String, nullable=True)
    reasoning = Column(Text, nullable=True)
    key_data_snapshot = Column(JSON, nullable=True)
    s1_sections_used = Column(JSON, nullable=True)

    ipo = relationship("IPO", back_populates="analyses")


class NewsEvent(Base):
    __tablename__ = "news_events"
    id = Column(Integer, primary_key=True, index=True)
    event_title = Column(String, index=True)
    event_date = Column(DateTime(timezone=True), nullable=True)
    source_url = Column(String, unique=True, nullable=False, index=True)
    source_name = Column(String, nullable=True)
    category = Column(String, nullable=True)
    processed_date = Column(DateTime(timezone=True),
                            default=lambda: datetime.now(timezone.utc))
    last_analyzed_date = Column(DateTime(timezone=True), nullable=True,
                                onupdate=lambda: datetime.now(timezone.utc))
    full_article_text = Column(Text, nullable=True)

    analyses = relationship("NewsEventAnalysis", back_populates="news_event", cascade="all, delete-orphan")

    __table_args__ = (UniqueConstraint('source_url', name='uq_news_source_url'),)


class NewsEventAnalysis(Base):
    __tablename__ = "news_event_analyses"
    id = Column(Integer, primary_key=True, index=True)
    news_event_id = Column(Integer, ForeignKey("news_events.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    sentiment = Column(String, nullable=True)
    sentiment_reasoning = Column(Text, nullable=True)

    affected_stocks_explicit = Column(JSON, nullable=True)
    affected_sectors_explicit = Column(JSON, nullable=True)

    news_summary_detailed = Column(Text, nullable=True)
    potential_impact_on_market = Column(Text, nullable=True)
    potential_impact_on_companies = Column(Text, nullable=True)
    potential_impact_on_sectors = Column(Text, nullable=True)

    mechanism_of_impact = Column(Text, nullable=True)
    estimated_timing_duration = Column(String, nullable=True)
    estimated_magnitude_direction = Column(String, nullable=True)
    confidence_of_assessment = Column(String, nullable=True)

    summary_for_email = Column(Text, nullable=True)
    key_news_snippets = Column(JSON, nullable=True)

    news_event = relationship("NewsEvent", back_populates="analyses")


class CachedAPIData(Base):
    __tablename__ = "cached_api_data"
    id = Column(Integer, primary_key=True, index=True)
    api_source = Column(String, index=True, nullable=False)
    request_url_or_params = Column(String, unique=True, nullable=False, index=True)
    response_data = Column(JSON, nullable=False)
    timestamp = Column(DateTime(timezone=True),
                       default=lambda: datetime.now(timezone.utc))  # default for Python-side timestamp
    expires_at = Column(DateTime(timezone=True), nullable=False, index=True)
---------- END models.py ----------


---------- news_analyzer.py ----------
# news_analyzer.py
import time
from sqlalchemy import inspect as sa_inspect  # For checking instance state
from sqlalchemy.orm import joinedload
from datetime import datetime, timezone, timedelta

from api_clients import FinnhubClient, GeminiAPIClient, scrape_article_content
from database import SessionLocal, get_db_session
from models import NewsEvent, NewsEventAnalysis
from error_handler import logger
from sqlalchemy.exc import SQLAlchemyError
from config import (
    MAX_NEWS_ARTICLES_PER_QUERY, MAX_NEWS_TO_ANALYZE_PER_RUN,
    NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI, MAX_GEMINI_TEXT_LENGTH
)


class NewsAnalyzer:
    def __init__(self):
        self.finnhub = FinnhubClient()
        self.gemini = GeminiAPIClient()
        self.db_session = next(get_db_session())

    def _close_session_if_active(self):
        if self.db_session and self.db_session.is_active:
            try:
                self.db_session.close()
                logger.debug("DB session closed in NewsAnalyzer.")
            except Exception as e_close:
                logger.warning(f"Error closing session in NewsAnalyzer: {e_close}")

    def fetch_market_news(self, category="general", count_to_fetch_from_api=MAX_NEWS_ARTICLES_PER_QUERY):
        logger.info(f"Fetching latest market news for category: {category} (max {count_to_fetch_from_api} from API)...")
        # Finnhub's /news endpoint doesn't have a 'count' param for items returned in one call, it returns recent news.
        # We might need to paginate or filter if we need more than one batch.
        # For now, one call and take top 'count_to_fetch_from_api'.
        news_items = self.finnhub.get_market_news(category=category)

        if news_items and isinstance(news_items, list):
            logger.info(f"Fetched {len(news_items)} news items from Finnhub.")
            return news_items[:count_to_fetch_from_api]
        else:
            logger.warning(f"Failed to fetch news or received unexpected format from Finnhub: {news_items}")
            return []

    def _get_or_create_news_event(self, news_item_from_api):
        self._ensure_news_event_session_active(news_item_from_api.get('headline', 'Unknown News'))

        source_url = news_item_from_api.get("url")
        if not source_url:
            logger.warning(f"News item missing URL, cannot process: {news_item_from_api.get('headline')}")
            return None  # Cannot reliably deduplicate or process without a URL

        event = self.db_session.query(NewsEvent).filter_by(source_url=source_url).first()

        full_article_text_scraped_now = None
        # Scrape only if event is new, or if it exists but full_article_text is missing
        if not event or (event and not event.full_article_text):
            logger.info(f"Attempting to scrape full article for: {source_url}")
            full_article_text_scraped_now = scrape_article_content(source_url)  # This is an API call (HTTP GET)
            time.sleep(1)  # Small delay after scraping
            if full_article_text_scraped_now:
                logger.info(f"Scraped ~{len(full_article_text_scraped_now)} chars for {source_url}")
            else:
                logger.warning(
                    f"Failed to scrape full article for {source_url}. Analysis will use summary if available.")

        current_time_utc = datetime.now(timezone.utc)
        if event:  # Event already exists in DB
            logger.debug(f"News event '{event.event_title[:70]}...' (URL: {source_url}) already in DB.")
            # If we just scraped text and it was missing, update the event
            if full_article_text_scraped_now and not event.full_article_text:
                logger.info(f"Updating existing event {event.id} with newly scraped full article text.")
                event.full_article_text = full_article_text_scraped_now
                event.processed_date = current_time_utc  # Update processed date as we've enhanced it
                try:
                    self.db_session.commit()
                except SQLAlchemyError as e:
                    self.db_session.rollback()
                    logger.error(f"Error updating full_article_text for existing event {source_url}: {e}")
            return event

        # Event is new, create it
        event_timestamp = news_item_from_api.get("datetime")
        event_datetime_utc = datetime.fromtimestamp(event_timestamp,
                                                    timezone.utc) if event_timestamp else current_time_utc

        new_event = NewsEvent(
            event_title=news_item_from_api.get("headline"),
            event_date=event_datetime_utc,
            source_url=source_url,
            source_name=news_item_from_api.get("source"),
            category=news_item_from_api.get("category"),
            full_article_text=full_article_text_scraped_now,  # Store scraped text
            processed_date=current_time_utc
        )
        self.db_session.add(new_event)
        try:
            self.db_session.commit()
            self.db_session.refresh(new_event)  # Get ID and other defaults loaded
            logger.info(f"Stored new news event: {new_event.event_title[:70]}... (ID: {new_event.id})")
            return new_event
        except SQLAlchemyError as e:
            self.db_session.rollback()
            logger.error(f"Database error storing new news event '{news_item_from_api.get('headline')}': {e}",
                         exc_info=True)
            return None

    def analyze_single_news_item(self, news_event_db):
        if not news_event_db:
            logger.error("analyze_single_news_item called with no NewsEvent DB object.")
            return None

        # Ensure the event object is bound to the current session
        news_event_db = self._ensure_news_event_is_bound(news_event_db)
        if not news_event_db:  # If binding failed
            return None

        headline = news_event_db.event_title
        content_for_analysis = news_event_db.full_article_text
        analysis_source_type = "full article"

        if not content_for_analysis:
            # Fallback to headline if no full article text (e.g. scraping failed or PDF)
            # A more robust system might use Finnhub summary if that was stored with NewsEvent.
            content_for_analysis = headline
            analysis_source_type = "headline only"
            logger.warning(f"No full article text for '{headline}'. Analyzing based on headline only.")

        # Truncate for Gemini if too long
        if len(content_for_analysis) > NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI:
            content_for_analysis = content_for_analysis[
                                   :NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI] + "\n... [CONTENT TRUNCATED FOR AI ANALYSIS] ..."
            logger.info(
                f"Truncated news content for '{headline}' to {NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI} chars for Gemini.")
            analysis_source_type += " (truncated)"

        logger.info(f"Analyzing news: '{headline[:70]}...' (using {analysis_source_type})")
        analysis_payload = {"key_news_snippets": {"headline": headline, "source_type_used": analysis_source_type}}

        # 1. Sentiment Analysis
        sentiment_context = f"News headline for context: {headline}"
        sentiment_response = self.gemini.analyze_sentiment_with_reasoning(content_for_analysis,
                                                                          context=sentiment_context)
        time.sleep(2)  # API courtesy
        if not sentiment_response.startswith("Error:"):
            try:  # Attempt to parse sentiment and reasoning
                parts = sentiment_response.split("Reasoning:", 1)
                if ":" in parts[0]:  # Expect "Sentiment: [Class]"
                    sentiment_class_text = parts[0].split(":", 1)[1].strip()
                    # Take first word, remove punctuation
                    analysis_payload["sentiment"] = sentiment_class_text.split(' ')[0].split('.')[0].split(',')[
                        0].strip()
                else:  # If no "Sentiment:" prefix, try to infer
                    analysis_payload["sentiment"] = parts[0].strip().split(' ')[0]  # First word as sentiment
                analysis_payload["sentiment_reasoning"] = parts[1].strip() if len(parts) > 1 else sentiment_response
            except Exception as e_parse_sent:
                logger.warning(
                    f"Could not robustly parse sentiment response for '{headline}': {sentiment_response}. Error: {e_parse_sent}. Storing raw.")
                analysis_payload["sentiment"] = "Error Parsing"
                analysis_payload["sentiment_reasoning"] = sentiment_response
        else:
            analysis_payload["sentiment"] = "AI Error"
            analysis_payload["sentiment_reasoning"] = sentiment_response

        # 2. Detailed Summary & Impact Analysis
        prompt_detailed_analysis = (
            f"News Headline: \"{headline}\"\n"
            f"News Content (may be truncated): \"{content_for_analysis}\"\n\n"
            f"Instructions for Analysis:\n"
            f"1. News Summary: Provide a comprehensive yet concise summary of this news article (3-5 key sentences).\n"
            f"2. Affected Entities: Identify specific companies (with ticker symbols if known and highly relevant) and/or specific industry sectors directly or significantly indirectly affected by this news. Explain why briefly for each.\n"
            f"3. Mechanism of Impact: For the primary affected entities, describe how this news will likely affect their fundamentals (e.g., revenue, costs, market share, customer sentiment) or market perception.\n"
            f"4. Estimated Timing & Duration: Estimate the likely timing (e.g., Immediate, Short-term <3mo, Medium-term 3-12mo, Long-term >1yr) and duration of the impact.\n"
            f"5. Estimated Magnitude & Direction: Estimate the potential magnitude (e.g., Low, Medium, High) and direction (e.g., Positive, Negative, Neutral/Mixed) of the impact on the primary affected entities.\n"
            f"6. Confidence Level: State your confidence (High, Medium, Low) in this overall impact assessment, briefly justifying it (e.g., based on clarity of news, directness of impact).\n"
            f"7. Investor Summary: Provide a final 2-sentence summary specifically for an investor, highlighting the most critical implication or takeaway.\n\n"
            f"Structure your response clearly with headings for each point (e.g., 'News Summary:', 'Affected Entities:', etc.)."
        )

        impact_analysis_response = self.gemini.generate_text(
            prompt_detailed_analysis[:MAX_GEMINI_TEXT_LENGTH])  # Ensure prompt length
        time.sleep(2)  # API courtesy

        if not impact_analysis_response.startswith("Error:"):
            analysis_payload["news_summary_detailed"] = self._parse_ai_section(impact_analysis_response,
                                                                               "News Summary:")
            analysis_payload["potential_impact_on_companies"] = self._parse_ai_section(impact_analysis_response,
                                                                                       ["Affected Entities:",
                                                                                        "Affected Companies:",
                                                                                        "Affected Stocks/Sectors:"])
            # If "Affected Sectors:" is a distinct section, try to get it too.
            sectors_text = self._parse_ai_section(impact_analysis_response, "Affected Sectors:")
            if sectors_text and not sectors_text.startswith("Section not found"):
                analysis_payload["potential_impact_on_sectors"] = sectors_text
            elif analysis_payload["potential_impact_on_companies"] and not analysis_payload.get(
                    "potential_impact_on_sectors"):  # If combined
                analysis_payload["potential_impact_on_sectors"] = analysis_payload["potential_impact_on_companies"]

            analysis_payload["mechanism_of_impact"] = self._parse_ai_section(impact_analysis_response,
                                                                             "Mechanism of Impact:")
            analysis_payload["estimated_timing_duration"] = self._parse_ai_section(impact_analysis_response,
                                                                                   ["Estimated Timing & Duration:",
                                                                                    "Estimated Timing:"])
            analysis_payload["estimated_magnitude_direction"] = self._parse_ai_section(impact_analysis_response, [
                "Estimated Magnitude & Direction:", "Estimated Magnitude/Direction:"])
            analysis_payload["confidence_of_assessment"] = self._parse_ai_section(impact_analysis_response,
                                                                                  "Confidence Level:")
            analysis_payload["summary_for_email"] = self._parse_ai_section(impact_analysis_response,
                                                                           ["Investor Summary:",
                                                                            "Final Summary for Investor:"])
        else:
            logger.error(
                f"Gemini failed to provide detailed impact analysis for '{headline}': {impact_analysis_response}")
            analysis_payload["news_summary_detailed"] = impact_analysis_response  # Store error message

        # Store analysis
        current_analysis_time = datetime.now(timezone.utc)
        news_analysis_entry = NewsEventAnalysis(
            news_event_id=news_event_db.id,
            analysis_date=current_analysis_time,
            sentiment=analysis_payload.get("sentiment"),
            sentiment_reasoning=analysis_payload.get("sentiment_reasoning"),
            news_summary_detailed=analysis_payload.get("news_summary_detailed"),
            potential_impact_on_companies=analysis_payload.get("potential_impact_on_companies"),
            potential_impact_on_sectors=analysis_payload.get("potential_impact_on_sectors"),
            mechanism_of_impact=analysis_payload.get("mechanism_of_impact"),
            estimated_timing_duration=analysis_payload.get("estimated_timing_duration"),
            estimated_magnitude_direction=analysis_payload.get("estimated_magnitude_direction"),
            confidence_of_assessment=analysis_payload.get("confidence_of_assessment"),
            summary_for_email=analysis_payload.get("summary_for_email"),
            key_news_snippets=analysis_payload.get("key_news_snippets")
            # TODO: Add logic to parse tickers/sectors from `potential_impact_on_companies` into `affected_stocks_explicit` JSON fields if needed.
        )
        self.db_session.add(news_analysis_entry)
        news_event_db.last_analyzed_date = current_analysis_time  # Update parent event

        try:
            self.db_session.commit()
            logger.info(
                f"Successfully analyzed and saved news: '{headline[:70]}...' (Analysis ID: {news_analysis_entry.id})")
        except SQLAlchemyError as e:
            self.db_session.rollback()
            logger.error(f"Database error saving news analysis for '{headline[:70]}...': {e}", exc_info=True)
            return None

        return news_analysis_entry

    def _parse_ai_section(self, ai_text, section_header_keywords):
        # This helper is now more robust, moved from IPOAnalyzer to be reusable if needed
        # (though for now it's kept within each class for simplicity of single file changes)
        if not ai_text or ai_text.startswith("Error:"): return "AI Error or No Text"

        if isinstance(section_header_keywords, str):
            keywords_to_check = [section_header_keywords.lower().strip()]
        else:
            keywords_to_check = [k.lower().strip() for k in section_header_keywords]

        lines = ai_text.split('\n')
        capture = False
        section_content = []

        # Define a list of all potential headers that could terminate a section.
        # This helps in accurately capturing multi-line content for the current section.
        all_known_headers_lower_prefixes = [
            "news summary:", "affected entities:", "affected companies:", "affected stocks/sectors:",
            "mechanism of impact:", "estimated timing & duration:", "estimated timing:",
            "estimated magnitude & direction:", "estimated magnitude/direction:",
            "confidence level:", "investor summary:", "final summary for investor:"
        ]  # Add other general section headers from other analyzers if this becomes shared utility.

        for i, line_original in enumerate(lines):
            line_stripped_lower = line_original.strip().lower()

            matched_current_keyword = None
            for kw_lower in keywords_to_check:
                # Check if the line starts with the keyword (potentially followed by a colon)
                if line_stripped_lower.startswith(kw_lower + ":") or line_stripped_lower == kw_lower:
                    matched_current_keyword = kw_lower
                    break

            if matched_current_keyword:
                capture = True
                # Get content on the same line after the header
                content_on_header_line = line_original.strip()[len(matched_current_keyword):].strip()
                if content_on_header_line.startswith(":"):
                    content_on_header_line = content_on_header_line[1:].strip()
                if content_on_header_line:
                    section_content.append(content_on_header_line)
                continue  # Move to next line

            if capture:
                # Check if the current line starts a *different* known section
                is_another_known_header = False
                for known_header_prefix in all_known_headers_lower_prefixes:
                    if line_stripped_lower.startswith(
                            known_header_prefix) and known_header_prefix not in keywords_to_check:
                        is_another_known_header = True
                        break

                if is_another_known_header:
                    break  # End capture for the current section

                section_content.append(line_original)  # Append the original line to preserve formatting

        return "\n".join(section_content).strip() if section_content else "Section not found or empty."

    def run_news_analysis_pipeline(self, category="general", count_to_fetch_from_api=MAX_NEWS_ARTICLES_PER_QUERY,
                                   count_to_analyze_this_run=MAX_NEWS_TO_ANALYZE_PER_RUN):
        fetched_news_items_api = self.fetch_market_news(category=category,
                                                        count_to_fetch_from_api=count_to_fetch_from_api)
        if not fetched_news_items_api:
            logger.info("No news items fetched from API for analysis.")
            self._close_session_if_active()
            return []

        analyzed_news_results = []
        newly_analyzed_count_this_run = 0

        reanalyze_older_than_days = 2  # Re-analyze if analysis is older, or if full text was missing and now found
        reanalyze_threshold_date = datetime.now(timezone.utc) - timedelta(days=reanalyze_older_than_days)

        for news_item_api_data in fetched_news_items_api:
            if newly_analyzed_count_this_run >= count_to_analyze_this_run:
                logger.info(
                    f"Reached analysis limit of {count_to_analyze_this_run} new/re-analyzed items for this run.")
                break

            try:
                news_event_db = self._get_or_create_news_event(
                    news_item_api_data)  # Scrapes/stores full text if new or missing
                if not news_event_db:
                    logger.warning(
                        f"Skipping news item as it could not be fetched or created in DB: {news_item_api_data.get('headline')}")
                    continue

                news_event_db = self._ensure_news_event_is_bound(news_event_db)  # Ensure session attachment
                if not news_event_db: continue

                analysis_needed = False
                # Check if analysis exists and is recent enough
                latest_analysis = None
                if news_event_db.analyses:  # Relationship is a list
                    # Sort by analysis_date descending to get the most recent
                    sorted_analyses = sorted(news_event_db.analyses, key=lambda x: x.analysis_date, reverse=True)
                    if sorted_analyses:
                        latest_analysis = sorted_analyses[0]

                if not latest_analysis:
                    analysis_needed = True
                    logger.info(
                        f"News '{news_event_db.event_title[:50]}...' (ID: {news_event_db.id}) requires new analysis (never analyzed).")
                elif latest_analysis.analysis_date < reanalyze_threshold_date:
                    analysis_needed = True
                    logger.info(
                        f"News '{news_event_db.event_title[:50]}...' (ID: {news_event_db.id}) requires re-analysis (analysis older than {reanalyze_older_than_days} days).")
                elif not news_event_db.full_article_text and latest_analysis:
                    # If it was analyzed but full text was missing (e.g. scraping failed before)
                    # _get_or_create_news_event tries to scrape again. If it succeeds now, we should re-analyze.
                    # The check `if full_article_text_scraped_now and not event.full_article_text:` in _get_or_create_news_event
                    # handles updating the event. If event.full_article_text is now populated, re-analyze.
                    if news_event_db.full_article_text:  # Check if it was successfully populated in this run
                        analysis_needed = True
                        logger.info(
                            f"News '{news_event_db.event_title[:50]}...' (ID: {news_event_db.id}) re-analyzing with newly scraped full text.")
                    else:
                        logger.info(
                            f"News '{news_event_db.event_title[:50]}...' (ID: {news_event_db.id}) already analyzed, full text still unavailable. Skipping re-analysis.")

                if analysis_needed:
                    analysis_result = self.analyze_single_news_item(news_event_db)
                    if analysis_result:
                        analyzed_news_results.append(analysis_result)
                        newly_analyzed_count_this_run += 1
                    time.sleep(3)  # API courtesy delay after each full analysis cycle
                else:
                    logger.info(
                        f"News '{news_event_db.event_title[:50]}...' (ID: {news_event_db.id}) already recently analyzed with available text. Skipping.")

            except Exception as e:
                logger.error(f"Failed to process or analyze news item '{news_item_api_data.get('headline')}': {e}",
                             exc_info=True)
                # Ensure session robustness for the next item
                if self.db_session and not self.db_session.is_active:
                    self.db_session = next(get_db_session())
                elif self.db_session:
                    self.db_session.rollback()  # Rollback current transaction if error occurred within loop item

        logger.info(
            f"News analysis pipeline completed. Newly analyzed/re-analyzed {newly_analyzed_count_this_run} items.")
        self._close_session_if_active()
        return analyzed_news_results

    def _ensure_news_event_session_active(self, news_identifier_for_log):
        if not self.db_session.is_active:
            logger.warning(f"Session for News '{news_identifier_for_log}' was inactive. Re-establishing.")
            self._close_session_if_active()
            self.db_session = next(get_db_session())

    def _ensure_news_event_is_bound(self, news_event_db_obj):
        """Ensures the news_event_db_obj is bound to the current active session."""
        if not news_event_db_obj: return None  # Should not happen

        self._ensure_news_event_session_active(
            news_event_db_obj.event_title[:50] if news_event_db_obj.event_title else 'Unknown News')

        instance_state = sa_inspect(news_event_db_obj)
        if not instance_state.session or instance_state.session is not self.db_session:
            logger.warning(
                f"NewsEvent DB entry '{news_event_db_obj.event_title[:50]}...' (ID: {news_event_db_obj.id if instance_state.has_identity else 'Transient'}) is not bound to current session. Merging.")
            try:
                if not instance_state.has_identity and news_event_db_obj.id is None:
                    # Try to find by URL if it's a new object that might already exist in this session's view due to prior ops
                    existing_in_session = self.db_session.query(NewsEvent).filter_by(
                        source_url=news_event_db_obj.source_url).first()
                    if existing_in_session:
                        news_event_db_obj = existing_in_session
                        logger.info(
                            f"Replaced transient NewsEvent for '{news_event_db_obj.source_url}' with instance from current session.")
                        return news_event_db_obj

                merged_event = self.db_session.merge(news_event_db_obj)
                logger.info(
                    f"Successfully merged/re-associated NewsEvent '{merged_event.event_title[:50]}...' (ID: {merged_event.id}) into current session.")
                return merged_event
            except Exception as e_merge:
                logger.error(
                    f"Failed to merge NewsEvent '{news_event_db_obj.event_title[:50]}...' into session: {e_merge}. Re-fetching as fallback.",
                    exc_info=True)
                fallback_event = None
                if instance_state.has_identity and news_event_db_obj.id:
                    fallback_event = self.db_session.query(NewsEvent).get(news_event_db_obj.id)
                elif news_event_db_obj.source_url:  # Try by unique URL
                    fallback_event = self.db_session.query(NewsEvent).filter_by(
                        source_url=news_event_db_obj.source_url).first()

                if not fallback_event:
                    logger.critical(
                        f"CRITICAL: Failed to re-associate NewsEvent '{news_event_db_obj.event_title[:50]}...' with current session after merge failure and could not re-fetch.")
                    return None  # Indicate critical failure
                logger.info(
                    f"Successfully re-fetched NewsEvent '{fallback_event.event_title[:50]}...' after merge failure.")
                return fallback_event
        return news_event_db_obj


if __name__ == '__main__':
    from database import init_db

    # init_db() # Ensure DB is initialized with new NewsEvent/Analysis model fields if changed

    logger.info("Starting standalone news analysis pipeline test...")
    analyzer = NewsAnalyzer()
    # Analyze a few new general news items
    results = analyzer.run_news_analysis_pipeline(category="general", count_to_fetch_from_api=10,
                                                  count_to_analyze_this_run=3)
    if results:
        logger.info(f"Pipeline processed {len(results)} news items this run.")
        for res_idx, res in enumerate(results):
            if hasattr(res, 'news_event') and res.news_event:  # Check if result object is valid
                logger.info(f"--- Result {res_idx + 1} ---")
                logger.info(f"News: {res.news_event.event_title[:100]}...")
                logger.info(f"Source: {res.news_event.source_url}")
                logger.info(f"Sentiment: {res.sentiment} - Reasoning: {res.sentiment_reasoning[:100]}...")
                logger.info(f"Investor Summary: {res.summary_for_email}")
                logger.info(f"Full Article Scraped: {'Yes' if res.news_event.full_article_text else 'No'}")
                if res.news_event.full_article_text:
                    logger.debug(f"Full Article Snippet: {res.news_event.full_article_text[:200]}...")
            else:
                logger.warning(
                    f"Processed news result item missing 'news_event' attribute or news_event is None. Result: {res}")
    else:
        logger.info("No new news items were processed in this run.")
---------- END news_analyzer.py ----------


---------- project_structure.py ----------
# project_to_file_backend.py
import os
import sys
from pathlib import Path

# --- Configuration ---

# Directories to exclude from both structure and content
EXCLUDED_DIRS = [
    '__pycache__',
    '.git',
    '.venv',        # Common virtual environment names
    'venv',
    'env',
    '.env',         # Exclude the .env file itself if it contains secrets
    'build',
    'dist',
    '*.egg-info',   # Python packaging artifacts
    '.pytest_cache',
    '.mypy_cache',
    '.vscode',
    '.idea',
    'node_modules', # If you happen to have node_modules
    'migrations',   # Often contains auto-generated code, review if needed
    'alembic'       # Alembic directory
]

# Specific files to exclude
EXCLUDED_FILES = [
    '.DS_Store',
    '*.pyc',
    '*.pyo',
    '*.pyd',
    '.env',         # Explicitly exclude again just in case
    'project_to_file_backend.py', # Exclude this script itself
    'project_structure_backend.txt', # Exclude the output file
    # Add any other specific files like local config overrides
]

# File extensions to treat as binary/media (content won't be included)
# Add more as needed (e.g., .jpg, .gif, .mp4, .db, .sqlite)
BINARY_EXTENSIONS = [
    '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.webp', '.ico',
    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
    '.zip', '.tar', '.gz', '.rar',
    '.mp3', '.wav', '.ogg',
    '.mp4', '.avi', '.mov', '.wmv',
    '.db', '.sqlite', '.sqlite3',
    '.pkl', '.joblib',
    '.pt', '.pth', '.onnx', # Model files
]

# Output file name
OUTPUT_FILE = 'project_structure_backend.txt'

# --- Script Logic ---

file_structure_tree = ""
file_contents = ""

def should_exclude(path: Path) -> bool:
    """Check if a path should be excluded based on configured lists."""
    # Check against excluded directory names
    if any(part in EXCLUDED_DIRS for part in path.parts):
        return True
    # Check against excluded directory patterns (like *.egg-info)
    if any(path.match(pattern) for pattern in EXCLUDED_DIRS if '*' in pattern):
         return True
    # Check against excluded file names and patterns
    if path.name in EXCLUDED_FILES:
        return True
    if any(path.match(pattern) for pattern in EXCLUDED_FILES if '*' in pattern):
        return True

    return False

def traverse_directory(dir_path: Path, indent: str = ''):
    """Recursively traverses directories and builds the structure/content strings."""
    global file_structure_tree
    global file_contents

    try:
        # Sort entries for consistent ordering: directories first, then files
        entries = sorted(list(dir_path.iterdir()), key=lambda p: (p.is_file(), p.name.lower()))
    except PermissionError:
        print(f"Warning: Permission denied for directory '{dir_path}'. Skipping.")
        return
    except FileNotFoundError:
         print(f"Warning: Directory '{dir_path}' not found during traversal (might have been deleted). Skipping.")
         return


    for entry in entries:
        if should_exclude(entry):
            continue

        if entry.is_dir():
            file_structure_tree += f"{indent}{entry.name}/\n"
            traverse_directory(entry, indent + '  ')
        elif entry.is_file():
            file_structure_tree += f"{indent}{entry.name}\n"
            file_extension = entry.suffix.lower()

            # Add separators for all files
            file_contents += f"\n---------- {entry.name} ----------\n"
            if file_extension in BINARY_EXTENSIONS:
                file_contents += f"(Binary file type {file_extension} - content not included)\n"
            else:
                try:
                    # Try reading with UTF-8 first
                    with entry.open('r', encoding='utf-8') as f:
                        file_contents += f.read() + "\n"
                except UnicodeDecodeError:
                    try:
                        # Fallback to latin-1 if UTF-8 fails
                        with entry.open('r', encoding='latin-1') as f:
                            file_contents += f.read() + "\n"
                        file_contents += "(Warning: Read file using latin-1 encoding due to UTF-8 decode error)\n"
                    except Exception as read_error_fallback:
                         file_contents += f"ERROR READING FILE (Fallback Failed): {read_error_fallback}\n"
                except Exception as read_error:
                    file_contents += f"ERROR READING FILE: {read_error}\n"

            file_contents += f"---------- END {entry.name} ----------\n\n"


def generate_project_structure_and_content(project_root: Path, output_file_path: Path):
    """Generates the combined structure and content file."""
    global file_structure_tree
    global file_contents
    file_structure_tree = "" # Reset global state
    file_contents = ""     # Reset global state

    print(f"Starting traversal from: {project_root}")
    traverse_directory(project_root)

    full_output = f"--- START OF FILE {output_file_path.name} ---\n\n"
    full_output += file_structure_tree
    full_output += "\n" # Separator between tree and content
    full_output += file_contents
    full_output += f"--- END OF FILE {output_file_path.name} ---\n"


    try:
        with output_file_path.open('w', encoding='utf-8') as f:
            f.write(full_output)
        print(f"Project structure and content written to '{output_file_path}'")
    except IOError as e:
        print(f"Error writing to output file '{output_file_path}': {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"An unexpected error occurred during file writing: {e}", file=sys.stderr)
        sys.exit(1)

# --- Main Execution ---
if __name__ == "__main__":
    project_root_path = Path(os.getcwd()) # Get current working directory as Path object
    output_path = project_root_path / OUTPUT_FILE

    print(f"Project Root: {project_root_path}")
    print(f"Output File: {output_path}")

    if not project_root_path.is_dir():
        print(f"Error: Project root directory '{project_root_path}' not found or is not a directory.", file=sys.stderr)
        sys.exit(1)

    try:
        generate_project_structure_and_content(project_root_path, output_path)
    except Exception as e:
        print(f"\nAn unexpected error occurred during execution: {e}", file=sys.stderr)
        sys.exit(1)
---------- END project_structure.py ----------


---------- requirements.txt ----------
# requirements.txt
sqlalchemy>=1.4,<2.0
requests>=2.32.0
psycopg2-binary>=2.8.0
pandas>=1.0.0
markdown2>=2.4.0
beautifulsoup4>=4.9.3
lxml>=4.6.3 
# Add other specific versions if needed
# python-dotenv # For managing environment variables if you choose to use .env files
---------- END requirements.txt ----------


---------- stock_analyzer.py ----------
# stock_analyzer.py
import pandas as pd
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timezone, timedelta
import math  # For DCF calculations
import time  # For API courtesy delays
import warnings  # For filtering warnings
from bs4 import XMLParsedAsHTMLWarning  # For filtering specific BS4 warning
import re  # For parsing AI response

# Filter the XMLParsedAsHTMLWarning from BeautifulSoup
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

from api_clients import (
    FinnhubClient, FinancialModelingPrepClient, AlphaVantageClient,
    EODHDClient, GeminiAPIClient, SECEDGARClient, extract_S1_text_sections
)
from database import SessionLocal, get_db_session
from models import Stock, StockAnalysis
from error_handler import logger
from sqlalchemy.exc import SQLAlchemyError
from config import (
    STOCK_FINANCIAL_YEARS, DEFAULT_DISCOUNT_RATE,
    DEFAULT_PERPETUAL_GROWTH_RATE, DEFAULT_FCF_PROJECTION_YEARS,
    TEN_K_KEY_SECTIONS, MAX_10K_SECTION_LENGTH_FOR_GEMINI,
    MAX_GEMINI_TEXT_LENGTH
)


# Helper function to safely get a numeric value from a dictionary
def safe_get_float(data_dict, key, default=None):
    val = data_dict.get(key)
    if val is None or val == "None" or val == "": return default  # Added empty string check
    try:
        return float(val)
    except (ValueError, TypeError):
        # logger.debug(f"safe_get_float: Could not convert key '{key}' value '{val}' to float. Returning default.")
        return default


# Helper function for CAGR calculation
def calculate_cagr(end_value, start_value, years):
    if start_value is None or end_value is None or not isinstance(years, (int, float)) or years <= 0: return None
    if start_value == 0: return None  # Avoid division by zero; growth is infinite or undefined
    # Handle negative values carefully: CAGR is typically for positive values.
    # If both are negative, it can be calculated but interpretation is complex.
    # If one is negative and other positive, CAGR is usually not meaningful.
    if start_value < 0 and end_value < 0:  # Both negative
        # Treat as positive for calculation, then adjust sign if needed (though CAGR is a rate)
        # This specific formula handles common cases for financial CAGR
        return ((float(end_value) / float(start_value)) ** (1 / float(years))) - 1
    if start_value < 0 or end_value < 0:  # One is negative, other positive/zero
        return None
    if end_value == 0 and start_value > 0:  # Value dropped to zero
        return -1.0  # Represents a 100% loss
    return ((float(end_value) / float(start_value)) ** (1 / float(years))) - 1


# Helper function for simple growth (YoY, QoQ)
def calculate_growth(current_value, previous_value):
    if previous_value is None or current_value is None: return None
    if float(previous_value) == 0:
        return None  # Or handle as infinite growth if current_value is positive
    try:
        return (float(current_value) - float(previous_value)) / abs(float(previous_value))
    except (ValueError, TypeError):
        return None


# Helper function to get value from a list of statement dicts (FMP style)
def get_value_from_statement_list(data_list, field, year_offset=0, report_date_for_log=None):
    if data_list and isinstance(data_list, list) and len(data_list) > year_offset:
        report = data_list[year_offset]
        if report and isinstance(report, dict):
            val = safe_get_float(report, field)
            if val is None:
                date_info = report_date_for_log or report.get('date', 'Unknown Date')
                # logger.debug(f"Field '{field}' not found or is None in FMP statement list for offset {year_offset} (Date: {date_info}).")
            return val
    # logger.debug(f"Could not get FMP statement field '{field}' for offset {year_offset}. Data list length {len(data_list) if data_list else 'N/A'}.")
    return None


# Helper function to get a specific concept value from Finnhub's reported financials
def get_finnhub_concept_value(finnhub_quarterly_reports_data, report_section_key, concept_names_list, quarter_offset=0):
    if not finnhub_quarterly_reports_data or len(finnhub_quarterly_reports_data) <= quarter_offset: return None
    report_data = finnhub_quarterly_reports_data[quarter_offset]
    if 'report' not in report_data or report_section_key not in report_data['report']: return None
    section_items = report_data['report'][report_section_key]
    if not section_items: return None
    for item in section_items:
        if item.get('concept') in concept_names_list or item.get('label') in concept_names_list:
            return safe_get_float(item, 'value')
    return None


# Helper function to get value from Alpha Vantage quarterly reports (list sorted oldest to newest)
def get_alphavantage_value(av_quarterly_reports, field_name, quarter_offset_from_latest=0):
    if not av_quarterly_reports or len(av_quarterly_reports) <= quarter_offset_from_latest:
        return None
    report_index = len(av_quarterly_reports) - 1 - quarter_offset_from_latest
    if report_index < 0: return None
    report = av_quarterly_reports[report_index]
    return safe_get_float(report, field_name)


class StockAnalyzer:
    def __init__(self, ticker):
        self.ticker = ticker.upper()
        self.finnhub = FinnhubClient()
        self.fmp = FinancialModelingPrepClient()
        self.alphavantage = AlphaVantageClient()
        self.eodhd = EODHDClient()
        self.gemini = GeminiAPIClient()
        self.sec_edgar = SECEDGARClient()

        self.db_session = next(get_db_session())
        self.stock_db_entry = None
        self._financial_data_cache = {}  # For storing fetched data during the analysis of one stock

        try:
            self._get_or_create_stock_entry()
        except Exception as e:
            logger.error(f"CRITICAL: Failed during _get_or_create_stock_entry for {self.ticker}: {e}", exc_info=True)
            self._close_session_if_active()
            # Re-raise as a more specific error to indicate initialization failure
            raise RuntimeError(
                f"StockAnalyzer for {self.ticker} could not be initialized due to DB/API issues during stock entry setup.") from e

    def _close_session_if_active(self):
        if self.db_session and self.db_session.is_active:
            try:
                self.db_session.close();
                logger.debug(f"DB session closed for {self.ticker}.")
            except Exception as e_close:
                logger.warning(f"Error closing session for {self.ticker}: {e_close}")

    def _get_or_create_stock_entry(self):
        if not self.db_session.is_active:  # Ensure session is active
            logger.warning(f"Session for {self.ticker} inactive in _get_or_create. Re-establishing.")
            self._close_session_if_active();  # Close stale session if any
            self.db_session = next(get_db_session())  # Get a fresh one

        self.stock_db_entry = self.db_session.query(Stock).filter_by(ticker=self.ticker).first()
        company_name, industry, sector, cik = None, None, None, None

        # Try FMP first for profile
        profile_fmp_list = self.fmp.get_company_profile(self.ticker);
        time.sleep(1)  # API courtesy
        if profile_fmp_list and isinstance(profile_fmp_list, list) and len(profile_fmp_list) > 0 and profile_fmp_list[
            0]:
            data = profile_fmp_list[0];
            self._financial_data_cache['profile_fmp'] = data
            company_name = data.get('companyName')
            industry = data.get('industry')
            sector = data.get('sector')
            cik = data.get('cik')  # FMP often provides CIK
            logger.info(f"Fetched profile from FMP for {self.ticker}.")
        else:
            logger.warning(f"FMP profile failed for {self.ticker}. Trying Finnhub.")
            profile_fh = self.finnhub.get_company_profile2(self.ticker);
            time.sleep(1)
            if profile_fh:
                self._financial_data_cache['profile_finnhub'] = profile_fh
                company_name = profile_fh.get('name')
                industry = profile_fh.get('finnhubIndustry')  # Finnhub has its own industry classification
                # Finnhub profile2 doesn't usually have sector or CIK directly
                logger.info(f"Fetched profile from Finnhub for {self.ticker}.")
            else:
                logger.warning(f"Finnhub profile failed for {self.ticker}. Trying Alpha Vantage Overview.")
                overview_av = self.alphavantage.get_company_overview(self.ticker);
                time.sleep(2)  # AV is slower
                if overview_av and overview_av.get("Symbol") == self.ticker:  # Check if response is valid
                    self._financial_data_cache['overview_alphavantage'] = overview_av
                    company_name = overview_av.get('Name')
                    industry = overview_av.get('Industry')
                    sector = overview_av.get('Sector')
                    cik = overview_av.get('CIK')  # Alpha Vantage sometimes has CIK
                    logger.info(f"Fetched overview from Alpha Vantage for {self.ticker}.")
                else:
                    logger.warning(f"All primary profile fetches (FMP, Finnhub, AV) failed for {self.ticker}.")

        if not company_name: company_name = self.ticker  # Fallback company name
        if not cik and self.ticker:  # If CIK still not found from any profile source
            logger.info(f"CIK not found from profiles for {self.ticker}. Querying SEC EDGAR.")
            cik_from_edgar = self.sec_edgar.get_cik_by_ticker(self.ticker);
            time.sleep(0.5)
            if cik_from_edgar:
                cik = cik_from_edgar
                logger.info(f"Fetched CIK {cik} from SEC EDGAR for {self.ticker}.")
            else:
                logger.warning(f"Could not fetch CIK from SEC EDGAR for {self.ticker}.")

        if not self.stock_db_entry:
            logger.info(f"Stock {self.ticker} not found in DB, creating new entry.")
            self.stock_db_entry = Stock(ticker=self.ticker, company_name=company_name, industry=industry, sector=sector,
                                        cik=cik)
            self.db_session.add(self.stock_db_entry)
            try:
                self.db_session.commit();
                self.db_session.refresh(self.stock_db_entry)
            except SQLAlchemyError as e:
                self.db_session.rollback();
                logger.error(f"Error creating stock entry for {self.ticker}: {e}");
                raise
        else:  # Stock entry exists, update if necessary
            updated = False
            if company_name and self.stock_db_entry.company_name != company_name: self.stock_db_entry.company_name = company_name; updated = True
            if industry and self.stock_db_entry.industry != industry: self.stock_db_entry.industry = industry; updated = True
            if sector and self.stock_db_entry.sector != sector: self.stock_db_entry.sector = sector; updated = True
            if cik and self.stock_db_entry.cik != cik:  # Update CIK if new one found or was null
                self.stock_db_entry.cik = cik;
                updated = True
            elif not self.stock_db_entry.cik and cik:  # If CIK was null and now we have one
                self.stock_db_entry.cik = cik;
                updated = True

            if updated:
                try:
                    self.db_session.commit();
                    self.db_session.refresh(self.stock_db_entry)
                    logger.info(f"Updated stock entry for {self.ticker}.")
                except SQLAlchemyError as e:
                    self.db_session.rollback();
                    logger.error(f"Error updating stock entry for {self.ticker}: {e}")
        logger.info(
            f"Stock entry for {self.ticker} (ID: {self.stock_db_entry.id if self.stock_db_entry else 'N/A'}, CIK: {self.stock_db_entry.cik if self.stock_db_entry and self.stock_db_entry.cik else 'N/A'}) ready.")

    def _fetch_financial_statements(self):
        logger.info(f"Fetching financial statements for {self.ticker}...")
        statements_cache = {  # Initialize with empty structures
            "fmp_income_annual": [], "fmp_balance_annual": [], "fmp_cashflow_annual": [],
            "finnhub_financials_quarterly_reported": {"data": []},  # Finnhub wraps in 'data'
            "alphavantage_income_quarterly": {"quarterlyReports": []},  # AV wraps in 'quarterlyReports'
            "alphavantage_balance_quarterly": {"quarterlyReports": []},
            "alphavantage_cashflow_quarterly": {"quarterlyReports": []}
        }
        try:
            # FMP Annual Statements (typically 5-10 years for free, more for paid)
            statements_cache["fmp_income_annual"] = self.fmp.get_financial_statements(self.ticker, "income-statement",
                                                                                      "annual",
                                                                                      STOCK_FINANCIAL_YEARS) or []
            time.sleep(1.5)  # API courtesy
            statements_cache["fmp_balance_annual"] = self.fmp.get_financial_statements(self.ticker,
                                                                                       "balance-sheet-statement",
                                                                                       "annual",
                                                                                       STOCK_FINANCIAL_YEARS) or []
            time.sleep(1.5)
            statements_cache["fmp_cashflow_annual"] = self.fmp.get_financial_statements(self.ticker,
                                                                                        "cash-flow-statement", "annual",
                                                                                        STOCK_FINANCIAL_YEARS) or []
            time.sleep(1.5)
            logger.info(
                f"FMP Annuals for {self.ticker}: Income({len(statements_cache['fmp_income_annual'])} records), Balance({len(statements_cache['fmp_balance_annual'])} records), Cashflow({len(statements_cache['fmp_cashflow_annual'])} records).")

            # Finnhub Quarterly Reported Financials (as filed, can be extensive)
            fh_q_data = self.finnhub.get_financials_reported(self.ticker, freq="quarterly")  # Fetches multiple quarters
            time.sleep(1.5)
            if fh_q_data and isinstance(fh_q_data, dict) and fh_q_data.get("data"):
                statements_cache["finnhub_financials_quarterly_reported"] = fh_q_data
                logger.info(f"Fetched {len(fh_q_data['data'])} quarterly reports from Finnhub for {self.ticker}.")
            else:
                logger.warning(f"Finnhub quarterly financials reported data missing or malformed for {self.ticker}.")

            # Alpha Vantage Quarterly Statements (API can be slow, hence longer sleep)
            av_income_q = self.alphavantage.get_income_statement_quarterly(self.ticker)
            time.sleep(15)  # AlphaVantage free tier is slow and has tight limits
            if av_income_q and isinstance(av_income_q, dict) and av_income_q.get("quarterlyReports"):
                statements_cache["alphavantage_income_quarterly"] = av_income_q
                logger.info(
                    f"Fetched {len(av_income_q['quarterlyReports'])} quarterly income reports from Alpha Vantage for {self.ticker}.")
            else:
                logger.warning(f"Alpha Vantage quarterly income reports missing or malformed for {self.ticker}.")

            av_balance_q = self.alphavantage.get_balance_sheet_quarterly(self.ticker)
            time.sleep(15)
            if av_balance_q and isinstance(av_balance_q, dict) and av_balance_q.get("quarterlyReports"):
                statements_cache["alphavantage_balance_quarterly"] = av_balance_q
                logger.info(
                    f"Fetched {len(av_balance_q['quarterlyReports'])} quarterly balance reports from Alpha Vantage for {self.ticker}.")
            else:
                logger.warning(f"Alpha Vantage quarterly balance reports missing or malformed for {self.ticker}.")

            av_cashflow_q = self.alphavantage.get_cash_flow_quarterly(self.ticker)
            time.sleep(15)
            if av_cashflow_q and isinstance(av_cashflow_q, dict) and av_cashflow_q.get("quarterlyReports"):
                statements_cache["alphavantage_cashflow_quarterly"] = av_cashflow_q
                logger.info(
                    f"Fetched {len(av_cashflow_q['quarterlyReports'])} quarterly cash flow reports from Alpha Vantage for {self.ticker}.")
            else:
                logger.warning(f"Alpha Vantage quarterly cash flow reports missing or malformed for {self.ticker}.")

        except Exception as e:
            logger.warning(f"Error during financial statements fetch for {self.ticker}: {e}.", exc_info=True)
        self._financial_data_cache['financial_statements'] = statements_cache
        return statements_cache

    def _fetch_key_metrics_and_profile_data(self):
        logger.info(f"Fetching key metrics and profile for {self.ticker}.")
        # FMP Key Metrics (Annual & Quarterly) - Quarterly might fail on free tier
        key_metrics_annual_fmp = self.fmp.get_key_metrics(self.ticker, "annual",
                                                          STOCK_FINANCIAL_YEARS + 2)  # +2 for CAGR calculations
        time.sleep(1.5);
        self._financial_data_cache['key_metrics_annual_fmp'] = key_metrics_annual_fmp or []

        key_metrics_quarterly_fmp = self.fmp.get_key_metrics(self.ticker, "quarterly",
                                                             8)  # Fetch recent 8 quarters for TTM if possible
        time.sleep(1.5)
        if key_metrics_quarterly_fmp is None:  # API call itself failed or returned None (e.g. 403 error logged by API client)
            logger.warning(
                f"FMP quarterly key metrics API call failed or returned None for {self.ticker}. Data will be empty.")
            self._financial_data_cache['key_metrics_quarterly_fmp'] = []
        else:
            self._financial_data_cache['key_metrics_quarterly_fmp'] = key_metrics_quarterly_fmp or []

        # Finnhub Basic Financials (often has TTM metrics)
        basic_fin_fh = self.finnhub.get_basic_financials(self.ticker)  # Provides 'metric' dict
        time.sleep(1.5);
        self._financial_data_cache['basic_financials_finnhub'] = basic_fin_fh or {}

        # Ensure FMP profile is loaded if not already (e.g. if AV or Finnhub profile was primary)
        if 'profile_fmp' not in self._financial_data_cache or not self._financial_data_cache.get('profile_fmp'):
            profile_fmp_list = self.fmp.get_company_profile(self.ticker);
            time.sleep(1.5)
            self._financial_data_cache['profile_fmp'] = profile_fmp_list[0] if profile_fmp_list and isinstance(
                profile_fmp_list, list) and profile_fmp_list[0] else {}

        logger.info(
            f"FMP KM Annual for {self.ticker}: {len(self._financial_data_cache['key_metrics_annual_fmp'])} records. "
            f"FMP KM Quarterly for {self.ticker}: {len(self._financial_data_cache['key_metrics_quarterly_fmp'])} records. "
            f"Finnhub Basic Financials for {self.ticker}: {'OK' if self._financial_data_cache.get('basic_financials_finnhub', {}).get('metric') else 'Data missing or Metric key not found'}.")

    def _calculate_derived_metrics(self):
        logger.info(f"Calculating derived metrics for {self.ticker}...")
        metrics = {"key_metrics_snapshot": {}}  # For storing specific data points used in email

        # Retrieve cached data
        statements = self._financial_data_cache.get('financial_statements', {})
        income_annual = sorted(statements.get('fmp_income_annual', []), key=lambda x: x.get("date", ""), reverse=True)
        balance_annual = sorted(statements.get('fmp_balance_annual', []), key=lambda x: x.get("date", ""), reverse=True)
        cashflow_annual = sorted(statements.get('fmp_cashflow_annual', []), key=lambda x: x.get("date", ""),
                                 reverse=True)

        av_income_q_reports = statements.get('alphavantage_income_quarterly', {}).get('quarterlyReports', [])
        fh_q_reports_list = statements.get('finnhub_financials_quarterly_reported', {}).get('data', [])

        key_metrics_annual = self._financial_data_cache.get('key_metrics_annual_fmp', [])
        key_metrics_quarterly = self._financial_data_cache.get('key_metrics_quarterly_fmp',
                                                               [])  # Might be empty if FMP restricted

        basic_fin_fh_metric = self._financial_data_cache.get('basic_financials_finnhub', {}).get('metric',
                                                                                                 {})  # Finnhub's basic fin data
        profile_fmp = self._financial_data_cache.get('profile_fmp', {})

        # Get latest available data points from FMP key metrics (annual and quarterly TTM) and Finnhub
        latest_km_q = key_metrics_quarterly[0] if key_metrics_quarterly and isinstance(key_metrics_quarterly, list) and \
                                                  key_metrics_quarterly[0] else {}
        latest_km_a = key_metrics_annual[0] if key_metrics_annual and isinstance(key_metrics_annual, list) and \
                                               key_metrics_annual[0] else {}

        # Valuation Ratios (Prefer TTM from FMP quarterly, fallback to FMP annual, then Finnhub TTM)
        metrics["pe_ratio"] = safe_get_float(latest_km_q, "peRatioTTM") or safe_get_float(latest_km_a,
                                                                                          "peRatio") or safe_get_float(
            basic_fin_fh_metric, "peTTM")
        metrics["pb_ratio"] = safe_get_float(latest_km_q, "priceToBookRatioTTM") or safe_get_float(latest_km_a,
                                                                                                   "pbRatio") or safe_get_float(
            basic_fin_fh_metric, "pbAnnual")  # Finnhub often calls this pbAnnual
        metrics["ps_ratio"] = safe_get_float(latest_km_q, "priceToSalesRatioTTM") or safe_get_float(latest_km_a,
                                                                                                    "priceSalesRatio") or safe_get_float(
            basic_fin_fh_metric, "psTTM")

        metrics["ev_to_sales"] = safe_get_float(latest_km_q, "enterpriseValueOverRevenueTTM") or safe_get_float(
            latest_km_a, "enterpriseValueOverRevenue")
        if metrics["ev_to_sales"] is None: logger.debug(
            f"{self.ticker}: EV/Sales is None. FMP Q TTM: {latest_km_q.get('enterpriseValueOverRevenueTTM')}, FMP A: {latest_km_a.get('enterpriseValueOverRevenue')}")

        metrics["ev_to_ebitda"] = safe_get_float(latest_km_q, "evToEbitdaTTM") or safe_get_float(latest_km_a,
                                                                                                 "evToEbitda")
        if metrics["ev_to_ebitda"] is None: logger.debug(
            f"{self.ticker}: EV/EBITDA is None. FMP Q TTM: {latest_km_q.get('evToEbitdaTTM')}, FMP A: {latest_km_a.get('evToEbitda')}")

        # Dividend Yield (FMP TTM, FMP Annual, Finnhub Annual - note Finnhub gives % not decimal)
        div_yield_fmp_q = safe_get_float(latest_km_q, "dividendYieldTTM")
        div_yield_fmp_a = safe_get_float(latest_km_a, "dividendYield")
        div_yield_fh = safe_get_float(basic_fin_fh_metric,
                                      "dividendYieldAnnual")  # This is a percentage value e.g. 1.5 for 1.5%
        if div_yield_fh is not None: div_yield_fh /= 100.0  # Convert to decimal
        metrics["dividend_yield"] = div_yield_fmp_q if div_yield_fmp_q is not None else (
            div_yield_fmp_a if div_yield_fmp_a is not None else div_yield_fh)

        # Profitability & EPS from FMP Annual Income Statements (most recent year)
        if income_annual:
            latest_ia = income_annual[0]
            metrics["eps"] = safe_get_float(latest_ia, "eps") or safe_get_float(latest_km_a,
                                                                                "eps")  # Fallback to KM eps
            metrics["net_profit_margin"] = safe_get_float(latest_ia, "netProfitMargin")
            if metrics["net_profit_margin"] is None: logger.debug(
                f"{self.ticker}: Net Profit Margin from FMP annual income ('{latest_ia.get('date')}') is None. Raw value: {latest_ia.get('netProfitMargin')}")
            metrics["gross_profit_margin"] = safe_get_float(latest_ia, "grossProfitMargin")
            if metrics["gross_profit_margin"] is None: logger.debug(
                f"{self.ticker}: Gross Profit Margin from FMP annual income ('{latest_ia.get('date')}') is None. Raw value: {latest_ia.get('grossProfitMargin')}")
            metrics["operating_profit_margin"] = safe_get_float(latest_ia,
                                                                "operatingIncomeRatio")  # FMP calls it operatingIncomeRatio

            ebit = safe_get_float(latest_ia, "operatingIncome")  # Can also use 'ebitda' - 'depreciationAndAmortization'
            interest_expense = safe_get_float(latest_ia, "interestExpense")
            if ebit is not None and interest_expense is not None and abs(
                    interest_expense) > 1e-6:  # Avoid division by zero/small num
                metrics["interest_coverage_ratio"] = ebit / abs(interest_expense)
            else:
                logger.debug(
                    f"{self.ticker}: Cannot calculate Interest Coverage. EBIT: {ebit}, Interest Expense: {interest_expense}")

        # Financial Health from FMP Annual Balance Sheets & Key Metrics
        if balance_annual:
            latest_ba = balance_annual[0]
            total_equity = safe_get_float(latest_ba, "totalStockholdersEquity")
            total_assets = safe_get_float(latest_ba, "totalAssets")
            latest_net_income = get_value_from_statement_list(income_annual, "netIncome", 0, latest_ba.get('date'))

            if total_equity and total_equity != 0 and latest_net_income is not None: metrics[
                "roe"] = latest_net_income / total_equity
            if total_assets and total_assets != 0 and latest_net_income is not None: metrics[
                "roa"] = latest_net_income / total_assets

            metrics["debt_to_equity"] = safe_get_float(latest_km_a, "debtToEquity")  # Prefer KM D/E
            if metrics["debt_to_equity"] is None:  # Calculate if not in KM
                total_debt_ba = safe_get_float(latest_ba, "totalDebt")
                if total_debt_ba is not None and total_equity and total_equity != 0:
                    metrics["debt_to_equity"] = total_debt_ba / total_equity

            current_assets = safe_get_float(latest_ba, "totalCurrentAssets")
            current_liabilities = safe_get_float(latest_ba, "totalCurrentLiabilities")
            if current_assets is not None and current_liabilities is not None and current_liabilities != 0:
                metrics["current_ratio"] = current_assets / current_liabilities

            cash_equivalents = safe_get_float(latest_ba, "cashAndCashEquivalents", 0)
            short_term_investments = safe_get_float(latest_ba, "shortTermInvestments", 0)
            net_receivables = safe_get_float(latest_ba, "netReceivables", 0)
            if current_liabilities is not None and current_liabilities != 0:
                metrics["quick_ratio"] = (
                                                     cash_equivalents + short_term_investments + net_receivables) / current_liabilities

        # Debt to EBITDA (from FMP annual KM or income statement + balance sheet)
        latest_annual_ebitda = safe_get_float(latest_km_a, "ebitda") or get_value_from_statement_list(income_annual,
                                                                                                      "ebitda", 0)
        if latest_annual_ebitda and latest_annual_ebitda != 0 and balance_annual:
            total_debt_val = get_value_from_statement_list(balance_annual, "totalDebt", 0)
            if total_debt_val is not None: metrics["debt_to_ebitda"] = total_debt_val / latest_annual_ebitda

        # Growth Rates (YoY, CAGR from FMP Annual Income Statements)
        metrics["revenue_growth_yoy"] = calculate_growth(get_value_from_statement_list(income_annual, "revenue", 0),
                                                         get_value_from_statement_list(income_annual, "revenue", 1))
        metrics["eps_growth_yoy"] = calculate_growth(get_value_from_statement_list(income_annual, "eps", 0),
                                                     get_value_from_statement_list(income_annual, "eps", 1))

        if len(income_annual) >= 3:  # Need 3 years of data for 2-year span for 3yr CAGR (end/start^(1/2)-1)
            metrics["revenue_growth_cagr_3yr"] = calculate_cagr(
                get_value_from_statement_list(income_annual, "revenue", 0),
                get_value_from_statement_list(income_annual, "revenue", 2), 2)
            metrics["eps_growth_cagr_3yr"] = calculate_cagr(get_value_from_statement_list(income_annual, "eps", 0),
                                                            get_value_from_statement_list(income_annual, "eps", 2), 2)
        if len(income_annual) >= 5:  # Need 5 years of data for 4-year span
            metrics["revenue_growth_cagr_5yr"] = calculate_cagr(
                get_value_from_statement_list(income_annual, "revenue", 0),
                get_value_from_statement_list(income_annual, "revenue", 4), 4)
            metrics["eps_growth_cagr_5yr"] = calculate_cagr(get_value_from_statement_list(income_annual, "eps", 0),
                                                            get_value_from_statement_list(income_annual, "eps", 4), 4)

        # Quarterly Revenue Growth (QoQ) - Prefer Alpha Vantage, fallback to Finnhub
        latest_q_revenue_av = get_alphavantage_value(av_income_q_reports, "totalRevenue", 0)  # Newest quarter
        previous_q_revenue_av = get_alphavantage_value(av_income_q_reports, "totalRevenue", 1)  # Quarter before newest
        if latest_q_revenue_av is not None and previous_q_revenue_av is not None:
            metrics["revenue_growth_qoq"] = calculate_growth(latest_q_revenue_av, previous_q_revenue_av)
            metrics["key_metrics_snapshot"]["q_revenue_source"], metrics["key_metrics_snapshot"][
                "latest_q_revenue"] = "AlphaVantage", latest_q_revenue_av
        else:  # Fallback to Finnhub if AV data is missing
            revenue_concepts_fh = ["Revenues", "RevenueFromContractWithCustomerExcludingAssessedTax", "TotalRevenues",
                                   "NetSales"]
            latest_q_revenue_fh = get_finnhub_concept_value(fh_q_reports_list, 'ic', revenue_concepts_fh,
                                                            0)  # Newest quarter
            previous_q_revenue_fh = get_finnhub_concept_value(fh_q_reports_list, 'ic', revenue_concepts_fh,
                                                              1)  # Quarter before
            if latest_q_revenue_fh is not None and previous_q_revenue_fh is not None:
                metrics["revenue_growth_qoq"] = calculate_growth(latest_q_revenue_fh, previous_q_revenue_fh)
                metrics["key_metrics_snapshot"]["q_revenue_source"], metrics["key_metrics_snapshot"][
                    "latest_q_revenue"] = "Finnhub", latest_q_revenue_fh
            else:
                logger.info(f"Could not calculate QoQ revenue for {self.ticker} from AlphaVantage or Finnhub.");
                metrics["revenue_growth_qoq"] = None

        # Free Cash Flow Metrics
        if cashflow_annual:
            fcf_latest_annual = get_value_from_statement_list(cashflow_annual, "freeCashFlow", 0)
            # Shares outstanding from FMP profile
            shares_outstanding = safe_get_float(profile_fmp, "sharesOutstanding") or \
                                 (safe_get_float(profile_fmp, "mktCap") / safe_get_float(profile_fmp, "price")
                                  if safe_get_float(profile_fmp, "price") and safe_get_float(profile_fmp,
                                                                                             "price") != 0 else None)

            if fcf_latest_annual is not None and shares_outstanding and shares_outstanding != 0:
                metrics["free_cash_flow_per_share"] = fcf_latest_annual / shares_outstanding
                market_cap_for_yield = safe_get_float(profile_fmp, "mktCap")
                if market_cap_for_yield and market_cap_for_yield != 0:
                    metrics["free_cash_flow_yield"] = fcf_latest_annual / market_cap_for_yield

            # FCF Trend (simple check on last 3 years of annual FCF)
            if len(cashflow_annual) >= 3:
                fcf0 = get_value_from_statement_list(cashflow_annual, "freeCashFlow", 0)
                fcf1 = get_value_from_statement_list(cashflow_annual, "freeCashFlow", 1)
                fcf2 = get_value_from_statement_list(cashflow_annual, "freeCashFlow", 2)
                # Ensure all are numbers before comparison
                if all(isinstance(x, (int, float)) for x in [fcf0, fcf1, fcf2] if x is not None):
                    if fcf0 is not None and fcf1 is not None and fcf2 is not None:
                        if fcf0 > fcf1 > fcf2:
                            metrics["free_cash_flow_trend"] = "Growing"
                        elif fcf0 < fcf1 < fcf2:
                            metrics["free_cash_flow_trend"] = "Declining"
                        else:
                            metrics["free_cash_flow_trend"] = "Mixed/Stable"
                    else:
                        metrics["free_cash_flow_trend"] = "Data Incomplete"  # Some FCF values are None
                else:
                    metrics["free_cash_flow_trend"] = "Non-Numeric Data"
            else:
                metrics["free_cash_flow_trend"] = "Data N/A (Less than 3 years)"

        # Retained Earnings Trend (simple check on last 3 years)
        if len(balance_annual) >= 3:
            re0 = get_value_from_statement_list(balance_annual, "retainedEarnings", 0)
            re1 = get_value_from_statement_list(balance_annual, "retainedEarnings", 1)
            re2 = get_value_from_statement_list(balance_annual, "retainedEarnings", 2)
            if all(isinstance(x, (int, float)) for x in [re0, re1, re2] if x is not None):
                if re0 is not None and re1 is not None and re2 is not None:
                    if re0 > re1 > re2:
                        metrics["retained_earnings_trend"] = "Growing"
                    elif re0 < re1 < re2:
                        metrics["retained_earnings_trend"] = "Declining"
                    else:
                        metrics["retained_earnings_trend"] = "Mixed/Stable"
                else:
                    metrics["retained_earnings_trend"] = "Data Incomplete"
            else:
                metrics["retained_earnings_trend"] = "Non-Numeric Data"
        else:
            metrics["retained_earnings_trend"] = "Data N/A (Less than 3 years)"

        # ROIC (Return on Invested Capital)
        if income_annual and balance_annual:
            # NOPAT = EBIT * (1 - Tax Rate)
            ebit_roic = get_value_from_statement_list(income_annual, "operatingIncome", 0)  # EBIT
            income_tax_expense_roic = get_value_from_statement_list(income_annual, "incomeTaxExpense", 0)
            income_before_tax_roic = get_value_from_statement_list(income_annual, "incomeBeforeTax", 0)

            effective_tax_rate = 0.21  # Default tax rate if cannot calculate
            if income_tax_expense_roic is not None and income_before_tax_roic is not None and income_before_tax_roic != 0:
                effective_tax_rate = income_tax_expense_roic / income_before_tax_roic

            nopat = None
            if ebit_roic is not None:
                nopat = ebit_roic * (1 - effective_tax_rate)

            # Invested Capital = Total Debt + Total Equity - Cash & Cash Equivalents
            total_debt_roic = get_value_from_statement_list(balance_annual, "totalDebt", 0)
            total_equity_roic = get_value_from_statement_list(balance_annual, "totalStockholdersEquity", 0)
            cash_equivalents_roic = get_value_from_statement_list(balance_annual, "cashAndCashEquivalents",
                                                                  0) or 0  # Default to 0 if None

            if total_debt_roic is not None and total_equity_roic is not None:
                invested_capital = total_debt_roic + total_equity_roic - cash_equivalents_roic
                if nopat is not None and invested_capital is not None and invested_capital != 0:
                    metrics["roic"] = nopat / invested_capital

        # Final cleanup: replace NaN/inf with None for all metrics
        final_metrics = {}
        for k, v in metrics.items():
            if k == "key_metrics_snapshot":  # Keep snapshot dict as is, assuming its values are clean
                final_metrics[k] = {sk: sv for sk, sv in v.items() if sv is not None and not (
                            isinstance(sv, float) and (math.isnan(sv) or math.isinf(sv)))}
            elif isinstance(v, float):
                final_metrics[k] = v if not (math.isnan(v) or math.isinf(v)) else None
            elif v is not None:  # Handles strings, bools, etc.
                final_metrics[k] = v
            else:  # Value is already None
                final_metrics[k] = None

        log_metrics = {k: v for k, v in final_metrics.items() if
                       k != "key_metrics_snapshot"}  # Exclude snapshot from this log line
        logger.info(f"Calculated metrics for {self.ticker}: {log_metrics}")
        self._financial_data_cache['calculated_metrics'] = final_metrics
        return final_metrics

    def _perform_dcf_analysis(self):
        logger.info(f"Performing simplified DCF analysis for {self.ticker}...")
        dcf_results = {
            "dcf_intrinsic_value": None, "dcf_upside_percentage": None,
            "dcf_assumptions": {
                "discount_rate": DEFAULT_DISCOUNT_RATE,
                "perpetual_growth_rate": DEFAULT_PERPETUAL_GROWTH_RATE,
                "projection_years": DEFAULT_FCF_PROJECTION_YEARS,
                "start_fcf": None,
                "fcf_growth_rates_projection": []  # Store the actual growth rates used
            }
        }

        cashflow_annual = sorted(
            self._financial_data_cache.get('financial_statements', {}).get('fmp_cashflow_annual', []),
            key=lambda x: x.get("date", ""), reverse=True)
        profile_fmp = self._financial_data_cache.get('profile_fmp', {})
        calculated_metrics = self._financial_data_cache.get('calculated_metrics', {})  # For FCF growth rate proxy

        current_price = safe_get_float(profile_fmp, "price")
        shares_outstanding = safe_get_float(profile_fmp, "sharesOutstanding") or \
                             (safe_get_float(profile_fmp, "mktCap") / current_price
                              if current_price and current_price != 0 else None)

        if not cashflow_annual or not profile_fmp or current_price is None or shares_outstanding is None or shares_outstanding == 0:
            logger.warning(
                f"Insufficient data for DCF for {self.ticker} (FCF statements, profile, price, or shares missing/zero).");
            return dcf_results

        current_fcf = get_value_from_statement_list(cashflow_annual, "freeCashFlow", 0)
        if current_fcf is None or current_fcf <= 10000:  # Need positive FCF, arbitrary small threshold
            logger.warning(f"Current FCF for {self.ticker} is {current_fcf}. DCF requires substantial positive FCF.");
            return dcf_results
        dcf_results["dcf_assumptions"]["start_fcf"] = current_fcf

        # Estimate initial FCF growth rate (prefer historical FCF CAGR, then revenue CAGR, then default)
        fcf_growth_rate_3yr_cagr = None
        if len(cashflow_annual) >= 4:  # Need 4 years data for 3-year CAGR over 3 periods
            fcf_start_for_cagr = get_value_from_statement_list(cashflow_annual, "freeCashFlow", 3)  # FCF 3 years ago
            if fcf_start_for_cagr and fcf_start_for_cagr > 0:  # Avoid issues with negative/zero start
                fcf_growth_rate_3yr_cagr = calculate_cagr(current_fcf, fcf_start_for_cagr, 3)

        initial_fcf_growth_rate = fcf_growth_rate_3yr_cagr if fcf_growth_rate_3yr_cagr is not None else \
            calculated_metrics.get("revenue_growth_cagr_3yr") if calculated_metrics.get(
                "revenue_growth_cagr_3yr") is not None else \
                calculated_metrics.get("revenue_growth_yoy") if calculated_metrics.get(
                    "revenue_growth_yoy") is not None else 0.05  # Default 5%

        if not isinstance(initial_fcf_growth_rate, (int, float)): initial_fcf_growth_rate = 0.05  # Ensure numeric
        initial_fcf_growth_rate = min(max(initial_fcf_growth_rate, -0.10), 0.20)  # Cap growth rate bounds for stability

        projected_fcfs = []
        last_projected_fcf = current_fcf
        # Linearly decline growth rate from initial to perpetual over projection_years
        growth_rate_decline_per_year = (initial_fcf_growth_rate - DEFAULT_PERPETUAL_GROWTH_RATE) / float(
            DEFAULT_FCF_PROJECTION_YEARS) \
            if DEFAULT_FCF_PROJECTION_YEARS > 0 else 0

        for i in range(DEFAULT_FCF_PROJECTION_YEARS):
            current_year_growth_rate = max(initial_fcf_growth_rate - (growth_rate_decline_per_year * i),
                                           DEFAULT_PERPETUAL_GROWTH_RATE)
            projected_fcf = last_projected_fcf * (1 + current_year_growth_rate);
            projected_fcfs.append(projected_fcf);
            last_projected_fcf = projected_fcf
            dcf_results["dcf_assumptions"]["fcf_growth_rates_projection"].append(round(current_year_growth_rate, 4))

        if not projected_fcfs: logger.error(f"DCF: No projected FCFs generated for {self.ticker}."); return dcf_results

        # Terminal Value Calculation (Gordon Growth Model)
        terminal_year_fcf = projected_fcfs[-1] * (1 + DEFAULT_PERPETUAL_GROWTH_RATE)
        terminal_value_denominator = DEFAULT_DISCOUNT_RATE - DEFAULT_PERPETUAL_GROWTH_RATE
        terminal_value = terminal_year_fcf / terminal_value_denominator if terminal_value_denominator > 1e-6 else 0  # Avoid div by zero
        if terminal_value_denominator <= 1e-6: logger.warning(
            f"DCF for {self.ticker}: Discount rate too close or below perpetual growth rate. Terminal Value may be unreliable.")

        # Discount projected FCFs and Terminal Value
        sum_discounted_fcf = sum(fcf / ((1 + DEFAULT_DISCOUNT_RATE) ** (i + 1)) for i, fcf in enumerate(projected_fcfs))
        discounted_terminal_value = terminal_value / ((1 + DEFAULT_DISCOUNT_RATE) ** DEFAULT_FCF_PROJECTION_YEARS)

        intrinsic_equity_value = sum_discounted_fcf + discounted_terminal_value

        if shares_outstanding != 0:
            intrinsic_value_per_share = intrinsic_equity_value / shares_outstanding;
            dcf_results["dcf_intrinsic_value"] = intrinsic_value_per_share
            if current_price and current_price != 0:
                dcf_results["dcf_upside_percentage"] = (intrinsic_value_per_share - current_price) / current_price

        logger.info(
            f"DCF for {self.ticker}: Intrinsic Value/Share: {dcf_results.get('dcf_intrinsic_value', 'N/A')}, "
            f"Upside: {dcf_results.get('dcf_upside_percentage', 'N/A') * 100 if dcf_results.get('dcf_upside_percentage') is not None else 'N/A'}%")
        self._financial_data_cache['dcf_results'] = dcf_results
        return dcf_results

    def _fetch_and_summarize_10k(self):
        logger.info(f"Fetching and attempting to summarize latest 10-K for {self.ticker}")
        summary_results = {"qualitative_sources_summary": {}}  # To store metadata about sources
        if not self.stock_db_entry or not self.stock_db_entry.cik:
            logger.warning(f"No CIK for {self.ticker}. Cannot fetch 10-K.");
            return summary_results

        # Try 10-K first, then 10-K/A (amendment)
        filing_url = self.sec_edgar.get_filing_document_url(self.stock_db_entry.cik, "10-K");
        time.sleep(0.5)
        if not filing_url:
            logger.info(f"No 10-K found for {self.ticker}, trying 10-K/A.")
            filing_url = self.sec_edgar.get_filing_document_url(self.stock_db_entry.cik, "10-K/A");
            time.sleep(0.5)

        if not filing_url:
            logger.warning(f"No 10-K or 10-K/A URL found for {self.ticker} (CIK: {self.stock_db_entry.cik})");
            return summary_results
        summary_results["qualitative_sources_summary"]["10k_filing_url_used"] = filing_url

        text_content = self.sec_edgar.get_filing_text(filing_url)  # Fetches from cache or SEC
        if not text_content:
            logger.warning(f"Failed to fetch/load 10-K text from {filing_url}");
            return summary_results
        logger.info(f"Fetched 10-K text (length: {len(text_content)}) for {self.ticker}. Extracting sections.")

        sections = extract_S1_text_sections(text_content, TEN_K_KEY_SECTIONS)  # Re-use S1 extractor logic
        company_name_for_prompt = self.stock_db_entry.company_name or self.ticker

        def summarize_section_with_gemini(section_text, section_name_for_prompt, company_name_ticker_prompt,
                                          max_len_gemini):
            if not section_text: return None, 0

            # Tailor context for Gemini based on section
            context_for_gemini = f"The following is the '{section_name_for_prompt}' section from the 10-K filing for {company_name_ticker_prompt}. "
            if section_name_for_prompt.lower() == "business":
                context_for_gemini += "Please summarize the company's core business operations, products/services, revenue generation model, and primary markets."
            elif section_name_for_prompt.lower() == "risk factors":
                context_for_gemini += "Identify and summarize the 3-5 most significant risk factors disclosed. Be concise."
            elif section_name_for_prompt.lower() == "management's discussion and analysis" or section_name_for_prompt.lower() == "mda":
                context_for_gemini += "Summarize key insights into financial performance drivers, financial condition, liquidity, and management's outlook or focus areas."
            else:  # Generic summary instruction
                context_for_gemini += "Please provide a concise factual summary of this section."

            summary = self.gemini.summarize_text_with_context(section_text, context_for_gemini, max_len_gemini)
            time.sleep(3)  # API courtesy
            return (summary if summary and not summary.startswith(
                "Error:") else f"AI summary error or no content for {section_name_for_prompt}."), len(section_text)

        summary_results["business_summary"], summary_results["qualitative_sources_summary"][
            "business_10k_source_length"] = \
            summarize_section_with_gemini(sections.get("business"), "Business",
                                          f"{company_name_for_prompt} ({self.ticker})",
                                          MAX_10K_SECTION_LENGTH_FOR_GEMINI)

        summary_results["risk_factors_summary"], summary_results["qualitative_sources_summary"][
            "risk_factors_10k_source_length"] = \
            summarize_section_with_gemini(sections.get("risk_factors"), "Risk Factors",
                                          f"{company_name_for_prompt} ({self.ticker})",
                                          MAX_10K_SECTION_LENGTH_FOR_GEMINI)

        summary_results["management_assessment_summary"], summary_results["qualitative_sources_summary"][
            "mda_10k_source_length"] = \
            summarize_section_with_gemini(sections.get("mda"), "Management's Discussion and Analysis",
                                          f"{company_name_for_prompt} ({self.ticker})",
                                          MAX_10K_SECTION_LENGTH_FOR_GEMINI)

        # Derived qualitative summaries using Gemini (e.g., competitive landscape, economic moat)
        # Ensure base summaries are strings before concatenation
        biz_summary_str = summary_results.get("business_summary", "") or ""
        mda_summary_str = summary_results.get("management_assessment_summary", "") or ""
        risk_summary_str = summary_results.get("risk_factors_summary", "") or ""

        # Competitive Landscape
        comp_input_text = (biz_summary_str + "\n" + mda_summary_str)[:MAX_GEMINI_TEXT_LENGTH].strip()
        if comp_input_text:
            comp_prompt = (
                f"Based on the business description and MD&A for {company_name_for_prompt} ({self.ticker}):\n\"\"\"\n{comp_input_text}\n\"\"\"\n"
                f"Describe the company's competitive landscape, identify its key competitors (if mentioned or inferable), and discuss its market positioning relative to them. Focus on factual statements from the provided text or reasonable inferences based on it.")
            comp_summary = self.gemini.generate_text(comp_prompt);
            time.sleep(3)
            if comp_summary and not comp_summary.startswith("Error:"): summary_results[
                "competitive_landscape_summary"] = comp_summary

        # Economic Moat
        comp_summary_str = summary_results.get("competitive_landscape_summary", "") or ""
        moat_input_text = (biz_summary_str + "\n" + comp_summary_str + "\n" + risk_summary_str)[
                          :MAX_GEMINI_TEXT_LENGTH].strip()
        if moat_input_text:  # and not summary_results.get("economic_moat_summary"): # Can generate even if some other source provides it, for 10-K perspective
            moat_prompt = (
                f"Analyze the primary economic moats (e.g., brand, network effects, switching costs, intangible assets, cost advantages) for {company_name_for_prompt} ({self.ticker}), "
                f"based on the following information:\n\"\"\"\n{moat_input_text}\n\"\"\"\nProvide a concise summary of its key moats and their strength.")
            moat_summary = self.gemini.generate_text(moat_prompt);
            time.sleep(3)
            if moat_summary and not moat_summary.startswith("Error:"): summary_results[
                "economic_moat_summary"] = moat_summary

        # Industry Trends
        industry_context_text = (biz_summary_str + "\nRelevant Industry: " + (
                    self.stock_db_entry.industry or "Not Specified") + "\nRelevant Sector: " + (
                                             self.stock_db_entry.sector or "Not Specified"))[
                                :MAX_GEMINI_TEXT_LENGTH].strip()
        if industry_context_text:  # and not summary_results.get("industry_trends_summary"):
            industry_prompt = (
                f"For {company_name_for_prompt} ({self.ticker}), operating in the '{self.stock_db_entry.industry}' industry, "
                f"consider the following context:\n\"\"\"\n{industry_context_text}\n\"\"\"\n"
                f"Analyze key trends, opportunities, and challenges relevant to this industry. How does the company appear to be positioned in relation to these trends?")
            industry_summary = self.gemini.generate_text(industry_prompt);
            time.sleep(3)
            if industry_summary and not industry_summary.startswith("Error:"): summary_results[
                "industry_trends_summary"] = industry_summary

        logger.info(f"10-K qualitative summaries generated for {self.ticker}.")
        self._financial_data_cache['10k_summaries'] = summary_results
        return summary_results

    def _parse_ai_investment_thesis_response(self, ai_response_text):
        # Default values
        parsed_data = {
            "investment_thesis_full": "AI response not fully processed or 'Investment Thesis:' section missing.",
            "investment_decision": "Review AI Output",  # Default if not parsed
            "strategy_type": "Not Specified by AI",
            "confidence_level": "Not Specified by AI",
            "reasoning": "AI response not fully processed or 'Key Reasoning Points:' section missing."
        }

        if not ai_response_text or ai_response_text.startswith("Error:"):
            error_message = ai_response_text if ai_response_text else "Error: Empty response from AI."
            parsed_data["investment_thesis_full"] = error_message
            parsed_data["reasoning"] = error_message
            parsed_data["investment_decision"] = "AI Error"
            parsed_data["strategy_type"] = "AI Error"
            parsed_data["confidence_level"] = "AI Error"
            return parsed_data

        text_content = ai_response_text.replace('\r\n', '\n').strip()

        # Define regex patterns for each section header
        # Using re.IGNORECASE | re.MULTILINE | re.DOTALL
        # The pattern captures everything after the header until the next known header or end of string.
        section_patterns = {
            "investment_thesis_full": re.compile(
                r"^\s*Investment Thesis:\s*\n?(.*?)(?=\n\s*(?:Investment Decision:|Strategy Type:|Confidence Level:|Key Reasoning Points:)|^\s*$|\Z)",
                re.I | re.M | re.S),
            "investment_decision": re.compile(
                r"^\s*Investment Decision:\s*\n?(.*?)(?=\n\s*(?:Investment Thesis:|Strategy Type:|Confidence Level:|Key Reasoning Points:)|^\s*$|\Z)",
                re.I | re.M | re.S),
            "strategy_type": re.compile(
                r"^\s*Strategy Type:\s*\n?(.*?)(?=\n\s*(?:Investment Thesis:|Investment Decision:|Confidence Level:|Key Reasoning Points:)|^\s*$|\Z)",
                re.I | re.M | re.S),
            "confidence_level": re.compile(
                r"^\s*Confidence Level:\s*\n?(.*?)(?=\n\s*(?:Investment Thesis:|Investment Decision:|Strategy Type:|Key Reasoning Points:)|^\s*$|\Z)",
                re.I | re.M | re.S),
            "reasoning": re.compile(
                r"^\s*Key Reasoning Points:\s*\n?(.*?)(?=\n\s*(?:Investment Thesis:|Investment Decision:|Strategy Type:|Confidence Level:)|^\s*$|\Z)",
                re.I | re.M | re.S)
        }

        found_any_section = False
        for key, pattern in section_patterns.items():
            match = pattern.search(text_content)
            if match:
                content = match.group(1).strip()
                if content:
                    # For single-line expected fields, often the first line of the match is enough
                    if key in ["investment_decision", "strategy_type", "confidence_level"]:
                        parsed_data[key] = content.split('\n')[0].strip()  # Take first line
                    else:
                        parsed_data[key] = content
                    found_any_section = True
                else:
                    logger.debug(f"Section '{key}' found but content was empty for {self.ticker}.")
            else:
                logger.debug(f"Section header for '{key}' not found via regex for {self.ticker}.")

        # If no sections were parsed by regex (e.g., AI didn't follow headings strictly)
        # and the response isn't an error, put the whole thing in the thesis for review.
        if not found_any_section and not ai_response_text.startswith("Error:"):
            logger.warning(
                f"Could not parse distinct sections from AI response for {self.ticker}. Full response in thesis.")
            parsed_data["investment_thesis_full"] = text_content
            # Decision, strategy, etc., will remain "Review AI Output" or "Not Specified"

        # A final check: if investment_decision is still "Review AI Output" but thesis contains a clear "Investment Decision:" line, try to grab it.
        # This is a fallback for the regex not catching it perfectly.
        if parsed_data["investment_decision"] == "Review AI Output" and "investment decision:" in text_content.lower():
            try:
                lines = text_content.split('\n')
                for i, line in enumerate(lines):
                    if "investment decision:" in line.lower():
                        decision_val = line.split(":", 1)[1].strip()
                        if decision_val: parsed_data["investment_decision"] = decision_val; break
            except Exception:
                pass  # Stick to "Review AI Output"

        return parsed_data

    def _determine_investment_thesis(self):
        logger.info(f"Synthesizing investment thesis for {self.ticker}...")
        metrics = self._financial_data_cache.get('calculated_metrics', {})
        qual_summaries = self._financial_data_cache.get('10k_summaries', {})
        dcf_results = self._financial_data_cache.get('dcf_results', {})
        profile = self._financial_data_cache.get('profile_fmp', {})  # For current price

        company_name = self.stock_db_entry.company_name or self.ticker
        industry = self.stock_db_entry.industry or "N/A"
        sector = self.stock_db_entry.sector or "N/A"

        prompt = f"Company: {company_name} ({self.ticker})\nIndustry: {industry}, Sector: {sector}\n\n"
        prompt += "Key Financial Metrics & Data:\n"
        # Select key metrics for the prompt
        metrics_for_prompt = {
            "P/E Ratio": metrics.get("pe_ratio"), "P/B Ratio": metrics.get("pb_ratio"),
            "P/S Ratio": metrics.get("ps_ratio"), "Dividend Yield": metrics.get("dividend_yield"),
            "ROE (Return on Equity)": metrics.get("roe"), "ROIC (Return on Invested Capital)": metrics.get("roic"),
            "Debt-to-Equity": metrics.get("debt_to_equity"),
            "Revenue Growth YoY": metrics.get("revenue_growth_yoy"),
            "Revenue Growth QoQ": metrics.get("revenue_growth_qoq"),
            "EPS Growth YoY": metrics.get("eps_growth_yoy"),
            "Net Profit Margin": metrics.get("net_profit_margin"),
            "Free Cash Flow Yield": metrics.get("free_cash_flow_yield"),
            "Free Cash Flow Trend": metrics.get("free_cash_flow_trend"),
            "Retained Earnings Trend": metrics.get("retained_earnings_trend"),
        }
        for name, val in metrics_for_prompt.items():
            if val is not None:
                if isinstance(val, float) and (
                        name.endswith("Yield") or "Growth" in name or "Margin" in name or name in [
                    "ROE (Return on Equity)", "ROIC (Return on Invested Capital)"]):
                    val_str = f"{val:.2%}"
                elif isinstance(val, float):
                    val_str = f"{val:.2f}"
                else:
                    val_str = str(val)
                prompt += f"- {name}: {val_str}\n"

        dcf_intrinsic_value = dcf_results.get("dcf_intrinsic_value")
        dcf_upside = dcf_results.get("dcf_upside_percentage")
        current_stock_price = profile.get("price")
        if current_stock_price is not None: prompt += f"- Current Stock Price: {current_stock_price:.2f}\n"
        if dcf_intrinsic_value is not None: prompt += f"- DCF Intrinsic Value per Share: {dcf_intrinsic_value:.2f}\n"
        if dcf_upside is not None: prompt += f"- DCF Upside/Downside: {dcf_upside:.2%}\n"
        prompt += "\n"

        prompt += "Qualitative Summaries (from 10-K & AI analysis):\n"
        qual_for_prompt = {
            "Business Model & Operations": qual_summaries.get("business_summary"),
            "Economic Moat": qual_summaries.get("economic_moat_summary"),
            "Industry Trends & Outlook": qual_summaries.get("industry_trends_summary"),
            "Competitive Landscape": qual_summaries.get("competitive_landscape_summary"),
            "Management's Discussion (MD&A Highlights)": qual_summaries.get("management_assessment_summary"),
            "Key Risk Factors": qual_summaries.get("risk_factors_summary"),
        }
        for name, text_val in qual_for_prompt.items():
            if text_val and isinstance(text_val, str):
                prompt += f"- {name}: {text_val[:250].replace('...', '').strip()}...\n"  # Truncate for prompt
        prompt += "\n"

        prompt += (
            "Instructions for AI: Based on all the above information, provide a detailed financial analysis. "
            "Structure your response *exactly* as follows, using these specific headings on separate lines:\n\n"
            "Investment Thesis:\n"
            "[Provide a comprehensive investment thesis (2-4 paragraphs) synthesizing all data, discussing positives and negatives, and the overall outlook.]\n\n"
            "Investment Decision:\n"
            "[State one of the following: Strong Buy, Buy, Hold, Monitor, Reduce, Sell, Avoid. Base this on the overall analysis.]\n\n"
            "Strategy Type:\n"
            "[Suggest an appropriate investment strategy, e.g., Value, GARP (Growth at a Reasonable Price), Growth, Income, Speculative, Special Situation, Turnaround.]\n\n"
            "Confidence Level:\n"
            "[State one of the following: High, Medium, Low. This reflects confidence in the *analysis and decision*, considering data quality and forecastability.]\n\n"
            "Key Reasoning Points:\n"
            "[Provide 3-7 bullet points summarizing the key reasons for the investment decision. Cover aspects like: \n"
            "* Valuation (e.g., DCF, comparables, P/E relative to growth/sector).\n"
            "* Financial Health & Performance (e.g., profitability, debt, cash flow trends).\n"
            "* Growth Outlook (e.g., revenue/EPS growth drivers, market expansion, innovation pipeline).\n"
            "* Economic Moat & Competitive Advantages (e.g., brand, patents, network effects, cost structure).\n"
            "* Key Risks (e.g., competition, macro factors, regulatory, company-specific issues).\n"
            "* Management & Strategy (e.g., effectiveness of strategy, capital allocation, execution track record, if known).\n"
            "Each point should be concise and directly support the decision.]"
        )

        final_prompt_for_gemini = prompt[:MAX_GEMINI_TEXT_LENGTH]  # Ensure prompt is within limits
        ai_response_text = self.gemini.generate_text(final_prompt_for_gemini)

        parsed_thesis_data = self._parse_ai_investment_thesis_response(ai_response_text)

        logger.info(
            f"Generated thesis for {self.ticker}. Parsed Decision: {parsed_thesis_data.get('investment_decision')}, Strategy: {parsed_thesis_data.get('strategy_type')}, Confidence: {parsed_thesis_data.get('confidence_level')}")
        return parsed_thesis_data

    def analyze(self):
        logger.info(f"Full analysis pipeline started for {self.ticker}...")
        final_data_for_db = {}  # This will hold all data to be saved to StockAnalysis model
        try:
            if not self.stock_db_entry:  # Should have been initialized
                logger.error(f"Stock DB entry for {self.ticker} is not initialized. Aborting analysis.");
                return None
            self._ensure_stock_db_entry_is_bound()  # Ensure SQLAlchemy session attachment

            # --- Data Fetching ---
            self._fetch_financial_statements()
            self._fetch_key_metrics_and_profile_data()

            # --- Calculations & Quantitative Analysis ---
            calculated_metrics = self._calculate_derived_metrics()
            final_data_for_db.update(calculated_metrics)

            dcf_analysis_results = self._perform_dcf_analysis()
            final_data_for_db.update(dcf_analysis_results)

            # --- Qualitative Analysis (10-K & AI Summaries) ---
            qualitative_analysis_results = self._fetch_and_summarize_10k()
            final_data_for_db.update(qualitative_analysis_results)

            # --- Final Synthesis & Decision ---
            investment_thesis_data = self._determine_investment_thesis()
            final_data_for_db.update(investment_thesis_data)

            # Create and populate the StockAnalysis DB entry
            analysis_entry = StockAnalysis(stock_id=self.stock_db_entry.id, analysis_date=datetime.now(timezone.utc))

            # Get all column names from the StockAnalysis model
            model_fields = [c.key for c in StockAnalysis.__table__.columns if
                            c.key not in ['id', 'stock_id', 'analysis_date']]

            for field_name in model_fields:
                if field_name in final_data_for_db:
                    value_to_set = final_data_for_db[field_name]

                    # Basic type checking/conversion for safety before setting attribute
                    target_column_type = getattr(StockAnalysis, field_name).type.python_type
                    if target_column_type == float:
                        if isinstance(value_to_set, str):  # Attempt conversion if string
                            try:
                                value_to_set = float(value_to_set)
                            except ValueError:
                                value_to_set = None
                        if isinstance(value_to_set, float) and (math.isnan(value_to_set) or math.isinf(value_to_set)):
                            value_to_set = None  # Store NaN/inf as None in DB
                    elif target_column_type == dict and not isinstance(value_to_set,
                                                                       dict):  # Ensure JSON fields are dicts
                        value_to_set = None  # Or {} if appropriate default
                    elif target_column_type == str and not isinstance(value_to_set, str):
                        value_to_set = str(value_to_set) if value_to_set is not None else None

                    setattr(analysis_entry, field_name, value_to_set)

            self.db_session.add(analysis_entry)
            self.stock_db_entry.last_analysis_date = analysis_entry.analysis_date  # Update stock's last analysis date
            self.db_session.commit()
            logger.info(f"Successfully analyzed and saved stock data: {self.ticker} (Analysis ID: {analysis_entry.id})")
            return analysis_entry

        except RuntimeError as rt_err:  # Catch initialization errors or other critical runtime issues
            logger.critical(f"Runtime error during full analysis for {self.ticker}: {rt_err}", exc_info=True);
            return None
        except Exception as e:
            logger.error(f"CRITICAL error in full analysis pipeline for {self.ticker}: {e}", exc_info=True)
            if self.db_session and self.db_session.is_active:
                try:
                    self.db_session.rollback(); logger.info(
                        f"Rolled back database transaction for {self.ticker} due to error.")
                except Exception as e_rb:
                    logger.error(f"Rollback error for {self.ticker}: {e_rb}")
            return None
        finally:
            self._close_session_if_active()  # Ensure session is closed

    def _ensure_stock_db_entry_is_bound(self):
        """Ensures the stock_db_entry is properly bound to the current active session."""
        if not self.stock_db_entry:
            logger.critical(
                f"CRITICAL: self.stock_db_entry is None for {self.ticker} at _ensure_stock_db_entry_is_bound.")
            raise RuntimeError(f"Stock entry for {self.ticker} is None during binding check.")

        if not self.db_session.is_active:  # If session became inactive for some reason
            logger.warning(f"DB Session for {self.ticker} was INACTIVE before binding check. Re-establishing.")
            self._close_session_if_active()  # Close potentially stale session
            self.db_session = next(get_db_session())  # Get a fresh session

            # After re-establishing session, self.stock_db_entry is likely detached. Re-fetch or re-attach.
            logger.info(f"Attempting to re-associate stock {self.ticker} with new session.")
            re_fetched_stock = self.db_session.query(Stock).filter(Stock.ticker == self.ticker).first()
            if not re_fetched_stock:
                logger.error(
                    f"Could not re-fetch stock {self.ticker} after session re-establishment. This might indicate a larger issue or first-time creation flow problem.")
                # Potentially try to re-run parts of _get_or_create_stock_entry if this is a common recovery path
                # For now, raise error as this state should be unusual if _get_or_create_stock_entry succeeded.
                raise RuntimeError(f"Failed to re-fetch stock {self.ticker} for new session. Analysis cannot proceed.")
            else:
                self.stock_db_entry = re_fetched_stock
                logger.info(f"Re-fetched and bound stock {self.ticker} (ID: {self.stock_db_entry.id}) to new session.")
            return  # Return after re-binding

        # Check if the instance is bound to *this* session object
        instance_state = sa_inspect(self.stock_db_entry)
        if not instance_state.session or instance_state.session is not self.db_session:
            obj_id_log = self.stock_db_entry.id if instance_state.has_identity else 'Transient/No ID'
            session_id_actual = id(instance_state.session) if instance_state.session else 'None'
            logger.warning(
                f"Stock {self.ticker} (ID: {obj_id_log}) is DETACHED or bound to a DIFFERENT session "
                f"(Expected session id: {id(self.db_session)}, Actual: {session_id_actual}). Attempting to merge.")
            try:
                # Merge the detached object into the current session
                self.stock_db_entry = self.db_session.merge(self.stock_db_entry)
                logger.info(
                    f"Successfully merged stock {self.ticker} (ID: {self.stock_db_entry.id}) into current session.")
            except Exception as e_merge:
                logger.error(
                    f"Failed to merge stock {self.ticker} into current session: {e_merge}. Attempting re-fetch as fallback.",
                    exc_info=True)
                # Fallback: try to re-fetch from DB using current session
                re_fetched_from_db = self.db_session.query(Stock).filter(Stock.ticker == self.ticker).first()
                if re_fetched_from_db:
                    self.stock_db_entry = re_fetched_from_db
                    logger.info(
                        f"Successfully re-fetched stock {self.ticker} (ID: {self.stock_db_entry.id}) after merge failure.")
                else:
                    # This is a more critical situation if re-fetch also fails.
                    logger.critical(
                        f"CRITICAL: Failed to bind stock {self.ticker} to current session after merge and re-fetch attempts.")
                    raise RuntimeError(f"Failed to bind stock {self.ticker} to session. Analysis cannot proceed.")


if __name__ == '__main__':
    from database import init_db

    # init_db() # Uncomment if DB schema needs to be created/updated

    logger.info("Starting standalone stock analysis test...")
    tickers_to_test = ["AAPL", "MSFT", "NKE"]  # Added NKE to re-test based on initial logs

    for ticker_symbol in tickers_to_test:
        analysis_result_obj = None
        try:
            logger.info(f"--- Analyzing {ticker_symbol} ---")
            analyzer_instance = StockAnalyzer(ticker=ticker_symbol)  # StockAnalyzer manages its own session
            analysis_result_obj = analyzer_instance.analyze()

            if analysis_result_obj and hasattr(analysis_result_obj, 'stock'):  # Check if analysis object is valid
                logger.info(
                    f"Analysis for {analysis_result_obj.stock.ticker} completed. "
                    f"Decision: {analysis_result_obj.investment_decision}, "
                    f"Strategy: {analysis_result_obj.strategy_type}, "
                    f"Confidence: {analysis_result_obj.confidence_level}"
                )
                if analysis_result_obj.dcf_intrinsic_value is not None:
                    logger.info(
                        f"DCF Value: {analysis_result_obj.dcf_intrinsic_value:.2f}, "
                        f"Upside: {analysis_result_obj.dcf_upside_percentage:.2% if analysis_result_obj.dcf_upside_percentage is not None else 'N/A'}"
                    )
                logger.info(
                    f"QoQ Revenue Growth: {analysis_result_obj.revenue_growth_qoq if analysis_result_obj.revenue_growth_qoq is not None else 'N/A'} "
                    f"(Source: {analysis_result_obj.key_metrics_snapshot.get('q_revenue_source', 'N/A') if analysis_result_obj.key_metrics_snapshot else 'N/A'})"
                )
                # Log a few more key metrics
                logger.info(
                    f"  P/E: {analysis_result_obj.pe_ratio}, P/B: {analysis_result_obj.pb_ratio}, ROE: {analysis_result_obj.roe}")
                # logger.debug(f"Full thesis for {ticker_symbol}: {analysis_result_obj.investment_thesis_full}") # For detailed check if needed
                # logger.debug(f"Reasoning for {ticker_symbol}: {analysis_result_obj.reasoning}")

            else:
                logger.error(f"Stock analysis pipeline FAILED or returned invalid result for {ticker_symbol}.")
        except RuntimeError as rt_err:  # Catch errors from StockAnalyzer.__init__
            logger.error(f"Could not run StockAnalyzer for {ticker_symbol} due to initialization error: {rt_err}")
        except Exception as e_main_loop:  # Catch any other unexpected errors during analysis
            logger.error(f"Unhandled error analyzing {ticker_symbol} in __main__ loop: {e_main_loop}", exc_info=True)
        finally:
            logger.info(f"--- Finished processing {ticker_symbol} ---")
            time.sleep(20)  # Respect API rate limits if running multiple tickers
---------- END stock_analyzer.py ----------

--- END OF FILE project_structure_backend.txt ---
