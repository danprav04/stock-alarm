--- START OF FILE project_structure_backend.txt ---

api_clients/
  __init__.py
  alphavantage_client.py
  base_client.py
  eodhd_client.py
  finnhub_client.py
  fmp_client.py
  gemini_client.py
  sec_edgar_client.py
core/
  __init__.py
  config.py
  logging_setup.py
database/
  __init__.py
  connection.py
  models.py
services/
  ipo_analyzer/
    __init__.py
    ai_analyzer.py
    data_fetcher.py
    db_handler.py
    helpers.py
    ipo_analyzer.py
  news_analyzer/
    __init__.py
    ai_analyzer.py
    data_fetcher.py
    db_handler.py
    news_analyzer.py
  stock_analyzer/
    __init__.py
    ai_synthesis.py
    data_fetcher.py
    dcf_analyzer.py
    helpers.py
    metrics_calculator.py
    qualitative_analyzer.py
    stock_analyzer.py
  __init__.py
  email_service.py
.gitignore
app_analysis.log
main.py
requirements.txt


---------- __init__.py ----------
# api_clients/__init__.py
from .base_client import APIClient, scrape_article_content, extract_S1_text_sections
from .finnhub_client import FinnhubClient
from .fmp_client import FinancialModelingPrepClient
from .alphavantage_client import AlphaVantageClient
from .eodhd_client import EODHDClient
from .sec_edgar_client import SECEDGARClient
from .gemini_client import GeminiAPIClient

__all__ = [
    "APIClient",
    "scrape_article_content",
    "extract_S1_text_sections",
    "FinnhubClient",
    "FinancialModelingPrepClient",
    "AlphaVantageClient",
    "EODHDClient",
    "SECEDGARClient",
    "GeminiAPIClient",
]
---------- END __init__.py ----------


---------- alphavantage_client.py ----------
# api_clients/alphavantage_client.py
from .base_client import APIClient
from core.config import ALPHA_VANTAGE_API_KEY
# Removed time module as hardcoded sleep is removed

class AlphaVantageClient(APIClient):
    def __init__(self):
        super().__init__("https://www.alphavantage.co", api_key_name="apikey", api_key_value=ALPHA_VANTAGE_API_KEY)

    def get_company_overview(self, ticker):
        params = {"function": "OVERVIEW", "symbol": ticker}
        # Base client handles retries and delays, including for rate limits (429)
        return self.request("GET", "/query", params=params, api_source_name="alphavantage_overview")

    def get_income_statement_quarterly(self, ticker):
        params = {"function": "INCOME_STATEMENT", "symbol": ticker}
        data = self.request("GET", "/query", params=params, api_source_name="alphavantage_income_quarterly")
        if data and isinstance(data.get("quarterlyReports"), list):
            data["quarterlyReports"].reverse() # Keep this logic if it's desired
        return data

    def get_balance_sheet_quarterly(self, ticker):
        params = {"function": "BALANCE_SHEET", "symbol": ticker}
        data = self.request("GET", "/query", params=params, api_source_name="alphavantage_balance_quarterly")
        if data and isinstance(data.get("quarterlyReports"), list):
            data["quarterlyReports"].reverse()
        return data

    def get_cash_flow_quarterly(self, ticker):
        params = {"function": "CASH_FLOW", "symbol": ticker}
        data = self.request("GET", "/query", params=params, api_source_name="alphavantage_cashflow_quarterly")
        if data and isinstance(data.get("quarterlyReports"), list):
            data["quarterlyReports"].reverse()
        return data
---------- END alphavantage_client.py ----------


---------- base_client.py ----------
# api_clients/base_client.py
import requests
import time
import json
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse

from core.config import (
    API_REQUEST_TIMEOUT, API_RETRY_ATTEMPTS, API_RETRY_DELAY,
    CACHE_EXPIRY_SECONDS
)
from core.logging_setup import logger
from database.connection import SessionLocal
from database.models import CachedAPIData


class APIClient:
    def __init__(self, base_url, api_key_name=None, api_key_value=None, headers=None):
        self.base_url = base_url
        self.api_key_name = api_key_name
        self.api_key_value = api_key_value
        self.headers = headers or {}
        if api_key_name and api_key_value:
            self.params = {api_key_name: api_key_value}
        else:
            self.params = {}

    def _get_cached_response(self, request_url_or_params_str):
        session = SessionLocal()
        try:
            current_time_utc = datetime.now(timezone.utc)
            cache_entry = session.query(CachedAPIData).filter(
                CachedAPIData.request_url_or_params == request_url_or_params_str,
                CachedAPIData.expires_at > current_time_utc
            ).first()
            if cache_entry:
                logger.info(f"Cache hit for: {request_url_or_params_str[:100]}...")
                return cache_entry.response_data
        except Exception as e:
            logger.error(f"Error reading from cache for '{request_url_or_params_str[:100]}...': {e}", exc_info=True)
        finally:
            session.close()
        return None

    def _cache_response(self, request_url_or_params_str, response_data, api_source):
        session = SessionLocal()
        try:
            now_utc = datetime.now(timezone.utc)
            expires_at_utc = now_utc + timedelta(seconds=CACHE_EXPIRY_SECONDS)

            session.query(CachedAPIData).filter(
                CachedAPIData.request_url_or_params == request_url_or_params_str).delete(synchronize_session=False)

            new_cache_entry = CachedAPIData(
                api_source=api_source,
                request_url_or_params=request_url_or_params_str,
                response_data=response_data,
                timestamp=now_utc,
                expires_at=expires_at_utc
            )
            session.add(new_cache_entry)
            session.commit()
            logger.info(f"Cached response for: {request_url_or_params_str[:100]}...")
        except Exception as e:
            logger.error(f"Error writing to cache for '{request_url_or_params_str[:100]}...': {e}", exc_info=True)
            session.rollback()
        finally:
            session.close()

    def request(self, method, endpoint, params=None, data=None, json_data=None, use_cache=True,
                api_source_name="unknown", is_json_response=True):

        # Determine if endpoint is a full URL
        parsed_endpoint = urlparse(endpoint)
        if parsed_endpoint.scheme and parsed_endpoint.netloc:
            url = endpoint  # Endpoint is already a full URL
        else:
            url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"

        current_call_params = params.copy() if params else {}
        full_query_params = self.params.copy()
        full_query_params.update(current_call_params)

        sorted_params = sorted(full_query_params.items()) if full_query_params else []
        param_string = "&".join([f"{k}={v}" for k, v in sorted_params])
        # For cache key, use the final URL (which might be just the endpoint if it was absolute)
        # and the param_string.
        cache_key_url_part = url  # Use the potentially absolute URL for the cache key
        cache_key_str = f"{method.upper()}:{cache_key_url_part}?{param_string}"

        if json_data:
            try:
                sorted_json_data_str = json.dumps(json_data, sort_keys=True, separators=(',', ':'))
                cache_key_str += f"|BODY:{sorted_json_data_str}"
            except TypeError as e:
                logger.warning(
                    f"Could not serialize json_data for cache key for {url}: {e}. Cache key may be less effective.")
                cache_key_str += f"|BODY_UNSERIALIZED:{str(json_data)}"

        if use_cache:
            cached_data = self._get_cached_response(cache_key_str)
            if cached_data is not None:
                return cached_data

        for attempt in range(API_RETRY_ATTEMPTS):
            try:
                response = requests.request(
                    method, url, params=full_query_params, data=data, json=json_data,
                    headers=self.headers, timeout=API_REQUEST_TIMEOUT
                )
                response.raise_for_status()

                if not is_json_response:
                    response_content = response.text
                    if use_cache:
                        self._cache_response(cache_key_str, response_content, api_source_name)
                    return response_content

                response_json = response.json()
                if use_cache:
                    self._cache_response(cache_key_str, response_json, api_source_name)
                return response_json

            except requests.exceptions.HTTPError as e:
                log_params_for_error = {k: (
                    str(v)[:4] + '******' + str(v)[-4:] if k == self.api_key_name and isinstance(v, str) and len(
                        str(v)) > 8 else v) for k, v in full_query_params.items()}
                log_headers_for_error = self.headers.copy()
                sensitive_header_keys = ["X-RapidAPI-Key", "Authorization", "Token", self.api_key_name]
                for h_key in sensitive_header_keys:
                    if h_key in log_headers_for_error and isinstance(log_headers_for_error[h_key], str) and len(
                            log_headers_for_error[h_key]) > 8:
                        log_headers_for_error[h_key] = log_headers_for_error[h_key][:4] + "******" + \
                                                       log_headers_for_error[h_key][-4:]
                status_code = e.response.status_code if e.response is not None else "Unknown"
                response_text_preview = e.response.text[:200] if e.response is not None else "No response body"
                logger.warning(
                    f"HTTP error on attempt {attempt + 1}/{API_RETRY_ATTEMPTS} for {method} {url} "
                    f"(Params: {log_params_for_error}, Headers: {log_headers_for_error}): "
                    f"{status_code} - {response_text_preview}..."
                )
                if api_source_name.startswith(
                        "alphavantage") and e.response is not None and "Our standard API call frequency is 25 requests per day." in e.response.text:
                    logger.error(f"Alpha Vantage API daily limit likely reached. Params: {log_params_for_error}")
                    return None
                if e.response is not None:
                    if status_code == 429:
                        delay = API_RETRY_DELAY * (2 ** attempt)
                        logger.info(f"Rate limit hit (429). Waiting for {delay} seconds.")
                        time.sleep(delay)
                    elif 500 <= status_code < 600:
                        delay = API_RETRY_DELAY * (2 ** attempt)
                        logger.info(f"Server error ({status_code}). Waiting for {delay} seconds before retry.")
                        time.sleep(delay)
                    elif status_code == 401 or status_code == 403:
                        logger.error(
                            f"Client error {status_code} (Unauthorized/Forbidden) for {url}. API key may be invalid or permissions lacking. No retry. Params: {log_params_for_error}")
                        return None
                    else:
                        # For 404s specifically on SEC Edgar filing text, don't log as error, just warning, as it might be a legitimate "not found"
                        if api_source_name == "edgar_filing_text_content" and status_code == 404:
                            logger.warning(
                                f"SEC Edgar returned 404 for {url}. Document may not exist or URL is incorrect.")
                        else:
                            logger.error(
                                f"Non-retryable client error {status_code} for {url}: {e.response.reason if e.response else 'Unknown reason'}",
                                exc_info=False)
                        return None
                else:
                    logger.error(f"HTTPError without response object for {url}. Cannot retry effectively.")
                    return None
            except requests.exceptions.RequestException as e:
                logger.warning(f"Request error on attempt {attempt + 1}/{API_RETRY_ATTEMPTS} for {url}: {e}")
                if attempt < API_RETRY_ATTEMPTS - 1:
                    delay = API_RETRY_DELAY * (2 ** attempt)
                    time.sleep(delay)
            except json.JSONDecodeError as e_json:
                logger.error(
                    f"JSON decode error for {url} on attempt {attempt + 1}. Response text: {response.text[:500] if 'response' in locals() else 'Response object not available'}... Error: {e_json}")
                if attempt < API_RETRY_ATTEMPTS - 1:
                    delay = API_RETRY_DELAY * (2 ** attempt)
                    time.sleep(delay)
                else:
                    return None
        logger.error(f"All {API_RETRY_ATTEMPTS} attempts failed for {url}. Last query params: {full_query_params}")
        return None


# --- Helper functions for scraping and parsing (moved from old api_clients.py) ---
def scrape_article_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9', 'Connection': 'keep-alive'
        }
        response = requests.get(url, headers=headers, timeout=API_REQUEST_TIMEOUT - 10, allow_redirects=True)
        response.raise_for_status()
        content_type = response.headers.get('content-type', '').lower()
        if 'html' not in content_type:
            logger.warning(f"Content type for {url} is not HTML ('{content_type}'). Skipping scrape.");
            return None

        soup = BeautifulSoup(response.content, 'lxml')

        for tag_name in ['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'iframe', 'noscript', 'link',
                         'meta', 'button', 'input', 'select', 'textarea', 'figure', 'figcaption']:
            for tag in soup.find_all(tag_name):
                tag.decompose()

        main_content_html = None
        selectors = [
            'article', 'main', 'div[role="main"]',
            'div[class*="article-body"]', 'div[class*="article-content"]', 'div[id*="article-body"]',
            'div[id*="article-content"]',
            'div[class*="post-content"]', 'div[class*="entry-content"]',
            'div[class*="story-body"]', 'div[class*="main-content"]', 'section[class*="content"]'
        ]
        for selector in selectors:
            tag = soup.select_one(selector)
            if tag:
                for unwanted_pattern in ['ad', 'social', 'related', 'share', 'comment', 'promo', 'sidebar', 'popup',
                                         'banner', 'meta-info', 'byline', 'author', 'timestamp', 'tags', 'breadcrumb',
                                         'pagination', 'tools', 'print-button', 'advertisement', 'figcaption',
                                         'read-more', 'newsletter', 'modal']:
                    for sub_tag in tag.find_all(
                            lambda t: any(unwanted_pattern in c.lower() for c in t.get('class', [])) or \
                                      any(unwanted_pattern in i.lower() for i in t.get('id', [])) or \
                                      unwanted_pattern in t.get('role', '').lower() or \
                                      unwanted_pattern in t.get('aria-label', '').lower()):
                        sub_tag.decompose()
                main_content_html = tag
                break

        article_text = ""
        if main_content_html:
            text_parts = []
            for element in main_content_html.find_all(
                    ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div', 'span', 'td', 'th']):
                text = element.get_text(separator=' ', strip=True)
                if text:
                    if element.name == 'div' and element.find(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
                        continue
                    text_parts.append(text)
            article_text = '\n'.join(filter(None, text_parts))
        elif soup.body:
            logger.info(f"Main content selectors failed for {url}, trying body text. This might be noisy.")
            article_text = soup.body.get_text(separator='\n', strip=True)
        else:
            logger.warning(f"Could not extract main content or body text from {url}.");
            return None

        article_text = re.sub(r'[ \t]+', ' ', article_text)
        article_text = re.sub(r'\n\s*\n', '\n\n', article_text)
        article_text = re.sub(r'\n{3,}', '\n\n', article_text).strip()

        if len(article_text) < 200:
            logger.info(
                f"Extracted text from {url} is very short ({len(article_text)} chars). Might be a stub, paywall, or primarily non-text content.")
        logger.info(f"Successfully scraped ~{len(article_text)} chars from {url}")
        return article_text

    except requests.exceptions.Timeout:
        logger.error(f"Timeout error scraping {url}.");
        return None
    except requests.exceptions.RequestException as e:
        logger.error(f"Request error scraping {url}: {e}");
        return None
    except Exception as e:
        logger.error(f"General error scraping {url}: {e}", exc_info=True);
        return None


def extract_S1_text_sections(filing_text, sections_map):
    if not filing_text or not sections_map: return {}
    extracted_sections = {}
    try:
        soup = BeautifulSoup(filing_text, 'lxml')
    except Exception:
        try:
            logger.warning("lxml parsing failed for SEC filing, trying html.parser.")
            soup = BeautifulSoup(filing_text, 'html.parser')
        except Exception as e_bs_parse:
            logger.error(
                f"BeautifulSoup failed to parse filing text with lxml and html.parser: {e_bs_parse}. Using raw text and regex matching might be less accurate.")
            normalized_text = re.sub(r'\s*\n\s*', '\n', filing_text.strip())
            normalized_text = ''.join(filter(lambda x: x.isprintable() or x.isspace(), normalized_text))
            soup = None

    if soup:
        for invisible_element_name in ['style', 'script', 'head', 'title', 'meta', 'link', 'noscript']:
            for element in soup.find_all(invisible_element_name):
                element.decompose()
        page_text = []
        for element in soup.find_all(
                ['p', 'div', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'td', 'tr', 'table', 'body']):
            text = element.get_text(separator='\n', strip=True)
            if text:
                page_text.append(text)
        normalized_text = '\n\n'.join(page_text)
        normalized_text = re.sub(r'\s*\n\s*', '\n', normalized_text)
        normalized_text = re.sub(r'\n{3,}', '\n\n', normalized_text)
        normalized_text = ''.join(filter(lambda x: x.isprintable() or x.isspace(), normalized_text))
    else:  # soup is None, normalized_text was already prepared
        pass

    section_patterns = []
    for key, patterns_list in sections_map.items():
        item_num_pattern_str = patterns_list[0].replace('.', r'\.?')
        base_item_regex = r"(?:ITEM|Item)\s*" + item_num_pattern_str.split()[-1] + r"\.?\s*:?\s*"
        if len(patterns_list) > 1:
            descriptive_name_regex = re.escape(patterns_list[1])
            start_regex_str_item_desc = base_item_regex + descriptive_name_regex
            section_patterns.append({"key": key, "start_regex": re.compile(start_regex_str_item_desc, re.IGNORECASE)})
            start_regex_str_desc_only = r"^\s*" + descriptive_name_regex + r"\s*$"
            section_patterns.append(
                {"key": key, "start_regex": re.compile(start_regex_str_desc_only, re.IGNORECASE | re.MULTILINE)})
        else:
            section_patterns.append({"key": key, "start_regex": re.compile(base_item_regex, re.IGNORECASE)})

    found_sections_matches = []
    for pattern_info in section_patterns:
        for match in pattern_info["start_regex"].finditer(normalized_text):
            found_sections_matches.append({
                "key": pattern_info["key"],
                "start": match.start(),
                "end_of_header": match.end(),
                "header_text": match.group(0).strip()
            })

    if not found_sections_matches:
        logger.warning("No sections extracted from SEC filing based on ITEM X or descriptive name patterns.");
        return {}

    found_sections_matches.sort(key=lambda x: x["start"])

    for i, current_sec_info in enumerate(found_sections_matches):
        start_index = current_sec_info["end_of_header"]
        end_index = len(normalized_text)
        for j in range(i + 1, len(found_sections_matches)):
            next_sec_info = found_sections_matches[j]
            if next_sec_info["key"] != current_sec_info["key"]:  # Stop if it's a header for a *different* section type
                # Check if the next header is a more specific version of the current section
                # e.g. current is "Item 1. Business", next is "Item 1A. Risk Factors" - this is fine
                # But if current is "Item 1. Business" and next is "Item 7. MD&A", then stop.
                # This logic might need refinement depending on how sections_map is structured.
                # For now, any different key means end of current section.
                end_index = next_sec_info["start"]
                break
        section_text = normalized_text[start_index:end_index].strip()
        section_text = re.sub(r'(?i)\btable\s+of\s+contents\b.*?\n', '', section_text, flags=re.MULTILINE)
        section_text = re.sub(r'^\s*(?:Page\s+\d+|\d+|PART\s+[IVXLCDM]+)\s*$', '', section_text, flags=re.MULTILINE)
        section_text = re.sub(r'\n{3,}', '\n\n', section_text).strip()

        if section_text:
            if current_sec_info["key"] not in extracted_sections or len(section_text) > len(
                    extracted_sections.get(current_sec_info["key"], "")):
                extracted_sections[current_sec_info["key"]] = section_text
                logger.debug(
                    f"Extracted section '{current_sec_info['key']}' (header: '{current_sec_info['header_text']}') len {len(section_text)}")

    if not extracted_sections:
        logger.warning("No text content could be extracted for any identified section headers after processing.")
    return extracted_sections
---------- END base_client.py ----------


---------- eodhd_client.py ----------
from .base_client import APIClient
from core.config import EODHD_API_KEY
from core.logging_setup import logger


class EODHDClient(APIClient):
    def __init__(self):
        super().__init__("https://eodhistoricaldata.com/api", api_key_name="api_token", api_key_value=EODHD_API_KEY)
        self.params["fmt"] = "json" # Default format

    def get_fundamental_data(self, ticker_with_exchange): # e.g., AAPL.US
        return self.request("GET", f"/fundamentals/{ticker_with_exchange}", api_source_name="eodhd_fundamentals")

    def get_ipo_calendar(self, from_date=None, to_date=None):
        params = {}
        if from_date: params["from"] = from_date
        if to_date: params["to"] = to_date
        logger.info("EODHDClient.get_ipo_calendar called. Data quality/availability may vary by subscription.")
        return self.request("GET", "/calendar/ipos", params=params, api_source_name="eodhd_ipo_calendar")
---------- END eodhd_client.py ----------


---------- finnhub_client.py ----------
from datetime import datetime, timedelta, timezone
from .base_client import APIClient
from core.config import FINNHUB_API_KEY


class FinnhubClient(APIClient):
    def __init__(self):
        super().__init__("https://finnhub.io/api/v1", api_key_name="token", api_key_value=FINNHUB_API_KEY)

    def get_market_news(self, category="general", min_id=0):
        params = {"category": category}
        if min_id > 0: params["minId"] = min_id
        return self.request("GET", "/news", params=params, api_source_name="finnhub_news")

    def get_company_profile2(self, ticker):
        return self.request("GET", "/stock/profile2", params={"symbol": ticker}, api_source_name="finnhub_profile")

    def get_financials_reported(self, ticker, freq="quarterly", count=20): # count specifies number of periods
        params = {"symbol": ticker, "freq": freq, "count": count}
        return self.request("GET", "/stock/financials-reported", params=params,
                            api_source_name="finnhub_financials_reported")

    def get_basic_financials(self, ticker, metric_type="all"):
        return self.request("GET", "/stock/metric", params={"symbol": ticker, "metric": metric_type},
                            api_source_name="finnhub_metrics")

    def get_ipo_calendar(self, from_date=None, to_date=None):
        if from_date is None: from_date = (datetime.now(timezone.utc) - timedelta(days=30)).strftime('%Y-%m-%d')
        if to_date is None: to_date = (datetime.now(timezone.utc) + timedelta(days=90)).strftime('%Y-%m-%d')
        params = {"from": from_date, "to": to_date}
        return self.request("GET", "/calendar/ipo", params=params, api_source_name="finnhub_ipo_calendar")

    def get_sec_filings(self, ticker, from_date=None, to_date=None):
        if from_date is None: from_date = (datetime.now(timezone.utc) - timedelta(days=365 * 2)).strftime('%Y-%m-%d')
        if to_date is None: to_date = datetime.now(timezone.utc).strftime('%Y-%m-%d')
        params = {"symbol": ticker, "from": from_date, "to": to_date}
        return self.request("GET", "/stock/filings", params=params, api_source_name="finnhub_filings")

    def get_company_peers(self, ticker):
        """Gets a list of company peers."""
        return self.request("GET", "/stock/peers", params={"symbol": ticker}, api_source_name="finnhub_peers")

---------- END finnhub_client.py ----------


---------- fmp_client.py ----------
# api_clients/fmp_client.py
from .base_client import APIClient
from core.config import FINANCIAL_MODELING_PREP_API_KEY
from core.logging_setup import logger


class FinancialModelingPrepClient(APIClient):
    def __init__(self):
        super().__init__("https://financialmodelingprep.com/api/v3", api_key_name="apikey",
                         api_key_value=FINANCIAL_MODELING_PREP_API_KEY)

    def get_ipo_calendar(self, from_date=None, to_date=None):
        # Note: FMP's free tier might not support this well or at all.
        params = {}
        if from_date: params["from"] = from_date
        if to_date: params["to"] = to_date
        logger.info("FinancialModelingPrepClient.get_ipo_calendar called. Availability depends on FMP subscription.")
        return self.request("GET", "/ipo_calendar", params=params, api_source_name="fmp_ipo_calendar")

    def get_financial_statements(self, ticker, statement_type="income-statement", period="quarter", limit=40):
        actual_limit = limit
        if period == "annual": actual_limit = min(limit, 15)
        elif period == "quarter": actual_limit = min(limit, 60)

        return self.request("GET", f"/{statement_type}/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_{statement_type.replace('-', '_')}_{period}")

    def get_income_statement_growth(self, ticker, period="quarter", limit=40):
        actual_limit = limit
        if period == "annual": actual_limit = min(limit, 15)
        elif period == "quarter": actual_limit = min(limit, 60)
        return self.request("GET", f"/income-statement-growth/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_income_statement_growth_{period}")

    def get_key_metrics(self, ticker, period="quarter", limit=40):
        actual_limit = limit
        if period == "annual": actual_limit = min(limit, 15)
        elif period == "quarter": actual_limit = min(limit, 60)
        return self.request("GET", f"/key-metrics/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_key_metrics_{period}")

    def get_ratios(self, ticker, period="quarter", limit=40):
        actual_limit = limit
        if period == "annual": actual_limit = min(limit, 15)
        elif period == "quarter": actual_limit = min(limit, 60)
        return self.request("GET", f"/ratios/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_ratios_{period}")

    def get_company_profile(self, ticker):
        return self.request("GET", f"/profile/{ticker}", params={}, api_source_name="fmp_profile")

    def get_analyst_estimates(self, ticker, period="annual"):
        logger.info(f"FMP get_analyst_estimates for {ticker} called. Availability depends on FMP subscription.")
        return self.request("GET", f"/analyst-estimates/{ticker}", params={"period": period},
                            api_source_name="fmp_analyst_estimates")
---------- END fmp_client.py ----------


---------- gemini_client.py ----------
# api_clients/gemini_client.py
import requests
import time
import json

from core.config import (
    GOOGLE_API_KEYS, API_REQUEST_TIMEOUT, API_RETRY_ATTEMPTS,
    API_RETRY_DELAY, GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE,
    GEMINI_MODEL_NAME, AI_JSON_OUTPUT_INSTRUCTION, GEMINI_MAX_OUTPUT_TOKENS
)
from core.logging_setup import logger


class GeminiAPIClient:
    def __init__(self):
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models"
        self.model_name = GEMINI_MODEL_NAME

    def _get_next_api_key_for_attempt(self, overall_attempt_num, max_attempts_per_key, total_keys):
        if total_keys == 0: return None, 0
        key_group_index = (overall_attempt_num // max_attempts_per_key) % total_keys
        api_key = GOOGLE_API_KEYS[key_group_index]
        current_retry_for_this_key = (overall_attempt_num % max_attempts_per_key) + 1
        logger.debug(
            f"Gemini: Using key ...{api_key[-4:]} (Index {key_group_index}), Attempt {current_retry_for_this_key}/{max_attempts_per_key}")
        return api_key, current_retry_for_this_key

    def _clean_json_string(self, raw_json_str):
        """Attempts to clean common issues in AI-generated JSON strings."""
        if not isinstance(raw_json_str, str):
            return raw_json_str  # Not a string, can't clean

        # Remove leading/trailing markdown code block fences if present
        cleaned_str = raw_json_str.strip()
        if cleaned_str.startswith("```json"):
            cleaned_str = cleaned_str[len("```json"):].strip()
        elif cleaned_str.startswith("```"):
            cleaned_str = cleaned_str[len("```"):].strip()

        if cleaned_str.endswith("```"):
            cleaned_str = cleaned_str[:-len("```")].strip()

        # Sometimes AI might wrap output in an outer quote, try to remove if it looks like it.
        if (cleaned_str.startswith('"') and cleaned_str.endswith('"')) or \
                (cleaned_str.startswith("'") and cleaned_str.endswith("'")):
            try_unquote = cleaned_str[1:-1]
            # Basic check: if unquoting makes it look like valid JSON (starts with { or [)
            if try_unquote.strip().startswith(("{", "[")):
                cleaned_str = try_unquote

        # Ensure newlines and tabs within strings are escaped (common AI mistake)
        # This is tricky. A more robust solution would be a proper parser that can handle some errors,
        # but for now, basic replacements for common patterns.
        # cleaned_str = cleaned_str.replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
        # The above is too aggressive. Let's rely on the LLM to get escaping right mostly.

        return cleaned_str

    def generate_text(self, prompt, model=None, output_format="text"):
        if model is None: model = self.model_name

        max_attempts_per_key = API_RETRY_ATTEMPTS
        total_keys = len(GOOGLE_API_KEYS)
        if total_keys == 0:
            logger.error("Gemini: No API keys configured in GOOGLE_API_KEYS.");
            return {"error": "No Google API keys."} if output_format == "json" else "Error: No Google API keys."

        # Append JSON instruction if needed
        final_prompt = prompt
        if output_format == "json" and AI_JSON_OUTPUT_INSTRUCTION not in prompt:
            final_prompt += f"\n\n{AI_JSON_OUTPUT_INSTRUCTION}"

        if len(final_prompt) > GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE:
            original_len = len(final_prompt)
            final_prompt = final_prompt[:GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE]
            logger.warning(
                f"Gemini prompt (original length {original_len}) exceeded hard limit {GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE}. "
                f"Truncated to {len(final_prompt)} chars."
            )
            trunc_note = "\n...[PROMPT TRUNCATED DUE TO EXCESSIVE LENGTH]..."
            if len(final_prompt) + len(trunc_note) <= GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE:
                final_prompt += trunc_note
            else:
                final_prompt = final_prompt[:GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE - len(trunc_note)] + trunc_note

        for overall_attempt_num in range(total_keys * max_attempts_per_key):
            api_key, current_retry_for_this_key = self._get_next_api_key_for_attempt(
                overall_attempt_num, max_attempts_per_key, total_keys
            )
            if api_key is None: break

            url = f"{self.base_url}/{model}:generateContent?key={api_key}"
            payload = {
                "contents": [{"parts": [{"text": final_prompt}]}],
                "generationConfig": {
                    "temperature": 0.5,  # Slightly lower for more factual JSON
                    "maxOutputTokens": GEMINI_MAX_OUTPUT_TOKENS,
                    "topP": 0.9, "topK": 40,
                    # "response_mime_type": "application/json" # Add if using models that support this directly
                },
                "safetySettings": [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                ]
            }
            # If the model supports it, setting response_mime_type in generationConfig
            # if output_format == "json" and model_supports_json_output: # model_supports_json_output would be a flag
            #    payload["generationConfig"]["response_mime_type"] = "application/json"

            try:
                response = requests.post(url, json=payload,
                                         timeout=API_REQUEST_TIMEOUT + 120)  # Increased timeout for potentially larger JSON
                response.raise_for_status()
                response_json = response.json()

                if response_json.get("promptFeedback", {}).get("blockReason"):
                    reason = response_json["promptFeedback"]["blockReason"]
                    logger.error(
                        f"Gemini prompt blocked for key ...{api_key[-4:]}. Reason: {reason}. Prompt: '{final_prompt[:150]}...'")
                    time.sleep(API_RETRY_DELAY);
                    continue

                if "candidates" in response_json and response_json["candidates"]:
                    candidate = response_json["candidates"][0]
                    finish_reason = candidate.get("finishReason")
                    if finish_reason not in [None, "STOP", "MAX_TOKENS", "MODEL_LENGTH", "OK",
                                             "OTHER"]:  # "OK" and "MODEL_LENGTH" added based on observations
                        logger.warning(
                            f"Gemini unusual finish reason: {finish_reason} for key ...{api_key[-4:]}. Prompt: '{final_prompt[:150]}...'")
                        if finish_reason == "SAFETY":
                            logger.error(
                                f"Gemini candidate content blocked by safety settings for key ...{api_key[-4:]}.")
                            time.sleep(API_RETRY_DELAY);
                            continue
                        # For other unusual reasons, if we expect JSON, this might be an issue.

                    content_part = candidate.get("content", {}).get("parts", [{}])[0]
                    if "text" in content_part:
                        raw_text_output = content_part["text"]
                        if output_format == "json":
                            cleaned_json_str = self._clean_json_string(raw_text_output)
                            try:
                                return json.loads(cleaned_json_str)
                            except json.JSONDecodeError as e_json_parse:
                                logger.error(
                                    f"Gemini response for key ...{api_key[-4:]} was not valid JSON after cleaning: {e_json_parse}. Raw text: '{raw_text_output[:500]}...'")
                                # Fallback or retry, or return error structure
                                # For now, return an error dict
                                if current_retry_for_this_key < max_attempts_per_key:  # retry if not last attempt for this key
                                    time.sleep(API_RETRY_DELAY * current_retry_for_this_key)
                                    continue
                                return {"error": "Failed to parse AI JSON response", "details": str(e_json_parse),
                                        "raw_response": raw_text_output[:500]}
                        else:  # output_format == "text"
                            return raw_text_output
                    else:
                        logger.error(
                            f"Gemini response missing 'text' in content part for key ...{api_key[-4:]}: {response_json}")
                else:
                    logger.error(
                        f"Gemini response malformed or no candidates for key ...{api_key[-4:]}: {response_json}")

            except requests.exceptions.HTTPError as e:
                response_text = e.response.text[:200] if e.response is not None else "N/A"
                status_code = e.response.status_code if e.response is not None else "N/A"
                logger.warning(
                    f"Gemini API HTTP error key ...{api_key[-4:]} attempt {current_retry_for_this_key}: {status_code} - {response_text}. Prompt: '{final_prompt[:150]}...'")
                if e.response is not None and e.response.status_code == 400:
                    if "API key not valid" in e.response.text or "API_KEY_INVALID" in e.response.text:
                        logger.error(
                            f"Gemini API key ...{api_key[-4:]} reported as invalid. Skipping further retries with this key for this call.")
                        overall_attempt_num = ((
                                                           overall_attempt_num // max_attempts_per_key) + 1) * max_attempts_per_key - 1  # Advance to next key group
                        continue
                    else:  # Other 400 errors
                        logger.error(
                            f"Gemini API Bad Request (400). Aborting for this prompt. Response: {e.response.text[:500]}")
                        return {"error": f"Gemini API bad request (400)", "details": e.response.text[
                                                                                     :200]} if output_format == "json" else f"Error: Gemini API bad request (400). {e.response.text[:200]}"
            except requests.exceptions.RequestException as e:
                logger.warning(
                    f"Gemini API request error key ...{api_key[-4:]} attempt {current_retry_for_this_key}: {e}. Prompt: '{final_prompt[:150]}...'")
            except json.JSONDecodeError as e_json_gemini:  # This is for parsing the *API's* response, not the LLM's content
                resp_text_for_log = response.text[:500] if 'response' in locals() and hasattr(response,
                                                                                              'text') else "N/A"
                logger.error(
                    f"Gemini API outer JSON decode error key ...{api_key[-4:]} attempt {current_retry_for_this_key}. Resp: {resp_text_for_log}. Err: {e_json_gemini}")

            if overall_attempt_num < (total_keys * max_attempts_per_key) - 1:
                time.sleep(API_RETRY_DELAY * current_retry_for_this_key)

        logger.error(
            f"All attempts ({total_keys * max_attempts_per_key}) for Gemini API failed for prompt: {final_prompt[:150]}...")
        return {
            "error": "Could not get response from Gemini API after multiple attempts."} if output_format == "json" else "Error: Could not get response from Gemini API after multiple attempts."

    def summarize_text_with_context(self, text_to_summarize, context_summary, desired_output_instruction,
                                    output_format="text"):
        # This method might be too generic now if specific JSON is needed for summarization.
        # It's kept for general text summarization. Specific summarization tasks might call generate_text directly.
        prompt = (
            f"Context: {context_summary}\n\n"
            f"Text to Analyze:\n\"\"\"\n{text_to_summarize}\n\"\"\"\n\n"
            f"Instructions: {desired_output_instruction}\n\n"
            f"Provide a concise and factual summary based on the text and guided by the context and instructions."
        )
        if output_format == "json":
            prompt += (
                f"\n\nOutput the summary in JSON format using the following structure: "
                f"{{\"summary\": \"Your summarized text here.\", \"keyPoints\": [\"Point 1\", \"Point 2\"]}}."
            )
        return self.generate_text(prompt, output_format=output_format)

    def analyze_sentiment_with_reasoning(self, text_to_analyze, context=""):
        prompt = (
            f"Analyze the sentiment of the following text. "
            f"Context for analysis (if any): '{context}'.\n\n"
            f"Text to Analyze:\n\"\"\"\n{text_to_analyze}\n\"\"\"\n\n"
            f"Instructions: Respond with the sentiment classification and reasoning. "
            f"Your entire response MUST be a single valid JSON object with the following structure: \n"
            f"{{\"sentiment\": \"Positive|Negative|Neutral\", \"reasoning\": \"A brief 1-2 sentence explanation, citing specific phrases from the text if possible to justify the sentiment.\"}}"
        )
        return self.generate_text(prompt, output_format="json")
---------- END gemini_client.py ----------


---------- sec_edgar_client.py ----------
import requests
import json
from datetime import datetime

from .base_client import APIClient
from core.config import EDGAR_USER_AGENT, API_REQUEST_TIMEOUT
from core.logging_setup import logger


class SECEDGARClient(APIClient):
    def __init__(self):
        self.company_tickers_url = "https://www.sec.gov/files/company_tickers.json"
        super().__init__("https://data.sec.gov/submissions/")
        self.headers = {"User-Agent": EDGAR_USER_AGENT, "Accept-Encoding": "gzip, deflate"}
        self._cik_map = None
        self._archives_base = "https://www.sec.gov/Archives/edgar/data/"

    def _load_cik_map(self):
        if self._cik_map is None:
            logger.info("Fetching CIK map from SEC...")
            cache_key_str = f"GET:{self.company_tickers_url}"
            cached_map = self._get_cached_response(cache_key_str)
            if cached_map:
                self._cik_map = cached_map
                logger.info(f"CIK map loaded from cache with {len(self._cik_map)} entries.")
                return self._cik_map

            try:
                response = requests.get(self.company_tickers_url, headers=self.headers, timeout=API_REQUEST_TIMEOUT)
                response.raise_for_status()
                data = response.json()
                self._cik_map = {item['ticker']: str(item['cik_str']).zfill(10)
                                 for item in data.values() if 'ticker' in item and 'cik_str' in item}
                self._cache_response(cache_key_str, self._cik_map, "sec_cik_map")
                logger.info(f"CIK map fetched and cached with {len(self._cik_map)} entries.")
            except requests.exceptions.RequestException as e:
                logger.error(f"Error fetching CIK map from SEC: {e}", exc_info=True)
                self._cik_map = {}
            except json.JSONDecodeError as e_json:
                logger.error(f"Error decoding CIK map JSON from SEC: {e_json}", exc_info=True)
                self._cik_map = {}
        return self._cik_map

    def get_cik_by_ticker(self, ticker):
        ticker = ticker.upper()
        try:
            cik_map = self._load_cik_map()
            return cik_map.get(ticker)
        except Exception as e:
            logger.error(f"Unexpected error in get_cik_by_ticker for {ticker}: {e}", exc_info=True)
            return None

    def get_company_filings_summary(self, cik):
        if not cik: return None
        formatted_cik_for_api = str(cik).zfill(10)
        return self.request("GET", f"CIK{formatted_cik_for_api}.json", api_source_name="edgar_filings_summary")

    def get_filing_document_url(self, cik, form_type="10-K", priordate_str=None, count=1):
        if not cik: return None if count == 1 else []
        company_summary = self.get_company_filings_summary(cik)

        if not company_summary or "filings" not in company_summary or "recent" not in company_summary["filings"]:
            logger.warning(f"No recent filings data for CIK {cik} in company summary.")
            return None if count == 1 else []

        recent_filings = company_summary["filings"]["recent"]
        target_filings_info = []

        required_keys = ["form", "accessionNumber", "primaryDocument", "filingDate"]
        min_len = float('inf')
        for key in required_keys:
            if key not in recent_filings or not isinstance(recent_filings[key], list):
                logger.warning(f"Missing or invalid '{key}' in recent filings for CIK {cik}.")
                return None if count == 1 else []
            min_len = min(min_len, len(recent_filings[key]))

        if min_len == float('inf') or min_len == 0:
             logger.warning(f"No usable filing entries for CIK {cik}.")
             return None if count == 1 else []

        forms = recent_filings["form"][:min_len]
        accession_numbers = recent_filings["accessionNumber"][:min_len]
        primary_documents = recent_filings["primaryDocument"][:min_len]
        filing_dates = recent_filings["filingDate"][:min_len]

        priordate_dt = None
        if priordate_str:
            try:
                priordate_dt = datetime.strptime(priordate_str, '%Y-%m-%d').date()
            except ValueError:
                logger.warning(f"Invalid priordate_str format: {priordate_str}. Should be YYYY-MM-DD. Ignoring.")

        for i, form_val in enumerate(forms):
            if form_val.upper() == form_type.upper():
                try:
                    current_filing_date = datetime.strptime(filing_dates[i], '%Y-%m-%d').date()
                except ValueError:
                    logger.warning(f"Invalid filingDate format '{filing_dates[i]}' for CIK {cik}, entry {i}. Skipping.")
                    continue

                if priordate_dt and current_filing_date > priordate_dt:
                    continue

                acc_num_no_hyphens = accession_numbers[i].replace('-', '')
                try:
                    cik_int_for_url = int(cik)
                except ValueError:
                    logger.error(f"CIK '{cik}' for URL construction is not a valid integer. Skipping filing.")
                    continue

                doc_url = f"{self._archives_base}{cik_int_for_url}/{acc_num_no_hyphens}/{primary_documents[i]}"
                target_filings_info.append({"url": doc_url, "date": current_filing_date, "form": form_val})

        if not target_filings_info:
            logger.info(f"No '{form_type}' filings found for CIK {cik} matching criteria.")
            return None if count == 1 else []

        target_filings_info.sort(key=lambda x: x["date"], reverse=True)

        if count == 1:
            return target_filings_info[0]["url"]
        else:
            return [f_info["url"] for f_info in target_filings_info[:count]]

    def get_filing_text(self, filing_url):
        if not filing_url: return None
        logger.info(f"Fetching filing text from: {filing_url}")
        try:
            text_content = self.request("GET", filing_url, use_cache=True,
                                        api_source_name="edgar_filing_text_content",
                                        is_json_response=False)
            if text_content:
                if isinstance(text_content, bytes):
                    try:
                        text_content = text_content.decode('utf-8')
                    except UnicodeDecodeError:
                        logger.warning(f"UTF-8 decode failed for {filing_url}, trying latin-1.")
                        text_content = text_content.decode('latin-1', errors='replace')
            return text_content
        except requests.exceptions.RequestException as e:
            logger.error(f"Error fetching SEC filing text from {filing_url}: {e}")
            return None
---------- END sec_edgar_client.py ----------


---------- __init__.py ----------
from .config import *
from .logging_setup import logger, setup_logging, handle_global_exception

__all__ = [
    # From config (example, list all you need)
    "GOOGLE_API_KEYS", "FINNHUB_API_KEY", "DATABASE_URL", "LOG_FILE_PATH",
    # From logging_setup
    "logger", "setup_logging", "handle_global_exception"
]
---------- END __init__.py ----------


---------- config.py ----------
# core/config.py

GOOGLE_API_KEYS = [
    "AIzaSyDLkwkVYBTUjabShS7VfdLkQTe7vZkxcjY", # Replace with your actual key
    "AIzaSyAjECAJZVZz6PzDaUVaAkgfcOeLXCPFA6Y", # Replace with your actual key
    "AIzaSyBRDIgN7ffBvoqAgaizQfuWRQExKc_oVig", # Replace with your actual key
    "AIzaSyC4XLSmSX4U2iuAqW_pvQ87eNyPaJwQpDo", # Replace with your actual key
]

FINNHUB_API_KEY = "d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390"  # Replace with your actual key
FINANCIAL_MODELING_PREP_API_KEY = "62ERGmJoqQgGD0nSGxRZS91TVzfz61uB"  # Replace with your actual key
EODHD_API_KEY = "683079df749c42.21476005"  # Replace with your actual key or "demo"
RAPIDAPI_UPCOMING_IPO_KEY = "0bd9b5144cmsh50c0e6d95c0b662p1cbdefjsn2d1cb0104cde"  # Replace with your actual key
ALPHA_VANTAGE_API_KEY = "HB6N4X55UTFGN2FP" # Replace with your actual Alpha Vantage Key

# SEC EDGAR Configuration
EDGAR_USER_AGENT = "FinancialAnalysisBot/1.0 YourCompanyName YourContactEmail@example.com"  # Be specific and polite

# Database Configuration
DATABASE_URL = "postgresql://avnadmin:AVNS_IeMYS-rv46Au9xqkza2@pg-4d810ff-daxiake-7258.d.aivencloud.com:26922/stock-alarm?sslmode=require"

# Email Configuration
EMAIL_HOST = "smtp-relay.brevo.com"
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_HOST_USER = "8dca1d001@smtp-brevo.com"
EMAIL_HOST_PASSWORD = "VrNUkDdcR5G9AL8P"
EMAIL_SENDER = "testypesty54@gmail.com"
EMAIL_RECIPIENT = "daniprav@gmail.com"

# Logging Configuration
LOG_FILE_PATH = "app_analysis.log"
LOG_LEVEL = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# API Client Settings
API_REQUEST_TIMEOUT = 45  # seconds
API_RETRY_ATTEMPTS = 3
API_RETRY_DELAY = 10  # seconds

# Gemini API Configuration
GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE = 400000 # Max input characters
GEMINI_MAX_OUTPUT_TOKENS = 8192 # Max output tokens

# Chunking for Summarization
SUMMARIZATION_CHUNK_SIZE_CHARS = 150000 # Reduced for better JSON handling within token limits
SUMMARIZATION_CHUNK_OVERLAP_CHARS = 5000
SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS = 300000 # Reduced

# Analysis Settings
MAX_NEWS_ARTICLES_PER_QUERY = 10
MAX_NEWS_TO_ANALYZE_PER_RUN = 5
MIN_MARKET_CAP = 1000000000
STOCK_FINANCIAL_YEARS = 7
IPO_ANALYSIS_REANALYZE_DAYS = 7

# Cache Settings
CACHE_EXPIRY_SECONDS = 3600 * 6

# DCF Analysis Defaults
DEFAULT_DISCOUNT_RATE = 0.09
DEFAULT_PERPETUAL_GROWTH_RATE = 0.025
DEFAULT_FCF_PROJECTION_YEARS = 5

# News Analysis
NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION = GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE - 20000 # Larger buffer for JSON structure

# IPO/10-K Sections
S1_KEY_SECTIONS = {
    "business": ["Item 1.", "Business"],
    "risk_factors": ["Item 1A.", "Risk Factors"],
    "mda": ["Item 7.", "Management's Discussion and Analysis of Financial Condition and Results of Operations"],
    "financial_statements": ["Item 8.", "Financial Statements and Supplementary Data"]
}
TEN_K_KEY_SECTIONS = S1_KEY_SECTIONS

# Stock Analyzer specific settings
MAX_COMPETITORS_TO_ANALYZE = 5
Q_REVENUE_SANITY_CHECK_DEVIATION_THRESHOLD = 0.30 # Lowered from 0.75
PRIORITY_REVENUE_SOURCES = ["fmp_quarterly", "finnhub_quarterly", "alphavantage_quarterly"]

# Default stock list for --all command in main.py
DEFAULT_STOCKS_FOR_ALL_MODE = ["AAPL", "MSFT", "GOOGL", "NVDA", "JPM", "NKE"]

# Gemini Model configuration
GEMINI_MODEL_NAME = "gemini-2.5-flash-preview-05-20" # Updated model

# System-wide AI response behavior
AI_JSON_OUTPUT_INSTRUCTION = (
    "IMPORTANT: Your entire response MUST be a single, valid JSON object. Do not include any text outside of this JSON structure. "
    "Ensure all strings within the JSON are properly escaped. Use the exact field names and structure specified in the prompt."
)
---------- END config.py ----------


---------- logging_setup.py ----------
import logging
import sys
from .config import LOG_FILE_PATH, LOG_LEVEL

_logging_configured = False

def setup_logging():
    """Configures logging for the application."""
    global _logging_configured
    if _logging_configured:
        return logging.getLogger()

    numeric_level = getattr(logging, LOG_LEVEL.upper(), None)
    if not isinstance(numeric_level, int):
        logging.warning(f"Invalid log level: {LOG_LEVEL} in config. Defaulting to INFO.")
        numeric_level = logging.INFO

    logger_obj = logging.getLogger()
    logger_obj.setLevel(numeric_level)

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')

    if not any(isinstance(h, logging.StreamHandler) for h in logger_obj.handlers):
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        logger_obj.addHandler(console_handler)

    if not any(isinstance(h, logging.FileHandler) and getattr(h, 'baseFilename', None) == LOG_FILE_PATH for h in logger_obj.handlers):
        try:
            file_handler = logging.FileHandler(LOG_FILE_PATH, mode='a')
            file_handler.setFormatter(formatter)
            logger_obj.addHandler(file_handler)
        except Exception as e:
            logging.error(f"Failed to set up file handler for {LOG_FILE_PATH}: {e}", exc_info=True)

    _logging_configured = True
    return logger_obj

logger = setup_logging()

def handle_global_exception(exc_type, exc_value, exc_traceback):
    """Custom global exception handler to log unhandled exceptions."""
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    logger.critical("Unhandled global exception:", exc_info=(exc_type, exc_value, exc_traceback))

# To use the global exception handler, uncomment the following line in your main script (e.g., main.py)
# sys.excepthook = handle_global_exception
---------- END logging_setup.py ----------


---------- __init__.py ----------
from .connection import Base, engine, SessionLocal, init_db, get_db_session
from .models import Stock, StockAnalysis, IPO, IPOAnalysis, NewsEvent, NewsEventAnalysis, CachedAPIData

__all__ = [
    "Base", "engine", "SessionLocal", "init_db", "get_db_session",
    "Stock", "StockAnalysis", "IPO", "IPOAnalysis",
    "NewsEvent", "NewsEventAnalysis", "CachedAPIData"
]
---------- END __init__.py ----------


---------- connection.py ----------
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, scoped_session
from core.config import DATABASE_URL
from core.logging_setup import logger

Base = declarative_base() # Moved from models.py

try:
    engine = create_engine(DATABASE_URL, pool_pre_ping=True)
    SessionFactory = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    SessionLocal = scoped_session(SessionFactory)

    # Base.query = SessionLocal.query_property() # This is optional

    def init_db():
        """Initializes the database and creates tables if they don't exist."""
        try:
            logger.info("Initializing database and creating tables...")
            # Import models here to ensure they are registered with Base metadata
            from . import models # noqa F401
            Base.metadata.create_all(bind=engine)
            logger.info("Database tables created successfully (if they didn't exist).")
        except Exception as e:
            logger.critical(f"CRITICAL Error initializing database: {e}", exc_info=True)
            raise

    def get_db_session():
        """Provides a database session. Caller is responsible for closing."""
        db = SessionLocal()
        try:
            yield db
        finally:
            # SessionLocal.remove() is called automatically by analyzers or when scope ends
            # For direct usage in main.py, ensure SessionLocal.remove() or db.close() is called.
             if db.is_active: # Check if session is still active before closing
                db.close()


except Exception as e:
    logger.critical(f"CRITICAL Failed to connect to database or setup SQLAlchemy: {e}", exc_info=True)
    raise
---------- END connection.py ----------


---------- models.py ----------
from sqlalchemy import Column, Integer, String, Float, DateTime, Text, JSON, ForeignKey, Boolean, Date, UniqueConstraint
from sqlalchemy.orm import relationship
from datetime import datetime, timezone
from .connection import Base  # Import Base from connection.py


class Stock(Base):
    __tablename__ = "stocks"
    id = Column(Integer, primary_key=True, index=True)
    ticker = Column(String, unique=True, index=True, nullable=False)
    company_name = Column(String)
    industry = Column(String, nullable=True)
    sector = Column(String, nullable=True)
    last_analysis_date = Column(DateTime(timezone=True),
                                default=lambda: datetime.now(timezone.utc),
                                onupdate=lambda: datetime.now(timezone.utc))
    cik = Column(String, nullable=True, index=True)

    analyses = relationship("StockAnalysis", back_populates="stock", cascade="all, delete-orphan")


class StockAnalysis(Base):
    __tablename__ = "stock_analyses"
    id = Column(Integer, primary_key=True, index=True)
    stock_id = Column(Integer, ForeignKey("stocks.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    pe_ratio = Column(Float, nullable=True)
    pb_ratio = Column(Float, nullable=True)
    ps_ratio = Column(Float, nullable=True)
    ev_to_sales = Column(Float, nullable=True)
    ev_to_ebitda = Column(Float, nullable=True)
    eps = Column(Float, nullable=True)
    roe = Column(Float, nullable=True)
    roa = Column(Float, nullable=True)
    roic = Column(Float, nullable=True)
    dividend_yield = Column(Float, nullable=True)
    debt_to_equity = Column(Float, nullable=True)
    debt_to_ebitda = Column(Float, nullable=True)
    interest_coverage_ratio = Column(Float, nullable=True)
    current_ratio = Column(Float, nullable=True)
    quick_ratio = Column(Float, nullable=True)
    revenue_growth_yoy = Column(Float, nullable=True)
    revenue_growth_qoq = Column(Float, nullable=True)
    revenue_growth_cagr_3yr = Column(Float, nullable=True)
    revenue_growth_cagr_5yr = Column(Float, nullable=True)
    eps_growth_yoy = Column(Float, nullable=True)
    eps_growth_cagr_3yr = Column(Float, nullable=True)
    eps_growth_cagr_5yr = Column(Float, nullable=True)
    net_profit_margin = Column(Float, nullable=True)
    gross_profit_margin = Column(Float, nullable=True)
    operating_profit_margin = Column(Float, nullable=True)
    free_cash_flow_per_share = Column(Float, nullable=True)
    free_cash_flow_yield = Column(Float, nullable=True)
    free_cash_flow_trend = Column(String, nullable=True)
    retained_earnings_trend = Column(String, nullable=True)
    dcf_intrinsic_value = Column(Float, nullable=True)
    dcf_upside_percentage = Column(Float, nullable=True)
    dcf_assumptions = Column(JSON, nullable=True)
    business_summary = Column(Text, nullable=True)
    economic_moat_summary = Column(Text, nullable=True)
    industry_trends_summary = Column(Text, nullable=True)
    competitive_landscape_summary = Column(Text, nullable=True)
    management_assessment_summary = Column(Text, nullable=True)
    risk_factors_summary = Column(Text, nullable=True)
    investment_thesis_full = Column(Text, nullable=True)
    investment_decision = Column(String, nullable=True)
    reasoning = Column(Text, nullable=True)
    strategy_type = Column(String, nullable=True)
    confidence_level = Column(String, nullable=True)
    key_metrics_snapshot = Column(JSON, nullable=True)
    qualitative_sources_summary = Column(JSON, nullable=True)

    stock = relationship("Stock", back_populates="analyses")


class IPO(Base):
    __tablename__ = "ipos"
    id = Column(Integer, primary_key=True, index=True)
    company_name = Column(String, index=True, nullable=False)
    symbol = Column(String, index=True, nullable=True)
    ipo_date_str = Column(String, nullable=True)
    ipo_date = Column(Date, nullable=True)
    expected_price_range_low = Column(Float, nullable=True)
    expected_price_range_high = Column(Float, nullable=True)
    expected_price_currency = Column(String, nullable=True, default="USD")
    offered_shares = Column(Float, nullable=True)
    total_shares_value = Column(Float, nullable=True)
    exchange = Column(String, nullable=True)
    status = Column(String, nullable=True)
    cik = Column(String, nullable=True, index=True)
    last_analysis_date = Column(DateTime(timezone=True),
                                default=lambda: datetime.now(timezone.utc),
                                onupdate=lambda: datetime.now(timezone.utc))
    s1_filing_url = Column(String, nullable=True)

    analyses = relationship("IPOAnalysis", back_populates="ipo", cascade="all, delete-orphan")
    __table_args__ = (UniqueConstraint('company_name', 'ipo_date_str', 'symbol', name='uq_ipo_name_date_symbol'),)


class IPOAnalysis(Base):
    __tablename__ = "ipo_analyses"
    id = Column(Integer, primary_key=True, index=True)
    ipo_id = Column(Integer, ForeignKey("ipos.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    s1_business_summary = Column(Text, nullable=True)
    s1_risk_factors_summary = Column(Text, nullable=True)
    s1_mda_summary = Column(Text, nullable=True)
    s1_financial_health_summary = Column(Text, nullable=True)
    competitive_landscape_summary = Column(Text, nullable=True)
    industry_outlook_summary = Column(Text, nullable=True)
    management_team_assessment = Column(Text, nullable=True)
    use_of_proceeds_summary = Column(Text, nullable=True)
    underwriter_quality_assessment = Column(String, nullable=True)
    business_model_summary = Column(Text, nullable=True)
    risk_factors_summary = Column(Text, nullable=True)
    pre_ipo_financials_summary = Column(Text, nullable=True)
    valuation_comparison_summary = Column(Text, nullable=True)
    investment_decision = Column(String, nullable=True)
    reasoning = Column(Text, nullable=True)
    key_data_snapshot = Column(JSON, nullable=True)
    s1_sections_used = Column(JSON, nullable=True)

    ipo = relationship("IPO", back_populates="analyses")


class NewsEvent(Base):
    __tablename__ = "news_events"
    id = Column(Integer, primary_key=True, index=True)
    event_title = Column(String, index=True)
    event_date = Column(DateTime(timezone=True), nullable=True)
    source_url = Column(String, unique=True, nullable=False, index=True)
    source_name = Column(String, nullable=True)
    category = Column(String, nullable=True)
    processed_date = Column(DateTime(timezone=True),
                            default=lambda: datetime.now(timezone.utc))
    last_analyzed_date = Column(DateTime(timezone=True), nullable=True,
                                onupdate=lambda: datetime.now(timezone.utc))
    full_article_text = Column(Text, nullable=True)

    analyses = relationship("NewsEventAnalysis", back_populates="news_event", cascade="all, delete-orphan")
    __table_args__ = (UniqueConstraint('source_url', name='uq_news_source_url'),)


class NewsEventAnalysis(Base):
    __tablename__ = "news_event_analyses"
    id = Column(Integer, primary_key=True, index=True)
    news_event_id = Column(Integer, ForeignKey("news_events.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    sentiment = Column(String, nullable=True)
    sentiment_reasoning = Column(Text, nullable=True)
    affected_stocks_explicit = Column(JSON, nullable=True)
    affected_sectors_explicit = Column(JSON, nullable=True)
    news_summary_detailed = Column(Text, nullable=True)
    potential_impact_on_market = Column(Text, nullable=True)
    potential_impact_on_companies = Column(Text, nullable=True)
    potential_impact_on_sectors = Column(Text, nullable=True)
    mechanism_of_impact = Column(Text, nullable=True)
    estimated_timing_duration = Column(String, nullable=True)
    estimated_magnitude_direction = Column(String, nullable=True)
    confidence_of_assessment = Column(String, nullable=True)
    summary_for_email = Column(Text, nullable=True)
    key_news_snippets = Column(JSON, nullable=True)

    news_event = relationship("NewsEvent", back_populates="analyses")


class CachedAPIData(Base):
    __tablename__ = "cached_api_data"
    id = Column(Integer, primary_key=True, index=True)
    api_source = Column(String, index=True, nullable=False)
    request_url_or_params = Column(String, unique=True, nullable=False, index=True)
    response_data = Column(JSON, nullable=False)
    timestamp = Column(DateTime(timezone=True),
                       default=lambda: datetime.now(timezone.utc))
    expires_at = Column(DateTime(timezone=True), nullable=False, index=True)
---------- END models.py ----------


---------- __init__.py ----------
# services/ipo_analyzer/__init__.py
from .ipo_analyzer import IPOAnalyzer

__all__ = ["IPOAnalyzer"]
---------- END __init__.py ----------


---------- ai_analyzer.py ----------
# services/ipo_analyzer/ai_analyzer.py
import time
import json  # For JSON parsing
from api_clients import extract_S1_text_sections
from core.logging_setup import logger
from core.config import S1_KEY_SECTIONS, SUMMARIZATION_CHUNK_SIZE_CHARS, AI_JSON_OUTPUT_INSTRUCTION


def _parse_generic_ai_json_response(ai_response_data, expected_keys_map,
                                    default_error_msg="AI Error or No Valid JSON."):
    """
    Generic parser for AI JSON responses.
    ai_response_data: The direct output from Gemini (expected to be a dict if successful).
    expected_keys_map: A dict mapping desired output keys to keys expected in AI's JSON.
                       e.g., {"business_model_summary": "businessModel"}
    """
    parsed_output = {key: default_error_msg for key in expected_keys_map.keys()}

    if not isinstance(ai_response_data, dict):
        logger.error(f"AI response is not a dictionary: {str(ai_response_data)[:200]}")
        return parsed_output  # All fields will have default_error_msg

    if ai_response_data.get("error"):
        logger.error(f"AI returned an error in JSON: {ai_response_data.get('error')}")
        # Propagate the error to all expected fields
        error_detail = ai_response_data.get('details', default_error_msg)
        for key in expected_keys_map.keys():
            parsed_output[key] = f"AI Error: {error_detail}"
        return parsed_output

    for target_key, ai_json_key_or_path in expected_keys_map.items():
        if isinstance(ai_json_key_or_path, str):  # Simple key
            value = ai_response_data.get(ai_json_key_or_path)
        elif isinstance(ai_json_key_or_path, list):  # Path for nested key e.g. ["section", "subsection"]
            temp_val = ai_response_data
            try:
                for k_part in ai_json_key_or_path:
                    temp_val = temp_val[k_part]
                value = temp_val
            except (KeyError, TypeError):
                value = None
        else:
            value = None

        if value is not None:
            # If value is a dict/list, convert back to string for text fields, or keep as is for JSON fields
            # This depends on how the final `analysis_payload` is structured and DB model.
            # For now, assume we want the structured data if it's complex, or text if it's simple.
            parsed_output[target_key] = value  # Store the direct value (could be string, list, dict)
        else:
            logger.warning(f"Key '{ai_json_key_or_path}' not found in AI JSON response for target '{target_key}'.")
            # Keep default_error_msg or a more specific "Key not found" message
            parsed_output[target_key] = f"Key '{ai_json_key_or_path}' not found in AI response."

    return parsed_output


def perform_ai_analysis_for_ipo(analyzer_instance, ipo_db_entry, s1_text, s1_url, ipo_api_data_raw):
    analysis_payload = {
        "key_data_snapshot": ipo_api_data_raw.copy() if ipo_api_data_raw else {},  # Make a copy to modify
        "s1_sections_used": {}
    }

    # Add s1_url to key_data_snapshot if available
    if s1_url:
        analysis_payload["key_data_snapshot"]["s1_filing_url_from_analysis"] = s1_url

    # Ensure all S1_KEY_SECTIONS are reported in s1_sections_used
    analysis_payload["s1_sections_used"] = {key_name: False for key_name in S1_KEY_SECTIONS.keys()}
    s1_sections = {}
    if s1_text:
        extracted_s1_data = extract_S1_text_sections(s1_text, S1_KEY_SECTIONS)
        s1_sections = extracted_s1_data # Keep the extracted text for use in prompts
        for key_name in S1_KEY_SECTIONS.keys():
            analysis_payload["s1_sections_used"][key_name] = bool(extracted_s1_data.get(key_name))
    else:
        # Already initialized to False above
        pass


    company_prompt_id = f"{ipo_db_entry.company_name} ({ipo_db_entry.symbol or 'N/A'})"
    max_section_len_for_prompt = SUMMARIZATION_CHUNK_SIZE_CHARS // 3  # Divide among sections

    biz_text_for_prompt = (s1_sections.get("business", "") or "")[:max_section_len_for_prompt]
    risk_text_for_prompt = (s1_sections.get("risk_factors", "") or "")[:max_section_len_for_prompt]
    mda_text_for_prompt = (s1_sections.get("mda", "") or "")[:max_section_len_for_prompt]

    prompt_context_parts = [f"IPO Analysis for: {company_prompt_id}"]
    if biz_text_for_prompt: prompt_context_parts.append(
        f"S-1 Business Summary Extract (truncated):\n {biz_text_for_prompt}...")
    if risk_text_for_prompt: prompt_context_parts.append(
        f"S-1 Risk Factors Extract (truncated):\n {risk_text_for_prompt}...")
    if mda_text_for_prompt: prompt_context_parts.append(f"S-1 MD&A Extract (truncated):\n {mda_text_for_prompt}...")

    # Use the modified key_data_snapshot (which might now include s1_filing_url_from_analysis)
    context_ipo_data = {k: analysis_payload["key_data_snapshot"].get(k) for k in
                        ["name", "symbol", "date", "price", "exchange", "status", "numberOfShares", "totalSharesValue", "s1_filing_url_from_analysis"]
                        if analysis_payload["key_data_snapshot"].get(k)}
    if context_ipo_data:
        prompt_context_parts.append(f"IPO Calendar Data (and S-1 URL if found): {json.dumps(context_ipo_data)}")
    full_prompt_context = "\n\n".join(prompt_context_parts)

    # --- Prompt 1: Business, Competition, Industry ---
    json_structure_prompt1 = """
    {
      "businessModel": {"summary": "Core operations, products/services, revenue generation model."},
      "competitiveLandscape": {"summary": "Key competitors, company's market position, differentiation."},
      "industryOutlook": {"summary": "Relevant industry trends, growth prospects, challenges."}
    }
    """
    prompt1_instruction = (
        f"{full_prompt_context}\n\n"
        f"Based on S-1 info (if provided) and IPO calendar data, analyze the IPO candidate. {AI_JSON_OUTPUT_INSTRUCTION}\n"
        f"Structure your JSON response with these exact top-level keys: \"businessModel\", \"competitiveLandscape\", \"industryOutlook\". Each should contain a \"summary\" field as a string."
        f"Example structure: {json_structure_prompt1}"
    )
    response1_data = analyzer_instance.gemini.generate_text(prompt1_instruction, output_format="json")
    time.sleep(1)

    parsed_response1 = _parse_generic_ai_json_response(response1_data, {
        "s1_business_summary": ["businessModel", "summary"],
        "competitive_landscape_summary": ["competitiveLandscape", "summary"],
        "industry_outlook_summary": ["industryOutlook", "summary"]
    })
    analysis_payload.update(parsed_response1)
    analysis_payload["business_model_summary"] = parsed_response1.get("s1_business_summary", "AI Error")

    # --- Prompt 2: Risks, Use of Proceeds, Financials ---
    json_structure_prompt2 = """
    {
      "keyRiskFactors": {"summary": "Top 3-5 specific risks from S-1, not generic ones. Explain potential impact."},
      "useOfIPOProceeds": {"summary": "How the company plans to use the funds raised."},
      "financialHealthSummary": {"summary": "Key financial performance trends (revenue, profit, burn rate), profitability, debt, liquidity from MD&A or inferred. If financials are missing from S-1, explicitly state this and the implications."}
    }
    """
    prompt2_instruction = (
        f"{full_prompt_context}\n\n"
        f"Analyze the IPO candidate. {AI_JSON_OUTPUT_INSTRUCTION}\n"
        f"Structure your JSON response with these exact top-level keys: \"keyRiskFactors\", \"useOfIPOProceeds\", \"financialHealthSummary\". Each should contain a \"summary\" field as a string."
        f"Example structure: {json_structure_prompt2}"
    )
    response2_data = analyzer_instance.gemini.generate_text(prompt2_instruction, output_format="json")
    time.sleep(1)

    parsed_response2 = _parse_generic_ai_json_response(response2_data, {
        "s1_risk_factors_summary": ["keyRiskFactors", "summary"],
        "use_of_proceeds_summary": ["useOfIPOProceeds", "summary"],
        "s1_financial_health_summary": ["financialHealthSummary", "summary"]
    })
    analysis_payload.update(parsed_response2)
    analysis_payload["risk_factors_summary"] = parsed_response2.get("s1_risk_factors_summary", "AI Error")
    analysis_payload["pre_ipo_financials_summary"] = parsed_response2.get("s1_financial_health_summary", "AI Error")
    analysis_payload["s1_mda_summary"] = parsed_response2.get("s1_financial_health_summary", "AI Error") # Often combined

    # --- Synthesis Prompt: Investment Decision and Reasoning ---
    synthesis_context_parts = [
        f"Synthesize an IPO investment perspective for {company_prompt_id} using the following information and previously analyzed S-1 summaries."
    ]
    # Add snippets of previously generated summaries (if successful)
    for key, display_name in [
        ("s1_business_summary", "Business Model Snippet"),
        ("s1_risk_factors_summary", "Key Risks Snippet"),
        ("s1_financial_health_summary", "Financial Health Snippet (MD&A based)")]: # Clarified source
        summary_text = analysis_payload.get(key)
        if summary_text and not isinstance(summary_text,
                                           dict) and "AI Error" not in summary_text and "not found" not in summary_text:
            synthesis_context_parts.append(f"{display_name}: {str(summary_text)[:250]}...")  # Ensure it's a string

    json_structure_synthesis = """
    {
      "investmentStance": "Monitor Closely|Potentially Attractive (with caveats)|High Risk/Speculative|Avoid|Further Diligence Required",
      "reasoning": ["Bullet point 1 explaining the stance...", "Bullet point 2..."],
      "criticalVerificationPoints": ["Specific item 1 to verify (e.g., if financials were missing, point this out as critical)...", "Specific item 2..."]
    }
    """
    synthesis_prompt_instruction = (
        "\n\nBased on the above, provide your investment perspective. "
        f"{AI_JSON_OUTPUT_INSTRUCTION}\n"
        f"Structure your JSON response with these exact keys: \"investmentStance\" (string), \"reasoning\" (list of strings), \"criticalVerificationPoints\" (list of strings)."
        f"Example structure: {json_structure_synthesis}"
    )
    full_synthesis_prompt = "\n\n".join(synthesis_context_parts) + synthesis_prompt_instruction

    synthesis_response_data = analyzer_instance.gemini.generate_text(full_synthesis_prompt, output_format="json")
    time.sleep(1)

    parsed_synthesis = _parse_generic_ai_json_response(synthesis_response_data, {
        "investment_decision": "investmentStance",
        "reasoning_points_list": "reasoning",  # Keep as list for now
        "critical_verification_points_list": "criticalVerificationPoints"
    })

    analysis_payload["investment_decision"] = parsed_synthesis.get("investment_decision", "Review AI Output")

    # Format reasoning and critical points into a single string for the DB 'reasoning' field
    reasoning_str_parts = []
    if isinstance(parsed_synthesis.get("reasoning_points_list"), list):
        reasoning_str_parts.append(
            "Reasoning:\n" + "\n".join([f"- {p}" for p in parsed_synthesis["reasoning_points_list"]]))
    elif parsed_synthesis.get("reasoning_points_list", "").startswith("AI Error"):  # If it's an error string
        reasoning_str_parts.append(f"Reasoning: {parsed_synthesis['reasoning_points_list']}")
    elif parsed_synthesis.get("reasoning_points_list"): # If it's just a string
        reasoning_str_parts.append(f"Reasoning:\n- {parsed_synthesis['reasoning_points_list']}")


    if isinstance(parsed_synthesis.get("critical_verification_points_list"), list):
        reasoning_str_parts.append("\nCritical Verification Points:\n" + "\n".join(
            [f"- {p}" for p in parsed_synthesis["critical_verification_points_list"]]))
    elif parsed_synthesis.get("critical_verification_points_list", "").startswith("AI Error"):
        reasoning_str_parts.append(
            f"\nCritical Verification Points: {parsed_synthesis['critical_verification_points_list']}")
    elif parsed_synthesis.get("critical_verification_points_list"): # If it's just a string
        reasoning_str_parts.append(f"\nCritical Verification Points:\n- {parsed_synthesis['critical_verification_points_list']}")


    analysis_payload["reasoning"] = "\n".join(reasoning_str_parts).strip()
    if not analysis_payload["reasoning"]:  # Fallback if parsing fails badly
        analysis_payload["reasoning"] = str(synthesis_response_data) if isinstance(synthesis_response_data,
                                                                                   dict) and synthesis_response_data.get(
            "error") else "AI reasoning synthesis failed."

    # Placeholders for other qualitative assessments
    analysis_payload["management_team_assessment"] = "Not explicitly analyzed by AI in this version."
    analysis_payload["underwriter_quality_assessment"] = "Not explicitly analyzed by AI in this version."
    analysis_payload[
        "valuation_comparison_summary"] = "Detailed valuation comparison not performed by AI in this version."

    return analysis_payload
---------- END ai_analyzer.py ----------


---------- data_fetcher.py ----------
# services/ipo_analyzer/data_fetcher.py
import time
from datetime import datetime, timedelta, timezone
from core.logging_setup import logger
from .helpers import parse_ipo_date_string
from sqlalchemy.exc import SQLAlchemyError


def fetch_upcoming_ipo_data(analyzer_instance):
    """Fetches upcoming IPO data from Finnhub."""
    logger.info("Fetching upcoming IPOs using Finnhub...")
    ipos_data_to_process = []
    today = datetime.now(timezone.utc)
    # Look back 60 days and forward 180 days for IPOs
    from_date = (today - timedelta(days=60)).strftime('%Y-%m-%d')
    to_date = (today + timedelta(days=180)).strftime('%Y-%m-%d')

    finnhub_response = analyzer_instance.finnhub.get_ipo_calendar(from_date=from_date, to_date=to_date)
    actual_ipo_list = []

    if finnhub_response and isinstance(finnhub_response, dict) and "ipoCalendar" in finnhub_response:
        actual_ipo_list = finnhub_response["ipoCalendar"]
        if not isinstance(actual_ipo_list, list):
            logger.warning(f"Finnhub response 'ipoCalendar' field is not a list. Found: {type(actual_ipo_list)}")
            actual_ipo_list = []  # Reset to empty list if not a list
        elif not actual_ipo_list:
            logger.info("Finnhub 'ipoCalendar' list is empty for the current period.")
    elif finnhub_response is None:  # Explicit check for None, indicating API failure handled by base_client
        logger.error("Failed to fetch IPOs from Finnhub (API call failed or returned None).")
    else:  # Other unexpected formats
        logger.info(f"No IPOs found or unexpected format from Finnhub. Response: {str(finnhub_response)[:200]}")

    if actual_ipo_list:  # Ensure it's a list and has items
        for ipo_api_data in actual_ipo_list:
            if not isinstance(ipo_api_data, dict):  # Skip if an item is not a dictionary
                logger.warning(f"Skipping non-dictionary item in Finnhub IPO calendar: {ipo_api_data}")
                continue

            price_range_raw = ipo_api_data.get("price")
            price_low, price_high = None, None
            if isinstance(price_range_raw, str) and price_range_raw.strip():  # e.g., "10.0-12.0" or "15.0"
                parts = price_range_raw.split('-', 1)
                try:
                    price_low = float(parts[0].strip())
                except:
                    pass  # pylint: disable=bare-except
                try:
                    price_high = float(parts[1].strip()) if len(parts) > 1 and parts[1].strip() else price_low
                except:
                    price_high = price_low if price_low is not None else None  # pylint: disable=bare-except
            elif isinstance(price_range_raw, (float, int)):  # If it's just a number
                price_low = float(price_range_raw)
                price_high = float(price_range_raw)

            parsed_date = parse_ipo_date_string(ipo_api_data.get("date"))

            ipos_data_to_process.append({
                "company_name": ipo_api_data.get("name"),
                "symbol": ipo_api_data.get("symbol"),
                "ipo_date_str": ipo_api_data.get("date"),  # Original string for DB
                "ipo_date": parsed_date,  # Parsed date object
                "expected_price_range_low": price_low,
                "expected_price_range_high": price_high,
                "exchange": ipo_api_data.get("exchange"),
                "status": ipo_api_data.get("status"),
                "offered_shares": ipo_api_data.get("numberOfShares"),  # Finnhub field name
                "total_shares_value": ipo_api_data.get("totalSharesValue"),  # Finnhub field name
                "source_api": "Finnhub",  # To track where the data came from
                "raw_data": ipo_api_data  # Store the raw dict for snapshot
            })
        logger.info(f"Successfully parsed {len(ipos_data_to_process)} IPOs from Finnhub API response.")

    # Deduplicate IPOs based on a composite key (name, symbol, date string)
    # This is important if multiple sources are ever combined or if an API returns duplicates
    unique_ipos = []
    seen_keys = set()
    for ipo_info in ipos_data_to_process:
        # Normalize key parts for better matching
        key_name = ipo_info.get("company_name", "").strip().lower() if ipo_info.get(
            "company_name") else "unknown_company"
        key_symbol = ipo_info.get("symbol", "").strip().upper() if ipo_info.get(
            "symbol") else "NO_SYMBOL"  # Handle missing symbols
        key_date = ipo_info.get("ipo_date_str", "")  # Use original date string for uniqueness

        unique_tuple = (key_name, key_symbol, key_date)
        if unique_tuple not in seen_keys:
            unique_ipos.append(ipo_info)
            seen_keys.add(unique_tuple)

    logger.info(f"Total unique IPOs fetched after deduplication: {len(unique_ipos)}")
    return unique_ipos


def fetch_s1_filing_data(analyzer_instance, db_session, ipo_db_entry):
    """Fetches S-1 filing text for a given IPO DB entry."""
    if not ipo_db_entry:
        return None, None

    target_cik = ipo_db_entry.cik

    # If CIK is not in the DB entry, try to get it using the symbol
    if not target_cik:
        if ipo_db_entry.symbol:
            target_cik = analyzer_instance.sec_edgar.get_cik_by_ticker(ipo_db_entry.symbol)
            time.sleep(0.5)  # SEC EDGAR rate limiting
            if target_cik:
                ipo_db_entry.cik = target_cik  # Update DB entry with found CIK
                try:
                    db_session.commit()
                except SQLAlchemyError as e:  # Catch potential commit errors
                    db_session.rollback()
                    logger.error(f"Failed to update CIK for {ipo_db_entry.company_name}: {e}")
            else:
                logger.warning(f"No CIK found via symbol {ipo_db_entry.symbol} for IPO '{ipo_db_entry.company_name}'.")
                return None, None  # Cannot proceed without CIK
        else:
            logger.warning(f"No CIK or symbol available for IPO '{ipo_db_entry.company_name}'. Cannot fetch S-1.")
            return None, None  # Cannot proceed

    logger.info(f"Attempting to fetch S-1/F-1 for {ipo_db_entry.company_name} (CIK: {target_cik})")
    s1_url = None
    # Try common S-1 and F-1 forms (including amendments)
    for form_type in ["S-1", "S-1/A", "F-1", "F-1/A"]:
        s1_url = analyzer_instance.sec_edgar.get_filing_document_url(cik=target_cik, form_type=form_type)
        time.sleep(0.5)  # SEC EDGAR rate limiting
        if s1_url:
            logger.info(f"Found {form_type} URL for {ipo_db_entry.company_name}: {s1_url}")
            break

    if s1_url:
        # Update the s1_filing_url in the database if it's new or different
        if ipo_db_entry.s1_filing_url != s1_url:
            ipo_db_entry.s1_filing_url = s1_url
            try:
                db_session.commit()
            except SQLAlchemyError as e:
                db_session.rollback()
                logger.warning(f"Failed to update S1 filing URL for {ipo_db_entry.company_name} due to: {e}")

        filing_text = analyzer_instance.sec_edgar.get_filing_text(s1_url)
        if filing_text:
            logger.info(f"Fetched S-1/F-1 text (length: {len(filing_text)}) for {ipo_db_entry.company_name}")
            return filing_text, s1_url
        else:
            logger.warning(f"Failed to fetch S-1/F-1 text from {s1_url}")
    else:
        logger.warning(f"No S-1 or F-1 URL found for {ipo_db_entry.company_name} (CIK: {target_cik}).")

    return None, None  # Return None if no text or URL found
---------- END data_fetcher.py ----------


---------- db_handler.py ----------
# services/ipo_analyzer/db_handler.py
import time
from database import IPO
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError


def get_or_create_ipo_db_entry(analyzer_instance, db_session, ipo_data_from_fetch):
    """Gets an existing IPO entry from the DB or creates a new one."""
    ipo_db_entry = None

    # Try to find by symbol first, as it's more likely to be unique if present
    if ipo_data_from_fetch.get("symbol"):
        ipo_db_entry = db_session.query(IPO).filter(IPO.symbol == ipo_data_from_fetch["symbol"]).first()

    # If not found by symbol, try by company name and IPO date string (original date string)
    if not ipo_db_entry and ipo_data_from_fetch.get("company_name") and ipo_data_from_fetch.get("ipo_date_str"):
        ipo_db_entry = db_session.query(IPO).filter(
            IPO.company_name == ipo_data_from_fetch["company_name"],
            IPO.ipo_date_str == ipo_data_from_fetch["ipo_date_str"]  # Match on the original string
        ).first()

    # Try to get CIK if not already available
    cik_to_store = ipo_data_from_fetch.get("cik")  # CIK might come from data fetch
    if not cik_to_store and ipo_data_from_fetch.get("symbol"):
        # If no CIK from fetched data, try to get it using symbol
        cik_to_store = analyzer_instance.sec_edgar.get_cik_by_ticker(ipo_data_from_fetch["symbol"])
        time.sleep(0.5)  # SEC EDGAR rate limiting
    elif not cik_to_store and ipo_db_entry and ipo_db_entry.symbol and not ipo_db_entry.cik:
        # If DB entry exists but has no CIK, try to get it
        cik_to_store = analyzer_instance.sec_edgar.get_cik_by_ticker(ipo_db_entry.symbol)
        time.sleep(0.5)

    if not ipo_db_entry:
        logger.info(f"IPO '{ipo_data_from_fetch.get('company_name')}' not found in DB, creating new entry.")
        ipo_db_entry = IPO(
            company_name=ipo_data_from_fetch.get("company_name"),
            symbol=ipo_data_from_fetch.get("symbol"),
            ipo_date_str=ipo_data_from_fetch.get("ipo_date_str"),
            ipo_date=ipo_data_from_fetch.get("ipo_date"),  # Parsed date
            expected_price_range_low=ipo_data_from_fetch.get("expected_price_range_low"),
            expected_price_range_high=ipo_data_from_fetch.get("expected_price_range_high"),
            offered_shares=ipo_data_from_fetch.get("offered_shares"),
            total_shares_value=ipo_data_from_fetch.get("total_shares_value"),
            exchange=ipo_data_from_fetch.get("exchange"),
            status=ipo_data_from_fetch.get("status"),
            cik=cik_to_store
        )
        db_session.add(ipo_db_entry)
        try:
            db_session.commit()
            db_session.refresh(ipo_db_entry)  # To get ID and other defaults
            logger.info(
                f"Created IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}, CIK: {ipo_db_entry.cik})")
        except SQLAlchemyError as e:
            db_session.rollback()
            logger.error(f"Error creating IPO DB entry for '{ipo_data_from_fetch.get('company_name')}': {e}",
                         exc_info=True)
            return None  # Return None if creation fails
    else:
        # IPO entry exists, update it if necessary
        updated = False
        fields_to_update = [
            "company_name", "symbol", "ipo_date_str", "ipo_date",
            "expected_price_range_low", "expected_price_range_high",
            "offered_shares", "total_shares_value", "exchange", "status"
        ]
        for field in fields_to_update:
            new_val = ipo_data_from_fetch.get(field)
            # Check if new_val is not None to avoid overwriting existing data with None
            # Also check if the value has actually changed
            if new_val is not None and getattr(ipo_db_entry, field) != new_val:
                setattr(ipo_db_entry, field, new_val)
                updated = True

        # Update CIK if a new one was found and it's different or was missing
        if cik_to_store and (ipo_db_entry.cik != cik_to_store or not ipo_db_entry.cik):
            ipo_db_entry.cik = cik_to_store
            updated = True

        if updated:
            try:
                db_session.commit()
                db_session.refresh(ipo_db_entry)
                logger.info(f"Updated IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}).")
            except SQLAlchemyError as e:
                db_session.rollback()
                logger.error(f"Error updating IPO DB entry for '{ipo_db_entry.company_name}': {e}", exc_info=True)
                # Potentially return the existing, un-updated entry or None depending on desired behavior

    return ipo_db_entry
---------- END db_handler.py ----------


---------- helpers.py ----------
# services/ipo_analyzer/helpers.py
from dateutil import parser as date_parser
from core.logging_setup import logger

def parse_ipo_date_string(date_str):
    if not date_str:
        return None
    try:
        # date_parser is quite flexible
        return date_parser.parse(date_str).date()
    except (ValueError, TypeError) as e:
        logger.warning(f"Could not parse IPO date string '{date_str}': {e}")
        return None
---------- END helpers.py ----------


---------- ipo_analyzer.py ----------
# services/ipo_analyzer/ipo_analyzer.py
# services/ipo_analyzer/ipo_analyzer.py
import time
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timedelta, timezone
import concurrent.futures

from api_clients import FinnhubClient, GeminiAPIClient, SECEDGARClient
from database import SessionLocal, IPO, IPOAnalysis  # Removed get_db_session, manage session per thread
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError
from core.config import IPO_ANALYSIS_REANALYZE_DAYS

# Import functions from submodules
from .helpers import parse_ipo_date_string
from .data_fetcher import fetch_upcoming_ipo_data, fetch_s1_filing_data
from .db_handler import get_or_create_ipo_db_entry
from .ai_analyzer import perform_ai_analysis_for_ipo

MAX_IPO_ANALYSIS_WORKERS = 1  # Module-level constant, adjust as needed based on API limits and system resources


class IPOAnalyzer:
    def __init__(self):
        self.finnhub = FinnhubClient()
        self.gemini = GeminiAPIClient()
        self.sec_edgar = SECEDGARClient()
        # Note: IPOAnalyzer orchestrates tasks, each task/thread will get its own DB session.

    def _analyze_single_ipo_task(self, db_session, ipo_data_from_fetch):
        """
        Analyzes a single IPO. This method is called by the thread worker.
        It uses the provided db_session.
        """
        ipo_identifier = ipo_data_from_fetch.get("company_name") or ipo_data_from_fetch.get("symbol") or "Unknown IPO"
        logger.info(
            f"Task: Starting analysis for IPO: {ipo_identifier} from source {ipo_data_from_fetch.get('source_api')}")

        ipo_db_entry = get_or_create_ipo_db_entry(self, db_session, ipo_data_from_fetch)
        if not ipo_db_entry:
            logger.error(
                f"Task: Could not get/create DB entry for IPO {ipo_identifier}. Aborting analysis for this item.")
            return None

        # Check if recent analysis exists and if significant data has changed
        reanalyze_threshold = datetime.now(timezone.utc) - timedelta(days=IPO_ANALYSIS_REANALYZE_DAYS)
        existing_analysis = db_session.query(IPOAnalysis).filter(IPOAnalysis.ipo_id == ipo_db_entry.id).order_by(
            IPOAnalysis.analysis_date.desc()).first()

        significant_change_detected = False
        if existing_analysis and existing_analysis.key_data_snapshot:
            snap = existing_analysis.key_data_snapshot
            snap_parsed_date = parse_ipo_date_string(snap.get("date"))

            # Compare key fields for significant changes
            if (ipo_db_entry.ipo_date != snap_parsed_date or
                    ipo_db_entry.status != snap.get("status") or
                    ipo_db_entry.expected_price_range_low != (
                            parse_ipo_date_string(snap.get("price").split('-')[0]) if isinstance(snap.get("price"),
                                                                                                 str) and '-' in snap.get(
                                "price") else snap.get("price")) or
                    # Crude price parsing for snapshot; assumes a simple number or "low-high"
                    ipo_db_entry.expected_price_range_high != (
                            parse_ipo_date_string(snap.get("price").split('-')[1]) if isinstance(snap.get("price"),
                                                                                                 str) and '-' in snap.get(
                                "price") and len(snap.get("price").split('-')) > 1 else snap.get("price"))):
                significant_change_detected = True
                logger.info(f"Task: Significant data change detected for {ipo_identifier}. Re-analyzing.")

        if existing_analysis and not significant_change_detected and existing_analysis.analysis_date >= reanalyze_threshold:
            logger.info(
                f"Task: Recent analysis for {ipo_identifier} exists (Date: {existing_analysis.analysis_date}) and no significant changes. Skipping re-analysis.")
            return existing_analysis

        s1_text, s1_url = fetch_s1_filing_data(self, db_session, ipo_db_entry)

        # Pass s1_url to the AI analysis function
        analysis_payload = perform_ai_analysis_for_ipo(self, ipo_db_entry, s1_text, s1_url,
                                                       ipo_data_from_fetch.get("raw_data", {}))

        current_time = datetime.now(timezone.utc)

        if existing_analysis:
            logger.info(f"Task: Updating existing analysis for {ipo_identifier} (ID: {existing_analysis.id})")
            for key, value in analysis_payload.items():
                setattr(existing_analysis, key, value)
            existing_analysis.analysis_date = current_time
            entry_to_save = existing_analysis
        else:
            logger.info(f"Task: Creating new analysis for {ipo_identifier}")
            entry_to_save = IPOAnalysis(
                ipo_id=ipo_db_entry.id,
                analysis_date=current_time,
                **analysis_payload
            )
            db_session.add(entry_to_save)

        ipo_db_entry.last_analysis_date = current_time

        try:
            db_session.commit()
            logger.info(f"Task: Saved IPO analysis for {ipo_identifier} (Analysis ID: {entry_to_save.id})")
        except SQLAlchemyError as e:
            db_session.rollback()
            logger.error(f"Task: DB error saving IPO analysis for {ipo_identifier}: {e}", exc_info=True)
            return None

        return entry_to_save

    def run_ipo_analysis_pipeline(self, upcoming_only=False, max_to_analyze=None):
        all_ipos_from_api = fetch_upcoming_ipo_data(self)
        analyzed_results = []

        if not all_ipos_from_api:
            logger.info("No upcoming IPOs found from data fetcher to analyze.")
            return []

        # 1. Pre-filter for essential data (e.g., company name)
        pre_filtered_ipos = []
        for ipo_data in all_ipos_from_api:
            if not ipo_data.get("company_name"):
                logger.debug(f"Skipping IPO due to missing company name: {ipo_data.get('symbol', 'N/A')}")
                continue
            pre_filtered_ipos.append(ipo_data)

        if not pre_filtered_ipos:
            logger.info("No IPOs remain after pre-filtering for essential data.")
            return []

        # 2. Sort by IPO date (earliest first). Parsed 'ipo_date' (date object) is used.
        #    None dates or unparseable dates are pushed to the end.
        pre_filtered_ipos.sort(key=lambda x: x.get("ipo_date") or datetime.max.date())

        # 3. Apply 'upcoming_only' and status filtering
        relevant_ipos_to_process = []
        today_date = datetime.now(timezone.utc).date()
        # Finnhub statuses: "Expected", "Priced", "Filed", "Withdrawn"
        # "Priced" can be upcoming if its date is in the future.
        # "Filed" and "Expected" are generally upcoming.
        valid_statuses_for_analysis = ["expected", "filed", "priced", "upcoming", "active"]  # Generic upcoming statuses

        if upcoming_only:
            for ipo_data in pre_filtered_ipos:
                ipo_date_obj = ipo_data.get("ipo_date")  # This is a date object
                status = ipo_data.get("status", "").lower()
                if ipo_date_obj and ipo_date_obj >= today_date and \
                        status in valid_statuses_for_analysis and status != "withdrawn":
                    relevant_ipos_to_process.append(ipo_data)
            logger.info(
                f"Filtered for upcoming IPOs only: {len(relevant_ipos_to_process)} IPOs remain for potential analysis.")
        else:
            # If not 'upcoming_only', still filter out "withdrawn" and ensure a valid status
            for ipo_data in pre_filtered_ipos:
                status = ipo_data.get("status", "").lower()
                if status in valid_statuses_for_analysis and status != "withdrawn":
                    relevant_ipos_to_process.append(ipo_data)
            logger.info(
                f"Not filtering for upcoming only. {len(relevant_ipos_to_process)} IPOs with valid status remain for potential analysis.")

        if not relevant_ipos_to_process:
            logger.info("No IPOs left after upcoming/status filtering.")
            return []

        # 4. Apply 'max_to_analyze' limit
        if max_to_analyze is not None and isinstance(max_to_analyze, int) and max_to_analyze > 0:
            if len(relevant_ipos_to_process) > max_to_analyze:
                logger.info(
                    f"Limiting IPOs to analyze to the earliest {max_to_analyze} from {len(relevant_ipos_to_process)} relevant IPOs.")
                relevant_ipos_to_process = relevant_ipos_to_process[:max_to_analyze]
            else:
                logger.info(
                    f"Number of relevant IPOs ({len(relevant_ipos_to_process)}) is within or equal to max_to_analyze ({max_to_analyze}). Analyzing all {len(relevant_ipos_to_process)}.")
        else:
            logger.info(
                f"No limit set for max_to_analyze, proceeding with {len(relevant_ipos_to_process)} relevant IPOs.")

        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_IPO_ANALYSIS_WORKERS) as executor:
            future_to_ipo_data = {}
            for ipo_data_for_task in relevant_ipos_to_process:  # Use the filtered list
                future = executor.submit(self._thread_worker_analyze_ipo, ipo_data_for_task)
                future_to_ipo_data[future] = ipo_data_for_task.get("company_name", "Unknown IPO")

            for future in concurrent.futures.as_completed(future_to_ipo_data):
                ipo_name = future_to_ipo_data[future]
                try:
                    result = future.result()
                    if result:
                        analyzed_results.append(result)
                except Exception as exc:
                    logger.error(f"IPO analysis for '{ipo_name}' generated an exception in thread: {exc}",
                                 exc_info=True)

        logger.info(
            f"IPO analysis pipeline completed. Processed {len(analyzed_results)} IPOs that required new/updated analysis from the filtered set.")
        return analyzed_results

    def _thread_worker_analyze_ipo(self, ipo_data_from_fetch):
        """
        Worker function for each thread. Manages its own DB session.
        """
        db_session = SessionLocal()
        try:
            return self._analyze_single_ipo_task(db_session, ipo_data_from_fetch)
        finally:
            SessionLocal.remove()
---------- END ipo_analyzer.py ----------


---------- __init__.py ----------
# services/news_analyzer/__init__.py
from .news_analyzer import NewsAnalyzer

__all__ = ["NewsAnalyzer"]
---------- END __init__.py ----------


---------- ai_analyzer.py ----------
# services/news_analyzer/ai_analyzer.py
import time
import json
from core.logging_setup import logger
from core.config import NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION, AI_JSON_OUTPUT_INSTRUCTION


def perform_ai_analysis_for_news_item(analyzer_instance, news_event_db_obj):
    headline = news_event_db_obj.event_title
    content_for_analysis = news_event_db_obj.full_article_text
    analysis_source_type = "full article"

    if not content_for_analysis:
        content_for_analysis = headline
        analysis_source_type = "headline only"
        logger.warning(f"No full article text for '{headline}'. Analyzing based on headline only.")

    if len(content_for_analysis) > NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION:
        content_for_analysis = content_for_analysis[
                               :NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION] + "\n... [CONTENT TRUNCATED] ..."
        logger.info(f"Truncated news content for '{headline}' for Gemini.")
        analysis_source_type += " (truncated)"

    logger.info(f"Analyzing news (JSON): '{headline[:70]}...' (using {analysis_source_type})")
    analysis_payload = {"key_news_snippets": {"headline": headline, "source_type_used": analysis_source_type}}

    # 1. Sentiment Analysis
    sentiment_json_structure = """
    {
      "sentiment": "Positive|Negative|Neutral",
      "reasoning": "A brief 1-2 sentence explanation, citing specific phrases from the text if possible."
    }
    """
    sentiment_prompt = (
        f"Analyze the sentiment of the following text. Context: News headline '{headline}'.\n\n"
        f"Text to Analyze:\n\"\"\"\n{content_for_analysis}\n\"\"\"\n\n"
        f"{AI_JSON_OUTPUT_INSTRUCTION} Structure it as: {sentiment_json_structure}"
    )
    sentiment_response_data = analyzer_instance.gemini.generate_text(sentiment_prompt, output_format="json")
    time.sleep(1)

    if isinstance(sentiment_response_data, dict) and not sentiment_response_data.get("error"):
        analysis_payload["sentiment"] = sentiment_response_data.get("sentiment", "Error Parsing")
        analysis_payload["sentiment_reasoning"] = sentiment_response_data.get("reasoning",
                                                                              "AI Error: Reasoning not provided.")
    else:
        logger.warning(
            f"Sentiment analysis failed or returned non-JSON for '{headline}'. Response: {sentiment_response_data}")
        analysis_payload["sentiment"] = "AI Error"
        analysis_payload["sentiment_reasoning"] = str(sentiment_response_data)

    # 2. Detailed Impact Analysis
    impact_json_structure = """
    {
      "newsSummary": "Comprehensive yet concise summary (3-5 key sentences).",
      "affectedEntities": [
        {"entityName": "Company Name", "tickerSymbol": "TICKER (if known and highly relevant)", "explanation": "Brief explanation of impact."},
        {"sectorName": "Industry Sector", "explanation": "Brief explanation of impact."}
      ],
      "mechanismOfImpact": "How news likely affects fundamentals (revenue, costs, market share) or market perception.",
      "estimatedTimingAndDuration": {"timing": "Immediate|Short-term (<3mo)|Medium-term (3-12mo)|Long-term (>1yr)", "duration": "Brief|Extended|Ongoing|etc."},
      "estimatedMagnitudeAndDirection": {"magnitude": "Low|Medium|High", "direction": "Positive|Negative|Neutral/Mixed"},
      "confidenceLevel": "High|Medium|Low",
      "confidenceJustification": "Brief justification for confidence level (e.g., clarity of news, directness of impact).",
      "investorSummary": "Final 2-sentence summary for an investor: most critical implication/takeaway."
    }
    """
    prompt_detailed_analysis = (
        f"News Headline: \"{headline}\"\n"
        f"News Content (may be truncated or headline only): \"\"\"\n{content_for_analysis}\n\"\"\"\n\n"
        f"Perform a detailed impact analysis. {AI_JSON_OUTPUT_INSTRUCTION}\n"
        f"Structure your JSON response as follows: {impact_json_structure}"
    )
    impact_analysis_response_data = analyzer_instance.gemini.generate_text(prompt_detailed_analysis,
                                                                           output_format="json")
    time.sleep(1)

    if isinstance(impact_analysis_response_data, dict) and not impact_analysis_response_data.get("error"):
        analysis_payload["news_summary_detailed"] = impact_analysis_response_data.get("newsSummary", "AI Error")

        # Handle affected entities carefully - it's a list of dicts
        affected_entities = impact_analysis_response_data.get("affectedEntities", [])
        if isinstance(affected_entities, list):
            analysis_payload["potential_impact_on_companies"] = json.dumps(
                [e for e in affected_entities if "entityName" in e])  # Store as JSON string
            analysis_payload["potential_impact_on_sectors"] = json.dumps(
                [e for e in affected_entities if "sectorName" in e])  # Store as JSON string
        else:
            analysis_payload["potential_impact_on_companies"] = "[]"  # Empty JSON array string
            analysis_payload["potential_impact_on_sectors"] = "[]"

        analysis_payload["mechanism_of_impact"] = impact_analysis_response_data.get("mechanismOfImpact", "AI Error")

        timing_duration = impact_analysis_response_data.get("estimatedTimingAndDuration", {})
        analysis_payload[
            "estimated_timing_duration"] = f"Timing: {timing_duration.get('timing', 'N/A')}, Duration: {timing_duration.get('duration', 'N/A')}" if isinstance(
            timing_duration, dict) else "AI Error"

        mag_direction = impact_analysis_response_data.get("estimatedMagnitudeAndDirection", {})
        analysis_payload[
            "estimated_magnitude_direction"] = f"Magnitude: {mag_direction.get('magnitude', 'N/A')}, Direction: {mag_direction.get('direction', 'N/A')}" if isinstance(
            mag_direction, dict) else "AI Error"

        analysis_payload["confidence_of_assessment"] = impact_analysis_response_data.get("confidenceLevel", "AI Error")
        if impact_analysis_response_data.get("confidenceJustification"):
            analysis_payload[
                "confidence_of_assessment"] += f" (Justification: {impact_analysis_response_data.get('confidenceJustification')})"

        analysis_payload["summary_for_email"] = impact_analysis_response_data.get("investorSummary", "AI Error")
    else:
        logger.error(
            f"Gemini failed to provide detailed impact analysis JSON for '{headline}': {impact_analysis_response_data}")
        error_indicator = str(impact_analysis_response_data)
        analysis_payload["news_summary_detailed"] = error_indicator
        analysis_payload["potential_impact_on_companies"] = "[]"
        analysis_payload["potential_impact_on_sectors"] = "[]"
        analysis_payload["mechanism_of_impact"] = error_indicator
        analysis_payload["estimated_timing_duration"] = error_indicator
        analysis_payload["estimated_magnitude_direction"] = error_indicator
        analysis_payload["confidence_of_assessment"] = error_indicator
        analysis_payload["summary_for_email"] = error_indicator

    return analysis_payload
---------- END ai_analyzer.py ----------


---------- data_fetcher.py ----------
# services/news_analyzer/data_fetcher.py
import time
from api_clients import scrape_article_content  # scrape_article_content is generic
from core.logging_setup import logger
from core.config import MAX_NEWS_ARTICLES_PER_QUERY  # Use default from config if not overridden


def fetch_market_news_from_api(analyzer_instance, category="general",
                               count_to_fetch_from_api=MAX_NEWS_ARTICLES_PER_QUERY):
    """Fetches market news from the Finnhub API."""
    logger.info(f"Fetching latest market news for category: {category} (max {count_to_fetch_from_api} from API)...")
    news_items = analyzer_instance.finnhub.get_market_news(
        category=category)  # Finnhub `get_market_news` doesn't have a count param in current client

    if news_items and isinstance(news_items, list):
        logger.info(f"Fetched {len(news_items)} news items from Finnhub.")
        # The Finnhub client's get_market_news might already limit, but we can slice here if needed
        return news_items[:count_to_fetch_from_api]
    else:
        logger.warning(f"Failed to fetch news or received unexpected format from Finnhub: {news_items}")
        return []


def scrape_news_article_content(news_url):
    """Scrapes content for a given news URL."""
    if not news_url:
        return None
    logger.info(f"Attempting to scrape full article for: {news_url}")
    full_article_text = scrape_article_content(news_url)  # This is the generic scraper
    time.sleep(1)  # Small delay after scraping
    if full_article_text:
        logger.info(f"Scraped ~{len(full_article_text)} chars for {news_url}")
    else:
        logger.warning(f"Failed to scrape full article for {news_url}.")
    return full_article_text
---------- END data_fetcher.py ----------


---------- db_handler.py ----------
# services/news_analyzer/db_handler.py
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timezone
from database import NewsEvent
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError
from .data_fetcher import scrape_news_article_content  # For scraping if needed during get/create


def _ensure_news_event_session_is_active(analyzer_instance, news_identifier_for_log="Unknown News"):
    """Ensures the NewsAnalyzer's DB session is active, re-establishing if necessary."""
    if not analyzer_instance.db_session or not analyzer_instance.db_session.is_active:
        logger.warning(f"Session for News '{news_identifier_for_log}' was inactive/closed. Re-establishing.")
        analyzer_instance._close_session_if_active()  # Call the NewsAnalyzer's close method
        analyzer_instance.db_session = next(
            analyzer_instance.get_new_db_session_generator())  # Get a fresh session via NewsAnalyzer's method
        logger.info(f"Re-established DB session for NewsAnalyzer processing '{news_identifier_for_log}'.")


def _ensure_news_event_is_bound_to_session(analyzer_instance, news_event_db_obj):
    """Ensures a NewsEvent DB object is bound to the NewsAnalyzer's current session."""
    if not news_event_db_obj:
        return None

    news_title_for_log = news_event_db_obj.event_title[:50] if news_event_db_obj.event_title else 'Unknown News'
    _ensure_news_event_session_is_active(analyzer_instance, news_title_for_log)

    instance_state = sa_inspect(news_event_db_obj)

    # If object is not in any session OR in a different session, merge it.
    if not instance_state.session or instance_state.session is not analyzer_instance.db_session:
        obj_id_log = news_event_db_obj.id if instance_state.has_identity else 'Transient'
        logger.warning(
            f"NewsEvent DB entry '{news_title_for_log}...' (ID: {obj_id_log}) not bound to current session. Merging."
        )
        try:
            # If it's a new object not yet persisted (no ID) but we found an existing one by URL, use that one.
            if not instance_state.has_identity and news_event_db_obj.id is None and news_event_db_obj.source_url:
                existing_in_session_by_url = analyzer_instance.db_session.query(NewsEvent).filter_by(
                    source_url=news_event_db_obj.source_url).first()
                if existing_in_session_by_url:
                    logger.info(
                        f"Replaced transient NewsEvent for '{news_event_db_obj.source_url}' with instance (ID: {existing_in_session_by_url.id}) from session during binding."
                    )
                    return existing_in_session_by_url  # Return the one already in session

            # Proceed with merge for objects with identity or new objects not found by URL
            merged_event = analyzer_instance.db_session.merge(news_event_db_obj)
            logger.info(
                f"Successfully merged NewsEvent '{merged_event.event_title[:50]}...' (ID: {merged_event.id}) into session."
            )
            return merged_event
        except Exception as e_merge:
            logger.error(
                f"Failed to merge NewsEvent '{news_title_for_log}...' into session: {e_merge}. Attempting re-fetch.",
                exc_info=True
            )
            # Fallback: Try to re-fetch the object using its ID or a unique key if merge fails
            fallback_event = None
            if instance_state.has_identity and news_event_db_obj.id:  # If it had an ID
                fallback_event = analyzer_instance.db_session.query(NewsEvent).get(news_event_db_obj.id)
            elif news_event_db_obj.source_url:  # If it had a URL (unique constraint)
                fallback_event = analyzer_instance.db_session.query(NewsEvent).filter_by(
                    source_url=news_event_db_obj.source_url).first()

            if not fallback_event:
                logger.critical(
                    f"CRITICAL: Failed to re-associate NewsEvent '{news_title_for_log}...' with session after merge failure and re-fetch attempt.");
                return None  # Critical failure
            logger.info(
                f"Successfully re-fetched NewsEvent '{fallback_event.event_title[:50]}...' (ID: {fallback_event.id}) into session after merge failure."
            )
            return fallback_event

    return news_event_db_obj  # Object is already bound correctly


def get_or_create_news_event_db_entry(analyzer_instance, news_item_from_api):
    """
    Gets an existing NewsEvent from the DB or creates a new one.
    Manages scraping of full article text if not already present.
    Uses the NewsAnalyzer's managed db_session.
    """
    session_log_id = news_item_from_api.get('headline', 'Unknown News API Item')
    _ensure_news_event_session_is_active(analyzer_instance, session_log_id)

    source_url = news_item_from_api.get("url")
    if not source_url:
        logger.warning(f"News item missing URL, cannot process: {news_item_from_api.get('headline')}")
        return None

    event = analyzer_instance.db_session.query(NewsEvent).filter_by(source_url=source_url).first()

    full_article_text_scraped_this_time = None
    # Scrape if event doesn't exist, or if it exists but has no full_article_text
    if not event or (event and not event.full_article_text):
        full_article_text_scraped_this_time = scrape_news_article_content(source_url)

    current_time_utc = datetime.now(timezone.utc)
    if event:
        logger.debug(f"News event '{event.event_title[:70]}...' (URL: {source_url}) already in DB.")
        # If we scraped text now and the existing event didn't have it, update the event
        if full_article_text_scraped_this_time and not event.full_article_text:
            logger.info(f"Updating existing event {event.id} with newly scraped full article text.")
            event.full_article_text = full_article_text_scraped_this_time
            event.processed_date = current_time_utc  # Update processed date as we added content
            try:
                analyzer_instance.db_session.commit()
            except SQLAlchemyError as e:
                analyzer_instance.db_session.rollback()
                logger.error(f"Error updating full_article_text for existing event {source_url}: {e}")
        return event  # Return existing event (possibly updated)

    # If event does not exist, create a new one
    event_timestamp = news_item_from_api.get("datetime")  # Finnhub provides UNIX timestamp
    event_datetime_utc = datetime.fromtimestamp(event_timestamp, timezone.utc) if event_timestamp else current_time_utc

    new_event = NewsEvent(
        event_title=news_item_from_api.get("headline"),
        event_date=event_datetime_utc,
        source_url=source_url,
        source_name=news_item_from_api.get("source"),
        category=news_item_from_api.get("category"),
        full_article_text=full_article_text_scraped_this_time,  # Use newly scraped text
        processed_date=current_time_utc
    )
    analyzer_instance.db_session.add(new_event)
    try:
        analyzer_instance.db_session.commit()
        analyzer_instance.db_session.refresh(new_event)  # Get ID and other defaults
        logger.info(f"Stored new news event: {new_event.event_title[:70]}... (ID: {new_event.id})")
        return new_event
    except SQLAlchemyError as e:
        analyzer_instance.db_session.rollback()
        logger.error(f"Database error storing new news event '{news_item_from_api.get('headline')}': {e}",
                     exc_info=True)
        # Attempt to find if it was created by a concurrent process due to unique constraint
        # This can happen if pipeline runs in parallel or if error handling is complex
        existing_after_error = analyzer_instance.db_session.query(NewsEvent).filter_by(source_url=source_url).first()
        if existing_after_error:
            logger.warning(
                f"Found existing event for {source_url} after commit error, likely due to race condition. Using existing.")
            return existing_after_error
        return None
---------- END db_handler.py ----------


---------- news_analyzer.py ----------
# services/news_analyzer/news_analyzer.py
import time
from datetime import datetime, timezone, timedelta
from database import SessionLocal, get_db_session, NewsEventAnalysis
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError
from core.config import MAX_NEWS_ARTICLES_PER_QUERY, MAX_NEWS_TO_ANALYZE_PER_RUN

from api_clients import FinnhubClient, GeminiAPIClient  # No scrape_article_content directly needed here

# Import from submodules
from .data_fetcher import fetch_market_news_from_api
from .db_handler import (
    get_or_create_news_event_db_entry,
    _ensure_news_event_is_bound_to_session,
    _ensure_news_event_session_is_active
)
from .ai_analyzer import perform_ai_analysis_for_news_item


class NewsAnalyzer:
    def __init__(self):
        self.finnhub = FinnhubClient()
        self.gemini = GeminiAPIClient()
        # NewsAnalyzer manages its own DB session for the duration of its pipeline run or instance lifetime.
        self.db_session_generator = get_db_session()  # Store the generator
        self.db_session = next(self.db_session_generator)  # Get the initial session
        logger.debug("NewsAnalyzer initialized with a new DB session.")

    def get_new_db_session_generator(self):
        """Allows db_handler to get a new session generator if needed."""
        return get_db_session()

    def _close_session_if_active(self):
        """Closes the NewsAnalyzer's current DB session if it's active."""
        if self.db_session and self.db_session.is_active:
            try:
                self.db_session.close()
                logger.debug("DB session closed in NewsAnalyzer.")
            except Exception as e_close:
                logger.warning(f"Error closing session in NewsAnalyzer: {e_close}")
        # db_session_generator does not need explicit closing here.
        # SessionLocal.remove() will be called by the context manager of get_db_session if it was used that way.
        # Or if SessionLocal() was directly used, it must be managed.
        # For now, we assume SessionLocal() created by get_db_session handles its own removal/closing when the generator is exhausted or via its yield finally block.

    def analyze_single_news_item_and_save(self, news_event_db_obj):
        """
        Orchestrates AI analysis for a single news item and saves the analysis.
        Uses the NewsAnalyzer's managed db_session.
        """
        if not news_event_db_obj:
            logger.error("analyze_single_news_item_and_save called with no NewsEvent DB object.")
            return None

        # Ensure the object is bound to the current session
        bound_news_event_db_obj = _ensure_news_event_is_bound_to_session(self, news_event_db_obj)
        if not bound_news_event_db_obj:
            logger.error(
                f"Failed to bind news event (ID: {news_event_db_obj.id if news_event_db_obj.id else 'N/A'}) to session. Cannot analyze.")
            return None

        news_event_to_analyze = bound_news_event_db_obj  # Use the (potentially merged/re-fetched) object

        analysis_payload = perform_ai_analysis_for_news_item(self, news_event_to_analyze)

        current_analysis_time = datetime.now(timezone.utc)
        news_analysis_entry = NewsEventAnalysis(
            news_event_id=news_event_to_analyze.id,
            analysis_date=current_analysis_time,
            **analysis_payload
        )

        self.db_session.add(news_analysis_entry)
        news_event_to_analyze.last_analyzed_date = current_analysis_time  # Update parent event

        try:
            self.db_session.commit()
            logger.info(
                f"Successfully analyzed and saved news: '{news_event_to_analyze.event_title[:70]}...' (Analysis ID: {news_analysis_entry.id})")
        except SQLAlchemyError as e:
            self.db_session.rollback()
            logger.error(f"Database error saving news analysis for '{news_event_to_analyze.event_title[:70]}...': {e}",
                         exc_info=True)
            return None  # Indicate failure

        return news_analysis_entry

    def run_news_analysis_pipeline(self, category="general", count_to_fetch_from_api=MAX_NEWS_ARTICLES_PER_QUERY,
                                   count_to_analyze_this_run=MAX_NEWS_TO_ANALYZE_PER_RUN):
        try:
            _ensure_news_event_session_is_active(self,
                                                 f"Pipeline Start: Category {category}")  # Ensure session is good at start

            fetched_news_items_api = fetch_market_news_from_api(self, category=category,
                                                                count_to_fetch_from_api=count_to_fetch_from_api)
            if not fetched_news_items_api:
                logger.info("No news items fetched from API for analysis.")
                return []

            analyzed_news_results = []
            newly_analyzed_count_this_run = 0

            # Define how old an analysis can be before re-analysis is considered
            reanalyze_older_than_days = 2
            reanalyze_threshold_date = datetime.now(timezone.utc) - timedelta(days=reanalyze_older_than_days)

            for news_item_api_data in fetched_news_items_api:
                if newly_analyzed_count_this_run >= count_to_analyze_this_run:
                    logger.info(
                        f"Reached analysis limit of {count_to_analyze_this_run} new/re-analyzed items for this run.")
                    break

                try:
                    # Get or create the NewsEvent DB entry. This also handles scraping.
                    news_event_db = get_or_create_news_event_db_entry(self, news_item_api_data)
                    if not news_event_db:
                        logger.warning(
                            f"Skipping news item (could not get/create in DB): {news_item_api_data.get('headline')}")
                        continue

                    # Ensure this db object (which might be new or existing) is bound to the session correctly
                    news_event_db = _ensure_news_event_is_bound_to_session(self, news_event_db)
                    if not news_event_db:  # If binding failed critically
                        logger.error(
                            f"Critical: Failed to bind news event {news_item_api_data.get('headline')} to session. Skipping.")
                        continue

                    analysis_needed = False
                    latest_analysis = None

                    # Check existing analyses for this news event
                    # Query sorted by date to get the most recent one
                    if news_event_db.id:  # Ensure event has an ID (i.e., it's persisted or merged)
                        latest_analysis = self.db_session.query(NewsEventAnalysis) \
                            .filter(NewsEventAnalysis.news_event_id == news_event_db.id) \
                            .order_by(NewsEventAnalysis.analysis_date.desc()) \
                            .first()

                    if not latest_analysis:
                        analysis_needed = True
                        logger.info(f"News '{news_event_db.event_title[:50]}...' requires new analysis.")
                    elif latest_analysis.analysis_date < reanalyze_threshold_date:
                        analysis_needed = True
                        logger.info(
                            f"News '{news_event_db.event_title[:50]}...' requires re-analysis (last analyzed {latest_analysis.analysis_date}, older than {reanalyze_older_than_days} days).")
                    # Check if full text became available since last analysis (if last analysis was headline-only)
                    elif news_event_db.full_article_text and latest_analysis and latest_analysis.key_news_snippets:
                        source_type_used_last_time = latest_analysis.key_news_snippets.get("source_type_used", "")
                        if "full article" not in source_type_used_last_time.lower():
                            analysis_needed = True
                            logger.info(
                                f"News '{news_event_db.event_title[:50]}...' re-analyzing with newly available/confirmed full text (was: {source_type_used_last_time}).")
                    else:
                        logger.info(
                            f"News '{news_event_db.event_title[:50]}...' already recently analyzed with available text. Skipping.")

                    if analysis_needed:
                        analysis_result = self.analyze_single_news_item_and_save(news_event_db)
                        if analysis_result:
                            analyzed_news_results.append(analysis_result)
                            newly_analyzed_count_this_run += 1
                        time.sleep(3)  # Delay between AI calls if multiple items are analyzed

                except Exception as e_item:  # Catch errors for a single item processing
                    logger.error(
                        f"Failed to process or analyze news item '{news_item_api_data.get('headline')}': {e_item}",
                        exc_info=True)
                    _ensure_news_event_session_is_active(self,
                                                         f"Error Recovery for {news_item_api_data.get('headline')}")  # Ensure session is active for next item
                    if self.db_session and self.db_session.is_active:
                        self.db_session.rollback()  # Rollback any partial transaction for this item

            logger.info(
                f"News analysis pipeline completed. Newly analyzed/re-analyzed {newly_analyzed_count_this_run} items.")
            return analyzed_news_results

        finally:
            self._close_session_if_active()  # Close session at the end of the pipeline run
---------- END news_analyzer.py ----------


---------- __init__.py ----------
# services/stock_analyzer/__init__.py
from .stock_analyzer import StockAnalyzer

__all__ = ["StockAnalyzer"]
---------- END __init__.py ----------


---------- ai_synthesis.py ----------
# services/stock_analyzer/ai_synthesis.py
import re
import json  # For parsing potential JSON responses
from core.logging_setup import logger
from .helpers import safe_get_float
from core.config import AI_JSON_OUTPUT_INSTRUCTION


def _parse_ai_investment_thesis_json_response(ticker_for_log, ai_response_data):
    parsed_data = {
        "investment_thesis_full": "AI response not fully processed or expected JSON fields missing.",
        "investment_decision": "Review AI Output",
        "strategy_type": "Not Specified by AI",
        "confidence_level": "Not Specified by AI",
        "reasoning": "AI response not fully processed or expected JSON fields missing."
    }

    if not isinstance(ai_response_data, dict):
        error_message = f"Error: AI response for thesis was not a valid dictionary. Response: {str(ai_response_data)[:500]}"
        parsed_data = {key: error_message for key in parsed_data}
        parsed_data["investment_decision"] = "AI Error"
        parsed_data["strategy_type"] = "AI Error"
        parsed_data["confidence_level"] = "AI Error"
        logger.error(f"AI thesis response for {ticker_for_log} is not a dict: {ai_response_data}")
        return parsed_data

    if ai_response_data.get("error"):
        error_message = f"AI Error: {ai_response_data.get('error_details', str(ai_response_data))}"
        parsed_data = {key: error_message for key in parsed_data}
        parsed_data["investment_decision"] = "AI Error"
        parsed_data["strategy_type"] = "AI Error"
        parsed_data["confidence_level"] = "AI Error"
        logger.error(f"AI thesis generation for {ticker_for_log} returned an error: {ai_response_data.get('error')}")
        return parsed_data

    # Expected JSON structure:
    # {
    #   "investmentThesis": "...",
    #   "investmentDecision": "Buy|Hold|Sell|Monitor|etc.",
    #   "strategyType": "GARP|Value|Growth|etc.",
    #   "confidenceLevel": "High|Medium|Low",
    #   "keyReasoningPoints": ["Point 1...", "Point 2..."],
    #   "dataQualityAcknowledgement": "..." (optional)
    # }

    parsed_data["investment_thesis_full"] = ai_response_data.get("investmentThesis",
                                                                 parsed_data["investment_thesis_full"])
    parsed_data["investment_decision"] = ai_response_data.get("investmentDecision", parsed_data["investment_decision"])
    parsed_data["strategy_type"] = ai_response_data.get("strategyType", parsed_data["strategy_type"])
    parsed_data["confidence_level"] = ai_response_data.get("confidenceLevel", parsed_data["confidence_level"])

    reasoning_points = ai_response_data.get("keyReasoningPoints")
    if isinstance(reasoning_points, list) and reasoning_points:
        parsed_data["reasoning"] = "\n".join([f"- {point}" for point in reasoning_points])
    elif isinstance(reasoning_points, str):  # Handle if AI gives a single string
        parsed_data["reasoning"] = reasoning_points
    else:
        parsed_data["reasoning"] = "Key reasoning points not provided in expected format by AI."

    data_quality_ack = ai_response_data.get("dataQualityAcknowledgement")
    if data_quality_ack:
        parsed_data["reasoning"] += f"\n\nAI Data Quality Acknowledgement: {data_quality_ack}"

    return parsed_data


def synthesize_investment_thesis(analyzer_instance):
    ticker = analyzer_instance.ticker
    logger.info(f"Synthesizing investment thesis for {ticker} using JSON format...")

    metrics = analyzer_instance._financial_data_cache.get('calculated_metrics', {})
    qual_summaries = analyzer_instance._financial_data_cache.get('10k_summaries', {})
    dcf_results = analyzer_instance._financial_data_cache.get('dcf_results', {})
    profile = analyzer_instance._financial_data_cache.get('profile_fmp', {})
    competitor_analysis_summary_data = analyzer_instance._financial_data_cache.get('competitor_analysis', {})

    company_name = analyzer_instance.stock_db_entry.company_name or ticker
    industry = analyzer_instance.stock_db_entry.industry or "N/A"
    sector = analyzer_instance.stock_db_entry.sector or "N/A"

    prompt = f"Company: {company_name} ({ticker})\nIndustry: {industry}, Sector: {sector}\n\n"
    prompt += "Key Financial Metrics & Data:\n"
    # ... (metrics formatting as before, unchanged)
    metrics_for_prompt = {
        "P/E Ratio": metrics.get("pe_ratio"), "P/B Ratio": metrics.get("pb_ratio"),
        "P/S Ratio": metrics.get("ps_ratio"), "Dividend Yield": metrics.get("dividend_yield"),
        "ROE": metrics.get("roe"), "ROIC": metrics.get("roic"),
        "Debt-to-Equity": metrics.get("debt_to_equity"), "Debt-to-EBITDA": metrics.get("debt_to_ebitda"),
        "Revenue Growth YoY": metrics.get("revenue_growth_yoy"),
        "Revenue Growth QoQ": metrics.get("revenue_growth_qoq"),
        f"Latest Quarterly Revenue (Source: {metrics.get('key_metrics_snapshot', {}).get('q_revenue_source', 'N/A')})": metrics.get(
            'key_metrics_snapshot', {}).get('latest_q_revenue'),
        "EPS Growth YoY": metrics.get("eps_growth_yoy"),
        "Net Profit Margin": metrics.get("net_profit_margin"),
        "Operating Profit Margin": metrics.get("operating_profit_margin"),
        "Free Cash Flow Yield": metrics.get("free_cash_flow_yield"),
        "FCF Trend (3yr)": metrics.get("free_cash_flow_trend"),
        "Retained Earnings Trend (3yr)": metrics.get("retained_earnings_trend"),
    }

    for name, val in metrics_for_prompt.items():
        if val is not None:
            formatted_val = val
            if isinstance(val, float):
                if any(kw in name.lower() for kw in ['yield', 'growth', 'margin', 'roe', 'roic']):
                    formatted_val = f'{val:.2%}'
                elif 'revenue' in name.lower() and 'growth' not in name.lower():
                    formatted_val = f'{val:,.0f}'
                else:
                    formatted_val = f'{val:.2f}'
            prompt += f"- {name}: {formatted_val}\n"

    current_stock_price = safe_get_float(profile, "price")
    dcf_iv = dcf_results.get("dcf_intrinsic_value")
    dcf_upside = dcf_results.get("dcf_upside_percentage")

    if current_stock_price is not None:
        prompt += f"- Current Stock Price: {current_stock_price:.2f}\n"
    if dcf_iv is not None:
        prompt += f"- DCF Intrinsic Value/Share (Base Case): {dcf_iv:.2f}\n"
    if dcf_upside is not None:
        prompt += f"- DCF Upside/Downside (Base Case): {dcf_upside:.2%}\n"

    if dcf_results.get("dcf_assumptions", {}).get("sensitivity_analysis"):
        prompt += "- DCF Sensitivity Highlights:\n"
        for s_idx, s_data in enumerate(dcf_results["dcf_assumptions"]["sensitivity_analysis"]):
            if s_idx < 2:
                upside_str = f"{s_data['upside']:.2%}" if s_data['upside'] is not None else "N/A"
                prompt += f"  - {s_data['scenario']}: IV {s_data['intrinsic_value']:.2f} (Upside: {upside_str})\n"
    prompt += "\n"

    prompt += "Qualitative Summaries (from 10-K & AI analysis):\n"

    # Helper to get AI summary text or fallback
    def get_summary_text(summary_data, key, fallback="N/A"):
        if isinstance(summary_data, dict) and summary_data.get(key):
            content = summary_data[key]
            if isinstance(content, dict) and "summary" in content:  # If it's a JSON summary object
                return content["summary"]
            elif isinstance(content, str):  # If it's already a string summary
                return content
        return fallback

    qual_for_prompt = {
        "Business Model": get_summary_text(qual_summaries.get("business_summary_data"), "summary"),
        "Economic Moat": get_summary_text(qual_summaries.get("economic_moat_summary_data"), "overallAssessment"),
        "Industry Trends & Positioning": get_summary_text(qual_summaries.get("industry_trends_summary_data"),
                                                          "companyPositioning"),
        "Competitive Landscape": get_summary_text(competitor_analysis_summary_data,
                                                  "landscapeOverview") or competitor_analysis_summary_data.get(
            "summary", "N/A"),
        "Management Discussion Highlights (MD&A)": get_summary_text(
            qual_summaries.get("management_assessment_summary_data"), "summary"),
        "Key Risk Factors (from 10-K)": get_summary_text(qual_summaries.get("risk_factors_summary_data"), "summary"),
    }

    for name, text_val in qual_for_prompt.items():
        if text_val and text_val != "N/A" and not text_val.startswith(
                ("AI analysis error", "Section not found", "Insufficient input")):
            prompt += f"- {name}:\n{text_val[:500].replace('...', '').strip()}...\n\n"
        elif text_val:
            prompt += f"- {name}: {text_val}\n\n"

    if analyzer_instance.data_quality_warnings:
        prompt += "IMPORTANT DATA QUALITY CONSIDERATIONS:\n"
        for i, warn_msg in enumerate(analyzer_instance.data_quality_warnings):
            prompt += f"- WARNING {i + 1}: {warn_msg}\n"
        prompt += "Acknowledge these warnings in your 'dataQualityAcknowledgement' field if they are significant.\n\n"

    prompt += (
        "Instructions for AI: Based on ALL the above information (quantitative, qualitative, DCF, competitor data, and data quality warnings), "
        "provide a detailed financial analysis and investment thesis. "
        f"Your entire response MUST be a single, valid JSON object. Do not include any text outside of this JSON structure. "
        "Use the following exact structure and field names:\n"
        "{\n"
        "  \"investmentThesis\": \"Comprehensive thesis (2-4 paragraphs) synthesizing all data. Discuss positives, negatives, outlook. If revenue growth is stagnant/negative but EPS growth is positive, explain drivers and sustainability. Address margin pressures or segment profitability changes.\",\n"
        "  \"investmentDecision\": \"Strong Buy|Buy|Hold|Monitor|Reduce|Sell|Avoid\",\n"
        "  \"strategyType\": \"Value|GARP|Growth|Income|Speculative|Special Situation|Turnaround\",\n"
        "  \"confidenceLevel\": \"High|Medium|Low (Reflect confidence in YOUR analysis, considering data quality and completeness)\",\n"
        "  \"keyReasoningPoints\": [\n"
        "    \"Bullet point 1: Valuation (DCF, comparables if any)\",\n"
        "    \"Bullet point 2: Financial Health & Profitability\",\n"
        "    \"Bullet point 3: Growth Prospects (Revenue & EPS)\",\n"
        "    \"Bullet point 4: Economic Moat & Competitive Position\",\n"
        "    \"Bullet point 5: Key Risks (including data quality issues if significant)\",\n"
        "    \"Bullet point 6: Management & Strategy (if inferable)\"\n"
        "  ],\n"
        "  \"dataQualityAcknowledgement\": \"Optional: Briefly state if data quality warnings significantly impacted your analysis or confidence.\"\n"
        "}\n"
    )

    ai_response_data = analyzer_instance.gemini.generate_text(prompt, output_format="json")
    parsed_thesis_data = _parse_ai_investment_thesis_json_response(ticker, ai_response_data)

    # Consolidate data quality warnings and adjust confidence
    # If there are CRITICAL warnings, or multiple warnings, confidence should be lowered.
    critical_warnings = [w for w in analyzer_instance.data_quality_warnings if "CRITICAL:" in w.upper()]
    significant_revenue_warnings = [w for w in analyzer_instance.data_quality_warnings if
                                    "REVENUE" in w.upper() and "DEVIATES" in w.upper()]

    current_confidence = parsed_thesis_data.get("confidence_level", "Not Specified by AI").lower()
    new_confidence = current_confidence
    confidence_adjustment_reason = ""

    if critical_warnings:
        new_confidence = "Low"
        confidence_adjustment_reason = f"Critical data quality warnings ({len(critical_warnings)}) present."
    elif significant_revenue_warnings or len(analyzer_instance.data_quality_warnings) >= 2:
        if current_confidence == "high":
            new_confidence = "Medium"
            confidence_adjustment_reason = "Significant data warnings or multiple issues."
        elif current_confidence == "medium":
            new_confidence = "Low"
            confidence_adjustment_reason = "Significant data warnings or multiple issues, and AI was already Medium."

    if new_confidence != current_confidence and current_confidence not in ["low", "ai error", "not specified by ai"]:
        logger.warning(
            f"Adjusting AI confidence for {ticker} from '{current_confidence.capitalize()}' to '{new_confidence.capitalize()}' due to: {confidence_adjustment_reason}")
        parsed_thesis_data["confidence_level"] = new_confidence.capitalize()
        # Add a note to reasoning if not already covered by AI's dataQualityAcknowledgement
        reasoning_update = f"\nSystem Note: Confidence adjusted to {new_confidence.capitalize()} due to data quality concerns ({confidence_adjustment_reason})."
        if "reasoning" in parsed_thesis_data and isinstance(parsed_thesis_data["reasoning"], str):
            if reasoning_update not in parsed_thesis_data["reasoning"]:
                parsed_thesis_data["reasoning"] += reasoning_update
        else:
            parsed_thesis_data["reasoning"] = reasoning_update

    logger.info(f"Generated thesis for {ticker}. Decision: {parsed_thesis_data.get('investment_decision')}, "
                f"Strategy: {parsed_thesis_data.get('strategy_type')}, Confidence: {parsed_thesis_data.get('confidence_level')}")

    return parsed_thesis_data
---------- END ai_synthesis.py ----------


---------- data_fetcher.py ----------
# services/stock_analyzer/data_fetcher.py
import time
from core.logging_setup import logger
from core.config import STOCK_FINANCIAL_YEARS

def fetch_financial_statements_data(analyzer_instance):
    """Fetches all necessary financial statements and stores them in analyzer_instance._financial_data_cache."""
    ticker = analyzer_instance.ticker
    logger.info(f"Fetching financial statements for {ticker}...")
    statements_cache = {
        "fmp_income_annual": [], "fmp_balance_annual": [], "fmp_cashflow_annual": [],
        "fmp_income_quarterly": [],
        "finnhub_financials_quarterly_reported": {"data": []},
        "alphavantage_income_quarterly": {"quarterlyReports": []},
        "alphavantage_balance_quarterly": {"quarterlyReports": []},
        "alphavantage_cashflow_quarterly": {"quarterlyReports": []}
    }
    try:
        # FMP Annuals
        statements_cache["fmp_income_annual"] = analyzer_instance.fmp.get_financial_statements(ticker, "income-statement", "annual", STOCK_FINANCIAL_YEARS) or []
        time.sleep(1.5)
        statements_cache["fmp_balance_annual"] = analyzer_instance.fmp.get_financial_statements(ticker, "balance-sheet-statement", "annual", STOCK_FINANCIAL_YEARS) or []
        time.sleep(1.5)
        statements_cache["fmp_cashflow_annual"] = analyzer_instance.fmp.get_financial_statements(ticker, "cash-flow-statement", "annual", STOCK_FINANCIAL_YEARS) or []
        time.sleep(1.5)
        logger.info(f"FMP Annuals for {ticker}: Income({len(statements_cache['fmp_income_annual'])}), Balance({len(statements_cache['fmp_balance_annual'])}), Cashflow({len(statements_cache['fmp_cashflow_annual'])}).")

        # FMP Quarterlies
        statements_cache["fmp_income_quarterly"] = analyzer_instance.fmp.get_financial_statements(ticker, "income-statement", "quarter", 8) or []
        time.sleep(1.5)
        logger.info(f"FMP Quarterly Income for {ticker}: {len(statements_cache['fmp_income_quarterly'])} records.")

        # Finnhub Quarterlies
        fh_q_data = analyzer_instance.finnhub.get_financials_reported(ticker, freq="quarterly", count=8)
        time.sleep(1.5)
        if fh_q_data and isinstance(fh_q_data, dict) and fh_q_data.get("data"):
            statements_cache["finnhub_financials_quarterly_reported"] = fh_q_data
            logger.info(f"Fetched {len(fh_q_data['data'])} quarterly reports from Finnhub for {ticker}.")
        else:
            logger.warning(f"Finnhub quarterly financials reported data missing or malformed for {ticker}.")

        # Alpha Vantage Quarterlies
        av_income_q = analyzer_instance.alphavantage.get_income_statement_quarterly(ticker)
        time.sleep(15) # Alpha Vantage free tier has strict rate limits
        if av_income_q and isinstance(av_income_q, dict) and av_income_q.get("quarterlyReports"):
            statements_cache["alphavantage_income_quarterly"] = av_income_q
            logger.info(f"Fetched {len(av_income_q['quarterlyReports'])} quarterly income reports from Alpha Vantage for {ticker}.")
        else:
            logger.warning(f"Alpha Vantage quarterly income reports missing or malformed for {ticker}.")

        av_balance_q = analyzer_instance.alphavantage.get_balance_sheet_quarterly(ticker)
        time.sleep(15)
        if av_balance_q and isinstance(av_balance_q, dict) and av_balance_q.get("quarterlyReports"):
            statements_cache["alphavantage_balance_quarterly"] = av_balance_q
            logger.info(f"Fetched {len(av_balance_q['quarterlyReports'])} quarterly balance reports from Alpha Vantage for {ticker}.")
        else:
            logger.warning(f"Alpha Vantage quarterly balance reports missing or malformed for {ticker}.")

        av_cashflow_q = analyzer_instance.alphavantage.get_cash_flow_quarterly(ticker)
        time.sleep(15)
        if av_cashflow_q and isinstance(av_cashflow_q, dict) and av_cashflow_q.get("quarterlyReports"):
            statements_cache["alphavantage_cashflow_quarterly"] = av_cashflow_q
            logger.info(f"Fetched {len(av_cashflow_q['quarterlyReports'])} quarterly cash flow reports from Alpha Vantage for {ticker}.")
        else:
            logger.warning(f"Alpha Vantage quarterly cash flow reports missing or malformed for {ticker}.")

    except Exception as e:
        logger.warning(f"Error during financial statements fetch for {ticker}: {e}.", exc_info=True)

    analyzer_instance._financial_data_cache['financial_statements'] = statements_cache


def fetch_key_metrics_and_profile_data(analyzer_instance):
    """Fetches key metrics and profile data, storing them in analyzer_instance._financial_data_cache."""
    ticker = analyzer_instance.ticker
    logger.info(f"Fetching key metrics and profile for {ticker}.")

    # FMP Key Metrics (Annual & Quarterly)
    analyzer_instance._financial_data_cache['key_metrics_annual_fmp'] = analyzer_instance.fmp.get_key_metrics(ticker, "annual", STOCK_FINANCIAL_YEARS + 2) or []
    time.sleep(1.5)
    key_metrics_quarterly_fmp = analyzer_instance.fmp.get_key_metrics(ticker, "quarterly", 8)
    time.sleep(1.5)
    analyzer_instance._financial_data_cache['key_metrics_quarterly_fmp'] = key_metrics_quarterly_fmp if key_metrics_quarterly_fmp is not None else []

    # Finnhub Basic Financials
    analyzer_instance._financial_data_cache['basic_financials_finnhub'] = analyzer_instance.finnhub.get_basic_financials(ticker) or {}
    time.sleep(1.5)

    # FMP Profile (if not already fetched during _get_or_create_stock_entry)
    if 'profile_fmp' not in analyzer_instance._financial_data_cache or not analyzer_instance._financial_data_cache.get('profile_fmp'):
        profile_fmp_list = analyzer_instance.fmp.get_company_profile(ticker)
        time.sleep(1.5)
        analyzer_instance._financial_data_cache['profile_fmp'] = profile_fmp_list[0] if profile_fmp_list and isinstance(profile_fmp_list, list) and profile_fmp_list[0] else {}

    logger.info(f"FMP KM Annual for {ticker}: {len(analyzer_instance._financial_data_cache['key_metrics_annual_fmp'])}. "
                f"FMP KM Quarterly for {ticker}: {len(analyzer_instance._financial_data_cache['key_metrics_quarterly_fmp'])}. "
                f"Finnhub Basic Financials for {ticker}: {'OK' if analyzer_instance._financial_data_cache.get('basic_financials_finnhub', {}).get('metric') else 'Data missing'}.")
---------- END data_fetcher.py ----------


---------- dcf_analyzer.py ----------
# services/stock_analyzer/dcf_analyzer.py
from core.logging_setup import logger
from .helpers import safe_get_float, get_value_from_statement_list, calculate_cagr
from core.config import (
    DEFAULT_DISCOUNT_RATE, DEFAULT_PERPETUAL_GROWTH_RATE,
    DEFAULT_FCF_PROJECTION_YEARS
)


def _calculate_dcf_value_internal(ticker_for_log, start_fcf, initial_growth, discount_rate, perpetual_growth,
                                  proj_years, shares_outstanding_val):
    projected_fcfs = []
    last_projected_fcf = start_fcf
    current_year_growth_rates = []

    # Linear decline in growth rate from initial_growth to perpetual_growth over proj_years
    growth_rate_decline_per_year = (initial_growth - perpetual_growth) / float(proj_years) if proj_years > 0 else 0

    for i in range(proj_years):
        current_year_growth_rate = max(initial_growth - (growth_rate_decline_per_year * i), perpetual_growth)
        projected_fcf = last_projected_fcf * (1 + current_year_growth_rate)
        projected_fcfs.append(projected_fcf)
        last_projected_fcf = projected_fcf
        current_year_growth_rates.append(round(current_year_growth_rate, 4))  # Store for assumptions

    if not projected_fcfs:  # Should not happen if proj_years > 0
        return None, []

    # Terminal Value Calculation
    terminal_year_fcf_for_tv = projected_fcfs[-1] * (1 + perpetual_growth)
    terminal_value_denominator = discount_rate - perpetual_growth

    terminal_value = 0
    if terminal_value_denominator <= 1e-6:  # Avoid division by zero or very small numbers, or negative if pgr > dr
        logger.warning(f"DCF for {ticker_for_log}: Discount rate ({discount_rate:.3f}) is too close to or less than "
                       f"perpetual growth rate ({perpetual_growth:.3f}). Terminal Value may be unreliable or infinite. Setting TV to 0.")
    else:
        terminal_value = terminal_year_fcf_for_tv / terminal_value_denominator

    # Discount FCFs and Terminal Value
    sum_discounted_fcf = sum(fcf / ((1 + discount_rate) ** (i + 1)) for i, fcf in enumerate(projected_fcfs))
    discounted_terminal_value = terminal_value / ((1 + discount_rate) ** proj_years)

    intrinsic_equity_value = sum_discounted_fcf + discounted_terminal_value

    if shares_outstanding_val is None or shares_outstanding_val == 0:
        logger.error(f"DCF for {ticker_for_log}: Shares outstanding is zero or None. Cannot calculate per share value.")
        return None, current_year_growth_rates

    return intrinsic_equity_value / shares_outstanding_val, current_year_growth_rates


def perform_dcf_analysis(analyzer_instance):
    ticker = analyzer_instance.ticker
    logger.info(f"Performing simplified DCF analysis for {ticker}...")

    dcf_results = {
        "dcf_intrinsic_value": None,
        "dcf_upside_percentage": None,
        "dcf_assumptions": {
            "discount_rate": DEFAULT_DISCOUNT_RATE,
            "perpetual_growth_rate": DEFAULT_PERPETUAL_GROWTH_RATE,
            "projection_years": DEFAULT_FCF_PROJECTION_YEARS,
            "start_fcf": None,
            "start_fcf_basis": "N/A",
            "fcf_growth_rates_projection": [],
            "initial_fcf_growth_rate_used": None,
            "initial_fcf_growth_rate_basis": "N/A",
            "sensitivity_analysis": []
        }
    }
    assumptions = dcf_results["dcf_assumptions"]

    # Retrieve necessary data from cache
    cashflow_annual_fmp = analyzer_instance._financial_data_cache.get('financial_statements', {}).get(
        'fmp_cashflow_annual', [])
    profile_fmp = analyzer_instance._financial_data_cache.get('profile_fmp', {})
    calculated_metrics = analyzer_instance._financial_data_cache.get('calculated_metrics', {})

    current_price = safe_get_float(profile_fmp, "price")
    shares_outstanding_profile = safe_get_float(profile_fmp, "sharesOutstanding")
    mkt_cap_profile = safe_get_float(profile_fmp, "mktCap")

    shares_outstanding_calc = (
                mkt_cap_profile / current_price) if mkt_cap_profile and current_price and current_price != 0 else None
    shares_outstanding = shares_outstanding_profile if shares_outstanding_profile is not None and shares_outstanding_profile > 0 else shares_outstanding_calc

    if not cashflow_annual_fmp or not profile_fmp or current_price is None or shares_outstanding is None or shares_outstanding == 0:
        logger.warning(
            f"Insufficient data for DCF for {ticker} (FCF statements, profile, price, or shares missing/zero).")
        analyzer_instance._financial_data_cache['dcf_results'] = dcf_results
        return dcf_results

    current_fcf_annual = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 0)
    if current_fcf_annual is None or current_fcf_annual <= 10000:  # Arbitrary small positive FCF threshold
        logger.warning(
            f"Current annual FCF for {ticker} is {current_fcf_annual}. DCF requires substantial positive FCF. Skipping DCF.")
        analyzer_instance._financial_data_cache['dcf_results'] = dcf_results
        return dcf_results

    assumptions["start_fcf"] = current_fcf_annual
    assumptions[
        "start_fcf_basis"] = f"Latest Annual FCF ({cashflow_annual_fmp[0].get('date') if cashflow_annual_fmp and cashflow_annual_fmp[0] else 'N/A'})"

    # Determine initial FCF growth rate
    fcf_growth_rate_3yr_cagr = None
    if len(cashflow_annual_fmp) >= 4:  # Need 4 data points for 3-year CAGR (current and 3 years prior)
        fcf_start_for_cagr = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 3)  # 3 years prior
        if fcf_start_for_cagr and fcf_start_for_cagr > 0 and current_fcf_annual > 0:  # Both positive for meaningful CAGR
            fcf_growth_rate_3yr_cagr = calculate_cagr(current_fcf_annual, fcf_start_for_cagr, 3)

    initial_fcf_growth_rate = DEFAULT_PERPETUAL_GROWTH_RATE  # Default
    assumptions["initial_fcf_growth_rate_basis"] = "Default (Perpetual Growth Rate)"

    if fcf_growth_rate_3yr_cagr is not None:
        initial_fcf_growth_rate = fcf_growth_rate_3yr_cagr
        assumptions["initial_fcf_growth_rate_basis"] = "Historical 3yr FCF CAGR"
    elif calculated_metrics.get("revenue_growth_cagr_3yr") is not None:
        initial_fcf_growth_rate = calculated_metrics["revenue_growth_cagr_3yr"]
        assumptions["initial_fcf_growth_rate_basis"] = "Proxy: Revenue Growth CAGR (3yr)"
    elif calculated_metrics.get("revenue_growth_yoy") is not None:
        initial_fcf_growth_rate = calculated_metrics["revenue_growth_yoy"]
        assumptions["initial_fcf_growth_rate_basis"] = "Proxy: Revenue Growth YoY"

    if not isinstance(initial_fcf_growth_rate, (int, float)):  # Ensure it's a number
        initial_fcf_growth_rate = DEFAULT_PERPETUAL_GROWTH_RATE

    # Cap and floor the initial growth rate to reasonable bounds
    initial_fcf_growth_rate = min(max(initial_fcf_growth_rate, -0.05), 0.15)  # e.g., -5% to 15%
    assumptions["initial_fcf_growth_rate_used"] = initial_fcf_growth_rate

    # Base Case DCF
    base_iv_per_share, base_fcf_growth_rates = _calculate_dcf_value_internal(
        ticker, assumptions["start_fcf"], assumptions["initial_fcf_growth_rate_used"],
        assumptions["discount_rate"], assumptions["perpetual_growth_rate"],
        assumptions["projection_years"], shares_outstanding
    )

    if base_iv_per_share is not None:
        dcf_results["dcf_intrinsic_value"] = base_iv_per_share
        assumptions["fcf_growth_rates_projection"] = base_fcf_growth_rates
        if current_price and current_price != 0:
            dcf_results["dcf_upside_percentage"] = (base_iv_per_share - current_price) / current_price
    else:
        logger.error(f"DCF base case calculation failed for {ticker}.")
        analyzer_instance._financial_data_cache['dcf_results'] = dcf_results
        return dcf_results  # Exit if base case fails

    # Sensitivity Analysis
    sensitivity_scenarios = [
        {"dr_adj": -0.005, "pgr_adj": 0.0, "label": "Discount Rate -0.5%"},
        {"dr_adj": +0.005, "pgr_adj": 0.0, "label": "Discount Rate +0.5%"},
        {"dr_adj": 0.0, "pgr_adj": -0.0025, "label": "Perp. Growth -0.25%"},
        {"dr_adj": 0.0, "pgr_adj": +0.0025, "label": "Perp. Growth +0.25%"}
    ]

    for scenario in sensitivity_scenarios:
        sens_dr = assumptions["discount_rate"] + scenario["dr_adj"]
        sens_pgr = assumptions["perpetual_growth_rate"] + scenario["pgr_adj"]

        # Ensure perpetual growth is less than discount rate for stable model
        if sens_pgr >= sens_dr - 0.001:  # Small margin
            logger.debug(
                f"Skipping DCF sensitivity scenario '{scenario['label']}' for {ticker} as PGR ({sens_pgr:.3f}) >= DR ({sens_dr:.3f}).")
            continue

        iv_sens, _ = _calculate_dcf_value_internal(
            ticker, assumptions["start_fcf"], assumptions["initial_fcf_growth_rate_used"],
            sens_dr, sens_pgr, assumptions["projection_years"], shares_outstanding
        )
        if iv_sens is not None:
            upside_sens = (iv_sens - current_price) / current_price if current_price and current_price != 0 else None
            assumptions["sensitivity_analysis"].append({
                "scenario": scenario["label"],
                "discount_rate": sens_dr,
                "perpetual_growth_rate": sens_pgr,
                "intrinsic_value": iv_sens,
                "upside": upside_sens
            })

    logger.info(f"DCF for {ticker}: Base IV/Share: {dcf_results.get('dcf_intrinsic_value', 'N/A'):.2f}, "
                f"Upside: {dcf_results.get('dcf_upside_percentage', 'N/A') * 100 if dcf_results.get('dcf_upside_percentage') is not None else 'N/A':.2f}%")

    analyzer_instance._financial_data_cache['dcf_results'] = dcf_results
    return dcf_results
---------- END dcf_analyzer.py ----------


---------- helpers.py ----------
# services/stock_analyzer/helpers.py
import math
from core.logging_setup import logger

def safe_get_float(data_dict, key, default=None):
    if data_dict is None or not isinstance(data_dict, dict): return default
    val = data_dict.get(key)
    if val is None or val == "None" or val == "" or str(val).lower() == "n/a" or str(val).lower() == "-": return default
    try: return float(val)
    except (ValueError, TypeError): return default

def calculate_cagr(end_value, start_value, years):
    if start_value is None or end_value is None or not isinstance(years, (int, float)) or years <= 0: return None
    if start_value == 0: return None
    if (start_value < 0 and end_value > 0) or (start_value > 0 and end_value < 0): return None
    if start_value < 0 and end_value < 0: return None # Or handle appropriately if CAGR for negative values is desired
    if end_value == 0 and start_value > 0: return -1.0 # Total loss
    try:
        return ((float(end_value) / float(start_value)) ** (1 / float(years))) - 1
    except (ValueError, TypeError, ZeroDivisionError): # ZeroDivisionError for years = 0, though already checked
        return None


def calculate_growth(current_value, previous_value):
    if previous_value is None or current_value is None: return None
    try:
        current_value_float = float(current_value)
        previous_value_float = float(previous_value)
        if previous_value_float == 0:
            return None if current_value_float == 0 else (float('inf') if current_value_float > 0 else float('-inf'))
        return (current_value_float - previous_value_float) / abs(previous_value_float)
    except (ValueError, TypeError):
        return None

def get_value_from_statement_list(data_list, field, year_offset=0, report_date_for_log=None):
    if data_list and isinstance(data_list, list) and len(data_list) > year_offset:
        report = data_list[year_offset]
        if report and isinstance(report, dict):
            val = safe_get_float(report, field)
            # Optional: detailed logging if value is None
            # if val is None:
            #     date_info = report_date_for_log or report.get('date', 'Unknown Date')
            #     logger.debug(f"Field '{field}' not found or invalid in report for {date_info} (offset {year_offset}).")
            return val
    return None

def get_finnhub_concept_value(finnhub_quarterly_reports_data, report_section_key, concept_names_list, quarter_offset=0):
    if not finnhub_quarterly_reports_data or len(finnhub_quarterly_reports_data) <= quarter_offset: return None
    report_data = finnhub_quarterly_reports_data[quarter_offset]
    if 'report' not in report_data or report_section_key not in report_data['report']: return None
    section_items = report_data['report'][report_section_key]
    if not section_items: return None
    for item in section_items:
        if item.get('concept') in concept_names_list or item.get('label') in concept_names_list:
            return safe_get_float(item, 'value')
    return None

def get_alphavantage_value(av_quarterly_reports, field_name, quarter_offset_from_latest=0):
    if not av_quarterly_reports or len(av_quarterly_reports) <= quarter_offset_from_latest: return None
    report = av_quarterly_reports[quarter_offset_from_latest]
    return safe_get_float(report, field_name)

def get_fmp_value(fmp_quarterly_reports, field_name, quarter_offset_from_latest=0):
    if not fmp_quarterly_reports or len(fmp_quarterly_reports) <= quarter_offset_from_latest: return None
    report = fmp_quarterly_reports[quarter_offset_from_latest]
    return safe_get_float(report, field_name)
---------- END helpers.py ----------


---------- metrics_calculator.py ----------
# services/stock_analyzer/metrics_calculator.py
import math
import json
from core.logging_setup import logger
from .helpers import (
    safe_get_float, calculate_cagr, calculate_growth,
    get_value_from_statement_list, get_fmp_value,
    get_alphavantage_value, get_finnhub_concept_value
)
from core.config import (
    Q_REVENUE_SANITY_CHECK_DEVIATION_THRESHOLD,
    PRIORITY_REVENUE_SOURCES
)


def _calculate_valuation_ratios(latest_km_q_fmp, latest_km_a_fmp, basic_fin_fh_metric, overview_av):
    ratios = {}
    ratios["pe_ratio"] = safe_get_float(latest_km_q_fmp, "peRatioTTM") or \
                         safe_get_float(latest_km_a_fmp, "peRatio") or \
                         safe_get_float(basic_fin_fh_metric, "peTTM")
    ratios["pb_ratio"] = safe_get_float(latest_km_q_fmp, "priceToBookRatioTTM") or \
                         safe_get_float(latest_km_a_fmp, "pbRatio") or \
                         safe_get_float(basic_fin_fh_metric, "pbAnnual")
    ratios["ps_ratio"] = safe_get_float(latest_km_q_fmp, "priceToSalesRatioTTM") or \
                         safe_get_float(latest_km_a_fmp, "priceSalesRatio") or \
                         safe_get_float(basic_fin_fh_metric, "psTTM")

    ratios["ev_to_sales"] = safe_get_float(latest_km_q_fmp, "enterpriseValueOverRevenueTTM") or \
                            safe_get_float(latest_km_a_fmp, "enterpriseValueOverRevenue") or \
                            safe_get_float(overview_av, "EVToRevenue")

    ratios["ev_to_ebitda"] = safe_get_float(latest_km_q_fmp, "evToEbitdaTTM") or \
                             safe_get_float(latest_km_a_fmp, "evToEbitda") or \
                             safe_get_float(overview_av, "EVToEBITDA")

    div_yield_fmp_q = safe_get_float(latest_km_q_fmp, "dividendYieldTTM")
    div_yield_fmp_a = safe_get_float(latest_km_a_fmp, "dividendYield")
    div_yield_fh_raw = safe_get_float(basic_fin_fh_metric, "dividendYieldAnnual")
    div_yield_fh = div_yield_fh_raw / 100.0 if div_yield_fh_raw is not None else None
    div_yield_av = safe_get_float(overview_av, "DividendYield")

    ratios["dividend_yield"] = div_yield_fmp_q if div_yield_fmp_q is not None else \
        (div_yield_fmp_a if div_yield_fmp_a is not None else \
             (div_yield_fh if div_yield_fh is not None else div_yield_av))
    return ratios


def _calculate_profitability_metrics(analyzer_instance, income_annual_fmp, balance_annual_fmp, latest_km_a_fmp,
                                     overview_av):
    metrics = {}
    ticker = analyzer_instance.ticker

    # From FMP Annual Income Statement
    latest_ia_fmp = income_annual_fmp[0] if income_annual_fmp else {}

    metrics["eps"] = safe_get_float(latest_ia_fmp, "eps") or \
                     safe_get_float(latest_km_a_fmp, "eps") or \
                     safe_get_float(overview_av, "EPS")

    metrics["net_profit_margin"] = safe_get_float(latest_ia_fmp, "netProfitMargin") or \
                                   safe_get_float(overview_av, "ProfitMargin")

    # Gross Profit Margin: FMP or (AV GrossProfitTTM / AV RevenueTTM)
    fmp_gross_margin = safe_get_float(latest_ia_fmp, "grossProfitMargin")
    if fmp_gross_margin is not None:
        metrics["gross_profit_margin"] = fmp_gross_margin
    else:
        av_gross_profit_ttm = safe_get_float(overview_av, "GrossProfitTTM")
        av_revenue_ttm = safe_get_float(overview_av, "RevenueTTM")
        if av_gross_profit_ttm is not None and av_revenue_ttm is not None and av_revenue_ttm != 0:
            metrics["gross_profit_margin"] = av_gross_profit_ttm / av_revenue_ttm
        else:
            metrics["gross_profit_margin"] = None

    metrics["operating_profit_margin"] = safe_get_float(latest_ia_fmp,
                                                        "operatingIncomeRatio")  # FMP specific for op margin
    # AlphaVantage overview_av also has "OperatingMarginTTM"
    if metrics["operating_profit_margin"] is None:
        metrics["operating_profit_margin"] = safe_get_float(overview_av, "OperatingMarginTTM")

    ebit_fmp = safe_get_float(latest_ia_fmp, "operatingIncome")
    interest_expense_fmp = safe_get_float(latest_ia_fmp, "interestExpense")
    if ebit_fmp is not None and interest_expense_fmp is not None and abs(interest_expense_fmp) > 1e-6:
        metrics["interest_coverage_ratio"] = ebit_fmp / abs(interest_expense_fmp)
    else:
        metrics["interest_coverage_ratio"] = None

    # ROE, ROA from various sources
    # Priority: FMP calculations > AlphaVantage direct > Finnhub direct
    # FMP calculation parts:
    total_equity_fmp = get_value_from_statement_list(balance_annual_fmp, "totalStockholdersEquity", 0)
    total_assets_fmp = get_value_from_statement_list(balance_annual_fmp, "totalAssets", 0)
    latest_net_income_fmp = get_value_from_statement_list(income_annual_fmp, "netIncome", 0)

    roe_fmp_calc = None
    if total_equity_fmp and total_equity_fmp != 0 and latest_net_income_fmp is not None:
        roe_fmp_calc = latest_net_income_fmp / total_equity_fmp

    roa_fmp_calc = None
    if total_assets_fmp and total_assets_fmp != 0 and latest_net_income_fmp is not None:
        roa_fmp_calc = latest_net_income_fmp / total_assets_fmp

    metrics["roe"] = roe_fmp_calc if roe_fmp_calc is not None else safe_get_float(overview_av, "ReturnOnEquityTTM")
    metrics["roa"] = roa_fmp_calc if roa_fmp_calc is not None else safe_get_float(overview_av, "ReturnOnAssetsTTM")

    # ROIC Calculation (Primarily FMP based due to detail needed)
    ebit_roic_fmp = get_value_from_statement_list(income_annual_fmp, "operatingIncome", 0)
    income_tax_expense_roic_fmp = get_value_from_statement_list(income_annual_fmp, "incomeTaxExpense", 0)
    income_before_tax_roic_fmp = get_value_from_statement_list(income_annual_fmp, "incomeBeforeTax", 0)

    effective_tax_rate = 0.21  # Default
    if income_tax_expense_roic_fmp is not None and income_before_tax_roic_fmp is not None and income_before_tax_roic_fmp != 0:
        calculated_tax_rate = income_tax_expense_roic_fmp / income_before_tax_roic_fmp
        if 0 <= calculated_tax_rate <= 0.50:
            effective_tax_rate = calculated_tax_rate
        else:
            logger.debug(
                f"Calculated tax rate {calculated_tax_rate:.2%} for {ticker} is unusual. Using default {effective_tax_rate:.2%}.")

    nopat_fmp = ebit_roic_fmp * (1 - effective_tax_rate) if ebit_roic_fmp is not None else None

    total_debt_roic_fmp = get_value_from_statement_list(balance_annual_fmp, "totalDebt", 0)
    cash_equivalents_roic_fmp = get_value_from_statement_list(balance_annual_fmp, "cashAndCashEquivalents", 0) or 0

    if total_debt_roic_fmp is not None and total_equity_fmp is not None:  # total_equity_fmp defined above
        invested_capital_fmp = total_debt_roic_fmp + total_equity_fmp - cash_equivalents_roic_fmp
        if nopat_fmp is not None and invested_capital_fmp is not None and invested_capital_fmp != 0:
            metrics["roic"] = nopat_fmp / invested_capital_fmp
        else:
            metrics["roic"] = None  # FMP ROIC calc failed
    else:
        metrics["roic"] = None  # FMP ROIC calc failed

    # AlphaVantage overview_av does not have ROIC directly.
    # Finnhub basic_financials might have roicAnnual, roicTTM under 'metric'
    if metrics["roic"] is None:
        fh_metrics = analyzer_instance._financial_data_cache.get('basic_financials_finnhub', {}).get('metric', {})
        metrics["roic"] = safe_get_float(fh_metrics, "roicTTM") or safe_get_float(fh_metrics, "roicAnnual")

    return metrics


def _calculate_financial_health_metrics(balance_annual_fmp, income_annual_fmp, latest_km_a_fmp, overview_av):
    metrics = {}
    latest_ba_fmp = balance_annual_fmp[0] if balance_annual_fmp else {}
    total_equity_fmp = safe_get_float(latest_ba_fmp, "totalStockholdersEquity")

    # Debt-to-Equity: FMP Key Metric > FMP Balance Sheet Calc > AlphaVantage Overview
    metrics["debt_to_equity"] = safe_get_float(latest_km_a_fmp, "debtToEquity")
    if metrics["debt_to_equity"] is None:
        total_debt_ba_fmp = safe_get_float(latest_ba_fmp, "totalDebt")
        if total_debt_ba_fmp is not None and total_equity_fmp and total_equity_fmp != 0:
            metrics["debt_to_equity"] = total_debt_ba_fmp / total_equity_fmp
    if metrics["debt_to_equity"] is None:
        # AlphaVantage has total debt and total equity in quarterly balance sheets, not directly in overview.
        # And DebtToEquityRatio is usually a TTM or annual metric. For now, stick to FMP.
        pass

    current_assets_fmp = safe_get_float(latest_ba_fmp, "totalCurrentAssets")
    current_liabilities_fmp = safe_get_float(latest_ba_fmp, "totalCurrentLiabilities")
    if current_assets_fmp is not None and current_liabilities_fmp is not None and current_liabilities_fmp != 0:
        metrics["current_ratio"] = current_assets_fmp / current_liabilities_fmp
    else:  # Fallback to AlphaVantage if FMP fails
        metrics["current_ratio"] = safe_get_float(overview_av, "CurrentRatio")

    cash_equivalents_fmp = safe_get_float(latest_ba_fmp, "cashAndCashEquivalents", 0)
    short_term_investments_fmp = safe_get_float(latest_ba_fmp, "shortTermInvestments", 0)
    net_receivables_fmp = safe_get_float(latest_ba_fmp, "netReceivables", 0)
    if current_liabilities_fmp is not None and current_liabilities_fmp != 0:  # Requires FMP current_liabilities
        metrics["quick_ratio"] = (
                                             cash_equivalents_fmp + short_term_investments_fmp + net_receivables_fmp) / current_liabilities_fmp
    else:  # Fallback to AlphaVantage if FMP fails
        # AlphaVantage overview_av does not directly provide quick ratio components in a simple way.
        # It might be in the full balance sheet. For now, if FMP fails, quick_ratio might be None.
        metrics["quick_ratio"] = None

    # Debt-to-EBITDA
    # Priority: FMP Key Metric > FMP Calc (Total Debt / EBITDA from Income Statement)
    latest_annual_ebitda_km_fmp = safe_get_float(latest_km_a_fmp, "ebitda")
    latest_annual_ebitda_is_fmp = get_value_from_statement_list(income_annual_fmp, "ebitda", 0)
    latest_annual_ebitda_fmp = latest_annual_ebitda_km_fmp if latest_annual_ebitda_km_fmp is not None else latest_annual_ebitda_is_fmp

    if latest_annual_ebitda_fmp and latest_annual_ebitda_fmp != 0:
        total_debt_val_fmp = get_value_from_statement_list(balance_annual_fmp, "totalDebt", 0)
        if total_debt_val_fmp is not None:
            metrics["debt_to_ebitda"] = total_debt_val_fmp / latest_annual_ebitda_fmp
        else:
            metrics["debt_to_ebitda"] = None
    else:
        metrics["debt_to_ebitda"] = None

    # AlphaVantage overview_av doesn't have DebtToEBITDA.

    return metrics


def _get_cross_validated_quarterly_revenue(analyzer_instance, statements_cache):
    ticker = analyzer_instance.ticker
    latest_q_revenue, previous_q_revenue, source_name, historical_revenues = None, None, None, []

    # Define revenue field names for each source
    revenue_fields = {
        "fmp_quarterly": "revenue",
        "alphavantage_quarterly": "totalRevenue",
        "finnhub_quarterly": ["Revenues", "RevenueFromContractWithCustomerExcludingAssessedTax", "TotalRevenues",
                              "NetSales"]  # List of possible concepts
    }

    for src_key in PRIORITY_REVENUE_SOURCES:
        try:
            if src_key == "fmp_quarterly" and statements_cache.get('fmp_income_quarterly'):
                reports = statements_cache['fmp_income_quarterly']
                if not reports: continue
                latest_val = get_fmp_value(reports, revenue_fields[src_key], 0)
                prev_val = get_fmp_value(reports, revenue_fields[src_key], 1) if len(reports) > 1 else None
                if latest_val is not None:
                    latest_q_revenue, previous_q_revenue, source_name = latest_val, prev_val, "FMP"
                    for i in range(min(len(reports), 5)):  # Get up to 5 historical points
                        h_val = get_fmp_value(reports, revenue_fields[src_key], i)
                        if h_val is not None: historical_revenues.append(h_val)
                    break
            elif src_key == "alphavantage_quarterly" and statements_cache.get('alphavantage_income_quarterly', {}).get(
                    'quarterlyReports'):
                reports = statements_cache['alphavantage_income_quarterly']['quarterlyReports']
                if not reports: continue
                latest_val = get_alphavantage_value(reports, revenue_fields[src_key], 0)
                prev_val = get_alphavantage_value(reports, revenue_fields[src_key], 1) if len(reports) > 1 else None
                if latest_val is not None:
                    latest_q_revenue, previous_q_revenue, source_name = latest_val, prev_val, "AlphaVantage"
                    for i in range(min(len(reports), 5)):
                        h_val = get_alphavantage_value(reports, revenue_fields[src_key], i)
                        if h_val is not None: historical_revenues.append(h_val)
                    break
            elif src_key == "finnhub_quarterly" and statements_cache.get('finnhub_financials_quarterly_reported',
                                                                         {}).get('data'):
                reports = statements_cache['finnhub_financials_quarterly_reported']['data']
                if not reports: continue
                latest_val = get_finnhub_concept_value(reports, 'ic', revenue_fields[src_key], 0)
                prev_val = get_finnhub_concept_value(reports, 'ic', revenue_fields[src_key], 1) if len(
                    reports) > 1 else None
                if latest_val is not None:
                    latest_q_revenue, previous_q_revenue, source_name = latest_val, prev_val, "Finnhub"
                    for i in range(min(len(reports), 5)):
                        h_val = get_finnhub_concept_value(reports, 'ic', revenue_fields[src_key], i)
                        if h_val is not None: historical_revenues.append(h_val)
                    break
        except Exception as e:
            logger.warning(f"Error processing quarterly revenue from {src_key} for {ticker}: {e}")
            continue

    avg_historical_q_revenue = None
    if historical_revenues:
        points_for_avg = [r for r in historical_revenues if r is not None and r > 0]  # Use only positive values
        # If latest_q_revenue is the first in historical_revenues, exclude it for avg calculation to compare against prior periods
        avg_base_points = points_for_avg[1:] if points_for_avg and points_for_avg[0] == latest_q_revenue and len(
            points_for_avg) > 1 else points_for_avg

        if len(avg_base_points) > 1:  # Need at least two points for a meaningful average
            avg_historical_q_revenue = sum(avg_base_points) / len(avg_base_points)
            if latest_q_revenue is not None and avg_historical_q_revenue > 0:  # Ensure avg is positive for deviation calc
                deviation = abs(latest_q_revenue - avg_historical_q_revenue) / avg_historical_q_revenue
                if deviation > Q_REVENUE_SANITY_CHECK_DEVIATION_THRESHOLD:
                    warning_msg = (
                        f"DATA QUALITY WARNING: Latest quarterly revenue ({latest_q_revenue:,.0f} from {source_name}) "
                        f"deviates by {deviation:.2%} from avg of recent historical quarters ({avg_historical_q_revenue:,.0f}). "
                        f"Review data accuracy.")
                    logger.warning(warning_msg)
                    analyzer_instance.data_quality_warnings.append(warning_msg)
        else:
            logger.info(
                f"Not enough historical quarterly revenue data (after filtering for positive values and excluding current if present) to perform sanity check for {ticker}.")
    else:
        logger.info(f"No historical quarterly revenue data found for sanity check for {ticker}.")

    if latest_q_revenue is None:
        logger.error(f"Could not determine latest quarterly revenue for {ticker} from any source.")
        analyzer_instance.data_quality_warnings.append("CRITICAL: Latest quarterly revenue could not be determined.")
    else:
        logger.info(f"Using latest quarterly revenue: {latest_q_revenue:,.0f} (Source: {source_name}) for {ticker}.")

    return latest_q_revenue, previous_q_revenue, source_name, avg_historical_q_revenue


def _calculate_growth_metrics(analyzer_instance, income_annual_fmp, statements_cache, overview_av):
    metrics = {"key_metrics_snapshot": {}}  # Initialize snapshot dict
    ticker = analyzer_instance.ticker

    # YoY Growth
    # FMP Annual Revenue
    fmp_revenue_y0 = get_value_from_statement_list(income_annual_fmp, "revenue", 0)
    fmp_revenue_y1 = get_value_from_statement_list(income_annual_fmp, "revenue", 1)

    # FMP Annual EPS
    fmp_eps_y0 = get_value_from_statement_list(income_annual_fmp, "eps", 0)
    fmp_eps_y1 = get_value_from_statement_list(income_annual_fmp, "eps", 1)

    metrics["revenue_growth_yoy"] = calculate_growth(fmp_revenue_y0, fmp_revenue_y1)
    metrics["eps_growth_yoy"] = calculate_growth(fmp_eps_y0, fmp_eps_y1)

    # AlphaVantage has "QuarterlyRevenueGrowthYOY", "QuarterlyEarningsGrowthYOY"
    # These are quarterly YoY. We are calculating annual YoY above.
    # We can add AV's TTM RevenueGrowth and EPSGrowth if available as fallbacks for YoY.
    # Overview_av has "RevenueGrowth" but it's usually for TTM or MRQ.
    # Let's stick to FMP for annual YoY growth for now due to clarity of period.

    # CAGR 3-year
    if len(income_annual_fmp) >= 3:
        metrics["revenue_growth_cagr_3yr"] = calculate_cagr(
            get_value_from_statement_list(income_annual_fmp, "revenue", 0),
            get_value_from_statement_list(income_annual_fmp, "revenue", 2), 2
        )
        metrics["eps_growth_cagr_3yr"] = calculate_cagr(
            get_value_from_statement_list(income_annual_fmp, "eps", 0),
            get_value_from_statement_list(income_annual_fmp, "eps", 2), 2
        )
    else:
        metrics["revenue_growth_cagr_3yr"] = None
        metrics["eps_growth_cagr_3yr"] = None

    # CAGR 5-year
    if len(income_annual_fmp) >= 5:
        metrics["revenue_growth_cagr_5yr"] = calculate_cagr(
            get_value_from_statement_list(income_annual_fmp, "revenue", 0),
            get_value_from_statement_list(income_annual_fmp, "revenue", 4), 4
        )
        metrics["eps_growth_cagr_5yr"] = calculate_cagr(
            get_value_from_statement_list(income_annual_fmp, "eps", 0),
            get_value_from_statement_list(income_annual_fmp, "eps", 4), 4
        )
    else:
        metrics["revenue_growth_cagr_5yr"] = None
        metrics["eps_growth_cagr_5yr"] = None

    # QoQ Revenue Growth
    latest_q_rev, prev_q_rev, rev_src_name, avg_hist_q_rev = _get_cross_validated_quarterly_revenue(analyzer_instance,
                                                                                                    statements_cache)

    if latest_q_rev is not None:
        metrics["key_metrics_snapshot"]["q_revenue_source"] = rev_src_name
        metrics["key_metrics_snapshot"]["latest_q_revenue"] = latest_q_rev
        metrics["key_metrics_snapshot"]["avg_historical_q_revenue_for_check"] = avg_hist_q_rev
        if prev_q_rev is not None:
            metrics["revenue_growth_qoq"] = calculate_growth(latest_q_rev, prev_q_rev)
        else:
            logger.info(
                f"Previous quarter revenue not available from source {rev_src_name} for {ticker}. Cannot calculate QoQ revenue growth.")
            metrics["revenue_growth_qoq"] = None
    else:  # Fallback to AlphaVantage QuarterlyRevenueGrowthYOY as a proxy if direct QoQ fails
        metrics["revenue_growth_qoq"] = safe_get_float(overview_av,
                                                       "QuarterlyRevenueGrowthYOY")  # Note: This is YOY not QOQ.
        metrics["key_metrics_snapshot"]["q_revenue_source"] = "N/A (or AV YOY as proxy)" if metrics[
                                                                                                "revenue_growth_qoq"] is None else "AlphaVantage (QuarterlyYoY as QoQ proxy)"
        metrics["key_metrics_snapshot"]["latest_q_revenue"] = None  # Can't determine specific latest Q revenue
        metrics["key_metrics_snapshot"]["avg_historical_q_revenue_for_check"] = None

    return metrics


def _calculate_cash_flow_and_trend_metrics(cashflow_annual_fmp, balance_annual_fmp, profile_fmp, overview_av):
    metrics = {}

    # FCF per Share & FCF Yield
    fcf_latest_annual_fmp = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 0)

    shares_outstanding_profile_fmp = safe_get_float(profile_fmp, "sharesOutstanding")
    mkt_cap_profile_fmp = safe_get_float(profile_fmp, "mktCap")
    price_profile_fmp = safe_get_float(profile_fmp, "price")

    # Use AlphaVantage SharesOutstanding if FMP's is missing
    shares_outstanding_av = safe_get_float(overview_av, "SharesOutstanding")
    shares_outstanding = shares_outstanding_profile_fmp if shares_outstanding_profile_fmp is not None and shares_outstanding_profile_fmp > 0 else shares_outstanding_av

    # Calculate shares outstanding if direct value is missing or zero, using mktCap and price
    if (
            shares_outstanding is None or shares_outstanding == 0) and mkt_cap_profile_fmp and price_profile_fmp and price_profile_fmp != 0:
        shares_outstanding = mkt_cap_profile_fmp / price_profile_fmp

    if fcf_latest_annual_fmp is not None and shares_outstanding and shares_outstanding != 0:
        metrics["free_cash_flow_per_share"] = fcf_latest_annual_fmp / shares_outstanding
        # Use MktCap from FMP profile first, then AV overview for FCF Yield
        mkt_cap_for_yield = mkt_cap_profile_fmp if mkt_cap_profile_fmp else safe_get_float(overview_av,
                                                                                           "MarketCapitalization")
        if mkt_cap_for_yield and mkt_cap_for_yield != 0:
            metrics["free_cash_flow_yield"] = fcf_latest_annual_fmp / mkt_cap_for_yield
        else:
            metrics["free_cash_flow_yield"] = None
    else:
        metrics["free_cash_flow_per_share"] = None
        metrics["free_cash_flow_yield"] = None

    # FCF Trend (3-year simple trend from FMP annual data)
    if len(cashflow_annual_fmp) >= 3:
        fcf0 = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 0)
        fcf1 = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 1)
        fcf2 = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 2)

        if all(isinstance(x, (int, float)) for x in [fcf0, fcf1, fcf2] if x is not None) and \
                all(x is not None for x in [fcf0, fcf1, fcf2]):
            if fcf0 > fcf1 > fcf2:
                metrics["free_cash_flow_trend"] = "Growing"
            elif fcf0 < fcf1 < fcf2:
                metrics["free_cash_flow_trend"] = "Declining"
            elif fcf0 > fcf1 and fcf1 < fcf2:
                metrics["free_cash_flow_trend"] = "Volatile (Dip then Rise)"
            elif fcf0 < fcf1 and fcf1 > fcf2:
                metrics["free_cash_flow_trend"] = "Volatile (Rise then Dip)"
            else:
                metrics["free_cash_flow_trend"] = "Mixed/Stable"
        else:
            metrics["free_cash_flow_trend"] = "Data Incomplete/Non-Numeric"
    else:
        metrics["free_cash_flow_trend"] = "Data N/A (<3 yrs)"

    # Retained Earnings Trend (3-year simple trend from FMP annual data)
    if len(balance_annual_fmp) >= 3:
        re0 = get_value_from_statement_list(balance_annual_fmp, "retainedEarnings", 0)
        re1 = get_value_from_statement_list(balance_annual_fmp, "retainedEarnings", 1)
        re2 = get_value_from_statement_list(balance_annual_fmp, "retainedEarnings", 2)

        if all(isinstance(x, (int, float)) for x in [re0, re1, re2] if x is not None) and \
                all(x is not None for x in [re0, re1, re2]):
            if re0 > re1 > re2:
                metrics["retained_earnings_trend"] = "Growing"
            elif re0 < re1 < re2:
                metrics["retained_earnings_trend"] = "Declining"
            else:
                metrics["retained_earnings_trend"] = "Mixed/Stable"
        else:
            metrics["retained_earnings_trend"] = "Data Incomplete/Non-Numeric"
    else:
        metrics["retained_earnings_trend"] = "Data N/A (<3 yrs)"

    return metrics


def calculate_all_derived_metrics(analyzer_instance):
    logger.info(f"Calculating derived metrics for {analyzer_instance.ticker}...")
    all_metrics_temp = {}

    # Retrieve cached data
    statements = analyzer_instance._financial_data_cache.get('financial_statements', {})
    income_annual_fmp = statements.get('fmp_income_annual', [])
    balance_annual_fmp = statements.get('fmp_balance_annual', [])
    cashflow_annual_fmp = statements.get('fmp_cashflow_annual', [])

    key_metrics_annual_fmp = analyzer_instance._financial_data_cache.get('key_metrics_annual_fmp', [])
    key_metrics_quarterly_fmp = analyzer_instance._financial_data_cache.get('key_metrics_quarterly_fmp', [])

    basic_fin_fh_metric = analyzer_instance._financial_data_cache.get('basic_financials_finnhub', {}).get('metric', {})
    profile_fmp = analyzer_instance._financial_data_cache.get('profile_fmp', {})
    # Ensure AlphaVantage overview is available
    overview_av = analyzer_instance._financial_data_cache.get('overview_alphavantage', {})
    if not overview_av:  # If somehow not fetched by stock_analyzer's init
        logger.warning(f"AlphaVantage overview data not found in cache for {analyzer_instance.ticker}. Fetching now.")
        overview_av = analyzer_instance.alphavantage.get_company_overview(analyzer_instance.ticker)
        analyzer_instance._financial_data_cache['overview_alphavantage'] = overview_av if overview_av else {}

    latest_km_q_fmp = key_metrics_quarterly_fmp[0] if key_metrics_quarterly_fmp else {}
    latest_km_a_fmp = key_metrics_annual_fmp[0] if key_metrics_annual_fmp else {}

    all_metrics_temp.update(
        _calculate_valuation_ratios(latest_km_q_fmp, latest_km_a_fmp, basic_fin_fh_metric, overview_av))
    all_metrics_temp.update(
        _calculate_profitability_metrics(analyzer_instance, income_annual_fmp, balance_annual_fmp, latest_km_a_fmp,
                                         overview_av))
    all_metrics_temp.update(
        _calculate_financial_health_metrics(balance_annual_fmp, income_annual_fmp, latest_km_a_fmp, overview_av))

    growth_metrics_result = _calculate_growth_metrics(analyzer_instance, income_annual_fmp, statements, overview_av)
    all_metrics_temp.update(growth_metrics_result)

    all_metrics_temp.update(
        _calculate_cash_flow_and_trend_metrics(cashflow_annual_fmp, balance_annual_fmp, profile_fmp, overview_av))

    final_metrics_cleaned = {}
    key_metrics_snapshot_data = all_metrics_temp.pop("key_metrics_snapshot", {})

    for k, v in all_metrics_temp.items():
        if isinstance(v, float):
            final_metrics_cleaned[k] = v if not (math.isnan(v) or math.isinf(v)) else None
        elif v is not None:
            final_metrics_cleaned[k] = v
        else:
            final_metrics_cleaned[k] = None

    final_metrics_cleaned["key_metrics_snapshot"] = {
        sk: sv for sk, sv in key_metrics_snapshot_data.items()
        if sv is not None and not (isinstance(sv, float) and (math.isnan(sv) or math.isinf(sv)))
    }

    log_metrics_display = {k: v for k, v in final_metrics_cleaned.items() if k != "key_metrics_snapshot"}
    logger.info(
        f"Calculated metrics for {analyzer_instance.ticker}: {json.dumps(log_metrics_display, indent=2, default=str)}")
    if final_metrics_cleaned["key_metrics_snapshot"]:
        logger.info(
            f"Key metrics snapshot for {analyzer_instance.ticker}: {json.dumps(final_metrics_cleaned['key_metrics_snapshot'], indent=2, default=str)}")

    analyzer_instance._financial_data_cache['calculated_metrics'] = final_metrics_cleaned
    return final_metrics_cleaned
---------- END metrics_calculator.py ----------


---------- qualitative_analyzer.py ----------
# services/stock_analyzer/qualitative_analyzer.py
import time
import json
from core.logging_setup import logger
from api_clients import extract_S1_text_sections
from core.config import (
    TEN_K_KEY_SECTIONS, SUMMARIZATION_CHUNK_SIZE_CHARS,
    SUMMARIZATION_CHUNK_OVERLAP_CHARS, SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS,
    MAX_COMPETITORS_TO_ANALYZE, AI_JSON_OUTPUT_INSTRUCTION
)
from .helpers import safe_get_float


def _summarize_text_chunked_for_json(analyzer_instance, text_to_summarize, base_context, section_specific_instruction,
                                     company_name_ticker_prompt, json_structure_example):
    gemini_client = analyzer_instance.gemini
    default_error_response = {"error": f"AI summary error or no content for '{base_context}'.", "summary": "",
                              "keyPoints": []}

    if not text_to_summarize or not isinstance(text_to_summarize, str) or not text_to_summarize.strip():
        return default_error_response, 0

    text_len = len(text_to_summarize)
    logger.info(
        f"Summarizing '{base_context}' for {company_name_ticker_prompt} (JSON output), original length: {text_len} chars.")

    final_prompt_instruction = (
        f"{section_specific_instruction}\n"
        f"Your entire response MUST be a single, valid JSON object structured as follows: {json_structure_example}"
    )

    if text_len < SUMMARIZATION_CHUNK_SIZE_CHARS:  # Adjusted for potentially larger JSON structure in prompt/output
        logger.info(
            f"Section length {text_len} is within single-pass limit ({SUMMARIZATION_CHUNK_SIZE_CHARS}). Summarizing directly for JSON.")
        summary_json = gemini_client.generate_text(
            f"Text to Summarize from '{base_context}' for {company_name_ticker_prompt}:\n\"\"\"\n{text_to_summarize}\n\"\"\"\n\n{final_prompt_instruction}",
            output_format="json"
        )
        time.sleep(2)
        if isinstance(summary_json, dict) and not summary_json.get("error"):
            return summary_json, text_len
        else:
            logger.error(f"Direct JSON summarization failed for '{base_context}'. Response: {summary_json}")
            return default_error_response, text_len

    # Chunked summarization
    logger.info(f"Section length {text_len} exceeds single-pass limit. Applying chunked summarization for JSON output.")
    chunks = []
    start = 0
    while start < text_len:
        end = start + SUMMARIZATION_CHUNK_SIZE_CHARS
        chunks.append(text_to_summarize[start:end])
        start = end - SUMMARIZATION_CHUNK_OVERLAP_CHARS if end < text_len else end

    chunk_summaries_text = []  # Store text summaries of chunks
    for i, chunk in enumerate(chunks):
        logger.info(
            f"Summarizing chunk {i + 1}/{len(chunks)} for '{base_context}' of {company_name_ticker_prompt} (length: {len(chunk)} chars) as text part.")
        # Summarize chunks into text first to avoid overly complex JSON handling for each small piece
        chunk_summary_text = gemini_client.summarize_text_with_context(
            chunk,
            f"This is chunk {i + 1} of {len(chunks)} from the '{base_context}' section for {company_name_ticker_prompt}.",
            f"Concisely summarize the key information in this chunk relevant to: {section_specific_instruction.splitlines()[0]}"
            # Simpler instruction for chunk
        )  # output_format="text" by default
        time.sleep(2)
        chunk_summaries_text.append(chunk_summary_text if chunk_summary_text and not chunk_summary_text.startswith(
            "Error:") else f"[AI error or no content for chunk {i + 1}]")

    if not chunk_summaries_text or all("[AI error" in s for s in chunk_summaries_text):
        return default_error_response, text_len

    concatenated_summaries = "\n\n---\n\n".join(chunk_summaries_text)
    logger.info(f"Concatenated chunk text summaries length for '{base_context}': {len(concatenated_summaries)} chars.")

    if len(concatenated_summaries) > SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS:  # If even concatenated text summaries are too long
        logger.warning(
            f"Concatenated text summaries for '{base_context}' too long ({len(concatenated_summaries)}). Truncating for final JSON generation.")
        concatenated_summaries = concatenated_summaries[:SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS]

    # Final pass: generate JSON from the concatenated text summaries
    logger.info(f"Generating final JSON summary for '{base_context}' from concatenated chunk summaries.")
    final_summary_json = gemini_client.generate_text(
        f"The following are collated summaries from different parts of the '{base_context}' section for {company_name_ticker_prompt}:\n\"\"\"\n{concatenated_summaries}\n\"\"\"\n\n"
        f"Synthesize these into a single, cohesive overview for '{base_context}'.\n{final_prompt_instruction}",
        output_format="json"
    )
    time.sleep(2)

    if isinstance(final_summary_json, dict) and not final_summary_json.get("error"):
        return final_summary_json, text_len
    else:
        logger.error(
            f"Final JSON summarization from chunks failed for '{base_context}'. Response: {final_summary_json}")
        # Fallback: return the concatenated text if JSON fails, wrapped in the error structure
        default_error_response[
            "summary"] = "AI error in final JSON summary pass. Concatenated text summaries provided instead."
        default_error_response["keyPoints"] = [concatenated_summaries[:1000]]  # Truncated
        return default_error_response, text_len


def fetch_and_summarize_10k_data(analyzer_instance):
    ticker = analyzer_instance.ticker
    logger.info(f"Fetching and attempting to summarize latest 10-K for {ticker} into JSON...")
    # Initialize with keys for where the JSON data itself will be stored
    summary_results = {
        "qualitative_sources_summary": {},
        "business_summary_data": None,
        "risk_factors_summary_data": None,
        "management_assessment_summary_data": None,  # MDA
        "economic_moat_summary_data": None,
        "industry_trends_summary_data": None,
    }

    if not analyzer_instance.stock_db_entry or not analyzer_instance.stock_db_entry.cik:
        logger.warning(f"No CIK for {ticker}. Cannot fetch 10-K.")
        # Populate error state for summary data fields
        for key in ["business_summary_data", "risk_factors_summary_data", "management_assessment_summary_data"]:
            summary_results[key] = {"error": "No CIK available for 10-K fetching."}
        return summary_results

    filing_url = analyzer_instance.sec_edgar.get_filing_document_url(analyzer_instance.stock_db_entry.cik, "10-K")
    time.sleep(0.5)
    if not filing_url:
        logger.info(f"No recent 10-K found for {ticker}, trying 10-K/A.")
        filing_url = analyzer_instance.sec_edgar.get_filing_document_url(analyzer_instance.stock_db_entry.cik, "10-K/A")
        time.sleep(0.5)

    if not filing_url:
        logger.warning(f"No 10-K or 10-K/A URL found for {ticker} (CIK: {analyzer_instance.stock_db_entry.cik})")
        for key in ["business_summary_data", "risk_factors_summary_data", "management_assessment_summary_data"]:
            summary_results[key] = {"error": "No 10-K or 10-K/A URL found."}
        return summary_results

    summary_results["qualitative_sources_summary"]["10k_filing_url_used"] = filing_url
    text_content = analyzer_instance.sec_edgar.get_filing_text(filing_url)

    if not text_content:
        logger.warning(f"Failed to fetch/load 10-K text from {filing_url}")
        for key in ["business_summary_data", "risk_factors_summary_data", "management_assessment_summary_data"]:
            summary_results[key] = {"error": f"Failed to fetch 10-K text from {filing_url}."}
        return summary_results

    logger.info(f"Fetched 10-K text (length: {len(text_content)}) for {ticker}. Extracting and summarizing sections.")
    sections = extract_S1_text_sections(text_content, TEN_K_KEY_SECTIONS)
    company_name_for_prompt = analyzer_instance.stock_db_entry.company_name or ticker

    # Define JSON structure for basic summaries
    basic_summary_json_structure = "{ \"summary\": \"Comprehensive summary text...\", \"keyPoints\": [\"Key point 1...\", \"Key point 2...\"] }"

    section_details = {
        "business": ("Business (Item 1)",
                     "Summarize the company's core business operations, primary products/services, revenue generation model, key customer segments, and primary markets. Highlight any recent strategic shifts mentioned.",
                     "business_summary_data"),
        "risk_factors": ("Risk Factors (Item 1A)",
                         "Identify and summarize the 3-5 most significant and company-specific risk factors disclosed. Focus on operational and strategic risks. Briefly explain the potential impact of each.",
                         "risk_factors_summary_data"),
        "mda": ("Management's Discussion and Analysis (Item 7)",
                "Summarize key insights into financial performance drivers (revenue, costs, profitability), financial condition (liquidity, capital resources), and management's outlook or significant focus areas. Note any discussion on margin pressures or segment performance changes.",
                "management_assessment_summary_data")  # Target key for summary_results
    }

    for section_key, (prompt_section_name, specific_instruction, target_summary_key) in section_details.items():
        section_text = sections.get(section_key)
        json_response = None
        source_len = 0
        if not section_text:
            logger.warning(f"Section '{prompt_section_name}' not found in 10-K for {ticker}.")
            json_response = {"error": f"Section '{prompt_section_name}' not found in 10-K document."}
        else:
            source_len = len(section_text)
            json_response, _ = _summarize_text_chunked_for_json(
                analyzer_instance, section_text, prompt_section_name,
                specific_instruction, f"{company_name_for_prompt} ({ticker})",
                basic_summary_json_structure
            )
        summary_results[target_summary_key] = json_response
        summary_results["qualitative_sources_summary"][f"{section_key}_10k_source_length"] = source_len
        logger.info(
            f"JSON Summary for '{prompt_section_name}' (source length {source_len}): {str(json_response)[:150].replace(chr(10), ' ')}...")

    # Economic Moat Analysis (derived from business and risk summaries)
    biz_summary_data = summary_results.get("business_summary_data", {})
    risk_summary_data = summary_results.get("risk_factors_summary_data", {})
    biz_summary_text = biz_summary_data.get("summary", "") if isinstance(biz_summary_data, dict) else ""
    risk_summary_text = risk_summary_data.get("summary", "") if isinstance(risk_summary_data, dict) else ""

    moat_json_structure = "{ \"moats\": [ { \"moatType\": \"Brand Strength|Network Effects|etc.\", \"evidence\": \"...\", \"strength\": \"Very Strong|Strong|Moderate|Weak\" } ], \"overallAssessment\": \"Overall summary of moat strength...\" }"
    if biz_summary_text or risk_summary_text:
        moat_input_text = (
            f"Business Summary:\n{biz_summary_text}\n\nRisk Factors Summary:\n{risk_summary_text}").strip()
        moat_prompt = (
            f"Analyze the primary economic moats for {company_name_for_prompt} ({ticker}), based on the following summaries from its 10-K:\n\n{moat_input_text}\n\n"
            f"Provide your analysis as a JSON object. {AI_JSON_OUTPUT_INSTRUCTION} Structure it as: {moat_json_structure}"
        )
        moat_summary_json = analyzer_instance.gemini.generate_text(moat_prompt, output_format="json")
        time.sleep(3)
        summary_results["economic_moat_summary_data"] = moat_summary_json if isinstance(moat_summary_json, dict) else {
            "error": "AI analysis for economic moat failed or returned non-JSON."}
    else:
        summary_results["economic_moat_summary_data"] = {
            "error": "Insufficient input from 10-K summaries for economic moat analysis."}

    # Industry Trends Analysis
    mda_summary_data = summary_results.get("management_assessment_summary_data", {})
    mda_summary_text = mda_summary_data.get("summary", "") if isinstance(mda_summary_data, dict) else ""

    industry_json_structure = "{ \"keyTrends\": [\"Trend 1...\", \"Trend 2...\"], \"opportunities\": [\"Opportunity 1...\"], \"challenges\": [\"Challenge 1...\"], \"companyPositioning\": \"How the company is positioned...\", \"overallOutlook\": \"Brief outlook statement...\" }"
    if biz_summary_text:
        industry_context_text = (
            f"Company: {company_name_for_prompt} ({ticker})\n"
            f"Industry: {analyzer_instance.stock_db_entry.industry or 'Not Specified'}\n"
            f"Sector: {analyzer_instance.stock_db_entry.sector or 'Not Specified'}\n\n"
            f"Business Summary (from 10-K):\n{biz_summary_text}\n\n"
            f"MD&A Highlights (from 10-K):\n{mda_summary_text}"
        ).strip()
        industry_prompt = (
            f"Based on the provided information for {company_name_for_prompt} ({ticker}):\n\n{industry_context_text}\n\n"
            f"Analyze key industry trends, opportunities, challenges, and the company's positioning. "
            f"{AI_JSON_OUTPUT_INSTRUCTION} Structure it as: {industry_json_structure}"
        )
        industry_summary_json = analyzer_instance.gemini.generate_text(industry_prompt, output_format="json")
        time.sleep(3)
        summary_results["industry_trends_summary_data"] = industry_summary_json if isinstance(industry_summary_json,
                                                                                              dict) else {
            "error": "AI analysis for industry trends failed or returned non-JSON."}
    else:
        summary_results["industry_trends_summary_data"] = {
            "error": "Insufficient input (Business Summary missing) for industry analysis."}

    # Remove the old string-based keys if they exist from a previous version, only keep _data suffixed keys for JSON
    for old_key in ["business_summary", "risk_factors_summary", "management_assessment_summary",
                    "economic_moat_summary", "industry_trends_summary"]:
        if old_key in summary_results:
            del summary_results[old_key]

    logger.info(f"10-K qualitative JSON summaries and AI interpretations generated for {ticker}.")
    analyzer_instance._financial_data_cache[
        '10k_summaries'] = summary_results  # This now stores dicts with JSON objects
    return summary_results


def fetch_and_analyze_competitors(analyzer_instance):
    ticker = analyzer_instance.ticker
    logger.info(f"Fetching and analyzing competitor data for {ticker} (JSON output)...")

    default_error_summary = {
        "summary": "Competitor analysis not performed or failed.",
        "landscapeOverview": "N/A",
        "companyPositioning": "N/A",
        "keyDifferentials": [],
        "competitionIntensity": "N/A",
        "peers_data": []
    }

    peers_data_finnhub = analyzer_instance.finnhub.get_company_peers(ticker)
    time.sleep(1)  # Reduced sleep

    if not peers_data_finnhub or not isinstance(peers_data_finnhub, list) or not peers_data_finnhub[0]:
        logger.warning(f"No direct peer data found from Finnhub for {ticker}.")
        analyzer_instance._financial_data_cache['competitor_analysis'] = {**default_error_summary,
                                                                          "summary": "No peer data found from primary source (Finnhub)."}
        return default_error_summary["summary"]  # For compatibility, return the summary string

    # ... (peer fetching logic as before, unchanged, up to peer_details_list)
    if isinstance(peers_data_finnhub[0], list):
        peers_data_finnhub = peers_data_finnhub[0]
    peer_tickers = [p for p in peers_data_finnhub if p and p.upper() != ticker.upper()][:MAX_COMPETITORS_TO_ANALYZE]

    if not peer_tickers:
        logger.info(f"No distinct competitor tickers found after filtering for {ticker}.")
        analyzer_instance._financial_data_cache['competitor_analysis'] = {**default_error_summary,
                                                                          "summary": "No distinct competitor tickers identified."}
        return default_error_summary["summary"]

    logger.info(f"Identified peers for {ticker}: {peer_tickers}. Fetching basic data for comparison.")
    peer_details_list = []
    for peer_ticker_symbol in peer_tickers:
        try:
            logger.debug(f"Fetching basic data for peer: {peer_ticker_symbol}")
            peer_profile_fmp_list = analyzer_instance.fmp.get_company_profile(peer_ticker_symbol);
            time.sleep(1)
            peer_profile_fmp = peer_profile_fmp_list[0] if peer_profile_fmp_list and isinstance(peer_profile_fmp_list,
                                                                                                list) and \
                                                           peer_profile_fmp_list[0] else {}

            peer_metrics_fmp_list = analyzer_instance.fmp.get_key_metrics(peer_ticker_symbol, period="annual", limit=1);
            time.sleep(1)
            peer_metrics_fmp = peer_metrics_fmp_list[0] if peer_metrics_fmp_list and isinstance(peer_metrics_fmp_list,
                                                                                                list) and \
                                                           peer_metrics_fmp_list[0] else {}

            peer_fh_basics = {}
            if not peer_metrics_fmp.get("peRatio") or not peer_metrics_fmp.get("priceSalesRatio"):
                peer_fh_basics_data = analyzer_instance.finnhub.get_basic_financials(peer_ticker_symbol);
                time.sleep(1)
                peer_fh_basics = peer_fh_basics_data.get("metric", {}) if peer_fh_basics_data else {}

            peer_name = peer_profile_fmp.get("companyName", peer_ticker_symbol)
            market_cap = safe_get_float(peer_profile_fmp, "mktCap")
            pe_ratio = safe_get_float(peer_metrics_fmp, "peRatio") or safe_get_float(peer_fh_basics, "peTTM")
            ps_ratio = safe_get_float(peer_metrics_fmp, "priceSalesRatio") or safe_get_float(peer_fh_basics, "psTTM")

            peer_info = {"ticker": peer_ticker_symbol, "name": peer_name, "market_cap": market_cap,
                         "pe_ratio": pe_ratio, "ps_ratio": ps_ratio}
            if peer_name != peer_ticker_symbol or market_cap or pe_ratio or ps_ratio:
                peer_details_list.append(peer_info)
        except Exception as e:
            logger.warning(f"Error fetching data for peer {peer_ticker_symbol}: {e}",
                           exc_info=False)  # exc_info False for brevity
        if len(peer_details_list) >= MAX_COMPETITORS_TO_ANALYZE:
            break
    # ... end of unchanged peer fetching logic

    if not peer_details_list:
        analyzer_instance._financial_data_cache['competitor_analysis'] = {**default_error_summary,
                                                                          "summary": "Could not fetch sufficient data for identified competitors."}
        return default_error_summary["summary"]

    company_name_for_prompt = analyzer_instance.stock_db_entry.company_name or ticker
    k_summaries = analyzer_instance._financial_data_cache.get('10k_summaries', {})
    biz_summary_10k_data = k_summaries.get('business_summary_data', {})
    biz_summary_10k_text = biz_summary_10k_data.get('summary',
                                                    "Business summary from 10-K not available or failed.") if isinstance(
        biz_summary_10k_data, dict) else "N/A"

    prompt_context = (
        f"Company being analyzed: {company_name_for_prompt} ({ticker}).\n"
        f"Its 10-K Business Summary extract: {biz_summary_10k_text[:1000]}...\n\n"  # Truncate for prompt
        f"Identified Competitors and their basic data:\n"
    )
    for peer in peer_details_list:
        mc_str = f"{peer['market_cap']:,.0f}" if peer['market_cap'] else "N/A"
        pe_str = f"{peer['pe_ratio']:.2f}" if peer['pe_ratio'] is not None else "N/A"
        ps_str = f"{peer['ps_ratio']:.2f}" if peer['ps_ratio'] is not None else "N/A"
        prompt_context += f"- {peer['name']} ({peer['ticker']}): Market Cap: {mc_str}, P/E: {pe_str}, P/S: {ps_str}\n"

    competitor_json_structure = """
    {
      "landscapeOverview": "General overview of the competitive landscape...",
      "companyPositioning": "Positioning of the analyzed company relative to competitors...",
      "keyDifferentials": [
        "Key difference 1 (e.g., scale, valuation, focus)...",
        "Key difference 2..."
      ],
      "competitionIntensity": "High|Medium|Low",
      "dataLimitations": "Optional: Note if competitor data was sparse or limited."
    }
    """
    comp_prompt = (
        f"{prompt_context}\n\n"
        f"Instruction: Based on the business summary of {company_name_for_prompt} and the list of its competitors with their financial metrics, "
        f"provide a concise analysis of the competitive landscape. Discuss {company_name_for_prompt}'s market positioning. "
        f"Highlight key differences in scale or valuation. Address competition intensity. Do not invent information. "
        f"{AI_JSON_OUTPUT_INSTRUCTION} Structure it as: {competitor_json_structure}"
    )

    comp_summary_json = analyzer_instance.gemini.generate_text(comp_prompt, output_format="json")
    time.sleep(3)

    final_competitor_analysis_data = {**default_error_summary, "peers_data": peer_details_list}  # Start with default

    if isinstance(comp_summary_json, dict) and not comp_summary_json.get("error"):
        # Update with AI generated fields if they exist
        for key in ["landscapeOverview", "companyPositioning", "keyDifferentials", "competitionIntensity",
                    "dataLimitations"]:
            if key in comp_summary_json:
                final_competitor_analysis_data[key] = comp_summary_json[key]
        final_competitor_analysis_data["summary"] = comp_summary_json.get("landscapeOverview", default_error_summary[
            "summary"])  # Main summary for email
    else:
        logger.error(
            f"AI synthesis of competitor data failed or returned non-JSON for {ticker}. Response: {comp_summary_json}")
        final_competitor_analysis_data["summary"] = "AI synthesis of competitor data failed."
        analyzer_instance.data_quality_warnings.append("Competitor analysis AI synthesis failed.")

    analyzer_instance._financial_data_cache['competitor_analysis'] = final_competitor_analysis_data
    logger.info(f"Competitor analysis JSON summary generated for {ticker}.")
    return final_competitor_analysis_data["summary"]  # Return string for compatibility
---------- END qualitative_analyzer.py ----------


---------- stock_analyzer.py ----------
# services/stock_analyzer/stock_analyzer.py
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timezone
import math
import time
import warnings
from bs4 import XMLParsedAsHTMLWarning
import json
import sqlalchemy  # Added import for sqlalchemy

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

from api_clients import (
    FinnhubClient, FinancialModelingPrepClient, AlphaVantageClient,
    EODHDClient, GeminiAPIClient, SECEDGARClient
)
from database import SessionLocal, get_db_session, Stock, StockAnalysis
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError

from .data_fetcher import fetch_financial_statements_data, fetch_key_metrics_and_profile_data
from .metrics_calculator import calculate_all_derived_metrics
from .dcf_analyzer import perform_dcf_analysis
from .qualitative_analyzer import fetch_and_summarize_10k_data, fetch_and_analyze_competitors
from .ai_synthesis import synthesize_investment_thesis


class StockAnalyzer:
    def __init__(self, ticker):
        self.ticker = ticker.upper()
        self.finnhub = FinnhubClient()
        self.fmp = FinancialModelingPrepClient()
        self.alphavantage = AlphaVantageClient()
        self.eodhd = EODHDClient()
        self.gemini = GeminiAPIClient()
        self.sec_edgar = SECEDGARClient()

        self.db_session = next(get_db_session())
        self.stock_db_entry = None
        self._financial_data_cache = {}
        self.data_quality_warnings = []

        try:
            self._get_or_create_stock_entry()
        except Exception as e:
            logger.error(f"CRITICAL: Failed during _get_or_create_stock_entry for {self.ticker}: {e}", exc_info=True)
            self._close_session_if_active()
            raise RuntimeError(
                f"StockAnalyzer for {self.ticker} could not be initialized due to DB/API issues during stock entry setup.") from e

    def _close_session_if_active(self):
        if self.db_session and self.db_session.is_active:
            try:
                self.db_session.close()
                logger.debug(f"DB session closed for {self.ticker}.")
            except Exception as e_close:
                logger.warning(f"Error closing session for {self.ticker}: {e_close}")

    def _get_or_create_stock_entry(self):
        if not self.db_session.is_active:
            logger.warning(f"Session for {self.ticker} inactive in _get_or_create. Re-establishing.")
            self._close_session_if_active()
            self.db_session = next(get_db_session())

        self.stock_db_entry = self.db_session.query(Stock).filter_by(ticker=self.ticker).first()

        company_name, industry, sector, cik = None, None, None, None
        profile_source_preference = ["fmp", "finnhub", "alphavantage"]

        for source in profile_source_preference:
            if source == "fmp":
                profile_fmp_list = self.fmp.get_company_profile(self.ticker)
                time.sleep(1)
                if profile_fmp_list and isinstance(profile_fmp_list, list) and profile_fmp_list[0]:
                    data = profile_fmp_list[0]
                    self._financial_data_cache['profile_fmp'] = data
                    company_name = data.get('companyName')
                    industry = data.get('industry')
                    sector = data.get('sector')
                    cik_val = data.get('cik')
                    if cik_val: cik = str(cik_val).zfill(10)
                    logger.info(f"Fetched profile from FMP for {self.ticker}.")
                    break
            elif source == "finnhub" and not company_name:
                profile_fh = self.finnhub.get_company_profile2(self.ticker)
                time.sleep(1)
                if profile_fh:
                    self._financial_data_cache['profile_finnhub'] = profile_fh
                    company_name = profile_fh.get('name')
                    industry = profile_fh.get('finnhubIndustry') or industry
                    logger.info(f"Fetched profile from Finnhub for {self.ticker}.")
                    break
            elif source == "alphavantage" and not company_name:
                overview_av = self.alphavantage.get_company_overview(self.ticker)

                if overview_av and overview_av.get("Symbol") == self.ticker:
                    self._financial_data_cache['overview_alphavantage'] = overview_av
                    company_name = overview_av.get('Name')
                    industry = overview_av.get('Industry') or industry
                    sector = overview_av.get('Sector') or sector
                    cik_val = overview_av.get('CIK')
                    if cik_val: cik = str(cik_val).zfill(10)
                    logger.info(f"Fetched overview from Alpha Vantage for {self.ticker}.")
                    break

        if not company_name:
            company_name = self.ticker
            logger.warning(f"All primary profile fetches failed or incomplete for {self.ticker}. Using ticker as name.")

        if not cik and self.ticker:
            logger.info(f"CIK not found from profiles for {self.ticker}. Querying SEC EDGAR CIK map.")
            cik_from_edgar = self.sec_edgar.get_cik_by_ticker(self.ticker)
            time.sleep(0.2)
            if cik_from_edgar:
                cik = str(cik_from_edgar).zfill(10)
                logger.info(f"Fetched CIK {cik} from SEC EDGAR CIK map for {self.ticker}.")
            else:
                logger.warning(f"Could not fetch CIK from SEC EDGAR CIK map for {self.ticker}.")

        if not self.stock_db_entry:
            logger.info(f"Stock {self.ticker} not found in DB, creating new entry.")
            self.stock_db_entry = Stock(
                ticker=self.ticker,
                company_name=company_name,
                industry=industry,
                sector=sector,
                cik=cik
            )
            self.db_session.add(self.stock_db_entry)
            try:
                self.db_session.commit()
                self.db_session.refresh(self.stock_db_entry)
            except SQLAlchemyError as e:
                self.db_session.rollback()
                logger.error(f"Error creating stock entry for {self.ticker}: {e}", exc_info=True)
                raise
        else:
            updated = False
            if company_name and self.stock_db_entry.company_name != company_name:
                self.stock_db_entry.company_name = company_name;
                updated = True
            if industry and self.stock_db_entry.industry != industry:
                self.stock_db_entry.industry = industry;
                updated = True
            if sector and self.stock_db_entry.sector != sector:
                self.stock_db_entry.sector = sector;
                updated = True
            if cik and (self.stock_db_entry.cik != cik or not self.stock_db_entry.cik):
                self.stock_db_entry.cik = cik;
                updated = True

            if updated:
                try:
                    self.db_session.commit()
                    self.db_session.refresh(self.stock_db_entry)
                    logger.info(f"Updated stock entry for {self.ticker} with new profile data.")
                except SQLAlchemyError as e:
                    self.db_session.rollback()
                    logger.error(f"Error updating stock entry for {self.ticker}: {e}")

        logger.info(
            f"Stock entry for {self.ticker} (ID: {self.stock_db_entry.id if self.stock_db_entry else 'N/A'}, CIK: {self.stock_db_entry.cik if self.stock_db_entry and self.stock_db_entry.cik else 'N/A'}) ready.")

    def _ensure_stock_db_entry_is_bound(self):
        if not self.stock_db_entry:
            raise RuntimeError(
                f"Stock entry for {self.ticker} is None during binding check. Prior initialization failure.")

        if not self.db_session.is_active:
            logger.warning(f"DB Session for {self.ticker} was INACTIVE before operation. Re-establishing.")
            self._close_session_if_active()
            self.db_session = next(get_db_session())
            re_fetched_stock = self.db_session.query(Stock).filter(Stock.ticker == self.ticker).first()
            if not re_fetched_stock:
                raise RuntimeError(
                    f"Failed to re-fetch stock {self.ticker} for new session after inactivity. Critical state.")
            self.stock_db_entry = re_fetched_stock
            logger.info(
                f"Re-fetched and bound stock {self.ticker} (ID: {self.stock_db_entry.id}) to new active session.")
            return

        instance_state = sa_inspect(self.stock_db_entry)
        if not instance_state.session or instance_state.session is not self.db_session:
            obj_id_log = self.stock_db_entry.id if instance_state.has_identity else 'Transient/No ID'
            logger.warning(
                f"Stock {self.ticker} (ID: {obj_id_log}) DETACHED or bound to DIFFERENT session. Attempting to merge.")
            try:
                self.stock_db_entry = self.db_session.merge(self.stock_db_entry)
                self.db_session.flush()
                logger.info(
                    f"Successfully merged stock {self.ticker} (ID: {self.stock_db_entry.id}) into current session.")
            except Exception as e_merge:
                logger.error(f"Failed to merge stock {self.ticker} into session: {e_merge}. Re-fetching as a fallback.",
                             exc_info=True)
                re_fetched_from_db_after_merge_fail = self.db_session.query(Stock).filter(
                    Stock.ticker == self.ticker).first()
                if re_fetched_from_db_after_merge_fail:
                    self.stock_db_entry = re_fetched_from_db_after_merge_fail
                    logger.info(
                        f"Successfully re-fetched stock {self.ticker} (ID: {self.stock_db_entry.id}) after merge failure.")
                else:
                    raise RuntimeError(
                        f"Failed to bind stock {self.ticker} to session after merge failure and re-fetch attempt. Analysis cannot proceed.")

    def analyze(self):
        logger.info(f"Full analysis pipeline started for {self.ticker}...")
        final_data_for_db = {}
        try:
            if not self.stock_db_entry:
                logger.error(f"Stock DB entry for {self.ticker} not initialized properly. Aborting analysis.")
                return None

            self._ensure_stock_db_entry_is_bound()

            fetch_financial_statements_data(self)
            fetch_key_metrics_and_profile_data(self)

            final_data_for_db.update(calculate_all_derived_metrics(self))
            final_data_for_db.update(perform_dcf_analysis(self))

            qual_summaries_data = fetch_and_summarize_10k_data(self)

            final_data_for_db["business_summary"] = qual_summaries_data.get("business_summary_data", {}).get("summary",
                                                                                                             "N/A")
            final_data_for_db["risk_factors_summary"] = qual_summaries_data.get("risk_factors_summary_data", {}).get(
                "summary", "N/A")
            final_data_for_db["management_assessment_summary"] = qual_summaries_data.get(
                "management_assessment_summary_data", {}).get("summary", "N/A")
            final_data_for_db["economic_moat_summary"] = qual_summaries_data.get("economic_moat_summary_data", {}).get(
                "overallAssessment", "N/A")
            final_data_for_db["industry_trends_summary"] = qual_summaries_data.get("industry_trends_summary_data",
                                                                                   {}).get("overallOutlook", "N/A")

            final_data_for_db["qualitative_sources_summary"] = qual_summaries_data.get("qualitative_sources_summary",
                                                                                       {})
            if "key_metrics_snapshot" not in final_data_for_db: final_data_for_db["key_metrics_snapshot"] = {}
            final_data_for_db["key_metrics_snapshot"]["10k_business_summary_data"] = qual_summaries_data.get(
                "business_summary_data")
            final_data_for_db["key_metrics_snapshot"]["10k_risk_factors_data"] = qual_summaries_data.get(
                "risk_factors_summary_data")
            final_data_for_db["key_metrics_snapshot"]["10k_mda_data"] = qual_summaries_data.get(
                "management_assessment_summary_data")
            final_data_for_db["key_metrics_snapshot"]["10k_economic_moat_data"] = qual_summaries_data.get(
                "economic_moat_summary_data")
            final_data_for_db["key_metrics_snapshot"]["10k_industry_trends_data"] = qual_summaries_data.get(
                "industry_trends_summary_data")

            final_data_for_db["competitive_landscape_summary"] = fetch_and_analyze_competitors(self)
            competitor_data_cache = self._financial_data_cache.get('competitor_analysis', {})
            if "key_metrics_snapshot" not in final_data_for_db: final_data_for_db["key_metrics_snapshot"] = {}
            final_data_for_db["key_metrics_snapshot"]["competitor_analysis_data"] = competitor_data_cache

            final_data_for_db.update(synthesize_investment_thesis(self))

            analysis_entry = StockAnalysis(stock_id=self.stock_db_entry.id, analysis_date=datetime.now(timezone.utc))
            model_fields = [c.key for c in StockAnalysis.__table__.columns if
                            c.key not in ['id', 'stock_id', 'analysis_date']]

            for field_name in model_fields:
                if field_name in final_data_for_db:
                    value_to_set = final_data_for_db[field_name]
                    target_column = getattr(StockAnalysis, field_name)
                    target_column_type = target_column.type.python_type if hasattr(target_column.type,
                                                                                   'python_type') else type(None)

                    if target_column_type == float:
                        if isinstance(value_to_set, str):
                            try:
                                value_to_set = float(value_to_set)
                            except ValueError:
                                value_to_set = None
                        if isinstance(value_to_set, float) and (math.isnan(value_to_set) or math.isinf(value_to_set)):
                            value_to_set = None
                    elif target_column_type == dict or isinstance(target_column.type, (
                    sqlalchemy.dialects.postgresql.JSONB, sqlalchemy.dialects.postgresql.JSON,
                    sqlalchemy.types.JSON)):  # Check for JSON types
                        if not isinstance(value_to_set, dict) and value_to_set is not None:
                            try:
                                parsed_json = json.loads(value_to_set) if isinstance(value_to_set, str) else None
                                if isinstance(parsed_json, dict):
                                    value_to_set = parsed_json
                                else:
                                    logger.warning(
                                        f"Field {field_name} expected dict/JSON, got {type(value_to_set)}. Value: '{str(value_to_set)[:100]}...'. Setting to error dict.")
                                    value_to_set = {"error": "Invalid data type received",
                                                    "original_value": str(value_to_set)[
                                                                      :200]} if value_to_set is not None else None
                            except json.JSONDecodeError:
                                logger.warning(
                                    f"Field {field_name} expected dict/JSON but failed to parse string: '{str(value_to_set)[:100]}...'. Setting to error dict.")
                                value_to_set = {"error": "Failed to parse JSON string",
                                                "original_value": str(value_to_set)[
                                                                  :200]} if value_to_set is not None else None
                    elif target_column_type == str and not isinstance(value_to_set, str) and value_to_set is not None:
                        value_to_set = str(value_to_set)

                    setattr(analysis_entry, field_name, value_to_set)

            self.db_session.add(analysis_entry)
            self.stock_db_entry.last_analysis_date = analysis_entry.analysis_date
            self.db_session.commit()
            logger.info(f"Successfully analyzed and saved stock data: {self.ticker} (Analysis ID: {analysis_entry.id})")
            return analysis_entry

        except RuntimeError as rt_err:
            logger.critical(f"Runtime error during full analysis for {self.ticker}: {rt_err}", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"CRITICAL error in full analysis pipeline for {self.ticker}: {e}", exc_info=True)
            if self.db_session and self.db_session.is_active:
                try:
                    self.db_session.rollback(); logger.info(
                        f"Rolled back DB transaction for {self.ticker} due to error.")
                except Exception as e_rb:
                    logger.error(f"Rollback error for {self.ticker}: {e_rb}")
            return None
        finally:
            self._close_session_if_active()
---------- END stock_analyzer.py ----------


---------- __init__.py ----------
# services/__init__.py
from .stock_analyzer.stock_analyzer import StockAnalyzer
from .ipo_analyzer.ipo_analyzer import IPOAnalyzer
from .news_analyzer.news_analyzer import NewsAnalyzer
from .email_service import EmailService

__all__ = [
    "StockAnalyzer",
    "IPOAnalyzer",
    "NewsAnalyzer",
    "EmailService",
]
---------- END __init__.py ----------


---------- email_service.py ----------
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from datetime import datetime, timezone
import json
import math
from markdown2 import Markdown

from core.config import (
    EMAIL_HOST, EMAIL_PORT, EMAIL_USE_TLS, EMAIL_HOST_USER,
    EMAIL_HOST_PASSWORD, EMAIL_SENDER, EMAIL_RECIPIENT
)
from core.logging_setup import logger
from database.models import StockAnalysis, IPOAnalysis, NewsEventAnalysis


class EmailService:
    def __init__(self):
        self.markdowner = Markdown(extras=["tables", "fenced-code-blocks", "break-on-newline"])

    def _md_to_html(self, md_text):
        if md_text is None: return "<p>N/A</p>"
        if isinstance(md_text, (dict, list)):
            return f"<pre>{json.dumps(md_text, indent=2)}</pre>"
        if not isinstance(md_text, str): md_text = str(md_text)
        if "<" in md_text and ">" in md_text and ("<p>" in md_text.lower() or "<div>" in md_text.lower()):
            return md_text
        return self.markdowner.convert(md_text)

    def _format_stock_analysis_html(self, analysis: StockAnalysis):
        if not analysis: return ""
        stock = analysis.stock

        def fmt_num(val, type="decimal", na_val="N/A"):
            if val is None or (isinstance(val, float) and (math.isnan(val) or math.isinf(val))): return na_val
            if type == "percent": return f"{val * 100:.2f}%"
            if type == "decimal": return f"{val:.2f}"
            return str(val)

        business_summary_html = self._md_to_html(analysis.business_summary)
        economic_moat_html = self._md_to_html(analysis.economic_moat_summary)
        industry_trends_html = self._md_to_html(analysis.industry_trends_summary)
        competitive_landscape_html = self._md_to_html(analysis.competitive_landscape_summary)
        management_assessment_html = self._md_to_html(analysis.management_assessment_summary)
        risk_factors_html = self._md_to_html(analysis.risk_factors_summary)
        investment_thesis_html = self._md_to_html(analysis.investment_thesis_full)
        reasoning_points_html = self._md_to_html(analysis.reasoning)

        dcf_assumptions_html = "<ul>"
        if analysis.dcf_assumptions and isinstance(analysis.dcf_assumptions, dict):
            assumptions_data = analysis.dcf_assumptions
            dcf_assumptions_html += f"<li>Discount Rate: {fmt_num(assumptions_data.get('discount_rate'), 'percent')}</li>"
            dcf_assumptions_html += f"<li>Perpetual Growth Rate: {fmt_num(assumptions_data.get('perpetual_growth_rate'), 'percent')}</li>"
            dcf_assumptions_html += f"<li>FCF Projection Years: {assumptions_data.get('projection_years', 'N/A')}</li>"
            dcf_assumptions_html += f"<li>Starting FCF: {fmt_num(assumptions_data.get('start_fcf'))}</li>"
            fcf_growth_proj = assumptions_data.get('fcf_growth_rates_projection')
            if fcf_growth_proj and isinstance(fcf_growth_proj, list):
                 dcf_assumptions_html += f"<li>Projected FCF Growth Rates: {', '.join([fmt_num(r, 'percent') for r in fcf_growth_proj])}</li>"
        else:
            dcf_assumptions_html += "<li>N/A</li>"
        dcf_assumptions_html += "</ul>"

        html = f"""
        <div class="analysis-block stock-analysis">
            <h2>Stock Analysis: {stock.company_name} ({stock.ticker})</h2>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Industry:</strong> {stock.industry or 'N/A'}, <strong>Sector:</strong> {stock.sector or 'N/A'}</p>
            <p><strong>Investment Decision:</strong> {analysis.investment_decision or 'N/A'}</p>
            <p><strong>Strategy Type:</strong> {analysis.strategy_type or 'N/A'}</p>
            <p><strong>Confidence Level:</strong> {analysis.confidence_level or 'N/A'}</p>
            <details>
                <summary><strong>Investment Thesis & Reasoning (Click to expand)</strong></summary>
                <h4>Full Thesis:</h4><div class="markdown-content">{investment_thesis_html}</div>
                <h4>Key Reasoning Points:</h4><div class="markdown-content">{reasoning_points_html}</div>
            </details>
            <details>
                <summary><strong>Key Financial Metrics (Click to expand)</strong></summary>
                <ul>
                    <li>P/E Ratio: {fmt_num(analysis.pe_ratio)}</li><li>P/B Ratio: {fmt_num(analysis.pb_ratio)}</li>
                    <li>P/S Ratio: {fmt_num(analysis.ps_ratio)}</li><li>EV/Sales: {fmt_num(analysis.ev_to_sales)}</li>
                    <li>EV/EBITDA: {fmt_num(analysis.ev_to_ebitda)}</li><li>EPS: {fmt_num(analysis.eps)}</li>
                    <li>ROE: {fmt_num(analysis.roe, 'percent')}</li><li>ROA: {fmt_num(analysis.roa, 'percent')}</li>
                    <li>ROIC: {fmt_num(analysis.roic, 'percent')}</li><li>Dividend Yield: {fmt_num(analysis.dividend_yield, 'percent')}</li>
                    <li>Debt-to-Equity: {fmt_num(analysis.debt_to_equity)}</li><li>Debt-to-EBITDA: {fmt_num(analysis.debt_to_ebitda)}</li>
                    <li>Interest Coverage: {fmt_num(analysis.interest_coverage_ratio)}x</li><li>Current Ratio: {fmt_num(analysis.current_ratio)}</li>
                    <li>Quick Ratio: {fmt_num(analysis.quick_ratio)}</li>
                    <li>Gross Profit Margin: {fmt_num(analysis.gross_profit_margin, 'percent')}</li>
                    <li>Operating Profit Margin: {fmt_num(analysis.operating_profit_margin, 'percent')}</li>
                    <li>Net Profit Margin: {fmt_num(analysis.net_profit_margin, 'percent')}</li>
                    <li>Revenue Growth YoY: {fmt_num(analysis.revenue_growth_yoy, 'percent')} (QoQ: {fmt_num(analysis.revenue_growth_qoq, 'percent')})</li>
                    <li>Revenue Growth CAGR (3yr/5yr): {fmt_num(analysis.revenue_growth_cagr_3yr, 'percent')} / {fmt_num(analysis.revenue_growth_cagr_5yr, 'percent')}</li>
                    <li>EPS Growth YoY: {fmt_num(analysis.eps_growth_yoy, 'percent')}</li>
                    <li>EPS Growth CAGR (3yr/5yr): {fmt_num(analysis.eps_growth_cagr_3yr, 'percent')} / {fmt_num(analysis.eps_growth_cagr_5yr, 'percent')}</li>
                    <li>FCF per Share: {fmt_num(analysis.free_cash_flow_per_share)}</li><li>FCF Yield: {fmt_num(analysis.free_cash_flow_yield, 'percent')}</li>
                    <li>FCF Trend: {analysis.free_cash_flow_trend or 'N/A'}</li><li>Retained Earnings Trend: {analysis.retained_earnings_trend or 'N/A'}</li>
                </ul>
            </details>
            <details>
                <summary><strong>DCF Analysis (Simplified) (Click to expand)</strong></summary>
                <ul>
                    <li>Intrinsic Value per Share: {fmt_num(analysis.dcf_intrinsic_value)}</li>
                    <li>Upside/Downside: {fmt_num(analysis.dcf_upside_percentage, 'percent')}</li>
                </ul>
                <p><em>Key Assumptions Used:</em></p>
                {dcf_assumptions_html}
            </details>
            <details>
                <summary><strong>Qualitative Analysis (from 10-K/Profile & AI) (Click to expand)</strong></summary>
                <p><strong>Business Summary:</strong></p><div class="markdown-content">{business_summary_html}</div>
                <p><strong>Economic Moat:</strong></p><div class="markdown-content">{economic_moat_html}</div>
                <p><strong>Industry Trends & Position:</strong></p><div class="markdown-content">{industry_trends_html}</div>
                <p><strong>Competitive Landscape:</strong></p><div class="markdown-content">{competitive_landscape_html}</div>
                <p><strong>Management Discussion Highlights (MD&A/Assessment):</strong></p><div class="markdown-content">{management_assessment_html}</div>
                <p><strong>Key Risk Factors:</strong></p><div class="markdown-content">{risk_factors_html}</div>
            </details>
            <details>
                <summary><strong>Supporting Data Snapshots (Click to expand)</strong></summary>
                <p><em>Key Metrics Data Points Used:</em></p><div class="markdown-content">{self._md_to_html(analysis.key_metrics_snapshot)}</div>
                <p><em>Qualitative Analysis Sources Summary:</em></p><div class="markdown-content">{self._md_to_html(analysis.qualitative_sources_summary)}</div>
            </details>
        </div>
        """
        return html

    def _format_ipo_analysis_html(self, analysis: IPOAnalysis):
        if not analysis: return ""
        ipo = analysis.ipo
        def fmt_price(val_low, val_high, currency="USD"):
            if val_low is None and val_high is None: return "N/A"
            if val_low is not None and val_high is not None:
                if val_low == val_high: return f"{val_low:.2f} {currency}"
                return f"{val_low:.2f} - {val_high:.2f} {currency}"
            if val_low is not None: return f"{val_low:.2f} {currency}"
            if val_high is not None: return f"{val_high:.2f} {currency}"
            return "N/A"
        reasoning_html = self._md_to_html(analysis.reasoning)
        s1_business_summary_html = self._md_to_html(analysis.s1_business_summary or analysis.business_model_summary)
        s1_risk_factors_summary_html = self._md_to_html(analysis.s1_risk_factors_summary or analysis.risk_factors_summary)
        s1_mda_summary_html = self._md_to_html(analysis.s1_mda_summary)
        s1_financial_health_summary_html = self._md_to_html(analysis.s1_financial_health_summary or analysis.pre_ipo_financials_summary)
        competitive_landscape_html = self._md_to_html(analysis.competitive_landscape_summary)
        industry_outlook_html = self._md_to_html(analysis.industry_outlook_summary)
        use_of_proceeds_html = self._md_to_html(analysis.use_of_proceeds_summary)
        management_team_html = self._md_to_html(analysis.management_team_assessment)
        underwriter_html = self._md_to_html(analysis.underwriter_quality_assessment)
        valuation_html = self._md_to_html(analysis.valuation_comparison_summary)
        html = f"""
        <div class="analysis-block ipo-analysis">
            <h2>IPO Analysis: {ipo.company_name} ({ipo.symbol or 'N/A'})</h2>
            <p><strong>Expected IPO Date:</strong> {ipo.ipo_date.strftime('%Y-%m-%d') if ipo.ipo_date else ipo.ipo_date_str or 'N/A'}</p>
            <p><strong>Expected Price Range:</strong> {fmt_price(ipo.expected_price_range_low, ipo.expected_price_range_high, ipo.expected_price_currency)}</p>
            <p><strong>Exchange:</strong> {ipo.exchange or 'N/A'}, <strong>Status:</strong> {ipo.status or 'N/A'}</p>
            <p><strong>S-1 Filing URL:</strong> {f'<a href="{ipo.s1_filing_url}">{ipo.s1_filing_url}</a>' if ipo.s1_filing_url else 'Not Found'}</p>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Preliminary Stance:</strong> {analysis.investment_decision or 'N/A'}</p>
            <details><summary><strong>AI Synthesized Reasoning & Critical Verification Points (Click to expand)</strong></summary><div class="markdown-content">{reasoning_html}</div></details>
            <details>
                <summary><strong>S-1 Based Summaries (if available) & AI Analysis (Click to expand)</strong></summary>
                <p><strong>Business Summary (S-1/inferred):</strong></p><div class="markdown-content">{s1_business_summary_html}</div>
                <p><strong>Competitive Landscape:</strong></p><div class="markdown-content">{competitive_landscape_html}</div>
                <p><strong>Industry Outlook:</strong></p><div class="markdown-content">{industry_outlook_html}</div>
                <p><strong>Risk Factors Summary (S-1/inferred):</strong></p><div class="markdown-content">{s1_risk_factors_summary_html}</div>
                <p><strong>Use of Proceeds (S-1/inferred):</strong></p><div class="markdown-content">{use_of_proceeds_html}</div>
                <p><strong>MD&A / Financial Health Summary (S-1/inferred):</strong></p><div class="markdown-content">{s1_mda_summary_html if s1_mda_summary_html and not s1_mda_summary_html.startswith("Section not found") else s1_financial_health_summary_html}</div>
                <p><strong>Management Team Assessment:</strong></p><div class="markdown-content">{management_team_html}</div>
                <p><strong>Underwriter Quality Assessment:</strong></p><div class="markdown-content">{underwriter_html}</div>
                <p><strong>Valuation Comparison Guidance:</strong></p><div class="markdown-content">{valuation_html}</div>
            </details>
            <details><summary><strong>Supporting Data (Click to expand)</strong></summary>
                <p><em>Raw IPO calendar API data:</em></p><div class="markdown-content">{self._md_to_html(analysis.key_data_snapshot)}</div>
                <p><em>S-1 Sections Used (True if found & used):</em></p><div class="markdown-content">{self._md_to_html(analysis.s1_sections_used)}</div>
            </details>
        </div>"""
        return html

    def _format_news_event_analysis_html(self, analysis: NewsEventAnalysis):
        if not analysis: return ""
        news_event = analysis.news_event
        sentiment_html = self._md_to_html(f"**Sentiment:** {analysis.sentiment or 'N/A'}\n**Reasoning:** {analysis.sentiment_reasoning or 'N/A'}")
        news_summary_detailed_html = self._md_to_html(analysis.news_summary_detailed)
        impact_companies_html = self._md_to_html(analysis.potential_impact_on_companies)
        impact_sectors_html = self._md_to_html(analysis.potential_impact_on_sectors)
        mechanism_html = self._md_to_html(analysis.mechanism_of_impact)
        timing_duration_html = self._md_to_html(analysis.estimated_timing_duration)
        magnitude_direction_html = self._md_to_html(analysis.estimated_magnitude_direction)
        confidence_html = self._md_to_html(analysis.confidence_of_assessment)
        investor_summary_html = self._md_to_html(analysis.summary_for_email)
        html = f"""
        <div class="analysis-block news-analysis">
            <h2>News/Event Analysis: {news_event.event_title}</h2>
            <p><strong>Event Date:</strong> {news_event.event_date.strftime('%Y-%m-%d %H:%M %Z') if news_event.event_date else 'N/A'}</p>
            <p><strong>Source:</strong> <a href="{news_event.source_url}">{news_event.source_name or news_event.source_url}</a></p>
            <p><strong>Full Article Scraped:</strong> {'Yes' if news_event.full_article_text else 'No (Analysis based on headline/summary if available)'}</p>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Investor Summary:</strong></p><div class="markdown-content">{investor_summary_html}</div>
            <details><summary><strong>Detailed AI Analysis (Click to expand)</strong></summary>
                <p><strong>Sentiment Analysis:</strong></p><div class="markdown-content">{sentiment_html}</div>
                <p><strong>Detailed News Summary:</strong></p><div class="markdown-content">{news_summary_detailed_html}</div>
                <p><strong>Potentially Affected Companies/Stocks:</strong></p><div class="markdown-content">{impact_companies_html}</div>
                <p><strong>Potentially Affected Sectors:</strong></p><div class="markdown-content">{impact_sectors_html}</div>
                <p><strong>Mechanism of Impact:</strong></p><div class="markdown-content">{mechanism_html}</div>
                <p><strong>Estimated Timing & Duration:</strong></p><div class="markdown-content">{timing_duration_html}</div>
                <p><strong>Estimated Magnitude & Direction:</strong></p><div class="markdown-content">{magnitude_direction_html}</div>
                <p><strong>Confidence of Assessment:</strong></p><div class="markdown-content">{confidence_html}</div>
            </details>
            <details><summary><strong>Key Snippets Used for Analysis (Click to expand)</strong></summary><div class="markdown-content">{self._md_to_html(analysis.key_news_snippets)}</div></details>
        </div>"""
        return html

    def create_summary_email(self, stock_analyses=None, ipo_analyses=None, news_analyses=None):
        if not any([stock_analyses, ipo_analyses, news_analyses]):
            logger.info("No analyses provided to create an email."); return None
        subject_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        subject = f"Financial Analysis Summary - {subject_date}"
        html_body = f"""
        <html><head><style>
            body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; background-color: #f4f4f4; line-height: 1.6; color: #333; }}
            .container {{ background-color: #ffffff; padding: 20px; border-radius: 8px; box-shadow: 0 0 15px rgba(0,0,0,0.1); max-width: 900px; margin: auto; }}
            .analysis-block {{ border: 1px solid #ddd; padding: 15px; margin-bottom: 25px; border-radius: 5px; background-color: #fdfdfd; box-shadow: 0 2px 4px rgba(0,0,0,0.05);}}
            .stock-analysis {{ border-left: 5px solid #4CAF50; }} .ipo-analysis {{ border-left: 5px solid #2196F3; }} .news-analysis {{ border-left: 5px solid #FFC107; }}
            h1 {{ color: #2c3e50; text-align: center; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
            h2 {{ color: #34495e; border-bottom: 1px solid #eee; padding-bottom: 5px; margin-top: 0; }}
            h4 {{ color: #555; margin-top: 15px; margin-bottom: 5px; }}
            details > summary {{ cursor: pointer; font-weight: bold; margin-bottom: 10px; color: #2980b9; padding: 5px; background-color: #ecf0f1; border-radius:3px; }}
            details[open] > summary {{ background-color: #dde5e8; }}
            pre {{ background-color: #eee; padding: 10px; border-radius: 4px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; font-size: 0.85em; border: 1px solid #ccc; }}
            ul {{ list-style-type: disc; margin-left: 20px; padding-left: 5px; }} li {{ margin-bottom: 8px; }}
            .markdown-content p {{ margin: 0.5em 0; }} .markdown-content ul, .markdown-content ol {{ margin-left: 20px; }}
            .markdown-content table {{ border-collapse: collapse; width: 100%; margin-bottom: 1em;}}
            .markdown-content th, .markdown-content td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }} .markdown-content th {{ background-color: #f2f2f2; }}
            .report-footer {{ text-align: center; font-size: 0.9em; color: #777; margin-top: 30px; }}
        </style></head><body><div class="container">
            <h1>Financial Analysis Report</h1>
            <p style="text-align:center; font-style:italic; color:#555;">Generated: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z')}</p>
            <p style="text-align:center; font-style:italic; color:#7f8c8d;"><em>This email contains automated analysis. Always do your own research before making investment decisions.</em></p>"""
        if stock_analyses: html_body += "<h2>Individual Stock Analyses</h2>"; [html_body := html_body + self._format_stock_analysis_html(sa) for sa in stock_analyses]
        if ipo_analyses: html_body += "<h2>Upcoming IPO Analyses</h2>"; [html_body := html_body + self._format_ipo_analysis_html(ia) for ia in ipo_analyses]
        if news_analyses: html_body += "<h2>Recent News & Event Analyses</h2>"; [html_body := html_body + self._format_news_event_analysis_html(na) for na in news_analyses]
        html_body += """<div class="report-footer"><p>© Automated Financial Analysis System</p></div></div></body></html>"""
        msg = MIMEMultipart('alternative'); msg['Subject'], msg['From'], msg['To'] = subject, EMAIL_SENDER, EMAIL_RECIPIENT
        msg.attach(MIMEText(html_body, 'html', 'utf-8')); return msg

    def send_email(self, message: MIMEMultipart):
        if not message: logger.error("No message object provided to send_email."); return False
        try:
            smtp_server = smtplib.SMTP(EMAIL_HOST, EMAIL_PORT, timeout=20)
            if EMAIL_USE_TLS: smtp_server.starttls()
            smtp_server.login(EMAIL_HOST_USER, EMAIL_HOST_PASSWORD)
            smtp_server.sendmail(EMAIL_SENDER, EMAIL_RECIPIENT, message.as_string())
            smtp_server.quit(); logger.info(f"Email sent successfully to {EMAIL_RECIPIENT}"); return True
        except smtplib.SMTPException as e_smtp: logger.error(f"SMTP error sending email: {e_smtp}", exc_info=True); return False
        except Exception as e: logger.error(f"General error sending email: {e}", exc_info=True); return False

if __name__ == '__main__':
    logger.info("Starting email service test...")
    class MockStock: __init__ = lambda self, ticker, company_name, industry="Tech", sector="Software": setattr(self, 'ticker', ticker) or setattr(self, 'company_name', company_name) or setattr(self, 'industry', industry) or setattr(self, 'sector', sector)
    class MockIPO: __init__ = lambda self, company_name, symbol, ipo_date_str="2025-07-15": setattr(self, 'company_name', company_name) or setattr(self, 'symbol', symbol) or setattr(self, 'ipo_date_str', ipo_date_str) or setattr(self, 'ipo_date', datetime.strptime(ipo_date_str, "%Y-%m-%d").date() if ipo_date_str else None) or setattr(self, 'expected_price_range_low', 18.00) or setattr(self, 'expected_price_range_high', 22.00) or setattr(self, 'expected_price_currency', "USD") or setattr(self, 'exchange', "NASDAQ") or setattr(self, 'status', "Filed") or setattr(self, 's1_filing_url', "http://example.com/s1")
    class MockNewsEvent: __init__ = lambda self, title, url, event_date_str="2025-05-25 10:00:00": setattr(self, 'event_title', title) or setattr(self, 'source_url', url) or setattr(self, 'source_name', "Mock News") or setattr(self, 'event_date', datetime.strptime(event_date_str, "%Y-%m-%d %H:%M:%S").replace(tzinfo=timezone.utc)) or setattr(self, 'full_article_text', "Full article text...")
    class MockStockAnalysis:
        def __init__(self, stock): self.stock, self.analysis_date, self.investment_decision, self.strategy_type, self.confidence_level, self.investment_thesis_full, self.reasoning = stock, datetime.now(timezone.utc), "Buy", "GARP", "Medium", "Thesis...", "Reasons..."
        self.dcf_assumptions = {"discount_rate": 0.095, "perpetual_growth_rate": 0.025, "projection_years": 5, "start_fcf": 1.2e9, "fcf_growth_rates_projection": [0.08, 0.07, 0.06, 0.05, 0.04]}
        self.pe_ratio, self.pb_ratio, self.ps_ratio, self.ev_to_sales, self.ev_to_ebitda, self.eps, self.roe, self.roa, self.roic, self.dividend_yield, self.debt_to_equity, self.debt_to_ebitda, self.interest_coverage_ratio, self.current_ratio, self.quick_ratio, self.gross_profit_margin, self.operating_profit_margin, self.net_profit_margin, self.revenue_growth_yoy, self.revenue_growth_qoq, self.revenue_growth_cagr_3yr, self.revenue_growth_cagr_5yr, self.eps_growth_yoy, self.eps_growth_cagr_3yr, self.eps_growth_cagr_5yr, self.free_cash_flow_per_share, self.free_cash_flow_yield, self.free_cash_flow_trend, self.retained_earnings_trend, self.dcf_intrinsic_value, self.dcf_upside_percentage, self.business_summary, self.economic_moat_summary, self.industry_trends_summary, self.competitive_landscape_summary, self.management_assessment_summary, self.risk_factors_summary, self.key_metrics_snapshot, self.qualitative_sources_summary = (18.5, 3.2, 2.5, 2.8, 12.0, 2.50, 0.22, 0.10, 0.15, 0.015, 0.5, 2.1, 8.0, 1.8, 1.2, 0.60, 0.20, 0.12, 0.15, 0.04, 0.12, 0.10, 0.20, 0.18, 0.15, 1.80, 0.05, "Growing", "Growing", 120.50, 0.205, "Biz Sum.", "Moat Sum.", "Ind Sum.", "Comp Sum.", "Mgmt Sum.", "Risk Sum.", {"price": 100}, {"10k_url": "url"})
    class MockIPOAnalysis: __init__ = lambda self, ipo: setattr(self, 'ipo', ipo) or setattr(self, 'analysis_date', datetime.now(timezone.utc))
    class MockNewsEventAnalysis: __init__ = lambda self, news_event: setattr(self, 'news_event', news_event) or setattr(self, 'analysis_date', datetime.now(timezone.utc))
    mock_sa = MockStockAnalysis(MockStock("MOCK", "MockCorp Inc."))
    mock_ipo_a = MockIPOAnalysis(MockIPO("NewIPO Inc.", "NIPO"))
    mock_news_a = MockNewsEventAnalysis(MockNewsEvent("Major Tech Breakthrough", "http://example.com/news1"))
    email_svc = EmailService()
    email_message = email_svc.create_summary_email(stock_analyses=[mock_sa], ipo_analyses=[mock_ipo_a], news_analyses=[mock_news_a])
    if email_message:
        logger.info("Email message created successfully.")
        output_filename = f"test_email_summary_refactored_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(output_filename, "w", encoding="utf-8") as f:
            payload_html = "";
            if email_message.is_multipart():
                for part in email_message.get_payload():
                    if part.get_content_type() == "text/html": payload_html = part.get_payload(decode=True).decode(part.get_content_charset() or 'utf-8'); break
            else: payload_html = email_message.get_payload(decode=True).decode(email_message.get_content_charset() or 'utf-8')
            if payload_html: f.write(payload_html); logger.info(f"Test email HTML saved to {output_filename}")
            else: logger.error("Could not extract HTML payload.")
        # if email_svc.send_email(email_message): logger.info("Test email sent.") else: logger.error("Failed to send test email.")
    else: logger.error("Failed to create email message.")
---------- END email_service.py ----------


---------- .gitignore ----------
venv
/__pycache__

---------- END .gitignore ----------


---------- app_analysis.log ----------
2025-05-28 15:05:21,035 - root - INFO - main:152 - ===================================================================
2025-05-28 15:05:21,036 - root - INFO - main:153 - Starting Financial Analysis Script at 2025-05-28 12:05:21 UTC
2025-05-28 15:05:21,037 - root - INFO - main:154 - ===================================================================
2025-05-28 15:05:21,039 - root - INFO - main:35 - --- Starting IPO Analysis Pipeline (Upcoming only: True, Max to analyze: 1) ---
2025-05-28 15:05:21,039 - root - INFO - data_fetcher:11 - Fetching upcoming IPOs using Finnhub...
2025-05-28 15:05:23,260 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/calendar/ipo?from=2025-03-29&to=2025-11-24&token=d0o7hphr01qqr9alj38gd...
2025-05-28 15:05:23,266 - root - INFO - data_fetcher:71 - Successfully parsed 148 IPOs from Finnhub API response.
2025-05-28 15:05:23,267 - root - INFO - data_fetcher:90 - Total unique IPOs fetched after deduplication: 148
2025-05-28 15:05:23,268 - root - INFO - ipo_analyzer:149 - Filtered for upcoming IPOs only: 2 IPOs remain for potential analysis.
2025-05-28 15:05:23,268 - root - INFO - ipo_analyzer:167 - Limiting IPOs to analyze to the earliest 1 from 2 relevant IPOs.
2025-05-28 15:05:23,271 - root - INFO - ipo_analyzer:36 - Task: Starting analysis for IPO: Wintergreen Acquisition Corp. from source Finnhub
2025-05-28 15:05:23,523 - root - INFO - sec_edgar_client:20 - Fetching CIK map from SEC...
2025-05-28 15:05:24,656 - root - INFO - base_client:65 - Cached response for: GET:https://www.sec.gov/files/company_tickers.json...
2025-05-28 15:05:24,657 - root - INFO - sec_edgar_client:35 - CIK map fetched and cached with 10046 entries.
2025-05-28 15:05:25,158 - root - INFO - db_handler:35 - IPO 'Wintergreen Acquisition Corp.' not found in DB, creating new entry.
2025-05-28 15:05:25,593 - root - INFO - db_handler:53 - Created IPO entry for 'Wintergreen Acquisition Corp.' (ID: 2, CIK: None)
2025-05-28 15:05:26,164 - root - WARNING - data_fetcher:114 - No CIK found via symbol WTGUU for IPO 'Wintergreen Acquisition Corp.'.
2025-05-28 15:05:55,179 - root - INFO - ipo_analyzer:90 - Task: Creating new analysis for Wintergreen Acquisition Corp.
2025-05-28 15:05:55,587 - root - INFO - ipo_analyzer:102 - Task: Saved IPO analysis for Wintergreen Acquisition Corp. (Analysis ID: 2)
2025-05-28 15:05:55,648 - root - INFO - ipo_analyzer:193 - IPO analysis pipeline completed. Processed 1 IPOs that required new/updated analysis from the filtered set.
2025-05-28 15:05:55,650 - root - INFO - main:146 - --- Main script execution finished. ---
2025-05-28 15:05:55,650 - root - INFO - main:159 - Financial Analysis Script finished at 2025-05-28 12:05:55 UTC
2025-05-28 15:05:55,651 - root - INFO - main:160 - Total execution time: 0:00:34.615045
2025-05-28 15:05:55,651 - root - INFO - main:161 - ===================================================================
2025-05-28 15:06:08,887 - root - INFO - main:152 - ===================================================================
2025-05-28 15:06:08,888 - root - INFO - main:153 - Starting Financial Analysis Script at 2025-05-28 12:06:08 UTC
2025-05-28 15:06:08,888 - root - INFO - main:154 - ===================================================================
2025-05-28 15:06:08,891 - root - INFO - main:57 - --- Generating Today's Email Summary ---
2025-05-28 15:06:10,305 - root - INFO - main:69 - Found 0 stock analyses, 1 IPO analyses, 0 news analyses since 2025-05-28 00:00:00 UTC for email.
2025-05-28 15:06:12,244 - root - INFO - email_service:250 - Email sent successfully to daniprav@gmail.com
2025-05-28 15:06:12,310 - root - INFO - main:146 - --- Main script execution finished. ---
2025-05-28 15:06:12,310 - root - INFO - main:159 - Financial Analysis Script finished at 2025-05-28 12:06:12 UTC
2025-05-28 15:06:12,310 - root - INFO - main:160 - Total execution time: 0:00:03.423380
2025-05-28 15:06:12,311 - root - INFO - main:161 - ===================================================================

---------- END app_analysis.log ----------


---------- main.py ----------
# main.py
import argparse
from datetime import datetime, timezone
from sqlalchemy.orm import joinedload
# import time # No longer needed for arbitrary sleeps
import sys

from database.connection import init_db, SessionLocal
from core.logging_setup import logger, handle_global_exception
from services import StockAnalyzer, IPOAnalyzer, NewsAnalyzer, EmailService
from database.models import StockAnalysis, IPOAnalysis, NewsEventAnalysis
from core.config import MAX_NEWS_TO_ANALYZE_PER_RUN, DEFAULT_STOCKS_FOR_ALL_MODE


def run_stock_analysis(tickers):
    logger.info(f"--- Starting Individual Stock Analysis for: {tickers} ---")
    results = []
    for ticker in tickers:
        try:
            analyzer = StockAnalyzer(ticker=ticker)
            analysis_result = analyzer.analyze()
            if analysis_result:
                results.append(analysis_result)
            else:
                logger.warning(f"Stock analysis for {ticker} did not return a result object.")
        except RuntimeError as rt_err:  # Catch specific init error
            logger.error(f"Could not run stock analysis for {ticker} due to critical init error: {rt_err}")
        except Exception as e:
            logger.error(f"Error analyzing stock {ticker}: {e}", exc_info=True)
        # Removed time.sleep(5) - API client's base_client handles delays/retries
    return results


def run_ipo_analysis(upcoming_only=False, max_to_analyze=None):
    logger.info(f"--- Starting IPO Analysis Pipeline (Upcoming only: {upcoming_only}, Max to analyze: {max_to_analyze or 'All relevant'}) ---")
    try:
        analyzer = IPOAnalyzer()
        results = analyzer.run_ipo_analysis_pipeline(upcoming_only=upcoming_only, max_to_analyze=max_to_analyze)
        return results
    except Exception as e:
        logger.error(f"Error during IPO analysis pipeline: {e}", exc_info=True)
        return []


def run_news_analysis(category="general", count_to_analyze=MAX_NEWS_TO_ANALYZE_PER_RUN):
    logger.info(f"--- Starting News Analysis Pipeline (Category: {category}, Max to Analyze: {count_to_analyze}) ---")
    try:
        analyzer = NewsAnalyzer()
        results = analyzer.run_news_analysis_pipeline(category=category, count_to_analyze_this_run=count_to_analyze)
        return results
    except Exception as e:
        logger.error(f"Error during news analysis pipeline: {e}", exc_info=True)
        return []


def generate_and_send_todays_email_summary():
    logger.info("--- Generating Today's Email Summary ---")
    db_session = SessionLocal()
    today_start_utc = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
    try:
        # Eager load related objects to prevent N+1 queries in email formatting
        recent_stock_analyses = db_session.query(StockAnalysis).options(joinedload(StockAnalysis.stock)).filter(
            StockAnalysis.analysis_date >= today_start_utc).all()
        recent_ipo_analyses = db_session.query(IPOAnalysis).options(joinedload(IPOAnalysis.ipo)).filter(
            IPOAnalysis.analysis_date >= today_start_utc).all()
        recent_news_analyses = db_session.query(NewsEventAnalysis).options(
            joinedload(NewsEventAnalysis.news_event)).filter(NewsEventAnalysis.analysis_date >= today_start_utc).all()

        logger.info(
            f"Found {len(recent_stock_analyses)} stock analyses, {len(recent_ipo_analyses)} IPO analyses, {len(recent_news_analyses)} news analyses since {today_start_utc.strftime('%Y-%m-%d %H:%M:%S %Z')} for email.")

        if not any([recent_stock_analyses, recent_ipo_analyses, recent_news_analyses]):
            logger.info("No new analyses performed recently to include in the email summary.")
            return

        email_svc = EmailService()
        email_message = email_svc.create_summary_email(
            stock_analyses=recent_stock_analyses,
            ipo_analyses=recent_ipo_analyses,
            news_analyses=recent_news_analyses
        )
        if email_message:
            email_svc.send_email(email_message)
        else:
            logger.error("Failed to create the email message (returned None).")
    except Exception as e:
        logger.error(f"Error generating or sending email summary: {e}", exc_info=True)
    finally:
        SessionLocal.remove()


def main():
    parser = argparse.ArgumentParser(description="Financial Analysis and Reporting Tool")
    parser.add_argument("--analyze-stocks", nargs="+", metavar="TICKER",
                        help="List of stock tickers to analyze (e.g., AAPL MSFT)")
    parser.add_argument("--analyze-ipos", action="store_true", help="Run IPO analysis pipeline.")
    parser.add_argument("--upcoming-ipos-only", action="store_true", help="When analyzing IPOs, process only those with future dates and relevant statuses.")
    parser.add_argument("--max-ipos-to-analyze", type=int, default=None, metavar="N",
                        help="Maximum number of IPOs to analyze in a run (applies after filtering, e.g., for upcoming). Default: all relevant.")
    parser.add_argument("--analyze-news", action="store_true", help="Run news analysis pipeline.")
    parser.add_argument("--news-category", default="general",
                        help="Category for news analysis (e.g., general, forex, crypto, merger).")
    parser.add_argument("--news-count-analyze", type=int, default=MAX_NEWS_TO_ANALYZE_PER_RUN,
                        help=f"Max number of new news items to analyze in this run (default from config: {MAX_NEWS_TO_ANALYZE_PER_RUN}).")
    parser.add_argument("--send-email", action="store_true",
                        help="Generate and send email summary of today's/recent analyses.")
    parser.add_argument("--init-db", action="store_true", help="Initialize the database (create tables).")
    parser.add_argument("--all", action="store_true",
                        help="Run all analyses (stocks from default list, IPOs with --upcoming-ipos-only and --max-ipos-to-analyze if specified, News) and send email.")

    args = parser.parse_args()

    if args.init_db:
        logger.info("Initializing database as per command line argument...")
        try:
            init_db()
            logger.info("Database initialization complete.")
        except Exception as e:
            logger.critical(f"Database initialization failed: {e}", exc_info=True)
            return  # Exit if DB init fails

    if args.all:
        logger.info(
            f"Running all analyses for default stocks: {DEFAULT_STOCKS_FOR_ALL_MODE}, IPOs (Upcoming only: {args.upcoming_ipos_only}, Max: {args.max_ipos_to_analyze or 'All'}), and News (max {args.news_count_analyze} items).")
        if DEFAULT_STOCKS_FOR_ALL_MODE:
            run_stock_analysis(DEFAULT_STOCKS_FOR_ALL_MODE)
        run_ipo_analysis(upcoming_only=args.upcoming_ipos_only, max_to_analyze=args.max_ipos_to_analyze)
        run_news_analysis(category=args.news_category, count_to_analyze=args.news_count_analyze)
        generate_and_send_todays_email_summary()
        logger.info("--- '--all' tasks finished. ---")
        return

    if args.analyze_stocks:
        run_stock_analysis(args.analyze_stocks)
    if args.analyze_ipos:
        run_ipo_analysis(upcoming_only=args.upcoming_ipos_only, max_to_analyze=args.max_ipos_to_analyze)
    if args.analyze_news:
        run_news_analysis(category=args.news_category, count_to_analyze=args.news_count_analyze)
    if args.send_email:
        generate_and_send_todays_email_summary()

    if not any([args.analyze_stocks, args.analyze_ipos, args.analyze_news, args.send_email, args.init_db, args.all]):
        logger.info("No action specified. Use --help for options.")
        parser.print_help()

    logger.info("--- Main script execution finished. ---")


if __name__ == "__main__":
    # sys.excepthook = handle_global_exception # Uncomment to enable global exception logging
    script_start_time = datetime.now(timezone.utc)
    logger.info("===================================================================")
    logger.info(f"Starting Financial Analysis Script at {script_start_time.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    logger.info("===================================================================")

    main()

    script_end_time = datetime.now(timezone.utc)
    logger.info(f"Financial Analysis Script finished at {script_end_time.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    logger.info(f"Total execution time: {script_end_time - script_start_time}")
    logger.info("===================================================================")
---------- END main.py ----------


---------- requirements.txt ----------
# requirements.txt
sqlalchemy>=1.4,<2.0
requests>=2.32.0
psycopg2-binary>=2.8.0
pandas>=1.0.0
markdown2>=2.4.0
beautifulsoup4>=4.9.3
lxml>=4.6.3 
# Add other specific versions if needed
# python-dotenv # For managing environment variables if you choose to use .env files
---------- END requirements.txt ----------

--- END OF FILE project_structure_backend.txt ---
