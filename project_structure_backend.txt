--- START OF FILE project_structure_backend.txt ---

api_clients/
  __init__.py
  alphavantage_client.py
  base_client.py
  eodhd_client.py
  finnhub_client.py
  fmp_client.py
  gemini_client.py
  sec_edgar_client.py
core/
  __init__.py
  config.py
  logging_setup.py
database/
  __init__.py
  connection.py
  models.py
services/
  ipo_analyzer/
    __init__.py
    ai_analyzer.py
    data_fetcher.py
    db_handler.py
    helpers.py
    ipo_analyzer.py
  news_analyzer/
    __init__.py
    ai_analyzer.py
    data_fetcher.py
    db_handler.py
    news_analyzer.py
  stock_analyzer/
    __init__.py
    ai_synthesis.py
    data_fetcher.py
    dcf_analyzer.py
    helpers.py
    metrics_calculator.py
    qualitative_analyzer.py
    stock_analyzer.py
  __init__.py
  email_service.py
.gitignore
app_analysis.log
main.py
requirements.txt


---------- __init__.py ----------
# api_clients/__init__.py
from .base_client import APIClient, scrape_article_content, extract_S1_text_sections
from .finnhub_client import FinnhubClient
from .fmp_client import FinancialModelingPrepClient
from .alphavantage_client import AlphaVantageClient
from .eodhd_client import EODHDClient
from .sec_edgar_client import SECEDGARClient
from .gemini_client import GeminiAPIClient

__all__ = [
    "APIClient",
    "scrape_article_content",
    "extract_S1_text_sections",
    "FinnhubClient",
    "FinancialModelingPrepClient",
    "AlphaVantageClient",
    "EODHDClient",
    "SECEDGARClient",
    "GeminiAPIClient",
]
---------- END __init__.py ----------


---------- alphavantage_client.py ----------
from .base_client import APIClient
from core.config import ALPHA_VANTAGE_API_KEY


class AlphaVantageClient(APIClient):
    def __init__(self):
        super().__init__("https://www.alphavantage.co", api_key_name="apikey", api_key_value=ALPHA_VANTAGE_API_KEY)

    def get_company_overview(self, ticker):
        params = {"function": "OVERVIEW", "symbol": ticker}
        return self.request("GET", "/query", params=params, api_source_name="alphavantage_overview")

    def get_income_statement_quarterly(self, ticker):
        params = {"function": "INCOME_STATEMENT", "symbol": ticker}
        data = self.request("GET", "/query", params=params, api_source_name="alphavantage_income_quarterly")
        if data and isinstance(data.get("quarterlyReports"), list):
            data["quarterlyReports"].reverse()
        return data

    def get_balance_sheet_quarterly(self, ticker):
        params = {"function": "BALANCE_SHEET", "symbol": ticker}
        data = self.request("GET", "/query", params=params, api_source_name="alphavantage_balance_quarterly")
        if data and isinstance(data.get("quarterlyReports"), list):
            data["quarterlyReports"].reverse()
        return data

    def get_cash_flow_quarterly(self, ticker):
        params = {"function": "CASH_FLOW", "symbol": ticker}
        data = self.request("GET", "/query", params=params, api_source_name="alphavantage_cashflow_quarterly")
        if data and isinstance(data.get("quarterlyReports"), list):
            data["quarterlyReports"].reverse()
        return data
---------- END alphavantage_client.py ----------


---------- base_client.py ----------
# api_clients/base_client.py
import requests
import time
import json
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse

from core.config import (
    API_REQUEST_TIMEOUT, API_RETRY_ATTEMPTS, API_RETRY_DELAY,
    CACHE_EXPIRY_SECONDS
)
from core.logging_setup import logger
from database.connection import SessionLocal
from database.models import CachedAPIData


class APIClient:
    def __init__(self, base_url, api_key_name=None, api_key_value=None, headers=None):
        self.base_url = base_url
        self.api_key_name = api_key_name
        self.api_key_value = api_key_value
        self.headers = headers or {}
        if api_key_name and api_key_value:
            self.params = {api_key_name: api_key_value}
        else:
            self.params = {}

    def _get_cached_response(self, request_url_or_params_str):
        session = SessionLocal()
        try:
            current_time_utc = datetime.now(timezone.utc)
            cache_entry = session.query(CachedAPIData).filter(
                CachedAPIData.request_url_or_params == request_url_or_params_str,
                CachedAPIData.expires_at > current_time_utc
            ).first()
            if cache_entry:
                logger.info(f"Cache hit for: {request_url_or_params_str[:100]}...")
                return cache_entry.response_data
        except Exception as e:
            logger.error(f"Error reading from cache for '{request_url_or_params_str[:100]}...': {e}", exc_info=True)
        finally:
            session.close()
        return None

    def _cache_response(self, request_url_or_params_str, response_data, api_source):
        session = SessionLocal()
        try:
            now_utc = datetime.now(timezone.utc)
            expires_at_utc = now_utc + timedelta(seconds=CACHE_EXPIRY_SECONDS)

            session.query(CachedAPIData).filter(
                CachedAPIData.request_url_or_params == request_url_or_params_str).delete(synchronize_session=False)

            new_cache_entry = CachedAPIData(
                api_source=api_source,
                request_url_or_params=request_url_or_params_str,
                response_data=response_data,
                timestamp=now_utc,
                expires_at=expires_at_utc
            )
            session.add(new_cache_entry)
            session.commit()
            logger.info(f"Cached response for: {request_url_or_params_str[:100]}...")
        except Exception as e:
            logger.error(f"Error writing to cache for '{request_url_or_params_str[:100]}...': {e}", exc_info=True)
            session.rollback()
        finally:
            session.close()

    def request(self, method, endpoint, params=None, data=None, json_data=None, use_cache=True,
                api_source_name="unknown", is_json_response=True):

        # Determine if endpoint is a full URL
        parsed_endpoint = urlparse(endpoint)
        if parsed_endpoint.scheme and parsed_endpoint.netloc:
            url = endpoint  # Endpoint is already a full URL
        else:
            url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"

        current_call_params = params.copy() if params else {}
        full_query_params = self.params.copy()
        full_query_params.update(current_call_params)

        sorted_params = sorted(full_query_params.items()) if full_query_params else []
        param_string = "&".join([f"{k}={v}" for k, v in sorted_params])
        # For cache key, use the final URL (which might be just the endpoint if it was absolute)
        # and the param_string.
        cache_key_url_part = url  # Use the potentially absolute URL for the cache key
        cache_key_str = f"{method.upper()}:{cache_key_url_part}?{param_string}"

        if json_data:
            try:
                sorted_json_data_str = json.dumps(json_data, sort_keys=True, separators=(',', ':'))
                cache_key_str += f"|BODY:{sorted_json_data_str}"
            except TypeError as e:
                logger.warning(
                    f"Could not serialize json_data for cache key for {url}: {e}. Cache key may be less effective.")
                cache_key_str += f"|BODY_UNSERIALIZED:{str(json_data)}"

        if use_cache:
            cached_data = self._get_cached_response(cache_key_str)
            if cached_data is not None:
                return cached_data

        for attempt in range(API_RETRY_ATTEMPTS):
            try:
                response = requests.request(
                    method, url, params=full_query_params, data=data, json=json_data,
                    headers=self.headers, timeout=API_REQUEST_TIMEOUT
                )
                response.raise_for_status()

                if not is_json_response:
                    response_content = response.text
                    if use_cache:
                        self._cache_response(cache_key_str, response_content, api_source_name)
                    return response_content

                response_json = response.json()
                if use_cache:
                    self._cache_response(cache_key_str, response_json, api_source_name)
                return response_json

            except requests.exceptions.HTTPError as e:
                log_params_for_error = {k: (
                    str(v)[:4] + '******' + str(v)[-4:] if k == self.api_key_name and isinstance(v, str) and len(
                        str(v)) > 8 else v) for k, v in full_query_params.items()}
                log_headers_for_error = self.headers.copy()
                sensitive_header_keys = ["X-RapidAPI-Key", "Authorization", "Token", self.api_key_name]
                for h_key in sensitive_header_keys:
                    if h_key in log_headers_for_error and isinstance(log_headers_for_error[h_key], str) and len(
                            log_headers_for_error[h_key]) > 8:
                        log_headers_for_error[h_key] = log_headers_for_error[h_key][:4] + "******" + \
                                                       log_headers_for_error[h_key][-4:]
                status_code = e.response.status_code if e.response is not None else "Unknown"
                response_text_preview = e.response.text[:200] if e.response is not None else "No response body"
                logger.warning(
                    f"HTTP error on attempt {attempt + 1}/{API_RETRY_ATTEMPTS} for {method} {url} "
                    f"(Params: {log_params_for_error}, Headers: {log_headers_for_error}): "
                    f"{status_code} - {response_text_preview}..."
                )
                if api_source_name.startswith(
                        "alphavantage") and e.response is not None and "Our standard API call frequency is 25 requests per day." in e.response.text:
                    logger.error(f"Alpha Vantage API daily limit likely reached. Params: {log_params_for_error}")
                    return None
                if e.response is not None:
                    if status_code == 429:
                        delay = API_RETRY_DELAY * (2 ** attempt)
                        logger.info(f"Rate limit hit (429). Waiting for {delay} seconds.")
                        time.sleep(delay)
                    elif 500 <= status_code < 600:
                        delay = API_RETRY_DELAY * (2 ** attempt)
                        logger.info(f"Server error ({status_code}). Waiting for {delay} seconds before retry.")
                        time.sleep(delay)
                    elif status_code == 401 or status_code == 403:
                        logger.error(
                            f"Client error {status_code} (Unauthorized/Forbidden) for {url}. API key may be invalid or permissions lacking. No retry. Params: {log_params_for_error}")
                        return None
                    else:
                        # For 404s specifically on SEC Edgar filing text, don't log as error, just warning, as it might be a legitimate "not found"
                        if api_source_name == "edgar_filing_text_content" and status_code == 404:
                            logger.warning(
                                f"SEC Edgar returned 404 for {url}. Document may not exist or URL is incorrect.")
                        else:
                            logger.error(
                                f"Non-retryable client error {status_code} for {url}: {e.response.reason if e.response else 'Unknown reason'}",
                                exc_info=False)
                        return None
                else:
                    logger.error(f"HTTPError without response object for {url}. Cannot retry effectively.")
                    return None
            except requests.exceptions.RequestException as e:
                logger.warning(f"Request error on attempt {attempt + 1}/{API_RETRY_ATTEMPTS} for {url}: {e}")
                if attempt < API_RETRY_ATTEMPTS - 1:
                    delay = API_RETRY_DELAY * (2 ** attempt)
                    time.sleep(delay)
            except json.JSONDecodeError as e_json:
                logger.error(
                    f"JSON decode error for {url} on attempt {attempt + 1}. Response text: {response.text[:500] if 'response' in locals() else 'Response object not available'}... Error: {e_json}")
                if attempt < API_RETRY_ATTEMPTS - 1:
                    delay = API_RETRY_DELAY * (2 ** attempt)
                    time.sleep(delay)
                else:
                    return None
        logger.error(f"All {API_RETRY_ATTEMPTS} attempts failed for {url}. Last query params: {full_query_params}")
        return None


# --- Helper functions for scraping and parsing (moved from old api_clients.py) ---
def scrape_article_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9', 'Connection': 'keep-alive'
        }
        response = requests.get(url, headers=headers, timeout=API_REQUEST_TIMEOUT - 10, allow_redirects=True)
        response.raise_for_status()
        content_type = response.headers.get('content-type', '').lower()
        if 'html' not in content_type:
            logger.warning(f"Content type for {url} is not HTML ('{content_type}'). Skipping scrape.");
            return None

        soup = BeautifulSoup(response.content, 'lxml')

        for tag_name in ['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'iframe', 'noscript', 'link',
                         'meta', 'button', 'input', 'select', 'textarea', 'figure', 'figcaption']:
            for tag in soup.find_all(tag_name):
                tag.decompose()

        main_content_html = None
        selectors = [
            'article', 'main', 'div[role="main"]',
            'div[class*="article-body"]', 'div[class*="article-content"]', 'div[id*="article-body"]',
            'div[id*="article-content"]',
            'div[class*="post-content"]', 'div[class*="entry-content"]',
            'div[class*="story-body"]', 'div[class*="main-content"]', 'section[class*="content"]'
        ]
        for selector in selectors:
            tag = soup.select_one(selector)
            if tag:
                for unwanted_pattern in ['ad', 'social', 'related', 'share', 'comment', 'promo', 'sidebar', 'popup',
                                         'banner', 'meta-info', 'byline', 'author', 'timestamp', 'tags', 'breadcrumb',
                                         'pagination', 'tools', 'print-button', 'advertisement', 'figcaption',
                                         'read-more', 'newsletter', 'modal']:
                    for sub_tag in tag.find_all(
                            lambda t: any(unwanted_pattern in c.lower() for c in t.get('class', [])) or \
                                      any(unwanted_pattern in i.lower() for i in t.get('id', [])) or \
                                      unwanted_pattern in t.get('role', '').lower() or \
                                      unwanted_pattern in t.get('aria-label', '').lower()):
                        sub_tag.decompose()
                main_content_html = tag
                break

        article_text = ""
        if main_content_html:
            text_parts = []
            for element in main_content_html.find_all(
                    ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div', 'span', 'td', 'th']):
                text = element.get_text(separator=' ', strip=True)
                if text:
                    if element.name == 'div' and element.find(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
                        continue
                    text_parts.append(text)
            article_text = '\n'.join(filter(None, text_parts))
        elif soup.body:
            logger.info(f"Main content selectors failed for {url}, trying body text. This might be noisy.")
            article_text = soup.body.get_text(separator='\n', strip=True)
        else:
            logger.warning(f"Could not extract main content or body text from {url}.");
            return None

        article_text = re.sub(r'[ \t]+', ' ', article_text)
        article_text = re.sub(r'\n\s*\n', '\n\n', article_text)
        article_text = re.sub(r'\n{3,}', '\n\n', article_text).strip()

        if len(article_text) < 200:
            logger.info(
                f"Extracted text from {url} is very short ({len(article_text)} chars). Might be a stub, paywall, or primarily non-text content.")
        logger.info(f"Successfully scraped ~{len(article_text)} chars from {url}")
        return article_text

    except requests.exceptions.Timeout:
        logger.error(f"Timeout error scraping {url}.");
        return None
    except requests.exceptions.RequestException as e:
        logger.error(f"Request error scraping {url}: {e}");
        return None
    except Exception as e:
        logger.error(f"General error scraping {url}: {e}", exc_info=True);
        return None


def extract_S1_text_sections(filing_text, sections_map):
    if not filing_text or not sections_map: return {}
    extracted_sections = {}
    try:
        soup = BeautifulSoup(filing_text, 'lxml')
    except Exception:
        try:
            logger.warning("lxml parsing failed for SEC filing, trying html.parser.")
            soup = BeautifulSoup(filing_text, 'html.parser')
        except Exception as e_bs_parse:
            logger.error(
                f"BeautifulSoup failed to parse filing text with lxml and html.parser: {e_bs_parse}. Using raw text and regex matching might be less accurate.")
            normalized_text = re.sub(r'\s*\n\s*', '\n', filing_text.strip())
            normalized_text = ''.join(filter(lambda x: x.isprintable() or x.isspace(), normalized_text))
            soup = None

    if soup:
        for invisible_element_name in ['style', 'script', 'head', 'title', 'meta', 'link', 'noscript']:
            for element in soup.find_all(invisible_element_name):
                element.decompose()
        page_text = []
        for element in soup.find_all(
                ['p', 'div', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'td', 'tr', 'table', 'body']):
            text = element.get_text(separator='\n', strip=True)
            if text:
                page_text.append(text)
        normalized_text = '\n\n'.join(page_text)
        normalized_text = re.sub(r'\s*\n\s*', '\n', normalized_text)
        normalized_text = re.sub(r'\n{3,}', '\n\n', normalized_text)
        normalized_text = ''.join(filter(lambda x: x.isprintable() or x.isspace(), normalized_text))
    else:  # soup is None, normalized_text was already prepared
        pass

    section_patterns = []
    for key, patterns_list in sections_map.items():
        item_num_pattern_str = patterns_list[0].replace('.', r'\.?')
        base_item_regex = r"(?:ITEM|Item)\s*" + item_num_pattern_str.split()[-1] + r"\.?\s*:?\s*"
        if len(patterns_list) > 1:
            descriptive_name_regex = re.escape(patterns_list[1])
            start_regex_str_item_desc = base_item_regex + descriptive_name_regex
            section_patterns.append({"key": key, "start_regex": re.compile(start_regex_str_item_desc, re.IGNORECASE)})
            start_regex_str_desc_only = r"^\s*" + descriptive_name_regex + r"\s*$"
            section_patterns.append(
                {"key": key, "start_regex": re.compile(start_regex_str_desc_only, re.IGNORECASE | re.MULTILINE)})
        else:
            section_patterns.append({"key": key, "start_regex": re.compile(base_item_regex, re.IGNORECASE)})

    found_sections_matches = []
    for pattern_info in section_patterns:
        for match in pattern_info["start_regex"].finditer(normalized_text):
            found_sections_matches.append({
                "key": pattern_info["key"],
                "start": match.start(),
                "end_of_header": match.end(),
                "header_text": match.group(0).strip()
            })

    if not found_sections_matches:
        logger.warning("No sections extracted from SEC filing based on ITEM X or descriptive name patterns.");
        return {}

    found_sections_matches.sort(key=lambda x: x["start"])

    for i, current_sec_info in enumerate(found_sections_matches):
        start_index = current_sec_info["end_of_header"]
        end_index = len(normalized_text)
        for j in range(i + 1, len(found_sections_matches)):
            next_sec_info = found_sections_matches[j]
            if next_sec_info["key"] != current_sec_info["key"]:  # Stop if it's a header for a *different* section type
                # Check if the next header is a more specific version of the current section
                # e.g. current is "Item 1. Business", next is "Item 1A. Risk Factors" - this is fine
                # But if current is "Item 1. Business" and next is "Item 7. MD&A", then stop.
                # This logic might need refinement depending on how sections_map is structured.
                # For now, any different key means end of current section.
                end_index = next_sec_info["start"]
                break
        section_text = normalized_text[start_index:end_index].strip()
        section_text = re.sub(r'(?i)\btable\s+of\s+contents\b.*?\n', '', section_text, flags=re.MULTILINE)
        section_text = re.sub(r'^\s*(?:Page\s+\d+|\d+|PART\s+[IVXLCDM]+)\s*$', '', section_text, flags=re.MULTILINE)
        section_text = re.sub(r'\n{3,}', '\n\n', section_text).strip()

        if section_text:
            if current_sec_info["key"] not in extracted_sections or len(section_text) > len(
                    extracted_sections.get(current_sec_info["key"], "")):
                extracted_sections[current_sec_info["key"]] = section_text
                logger.debug(
                    f"Extracted section '{current_sec_info['key']}' (header: '{current_sec_info['header_text']}') len {len(section_text)}")

    if not extracted_sections:
        logger.warning("No text content could be extracted for any identified section headers after processing.")
    return extracted_sections
---------- END base_client.py ----------


---------- eodhd_client.py ----------
from .base_client import APIClient
from core.config import EODHD_API_KEY
from core.logging_setup import logger


class EODHDClient(APIClient):
    def __init__(self):
        super().__init__("https://eodhistoricaldata.com/api", api_key_name="api_token", api_key_value=EODHD_API_KEY)
        self.params["fmt"] = "json" # Default format

    def get_fundamental_data(self, ticker_with_exchange): # e.g., AAPL.US
        return self.request("GET", f"/fundamentals/{ticker_with_exchange}", api_source_name="eodhd_fundamentals")

    def get_ipo_calendar(self, from_date=None, to_date=None):
        params = {}
        if from_date: params["from"] = from_date
        if to_date: params["to"] = to_date
        logger.info("EODHDClient.get_ipo_calendar called. Data quality/availability may vary by subscription.")
        return self.request("GET", "/calendar/ipos", params=params, api_source_name="eodhd_ipo_calendar")
---------- END eodhd_client.py ----------


---------- finnhub_client.py ----------
from datetime import datetime, timedelta, timezone
from .base_client import APIClient
from core.config import FINNHUB_API_KEY


class FinnhubClient(APIClient):
    def __init__(self):
        super().__init__("https://finnhub.io/api/v1", api_key_name="token", api_key_value=FINNHUB_API_KEY)

    def get_market_news(self, category="general", min_id=0):
        params = {"category": category}
        if min_id > 0: params["minId"] = min_id
        return self.request("GET", "/news", params=params, api_source_name="finnhub_news")

    def get_company_profile2(self, ticker):
        return self.request("GET", "/stock/profile2", params={"symbol": ticker}, api_source_name="finnhub_profile")

    def get_financials_reported(self, ticker, freq="quarterly", count=20): # count specifies number of periods
        params = {"symbol": ticker, "freq": freq, "count": count}
        return self.request("GET", "/stock/financials-reported", params=params,
                            api_source_name="finnhub_financials_reported")

    def get_basic_financials(self, ticker, metric_type="all"):
        return self.request("GET", "/stock/metric", params={"symbol": ticker, "metric": metric_type},
                            api_source_name="finnhub_metrics")

    def get_ipo_calendar(self, from_date=None, to_date=None):
        if from_date is None: from_date = (datetime.now(timezone.utc) - timedelta(days=30)).strftime('%Y-%m-%d')
        if to_date is None: to_date = (datetime.now(timezone.utc) + timedelta(days=90)).strftime('%Y-%m-%d')
        params = {"from": from_date, "to": to_date}
        return self.request("GET", "/calendar/ipo", params=params, api_source_name="finnhub_ipo_calendar")

    def get_sec_filings(self, ticker, from_date=None, to_date=None):
        if from_date is None: from_date = (datetime.now(timezone.utc) - timedelta(days=365 * 2)).strftime('%Y-%m-%d')
        if to_date is None: to_date = datetime.now(timezone.utc).strftime('%Y-%m-%d')
        params = {"symbol": ticker, "from": from_date, "to": to_date}
        return self.request("GET", "/stock/filings", params=params, api_source_name="finnhub_filings")

    def get_company_peers(self, ticker):
        """Gets a list of company peers."""
        return self.request("GET", "/stock/peers", params={"symbol": ticker}, api_source_name="finnhub_peers")

---------- END finnhub_client.py ----------


---------- fmp_client.py ----------
# api_clients/fmp_client.py
from .base_client import APIClient
from core.config import FINANCIAL_MODELING_PREP_API_KEY
from core.logging_setup import logger


class FinancialModelingPrepClient(APIClient):
    def __init__(self):
        super().__init__("https://financialmodelingprep.com/api/v3", api_key_name="apikey",
                         api_key_value=FINANCIAL_MODELING_PREP_API_KEY)

    def get_ipo_calendar(self, from_date=None, to_date=None):
        # Note: FMP's free tier might not support this well or at all.
        params = {}
        if from_date: params["from"] = from_date
        if to_date: params["to"] = to_date
        logger.info("FinancialModelingPrepClient.get_ipo_calendar called. Availability depends on FMP subscription.")
        return self.request("GET", "/ipo_calendar", params=params, api_source_name="fmp_ipo_calendar")

    def get_financial_statements(self, ticker, statement_type="income-statement", period="quarter", limit=40):
        actual_limit = limit
        if period == "annual": actual_limit = min(limit, 15)
        elif period == "quarter": actual_limit = min(limit, 60)

        return self.request("GET", f"/{statement_type}/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_{statement_type.replace('-', '_')}_{period}")

    def get_income_statement_growth(self, ticker, period="quarter", limit=40):
        actual_limit = limit
        if period == "annual": actual_limit = min(limit, 15)
        elif period == "quarter": actual_limit = min(limit, 60)
        return self.request("GET", f"/income-statement-growth/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_income_statement_growth_{period}")

    def get_key_metrics(self, ticker, period="quarter", limit=40):
        actual_limit = limit
        if period == "annual": actual_limit = min(limit, 15)
        elif period == "quarter": actual_limit = min(limit, 60)
        return self.request("GET", f"/key-metrics/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_key_metrics_{period}")

    def get_ratios(self, ticker, period="quarter", limit=40):
        actual_limit = limit
        if period == "annual": actual_limit = min(limit, 15)
        elif period == "quarter": actual_limit = min(limit, 60)
        return self.request("GET", f"/ratios/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_ratios_{period}")

    def get_company_profile(self, ticker):
        return self.request("GET", f"/profile/{ticker}", params={}, api_source_name="fmp_profile")

    def get_analyst_estimates(self, ticker, period="annual"):
        logger.info(f"FMP get_analyst_estimates for {ticker} called. Availability depends on FMP subscription.")
        return self.request("GET", f"/analyst-estimates/{ticker}", params={"period": period},
                            api_source_name="fmp_analyst_estimates")
---------- END fmp_client.py ----------


---------- gemini_client.py ----------
import requests
import time
import json

from core.config import (
    GOOGLE_API_KEYS, API_REQUEST_TIMEOUT, API_RETRY_ATTEMPTS,
    API_RETRY_DELAY, GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE
)
from core.logging_setup import logger


class GeminiAPIClient:
    def __init__(self):
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models"
        self.model_name = "gemini-2.5-flash-preview-05-20"

    def _get_next_api_key_for_attempt(self, overall_attempt_num, max_attempts_per_key, total_keys):
        if total_keys == 0: return None, 0
        key_group_index = (overall_attempt_num // max_attempts_per_key) % total_keys
        api_key = GOOGLE_API_KEYS[key_group_index]
        current_retry_for_this_key = (overall_attempt_num % max_attempts_per_key) + 1
        logger.debug(f"Gemini: Using key ...{api_key[-4:]} (Index {key_group_index}), Attempt {current_retry_for_this_key}/{max_attempts_per_key}")
        return api_key, current_retry_for_this_key

    def generate_text(self, prompt, model=None):
        if model is None: model = self.model_name

        max_attempts_per_key = API_RETRY_ATTEMPTS
        total_keys = len(GOOGLE_API_KEYS)
        if total_keys == 0:
            logger.error("Gemini: No API keys configured in GOOGLE_API_KEYS."); return "Error: No Google API keys."

        if len(prompt) > GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE:
            original_len = len(prompt)
            prompt = prompt[:GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE]
            logger.warning(
                f"Gemini prompt (original length {original_len}) exceeded hard limit {GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE}. "
                f"Truncated to {len(prompt)} chars."
            )
            trunc_note = "\n...[PROMPT TRUNCATED DUE TO EXCESSIVE LENGTH]..."
            if len(prompt) + len(trunc_note) <= GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE:
                prompt += trunc_note
            else:
                prompt = prompt[:GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE - len(trunc_note)] + trunc_note

        for overall_attempt_num in range(total_keys * max_attempts_per_key):
            api_key, current_retry_for_this_key = self._get_next_api_key_for_attempt(
                overall_attempt_num, max_attempts_per_key, total_keys
            )
            if api_key is None: break

            url = f"{self.base_url}/{model}:generateContent?key={api_key}"
            payload = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {
                    "temperature": 0.6, "maxOutputTokens": 8192,
                    "topP": 0.9, "topK": 40
                },
                "safetySettings": [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                ]
            }
            try:
                response = requests.post(url, json=payload, timeout=API_REQUEST_TIMEOUT + 120)
                response.raise_for_status()
                response_json = response.json()

                if response_json.get("promptFeedback", {}).get("blockReason"):
                    reason = response_json["promptFeedback"]["blockReason"]
                    logger.error(f"Gemini prompt blocked for key ...{api_key[-4:]}. Reason: {reason}. Prompt: '{prompt[:150]}...'")
                    time.sleep(API_RETRY_DELAY); continue

                if "candidates" in response_json and response_json["candidates"]:
                    candidate = response_json["candidates"][0]
                    finish_reason = candidate.get("finishReason")
                    if finish_reason not in [None, "STOP", "MAX_TOKENS", "MODEL_LENGTH", "OK", "OTHER"]:
                        logger.warning(f"Gemini unusual finish reason: {finish_reason} for key ...{api_key[-4:]}. Prompt: '{prompt[:150]}...'")
                        if finish_reason == "SAFETY":
                            logger.error(f"Gemini candidate content blocked by safety settings for key ...{api_key[-4:]}.")
                            time.sleep(API_RETRY_DELAY); continue

                    content_part = candidate.get("content", {}).get("parts", [{}])[0]
                    if "text" in content_part:
                        return content_part["text"]
                    else:
                        logger.error(f"Gemini response missing 'text' in content part for key ...{api_key[-4:]}: {response_json}")
                else:
                    logger.error(f"Gemini response malformed or no candidates for key ...{api_key[-4:]}: {response_json}")

            except requests.exceptions.HTTPError as e:
                response_text = e.response.text[:200] if e.response is not None else "N/A"
                status_code = e.response.status_code if e.response is not None else "N/A"
                logger.warning(
                    f"Gemini API HTTP error key ...{api_key[-4:]} attempt {current_retry_for_this_key}: {status_code} - {response_text}. Prompt: '{prompt[:150]}...'")
                if e.response is not None and e.response.status_code == 400:
                    if "API key not valid" in e.response.text or "API_KEY_INVALID" in e.response.text:
                        logger.error(f"Gemini API key ...{api_key[-4:]} reported as invalid. Skipping further retries with this key for this call.")
                        overall_attempt_num = ( (overall_attempt_num // max_attempts_per_key) + 1) * max_attempts_per_key -1
                        continue
                    else:
                        logger.error(f"Gemini API Bad Request (400). Aborting for this prompt. Response: {e.response.text[:500]}")
                        return f"Error: Gemini API bad request (400). {e.response.text[:200]}"
            except requests.exceptions.RequestException as e:
                logger.warning(f"Gemini API request error key ...{api_key[-4:]} attempt {current_retry_for_this_key}: {e}. Prompt: '{prompt[:150]}...'")
            except json.JSONDecodeError as e_json_gemini:
                resp_text_for_log = response.text[:500] if 'response' in locals() and hasattr(response, 'text') else "N/A"
                logger.error(f"Gemini API JSON decode error key ...{api_key[-4:]} attempt {current_retry_for_this_key}. Resp: {resp_text_for_log}. Err: {e_json_gemini}")

            if overall_attempt_num < (total_keys * max_attempts_per_key) - 1:
                time.sleep(API_RETRY_DELAY * current_retry_for_this_key)

        logger.error(f"All attempts ({total_keys * max_attempts_per_key}) for Gemini API failed for prompt: {prompt[:150]}...")
        return "Error: Could not get response from Gemini API after multiple attempts."

    def summarize_text_with_context(self, text_to_summarize, context_summary, desired_output_instruction):
        prompt = (
            f"Context: {context_summary}\n\n"
            f"Text to Analyze:\n\"\"\"\n{text_to_summarize}\n\"\"\"\n\n"
            f"Instructions: {desired_output_instruction}\n\n"
            f"Provide a concise and factual summary based on the text and guided by the context and instructions."
        )
        return self.generate_text(prompt)

    def analyze_sentiment_with_reasoning(self, text_to_analyze, context=""):
        prompt = (
            f"Analyze the sentiment of the following text. "
            f"Context for analysis (if any): '{context}'.\n\n"
            f"Text to Analyze:\n\"\"\"\n{text_to_analyze}\n\"\"\"\n\n"
            f"Instructions: Respond with the sentiment classification and reasoning, structured as follows:\n"
            f"Sentiment: [Choose one: Positive, Negative, Neutral]\n"
            f"Reasoning: [Provide a brief 1-2 sentence explanation, citing specific phrases from the text if possible to justify the sentiment.]"
        )
        return self.generate_text(prompt)
---------- END gemini_client.py ----------


---------- sec_edgar_client.py ----------
import requests
import json
from datetime import datetime

from .base_client import APIClient
from core.config import EDGAR_USER_AGENT, API_REQUEST_TIMEOUT
from core.logging_setup import logger


class SECEDGARClient(APIClient):
    def __init__(self):
        self.company_tickers_url = "https://www.sec.gov/files/company_tickers.json"
        super().__init__("https://data.sec.gov/submissions/")
        self.headers = {"User-Agent": EDGAR_USER_AGENT, "Accept-Encoding": "gzip, deflate"}
        self._cik_map = None
        self._archives_base = "https://www.sec.gov/Archives/edgar/data/"

    def _load_cik_map(self):
        if self._cik_map is None:
            logger.info("Fetching CIK map from SEC...")
            cache_key_str = f"GET:{self.company_tickers_url}"
            cached_map = self._get_cached_response(cache_key_str)
            if cached_map:
                self._cik_map = cached_map
                logger.info(f"CIK map loaded from cache with {len(self._cik_map)} entries.")
                return self._cik_map

            try:
                response = requests.get(self.company_tickers_url, headers=self.headers, timeout=API_REQUEST_TIMEOUT)
                response.raise_for_status()
                data = response.json()
                self._cik_map = {item['ticker']: str(item['cik_str']).zfill(10)
                                 for item in data.values() if 'ticker' in item and 'cik_str' in item}
                self._cache_response(cache_key_str, self._cik_map, "sec_cik_map")
                logger.info(f"CIK map fetched and cached with {len(self._cik_map)} entries.")
            except requests.exceptions.RequestException as e:
                logger.error(f"Error fetching CIK map from SEC: {e}", exc_info=True)
                self._cik_map = {}
            except json.JSONDecodeError as e_json:
                logger.error(f"Error decoding CIK map JSON from SEC: {e_json}", exc_info=True)
                self._cik_map = {}
        return self._cik_map

    def get_cik_by_ticker(self, ticker):
        ticker = ticker.upper()
        try:
            cik_map = self._load_cik_map()
            return cik_map.get(ticker)
        except Exception as e:
            logger.error(f"Unexpected error in get_cik_by_ticker for {ticker}: {e}", exc_info=True)
            return None

    def get_company_filings_summary(self, cik):
        if not cik: return None
        formatted_cik_for_api = str(cik).zfill(10)
        return self.request("GET", f"CIK{formatted_cik_for_api}.json", api_source_name="edgar_filings_summary")

    def get_filing_document_url(self, cik, form_type="10-K", priordate_str=None, count=1):
        if not cik: return None if count == 1 else []
        company_summary = self.get_company_filings_summary(cik)

        if not company_summary or "filings" not in company_summary or "recent" not in company_summary["filings"]:
            logger.warning(f"No recent filings data for CIK {cik} in company summary.")
            return None if count == 1 else []

        recent_filings = company_summary["filings"]["recent"]
        target_filings_info = []

        required_keys = ["form", "accessionNumber", "primaryDocument", "filingDate"]
        min_len = float('inf')
        for key in required_keys:
            if key not in recent_filings or not isinstance(recent_filings[key], list):
                logger.warning(f"Missing or invalid '{key}' in recent filings for CIK {cik}.")
                return None if count == 1 else []
            min_len = min(min_len, len(recent_filings[key]))

        if min_len == float('inf') or min_len == 0:
             logger.warning(f"No usable filing entries for CIK {cik}.")
             return None if count == 1 else []

        forms = recent_filings["form"][:min_len]
        accession_numbers = recent_filings["accessionNumber"][:min_len]
        primary_documents = recent_filings["primaryDocument"][:min_len]
        filing_dates = recent_filings["filingDate"][:min_len]

        priordate_dt = None
        if priordate_str:
            try:
                priordate_dt = datetime.strptime(priordate_str, '%Y-%m-%d').date()
            except ValueError:
                logger.warning(f"Invalid priordate_str format: {priordate_str}. Should be YYYY-MM-DD. Ignoring.")

        for i, form_val in enumerate(forms):
            if form_val.upper() == form_type.upper():
                try:
                    current_filing_date = datetime.strptime(filing_dates[i], '%Y-%m-%d').date()
                except ValueError:
                    logger.warning(f"Invalid filingDate format '{filing_dates[i]}' for CIK {cik}, entry {i}. Skipping.")
                    continue

                if priordate_dt and current_filing_date > priordate_dt:
                    continue

                acc_num_no_hyphens = accession_numbers[i].replace('-', '')
                try:
                    cik_int_for_url = int(cik)
                except ValueError:
                    logger.error(f"CIK '{cik}' for URL construction is not a valid integer. Skipping filing.")
                    continue

                doc_url = f"{self._archives_base}{cik_int_for_url}/{acc_num_no_hyphens}/{primary_documents[i]}"
                target_filings_info.append({"url": doc_url, "date": current_filing_date, "form": form_val})

        if not target_filings_info:
            logger.info(f"No '{form_type}' filings found for CIK {cik} matching criteria.")
            return None if count == 1 else []

        target_filings_info.sort(key=lambda x: x["date"], reverse=True)

        if count == 1:
            return target_filings_info[0]["url"]
        else:
            return [f_info["url"] for f_info in target_filings_info[:count]]

    def get_filing_text(self, filing_url):
        if not filing_url: return None
        logger.info(f"Fetching filing text from: {filing_url}")
        try:
            text_content = self.request("GET", filing_url, use_cache=True,
                                        api_source_name="edgar_filing_text_content",
                                        is_json_response=False)
            if text_content:
                if isinstance(text_content, bytes):
                    try:
                        text_content = text_content.decode('utf-8')
                    except UnicodeDecodeError:
                        logger.warning(f"UTF-8 decode failed for {filing_url}, trying latin-1.")
                        text_content = text_content.decode('latin-1', errors='replace')
            return text_content
        except requests.exceptions.RequestException as e:
            logger.error(f"Error fetching SEC filing text from {filing_url}: {e}")
            return None
---------- END sec_edgar_client.py ----------


---------- __init__.py ----------
from .config import *
from .logging_setup import logger, setup_logging, handle_global_exception

__all__ = [
    # From config (example, list all you need)
    "GOOGLE_API_KEYS", "FINNHUB_API_KEY", "DATABASE_URL", "LOG_FILE_PATH",
    # From logging_setup
    "logger", "setup_logging", "handle_global_exception"
]
---------- END __init__.py ----------


---------- config.py ----------
# core/config.py

GOOGLE_API_KEYS = [
    "AIzaSyDLkwkVYBTUjabShS7VfdLkQTe7vZkxcjY", # Replace with your actual key
    "AIzaSyAjECAJZVZz6PzDaUVaAkgfcOeLXCPFA6Y", # Replace with your actual key
    "AIzaSyBRDIgN7ffBvoqAgaizQfuWRQExKc_oVig", # Replace with your actual key
    "AIzaSyC4XLSmSX4U2iuAqW_pvQ87eNyPaJwQpDo", # Replace with your actual key
]

FINNHUB_API_KEY = "d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390"  # Replace with your actual key
FINANCIAL_MODELING_PREP_API_KEY = "62ERGmJoqQgGD0nSGxRZS91TVzfz61uB"  # Replace with your actual key
EODHD_API_KEY = "683079df749c42.21476005"  # Replace with your actual key or "demo"
RAPIDAPI_UPCOMING_IPO_KEY = "0bd9b5144cmsh50c0e6d95c0b662p1cbdefjsn2d1cb0104cde"  # Replace with your actual key
ALPHA_VANTAGE_API_KEY = "HB6N4X55UTFGN2FP" # Replace with your actual Alpha Vantage Key

# SEC EDGAR Configuration
EDGAR_USER_AGENT = "FinancialAnalysisBot/1.0 YourCompanyName YourContactEmail@example.com"  # Be specific and polite

# Database Configuration
DATABASE_URL = "postgresql://avnadmin:AVNS_IeMYS-rv46Au9xqkza2@pg-4d810ff-daxiake-7258.d.aivencloud.com:26922/stock-alarm?sslmode=require"

# Email Configuration
EMAIL_HOST = "smtp-relay.brevo.com"
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_HOST_USER = "8dca1d001@smtp-brevo.com"
EMAIL_HOST_PASSWORD = "VrNUkDdcR5G9AL8P"
EMAIL_SENDER = "testypesty54@gmail.com"
EMAIL_RECIPIENT = "daniprav@gmail.com"

# Logging Configuration
LOG_FILE_PATH = "app_analysis.log"
LOG_LEVEL = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# API Client Settings
API_REQUEST_TIMEOUT = 45  # seconds
API_RETRY_ATTEMPTS = 3
API_RETRY_DELAY = 10  # seconds

# Gemini API Configuration
GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE = 400000

# Chunking for Summarization
SUMMARIZATION_CHUNK_SIZE_CHARS = 500000
SUMMARIZATION_CHUNK_OVERLAP_CHARS = 5000
SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS = 500000

# Analysis Settings
MAX_NEWS_ARTICLES_PER_QUERY = 10
MAX_NEWS_TO_ANALYZE_PER_RUN = 5
MIN_MARKET_CAP = 1000000000
STOCK_FINANCIAL_YEARS = 7
IPO_ANALYSIS_REANALYZE_DAYS = 7

# Cache Settings
CACHE_EXPIRY_SECONDS = 3600 * 6

# DCF Analysis Defaults
DEFAULT_DISCOUNT_RATE = 0.09
DEFAULT_PERPETUAL_GROWTH_RATE = 0.025
DEFAULT_FCF_PROJECTION_YEARS = 5

# News Analysis
NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION = GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE - 10000

# IPO/10-K Sections
S1_KEY_SECTIONS = {
    "business": ["Item 1.", "Business"],
    "risk_factors": ["Item 1A.", "Risk Factors"],
    "mda": ["Item 7.", "Management's Discussion and Analysis of Financial Condition and Results of Operations"],
    "financial_statements": ["Item 8.", "Financial Statements and Supplementary Data"]
}
TEN_K_KEY_SECTIONS = S1_KEY_SECTIONS

# Stock Analyzer specific settings
MAX_COMPETITORS_TO_ANALYZE = 5
Q_REVENUE_SANITY_CHECK_DEVIATION_THRESHOLD = 0.75
PRIORITY_REVENUE_SOURCES = ["fmp_quarterly", "finnhub_quarterly", "alphavantage_quarterly"]
---------- END config.py ----------


---------- logging_setup.py ----------
import logging
import sys
from .config import LOG_FILE_PATH, LOG_LEVEL

_logging_configured = False

def setup_logging():
    """Configures logging for the application."""
    global _logging_configured
    if _logging_configured:
        return logging.getLogger()

    numeric_level = getattr(logging, LOG_LEVEL.upper(), None)
    if not isinstance(numeric_level, int):
        logging.warning(f"Invalid log level: {LOG_LEVEL} in config. Defaulting to INFO.")
        numeric_level = logging.INFO

    logger_obj = logging.getLogger()
    logger_obj.setLevel(numeric_level)

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')

    if not any(isinstance(h, logging.StreamHandler) for h in logger_obj.handlers):
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        logger_obj.addHandler(console_handler)

    if not any(isinstance(h, logging.FileHandler) and getattr(h, 'baseFilename', None) == LOG_FILE_PATH for h in logger_obj.handlers):
        try:
            file_handler = logging.FileHandler(LOG_FILE_PATH, mode='a')
            file_handler.setFormatter(formatter)
            logger_obj.addHandler(file_handler)
        except Exception as e:
            logging.error(f"Failed to set up file handler for {LOG_FILE_PATH}: {e}", exc_info=True)

    _logging_configured = True
    return logger_obj

logger = setup_logging()

def handle_global_exception(exc_type, exc_value, exc_traceback):
    """Custom global exception handler to log unhandled exceptions."""
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    logger.critical("Unhandled global exception:", exc_info=(exc_type, exc_value, exc_traceback))

# To use the global exception handler, uncomment the following line in your main script (e.g., main.py)
# sys.excepthook = handle_global_exception
---------- END logging_setup.py ----------


---------- __init__.py ----------
from .connection import Base, engine, SessionLocal, init_db, get_db_session
from .models import Stock, StockAnalysis, IPO, IPOAnalysis, NewsEvent, NewsEventAnalysis, CachedAPIData

__all__ = [
    "Base", "engine", "SessionLocal", "init_db", "get_db_session",
    "Stock", "StockAnalysis", "IPO", "IPOAnalysis",
    "NewsEvent", "NewsEventAnalysis", "CachedAPIData"
]
---------- END __init__.py ----------


---------- connection.py ----------
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, scoped_session
from core.config import DATABASE_URL
from core.logging_setup import logger

Base = declarative_base() # Moved from models.py

try:
    engine = create_engine(DATABASE_URL, pool_pre_ping=True)
    SessionFactory = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    SessionLocal = scoped_session(SessionFactory)

    # Base.query = SessionLocal.query_property() # This is optional

    def init_db():
        """Initializes the database and creates tables if they don't exist."""
        try:
            logger.info("Initializing database and creating tables...")
            # Import models here to ensure they are registered with Base metadata
            from . import models # noqa F401
            Base.metadata.create_all(bind=engine)
            logger.info("Database tables created successfully (if they didn't exist).")
        except Exception as e:
            logger.critical(f"CRITICAL Error initializing database: {e}", exc_info=True)
            raise

    def get_db_session():
        """Provides a database session. Caller is responsible for closing."""
        db = SessionLocal()
        try:
            yield db
        finally:
            # SessionLocal.remove() is called automatically by analyzers or when scope ends
            # For direct usage in main.py, ensure SessionLocal.remove() or db.close() is called.
             if db.is_active: # Check if session is still active before closing
                db.close()


except Exception as e:
    logger.critical(f"CRITICAL Failed to connect to database or setup SQLAlchemy: {e}", exc_info=True)
    raise
---------- END connection.py ----------


---------- models.py ----------
from sqlalchemy import Column, Integer, String, Float, DateTime, Text, JSON, ForeignKey, Boolean, Date, UniqueConstraint
from sqlalchemy.orm import relationship
from datetime import datetime, timezone
from .connection import Base  # Import Base from connection.py


class Stock(Base):
    __tablename__ = "stocks"
    id = Column(Integer, primary_key=True, index=True)
    ticker = Column(String, unique=True, index=True, nullable=False)
    company_name = Column(String)
    industry = Column(String, nullable=True)
    sector = Column(String, nullable=True)
    last_analysis_date = Column(DateTime(timezone=True),
                                default=lambda: datetime.now(timezone.utc),
                                onupdate=lambda: datetime.now(timezone.utc))
    cik = Column(String, nullable=True, index=True)

    analyses = relationship("StockAnalysis", back_populates="stock", cascade="all, delete-orphan")


class StockAnalysis(Base):
    __tablename__ = "stock_analyses"
    id = Column(Integer, primary_key=True, index=True)
    stock_id = Column(Integer, ForeignKey("stocks.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    pe_ratio = Column(Float, nullable=True)
    pb_ratio = Column(Float, nullable=True)
    ps_ratio = Column(Float, nullable=True)
    ev_to_sales = Column(Float, nullable=True)
    ev_to_ebitda = Column(Float, nullable=True)
    eps = Column(Float, nullable=True)
    roe = Column(Float, nullable=True)
    roa = Column(Float, nullable=True)
    roic = Column(Float, nullable=True)
    dividend_yield = Column(Float, nullable=True)
    debt_to_equity = Column(Float, nullable=True)
    debt_to_ebitda = Column(Float, nullable=True)
    interest_coverage_ratio = Column(Float, nullable=True)
    current_ratio = Column(Float, nullable=True)
    quick_ratio = Column(Float, nullable=True)
    revenue_growth_yoy = Column(Float, nullable=True)
    revenue_growth_qoq = Column(Float, nullable=True)
    revenue_growth_cagr_3yr = Column(Float, nullable=True)
    revenue_growth_cagr_5yr = Column(Float, nullable=True)
    eps_growth_yoy = Column(Float, nullable=True)
    eps_growth_cagr_3yr = Column(Float, nullable=True)
    eps_growth_cagr_5yr = Column(Float, nullable=True)
    net_profit_margin = Column(Float, nullable=True)
    gross_profit_margin = Column(Float, nullable=True)
    operating_profit_margin = Column(Float, nullable=True)
    free_cash_flow_per_share = Column(Float, nullable=True)
    free_cash_flow_yield = Column(Float, nullable=True)
    free_cash_flow_trend = Column(String, nullable=True)
    retained_earnings_trend = Column(String, nullable=True)
    dcf_intrinsic_value = Column(Float, nullable=True)
    dcf_upside_percentage = Column(Float, nullable=True)
    dcf_assumptions = Column(JSON, nullable=True)
    business_summary = Column(Text, nullable=True)
    economic_moat_summary = Column(Text, nullable=True)
    industry_trends_summary = Column(Text, nullable=True)
    competitive_landscape_summary = Column(Text, nullable=True)
    management_assessment_summary = Column(Text, nullable=True)
    risk_factors_summary = Column(Text, nullable=True)
    investment_thesis_full = Column(Text, nullable=True)
    investment_decision = Column(String, nullable=True)
    reasoning = Column(Text, nullable=True)
    strategy_type = Column(String, nullable=True)
    confidence_level = Column(String, nullable=True)
    key_metrics_snapshot = Column(JSON, nullable=True)
    qualitative_sources_summary = Column(JSON, nullable=True)

    stock = relationship("Stock", back_populates="analyses")


class IPO(Base):
    __tablename__ = "ipos"
    id = Column(Integer, primary_key=True, index=True)
    company_name = Column(String, index=True, nullable=False)
    symbol = Column(String, index=True, nullable=True)
    ipo_date_str = Column(String, nullable=True)
    ipo_date = Column(Date, nullable=True)
    expected_price_range_low = Column(Float, nullable=True)
    expected_price_range_high = Column(Float, nullable=True)
    expected_price_currency = Column(String, nullable=True, default="USD")
    offered_shares = Column(Float, nullable=True)
    total_shares_value = Column(Float, nullable=True)
    exchange = Column(String, nullable=True)
    status = Column(String, nullable=True)
    cik = Column(String, nullable=True, index=True)
    last_analysis_date = Column(DateTime(timezone=True),
                                default=lambda: datetime.now(timezone.utc),
                                onupdate=lambda: datetime.now(timezone.utc))
    s1_filing_url = Column(String, nullable=True)

    analyses = relationship("IPOAnalysis", back_populates="ipo", cascade="all, delete-orphan")
    __table_args__ = (UniqueConstraint('company_name', 'ipo_date_str', 'symbol', name='uq_ipo_name_date_symbol'),)


class IPOAnalysis(Base):
    __tablename__ = "ipo_analyses"
    id = Column(Integer, primary_key=True, index=True)
    ipo_id = Column(Integer, ForeignKey("ipos.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    s1_business_summary = Column(Text, nullable=True)
    s1_risk_factors_summary = Column(Text, nullable=True)
    s1_mda_summary = Column(Text, nullable=True)
    s1_financial_health_summary = Column(Text, nullable=True)
    competitive_landscape_summary = Column(Text, nullable=True)
    industry_outlook_summary = Column(Text, nullable=True)
    management_team_assessment = Column(Text, nullable=True)
    use_of_proceeds_summary = Column(Text, nullable=True)
    underwriter_quality_assessment = Column(String, nullable=True)
    business_model_summary = Column(Text, nullable=True)
    risk_factors_summary = Column(Text, nullable=True)
    pre_ipo_financials_summary = Column(Text, nullable=True)
    valuation_comparison_summary = Column(Text, nullable=True)
    investment_decision = Column(String, nullable=True)
    reasoning = Column(Text, nullable=True)
    key_data_snapshot = Column(JSON, nullable=True)
    s1_sections_used = Column(JSON, nullable=True)

    ipo = relationship("IPO", back_populates="analyses")


class NewsEvent(Base):
    __tablename__ = "news_events"
    id = Column(Integer, primary_key=True, index=True)
    event_title = Column(String, index=True)
    event_date = Column(DateTime(timezone=True), nullable=True)
    source_url = Column(String, unique=True, nullable=False, index=True)
    source_name = Column(String, nullable=True)
    category = Column(String, nullable=True)
    processed_date = Column(DateTime(timezone=True),
                            default=lambda: datetime.now(timezone.utc))
    last_analyzed_date = Column(DateTime(timezone=True), nullable=True,
                                onupdate=lambda: datetime.now(timezone.utc))
    full_article_text = Column(Text, nullable=True)

    analyses = relationship("NewsEventAnalysis", back_populates="news_event", cascade="all, delete-orphan")
    __table_args__ = (UniqueConstraint('source_url', name='uq_news_source_url'),)


class NewsEventAnalysis(Base):
    __tablename__ = "news_event_analyses"
    id = Column(Integer, primary_key=True, index=True)
    news_event_id = Column(Integer, ForeignKey("news_events.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    sentiment = Column(String, nullable=True)
    sentiment_reasoning = Column(Text, nullable=True)
    affected_stocks_explicit = Column(JSON, nullable=True)
    affected_sectors_explicit = Column(JSON, nullable=True)
    news_summary_detailed = Column(Text, nullable=True)
    potential_impact_on_market = Column(Text, nullable=True)
    potential_impact_on_companies = Column(Text, nullable=True)
    potential_impact_on_sectors = Column(Text, nullable=True)
    mechanism_of_impact = Column(Text, nullable=True)
    estimated_timing_duration = Column(String, nullable=True)
    estimated_magnitude_direction = Column(String, nullable=True)
    confidence_of_assessment = Column(String, nullable=True)
    summary_for_email = Column(Text, nullable=True)
    key_news_snippets = Column(JSON, nullable=True)

    news_event = relationship("NewsEvent", back_populates="analyses")


class CachedAPIData(Base):
    __tablename__ = "cached_api_data"
    id = Column(Integer, primary_key=True, index=True)
    api_source = Column(String, index=True, nullable=False)
    request_url_or_params = Column(String, unique=True, nullable=False, index=True)
    response_data = Column(JSON, nullable=False)
    timestamp = Column(DateTime(timezone=True),
                       default=lambda: datetime.now(timezone.utc))
    expires_at = Column(DateTime(timezone=True), nullable=False, index=True)
---------- END models.py ----------


---------- __init__.py ----------
# services/ipo_analyzer/__init__.py
from .ipo_analyzer import IPOAnalyzer

__all__ = ["IPOAnalyzer"]
---------- END __init__.py ----------


---------- ai_analyzer.py ----------
# services/ipo_analyzer/ai_analyzer.py
import time
from api_clients import extract_S1_text_sections
from core.logging_setup import logger
from core.config import S1_KEY_SECTIONS, SUMMARIZATION_CHUNK_SIZE_CHARS


def _parse_ai_section_response(ai_text, section_header_keywords):
    """
    Parses a specific section from a larger AI-generated text block.
    section_header_keywords can be a string or a list of strings.
    """
    if not ai_text or ai_text.startswith("Error:"):
        return "AI Error or No Text"

    # Normalize keywords
    keywords_to_check = [k.lower() for k in (
        [section_header_keywords] if isinstance(section_header_keywords, str) else section_header_keywords)]

    lines = ai_text.split('\n')
    capture_content = False
    section_content_lines = []

    # List of all known headers to stop capturing when a new section starts
    # This needs to be comprehensive for the specific AI output format
    all_known_headers_lower = [
        "business model:", "competitive landscape:", "industry outlook:",
        "key risk factors:", "risk factors summary:", "use of ipo proceeds:",
        "financial health from md&a:", "financial health summary:", "md&a summary:",
        "investment stance:", "reasoning:", "critical verification points:"
        # Add any other headers used by Gemini prompts here
    ]

    for line in lines:
        normalized_line_stripped = line.strip().lower()

        # Check if the line starts with one of the target keywords
        matched_current_keyword = next((kw for kw in keywords_to_check if normalized_line_stripped.startswith(
            kw + ":") or normalized_line_stripped == kw), None)

        if matched_current_keyword:
            capture_content = True
            # Get content on the same line as the header, after the colon
            line_content_after_header = line.strip()[len(matched_current_keyword):].lstrip(':').strip()
            if line_content_after_header:
                section_content_lines.append(line_content_after_header)
            continue  # Move to next line

        if capture_content:
            # Check if this line is a different known header (and not one of the target keywords)
            is_another_known_header = any(
                normalized_line_stripped.startswith(h_prefix) for h_prefix in all_known_headers_lower if
                h_prefix not in keywords_to_check)
            if is_another_known_header:
                break  # Stop capturing, new section started
            section_content_lines.append(line)  # Append the original line to preserve formatting

    return "\n".join(section_content_lines).strip() if section_content_lines else "Section not found or empty."


def _parse_ai_synthesis_response(ai_response):
    """Parses the AI synthesis response for investment decision and reasoning."""
    parsed_data = {}
    if ai_response.startswith("Error:") or not ai_response:
        parsed_data["investment_decision"] = "AI Error"
        parsed_data["reasoning"] = ai_response if ai_response else "AI Error: Empty response."
        return parsed_data

    parsed_data["investment_decision"] = _parse_ai_section_response(ai_response, "Investment Stance")
    # Combine Reasoning and Critical Verification Points into one 'reasoning' field
    reasoning_text = _parse_ai_section_response(ai_response, "Reasoning")
    critical_points_text = _parse_ai_section_response(ai_response, "Critical Verification Points")

    combined_reasoning = []
    if reasoning_text and not reasoning_text.startswith("Section not found"):
        combined_reasoning.append(reasoning_text)
    if critical_points_text and not critical_points_text.startswith("Section not found"):
        combined_reasoning.append(f"Critical Verification Points:\n{critical_points_text}")

    parsed_data["reasoning"] = "\n\n".join(combined_reasoning).strip()

    if parsed_data["reasoning"].startswith("Section not found") or not parsed_data["reasoning"]:
        parsed_data["reasoning"] = ai_response  # Fallback to full response if parsing specific parts fails

    if parsed_data["investment_decision"].startswith("Section not found"):
        parsed_data["investment_decision"] = "Review AI Output"  # Default if parsing fails

    return parsed_data


def perform_ai_analysis_for_ipo(analyzer_instance, ipo_db_entry, s1_text, ipo_api_data_raw):
    """Performs AI-driven analysis using S-1 text and other IPO data."""
    analysis_payload = {
        "key_data_snapshot": ipo_api_data_raw,  # Store the raw API data for this IPO
        "s1_sections_used": {}  # Track which S-1 sections were found and used
    }

    s1_sections = {}
    if s1_text:
        s1_sections = extract_S1_text_sections(s1_text, S1_KEY_SECTIONS)
        analysis_payload["s1_sections_used"] = {k: bool(v) for k, v in s1_sections.items()}
    else:  # No S1 text, all sections will be marked as not used
        analysis_payload["s1_sections_used"] = {k: False for k in S1_KEY_SECTIONS.keys()}

    company_prompt_id = f"{ipo_db_entry.company_name} ({ipo_db_entry.symbol or 'N/A'})"
    max_section_len_for_prompt = SUMMARIZATION_CHUNK_SIZE_CHARS  # Max length for each section text in prompt

    # Prepare context from S-1 sections (truncated)
    biz_text_for_prompt = (s1_sections.get("business", "") or "")[:max_section_len_for_prompt]
    risk_text_for_prompt = (s1_sections.get("risk_factors", "") or "")[:max_section_len_for_prompt]
    mda_text_for_prompt = (s1_sections.get("mda", "") or "")[:max_section_len_for_prompt]

    prompt_context_parts = [f"IPO Analysis for: {company_prompt_id}"]
    if biz_text_for_prompt: prompt_context_parts.append(f"S-1 Business Summary Extract: {biz_text_for_prompt}")
    if risk_text_for_prompt: prompt_context_parts.append(f"S-1 Risk Factors Extract: {risk_text_for_prompt}")
    if mda_text_for_prompt: prompt_context_parts.append(f"S-1 MD&A Extract: {mda_text_for_prompt}")

    # Add raw IPO data to prompt context for better AI understanding
    if ipo_api_data_raw:
        context_ipo_data = {
            "name": ipo_api_data_raw.get("name"), "symbol": ipo_api_data_raw.get("symbol"),
            "date": ipo_api_data_raw.get("date"), "price": ipo_api_data_raw.get("price"),
            "exchange": ipo_api_data_raw.get("exchange"), "status": ipo_api_data_raw.get("status")
        }
        prompt_context_parts.append(f"IPO Calendar Data: {context_ipo_data}")

    full_prompt_context = "\n\n".join(prompt_context_parts)

    # Gemini Prompt 1: Business, Competition, Industry
    prompt1_instruction = (
        f"{full_prompt_context}\n\n"
        "Based on the S-1 information (if provided) and IPO calendar data, summarize the following for this IPO candidate:\n"
        "1. Business Model: (Core operations, products/services, revenue generation)\n"
        "2. Competitive Landscape: (Key competitors, company's market position, differentiation)\n"
        "3. Industry Outlook: (Relevant industry trends, growth prospects, challenges)\n"
        "Structure your response clearly with these headings."
    )
    response1 = analyzer_instance.gemini.generate_text(prompt1_instruction)
    time.sleep(3)  # API call delay
    analysis_payload["s1_business_summary"] = _parse_ai_section_response(response1,
                                                                         "Business Model")  # Will be aliased to business_model_summary too
    analysis_payload["competitive_landscape_summary"] = _parse_ai_section_response(response1, "Competitive Landscape")
    analysis_payload["industry_outlook_summary"] = _parse_ai_section_response(response1, "Industry Outlook")

    # Gemini Prompt 2: Risks, Use of Proceeds, Financials
    prompt2_instruction = (
        f"{full_prompt_context}\n\n"
        "Based on the S-1 information (if provided) and IPO calendar data, summarize the following for this IPO candidate:\n"
        "1. Key Risk Factors: (Top 3-5 specific risks from S-1, not generic market risks)\n"
        "2. Use of IPO Proceeds: (How the company plans to use the funds raised)\n"
        "3. Financial Health Summary (from MD&A or inferred): (Key financial performance trends, profitability, debt, liquidity)\n"
        "Structure your response clearly with these headings."
    )
    response2 = analyzer_instance.gemini.generate_text(prompt2_instruction)
    time.sleep(3)  # API call delay
    analysis_payload["s1_risk_factors_summary"] = _parse_ai_section_response(response2, ["Key Risk Factors",
                                                                                         "Risk Factors Summary"])
    analysis_payload["use_of_proceeds_summary"] = _parse_ai_section_response(response2, "Use of IPO Proceeds")
    analysis_payload["s1_financial_health_summary"] = _parse_ai_section_response(response2, ["Financial Health Summary",
                                                                                             "Financial Health from MD&A"])

    # Alias for consistency with older DB fields if they exist or for email service
    analysis_payload["business_model_summary"] = analysis_payload["s1_business_summary"]
    analysis_payload["risk_factors_summary"] = analysis_payload["s1_risk_factors_summary"]
    analysis_payload["pre_ipo_financials_summary"] = analysis_payload["s1_financial_health_summary"]  # Alias
    analysis_payload["s1_mda_summary"] = analysis_payload[
        "s1_financial_health_summary"]  # MDA summary is often about financial health

    # Synthesis Prompt: Investment Decision and Reasoning
    synthesis_prompt_parts = [
        f"Synthesize an IPO investment perspective for {company_prompt_id} using the following information and previously analyzed S-1 summaries."]
    if analysis_payload.get('s1_business_summary') and "Section not found" not in analysis_payload[
        's1_business_summary'] and "AI Error" not in analysis_payload['s1_business_summary']:
        synthesis_prompt_parts.append(f"Business Model Snippet: {analysis_payload['s1_business_summary'][:200]}...")
    if analysis_payload.get('s1_risk_factors_summary') and "Section not found" not in analysis_payload[
        's1_risk_factors_summary'] and "AI Error" not in analysis_payload['s1_risk_factors_summary']:
        synthesis_prompt_parts.append(f"Key Risks Snippet: {analysis_payload['s1_risk_factors_summary'][:200]}...")
    if analysis_payload.get('s1_financial_health_summary') and "Section not found" not in analysis_payload[
        's1_financial_health_summary'] and "AI Error" not in analysis_payload['s1_financial_health_summary']:
        synthesis_prompt_parts.append(
            f"Financial Health Snippet: {analysis_payload['s1_financial_health_summary'][:200]}...")

    synthesis_prompt_parts.append(
        "Instructions: Provide the following structured response:\n"
        "Investment Stance: [Choose ONE: Monitor Closely, Potentially Attractive (with caveats), High Risk/Speculative, Avoid, Further Diligence Required]\n"
        "Reasoning: [Provide 2-4 bullet points explaining the stance, highlighting key pros and cons. Refer to business model, risks, financials, market conditions if relevant.]\n"
        "Critical Verification Points: [List 2-3 specific items from the S-1 (if available) or aspects of the business model that an investor should verify or scrutinize further.]"
    )
    synthesis_prompt = "\n\n".join(synthesis_prompt_parts)

    gemini_synthesis_response = analyzer_instance.gemini.generate_text(synthesis_prompt)
    time.sleep(3)  # API call delay
    parsed_synthesis = _parse_ai_synthesis_response(gemini_synthesis_response)
    analysis_payload.update(parsed_synthesis)  # Adds 'investment_decision' and 'reasoning'

    # Placeholder for other qualitative assessments if needed in future
    analysis_payload["management_team_assessment"] = "Not explicitly analyzed by AI in this version."
    analysis_payload["underwriter_quality_assessment"] = "Not explicitly analyzed by AI in this version."
    analysis_payload[
        "valuation_comparison_summary"] = "Detailed valuation comparison not performed by AI in this version."

    return analysis_payload

---------- END ai_analyzer.py ----------


---------- data_fetcher.py ----------
# services/ipo_analyzer/data_fetcher.py
import time
from datetime import datetime, timedelta, timezone
from core.logging_setup import logger
from .helpers import parse_ipo_date_string
from sqlalchemy.exc import SQLAlchemyError


def fetch_upcoming_ipo_data(analyzer_instance):
    """Fetches upcoming IPO data from Finnhub."""
    logger.info("Fetching upcoming IPOs using Finnhub...")
    ipos_data_to_process = []
    today = datetime.now(timezone.utc)
    # Look back 60 days and forward 180 days for IPOs
    from_date = (today - timedelta(days=60)).strftime('%Y-%m-%d')
    to_date = (today + timedelta(days=180)).strftime('%Y-%m-%d')

    finnhub_response = analyzer_instance.finnhub.get_ipo_calendar(from_date=from_date, to_date=to_date)
    actual_ipo_list = []

    if finnhub_response and isinstance(finnhub_response, dict) and "ipoCalendar" in finnhub_response:
        actual_ipo_list = finnhub_response["ipoCalendar"]
        if not isinstance(actual_ipo_list, list):
            logger.warning(f"Finnhub response 'ipoCalendar' field is not a list. Found: {type(actual_ipo_list)}")
            actual_ipo_list = []  # Reset to empty list if not a list
        elif not actual_ipo_list:
            logger.info("Finnhub 'ipoCalendar' list is empty for the current period.")
    elif finnhub_response is None:  # Explicit check for None, indicating API failure handled by base_client
        logger.error("Failed to fetch IPOs from Finnhub (API call failed or returned None).")
    else:  # Other unexpected formats
        logger.info(f"No IPOs found or unexpected format from Finnhub. Response: {str(finnhub_response)[:200]}")

    if actual_ipo_list:  # Ensure it's a list and has items
        for ipo_api_data in actual_ipo_list:
            if not isinstance(ipo_api_data, dict):  # Skip if an item is not a dictionary
                logger.warning(f"Skipping non-dictionary item in Finnhub IPO calendar: {ipo_api_data}")
                continue

            price_range_raw = ipo_api_data.get("price")
            price_low, price_high = None, None
            if isinstance(price_range_raw, str) and price_range_raw.strip():  # e.g., "10.0-12.0" or "15.0"
                parts = price_range_raw.split('-', 1)
                try:
                    price_low = float(parts[0].strip())
                except:
                    pass  # pylint: disable=bare-except
                try:
                    price_high = float(parts[1].strip()) if len(parts) > 1 and parts[1].strip() else price_low
                except:
                    price_high = price_low if price_low is not None else None  # pylint: disable=bare-except
            elif isinstance(price_range_raw, (float, int)):  # If it's just a number
                price_low = float(price_range_raw)
                price_high = float(price_range_raw)

            parsed_date = parse_ipo_date_string(ipo_api_data.get("date"))

            ipos_data_to_process.append({
                "company_name": ipo_api_data.get("name"),
                "symbol": ipo_api_data.get("symbol"),
                "ipo_date_str": ipo_api_data.get("date"),  # Original string for DB
                "ipo_date": parsed_date,  # Parsed date object
                "expected_price_range_low": price_low,
                "expected_price_range_high": price_high,
                "exchange": ipo_api_data.get("exchange"),
                "status": ipo_api_data.get("status"),
                "offered_shares": ipo_api_data.get("numberOfShares"),  # Finnhub field name
                "total_shares_value": ipo_api_data.get("totalSharesValue"),  # Finnhub field name
                "source_api": "Finnhub",  # To track where the data came from
                "raw_data": ipo_api_data  # Store the raw dict for snapshot
            })
        logger.info(f"Successfully parsed {len(ipos_data_to_process)} IPOs from Finnhub API response.")

    # Deduplicate IPOs based on a composite key (name, symbol, date string)
    # This is important if multiple sources are ever combined or if an API returns duplicates
    unique_ipos = []
    seen_keys = set()
    for ipo_info in ipos_data_to_process:
        # Normalize key parts for better matching
        key_name = ipo_info.get("company_name", "").strip().lower() if ipo_info.get(
            "company_name") else "unknown_company"
        key_symbol = ipo_info.get("symbol", "").strip().upper() if ipo_info.get(
            "symbol") else "NO_SYMBOL"  # Handle missing symbols
        key_date = ipo_info.get("ipo_date_str", "")  # Use original date string for uniqueness

        unique_tuple = (key_name, key_symbol, key_date)
        if unique_tuple not in seen_keys:
            unique_ipos.append(ipo_info)
            seen_keys.add(unique_tuple)

    logger.info(f"Total unique IPOs fetched after deduplication: {len(unique_ipos)}")
    return unique_ipos


def fetch_s1_filing_data(analyzer_instance, db_session, ipo_db_entry):
    """Fetches S-1 filing text for a given IPO DB entry."""
    if not ipo_db_entry:
        return None, None

    target_cik = ipo_db_entry.cik

    # If CIK is not in the DB entry, try to get it using the symbol
    if not target_cik:
        if ipo_db_entry.symbol:
            target_cik = analyzer_instance.sec_edgar.get_cik_by_ticker(ipo_db_entry.symbol)
            time.sleep(0.5)  # SEC EDGAR rate limiting
            if target_cik:
                ipo_db_entry.cik = target_cik  # Update DB entry with found CIK
                try:
                    db_session.commit()
                except SQLAlchemyError as e:  # Catch potential commit errors
                    db_session.rollback()
                    logger.error(f"Failed to update CIK for {ipo_db_entry.company_name}: {e}")
            else:
                logger.warning(f"No CIK found via symbol {ipo_db_entry.symbol} for IPO '{ipo_db_entry.company_name}'.")
                return None, None  # Cannot proceed without CIK
        else:
            logger.warning(f"No CIK or symbol available for IPO '{ipo_db_entry.company_name}'. Cannot fetch S-1.")
            return None, None  # Cannot proceed

    logger.info(f"Attempting to fetch S-1/F-1 for {ipo_db_entry.company_name} (CIK: {target_cik})")
    s1_url = None
    # Try common S-1 and F-1 forms (including amendments)
    for form_type in ["S-1", "S-1/A", "F-1", "F-1/A"]:
        s1_url = analyzer_instance.sec_edgar.get_filing_document_url(cik=target_cik, form_type=form_type)
        time.sleep(0.5)  # SEC EDGAR rate limiting
        if s1_url:
            logger.info(f"Found {form_type} URL for {ipo_db_entry.company_name}: {s1_url}")
            break

    if s1_url:
        # Update the s1_filing_url in the database if it's new or different
        if ipo_db_entry.s1_filing_url != s1_url:
            ipo_db_entry.s1_filing_url = s1_url
            try:
                db_session.commit()
            except SQLAlchemyError as e:
                db_session.rollback()
                logger.warning(f"Failed to update S1 filing URL for {ipo_db_entry.company_name} due to: {e}")

        filing_text = analyzer_instance.sec_edgar.get_filing_text(s1_url)
        if filing_text:
            logger.info(f"Fetched S-1/F-1 text (length: {len(filing_text)}) for {ipo_db_entry.company_name}")
            return filing_text, s1_url
        else:
            logger.warning(f"Failed to fetch S-1/F-1 text from {s1_url}")
    else:
        logger.warning(f"No S-1 or F-1 URL found for {ipo_db_entry.company_name} (CIK: {target_cik}).")

    return None, None  # Return None if no text or URL found
---------- END data_fetcher.py ----------


---------- db_handler.py ----------
# services/ipo_analyzer/db_handler.py
import time
from database import IPO
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError


def get_or_create_ipo_db_entry(analyzer_instance, db_session, ipo_data_from_fetch):
    """Gets an existing IPO entry from the DB or creates a new one."""
    ipo_db_entry = None

    # Try to find by symbol first, as it's more likely to be unique if present
    if ipo_data_from_fetch.get("symbol"):
        ipo_db_entry = db_session.query(IPO).filter(IPO.symbol == ipo_data_from_fetch["symbol"]).first()

    # If not found by symbol, try by company name and IPO date string (original date string)
    if not ipo_db_entry and ipo_data_from_fetch.get("company_name") and ipo_data_from_fetch.get("ipo_date_str"):
        ipo_db_entry = db_session.query(IPO).filter(
            IPO.company_name == ipo_data_from_fetch["company_name"],
            IPO.ipo_date_str == ipo_data_from_fetch["ipo_date_str"]  # Match on the original string
        ).first()

    # Try to get CIK if not already available
    cik_to_store = ipo_data_from_fetch.get("cik")  # CIK might come from data fetch
    if not cik_to_store and ipo_data_from_fetch.get("symbol"):
        # If no CIK from fetched data, try to get it using symbol
        cik_to_store = analyzer_instance.sec_edgar.get_cik_by_ticker(ipo_data_from_fetch["symbol"])
        time.sleep(0.5)  # SEC EDGAR rate limiting
    elif not cik_to_store and ipo_db_entry and ipo_db_entry.symbol and not ipo_db_entry.cik:
        # If DB entry exists but has no CIK, try to get it
        cik_to_store = analyzer_instance.sec_edgar.get_cik_by_ticker(ipo_db_entry.symbol)
        time.sleep(0.5)

    if not ipo_db_entry:
        logger.info(f"IPO '{ipo_data_from_fetch.get('company_name')}' not found in DB, creating new entry.")
        ipo_db_entry = IPO(
            company_name=ipo_data_from_fetch.get("company_name"),
            symbol=ipo_data_from_fetch.get("symbol"),
            ipo_date_str=ipo_data_from_fetch.get("ipo_date_str"),
            ipo_date=ipo_data_from_fetch.get("ipo_date"),  # Parsed date
            expected_price_range_low=ipo_data_from_fetch.get("expected_price_range_low"),
            expected_price_range_high=ipo_data_from_fetch.get("expected_price_range_high"),
            offered_shares=ipo_data_from_fetch.get("offered_shares"),
            total_shares_value=ipo_data_from_fetch.get("total_shares_value"),
            exchange=ipo_data_from_fetch.get("exchange"),
            status=ipo_data_from_fetch.get("status"),
            cik=cik_to_store
        )
        db_session.add(ipo_db_entry)
        try:
            db_session.commit()
            db_session.refresh(ipo_db_entry)  # To get ID and other defaults
            logger.info(
                f"Created IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}, CIK: {ipo_db_entry.cik})")
        except SQLAlchemyError as e:
            db_session.rollback()
            logger.error(f"Error creating IPO DB entry for '{ipo_data_from_fetch.get('company_name')}': {e}",
                         exc_info=True)
            return None  # Return None if creation fails
    else:
        # IPO entry exists, update it if necessary
        updated = False
        fields_to_update = [
            "company_name", "symbol", "ipo_date_str", "ipo_date",
            "expected_price_range_low", "expected_price_range_high",
            "offered_shares", "total_shares_value", "exchange", "status"
        ]
        for field in fields_to_update:
            new_val = ipo_data_from_fetch.get(field)
            # Check if new_val is not None to avoid overwriting existing data with None
            # Also check if the value has actually changed
            if new_val is not None and getattr(ipo_db_entry, field) != new_val:
                setattr(ipo_db_entry, field, new_val)
                updated = True

        # Update CIK if a new one was found and it's different or was missing
        if cik_to_store and (ipo_db_entry.cik != cik_to_store or not ipo_db_entry.cik):
            ipo_db_entry.cik = cik_to_store
            updated = True

        if updated:
            try:
                db_session.commit()
                db_session.refresh(ipo_db_entry)
                logger.info(f"Updated IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}).")
            except SQLAlchemyError as e:
                db_session.rollback()
                logger.error(f"Error updating IPO DB entry for '{ipo_db_entry.company_name}': {e}", exc_info=True)
                # Potentially return the existing, un-updated entry or None depending on desired behavior

    return ipo_db_entry
---------- END db_handler.py ----------


---------- helpers.py ----------
# services/ipo_analyzer/helpers.py
from dateutil import parser as date_parser
from core.logging_setup import logger

def parse_ipo_date_string(date_str):
    if not date_str:
        return None
    try:
        # date_parser is quite flexible
        return date_parser.parse(date_str).date()
    except (ValueError, TypeError) as e:
        logger.warning(f"Could not parse IPO date string '{date_str}': {e}")
        return None
---------- END helpers.py ----------


---------- ipo_analyzer.py ----------
# services/ipo_analyzer/ipo_analyzer.py
import time
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timedelta, timezone
import concurrent.futures

from api_clients import FinnhubClient, GeminiAPIClient, SECEDGARClient
from database import SessionLocal, IPO, IPOAnalysis  # Removed get_db_session, manage session per thread
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError
from core.config import IPO_ANALYSIS_REANALYZE_DAYS

# Import functions from submodules
from .helpers import parse_ipo_date_string
from .data_fetcher import fetch_upcoming_ipo_data, fetch_s1_filing_data
from .db_handler import get_or_create_ipo_db_entry
from .ai_analyzer import perform_ai_analysis_for_ipo

MAX_IPO_ANALYSIS_WORKERS = 1  # Module-level constant, adjust as needed based on API limits and system resources


class IPOAnalyzer:
    def __init__(self):
        self.finnhub = FinnhubClient()
        self.gemini = GeminiAPIClient()
        self.sec_edgar = SECEDGARClient()
        # Note: IPOAnalyzer orchestrates tasks, each task/thread will get its own DB session.

    def _analyze_single_ipo_task(self, db_session, ipo_data_from_fetch):
        """
        Analyzes a single IPO. This method is called by the thread worker.
        It uses the provided db_session.
        """
        ipo_identifier = ipo_data_from_fetch.get("company_name") or ipo_data_from_fetch.get("symbol") or "Unknown IPO"
        logger.info(
            f"Task: Starting analysis for IPO: {ipo_identifier} from source {ipo_data_from_fetch.get('source_api')}")

        ipo_db_entry = get_or_create_ipo_db_entry(self, db_session, ipo_data_from_fetch)
        if not ipo_db_entry:
            logger.error(
                f"Task: Could not get/create DB entry for IPO {ipo_identifier}. Aborting analysis for this item.")
            return None

        # Check if recent analysis exists and if significant data has changed
        reanalyze_threshold = datetime.now(timezone.utc) - timedelta(days=IPO_ANALYSIS_REANALYZE_DAYS)
        existing_analysis = db_session.query(IPOAnalysis).filter(IPOAnalysis.ipo_id == ipo_db_entry.id).order_by(
            IPOAnalysis.analysis_date.desc()).first()

        significant_change_detected = False
        if existing_analysis and existing_analysis.key_data_snapshot:
            # Compare key fields from the snapshot with current ipo_db_entry
            snap = existing_analysis.key_data_snapshot  # This is the raw_data from previous fetch
            snap_parsed_date = parse_ipo_date_string(snap.get("date"))  # Parse date from snapshot for comparison

            if (ipo_db_entry.ipo_date != snap_parsed_date or
                    ipo_db_entry.status != snap.get("status") or
                    ipo_db_entry.expected_price_range_low != snap.get(
                        "price_range_low") or  # Assuming snapshot keys match
                    ipo_db_entry.expected_price_range_high != snap.get("price_range_high")):
                significant_change_detected = True
                logger.info(f"Task: Significant data change detected for {ipo_identifier}. Re-analyzing.")

        if existing_analysis and not significant_change_detected and existing_analysis.analysis_date >= reanalyze_threshold:
            logger.info(
                f"Task: Recent analysis for {ipo_identifier} exists (Date: {existing_analysis.analysis_date}) and no significant changes. Skipping re-analysis.")
            return existing_analysis  # Return existing analysis if up-to-date and no major changes

        # Fetch S-1 data
        s1_text, s1_url = fetch_s1_filing_data(self, db_session, ipo_db_entry)
        # s1_url is already saved to ipo_db_entry by fetch_s1_filing_data if found

        # Perform AI analysis
        # ipo_data_from_fetch contains 'raw_data' which is the original API response
        analysis_payload = perform_ai_analysis_for_ipo(self, ipo_db_entry, s1_text,
                                                       ipo_data_from_fetch.get("raw_data", {}))

        current_time = datetime.now(timezone.utc)

        if existing_analysis:  # Update existing analysis entry
            logger.info(f"Task: Updating existing analysis for {ipo_identifier} (ID: {existing_analysis.id})")
            for key, value in analysis_payload.items():
                setattr(existing_analysis, key, value)
            existing_analysis.analysis_date = current_time
            entry_to_save = existing_analysis
        else:  # Create new analysis entry
            logger.info(f"Task: Creating new analysis for {ipo_identifier}")
            entry_to_save = IPOAnalysis(
                ipo_id=ipo_db_entry.id,
                analysis_date=current_time,
                **analysis_payload
            )
            db_session.add(entry_to_save)

        ipo_db_entry.last_analysis_date = current_time  # Update parent IPO's last analysis date

        try:
            db_session.commit()
            logger.info(f"Task: Saved IPO analysis for {ipo_identifier} (Analysis ID: {entry_to_save.id})")
        except SQLAlchemyError as e:
            db_session.rollback()
            logger.error(f"Task: DB error saving IPO analysis for {ipo_identifier}: {e}", exc_info=True)
            return None  # Indicate failure

        return entry_to_save

    def run_ipo_analysis_pipeline(self):
        all_upcoming_ipos_from_api = fetch_upcoming_ipo_data(self)
        analyzed_results = []

        if not all_upcoming_ipos_from_api:
            logger.info("No upcoming IPOs found to analyze.")
            return []

        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_IPO_ANALYSIS_WORKERS) as executor:
            future_to_ipo_data = {}
            for ipo_data in all_upcoming_ipos_from_api:
                # Filter IPOs that are relevant for analysis (e.g., not withdrawn, has a name)
                status = ipo_data.get("status", "").lower()
                # Define statuses that are worth analyzing
                relevant_statuses = ["expected", "filed", "priced", "upcoming", "active"]  # Adjust as needed

                if status not in relevant_statuses or not ipo_data.get("company_name"):
                    logger.debug(
                        f"Skipping IPO '{ipo_data.get('company_name', 'N/A')}' due to status '{status}' or missing name.")
                    continue

                # Submit the task to the executor
                future = executor.submit(self._thread_worker_analyze_ipo, ipo_data)
                future_to_ipo_data[future] = ipo_data.get("company_name", "Unknown IPO")

            for future in concurrent.futures.as_completed(future_to_ipo_data):
                ipo_name = future_to_ipo_data[future]
                try:
                    result = future.result()  # This will block until the future is complete
                    if result:
                        analyzed_results.append(result)
                except Exception as exc:
                    logger.error(f"IPO analysis for '{ipo_name}' generated an exception in thread: {exc}",
                                 exc_info=True)

        logger.info(
            f"IPO analysis pipeline completed. Processed {len(analyzed_results)} IPOs that required new/updated analysis.")
        return analyzed_results

    def _thread_worker_analyze_ipo(self, ipo_data_from_fetch):
        """
        Worker function for each thread. Manages its own DB session.
        """
        db_session = SessionLocal()  # Create a new session for this thread
        try:
            return self._analyze_single_ipo_task(db_session, ipo_data_from_fetch)
        finally:
            SessionLocal.remove()  # Remove the session, effectively closing it for this thread
---------- END ipo_analyzer.py ----------


---------- __init__.py ----------
# services/news_analyzer/__init__.py
from .news_analyzer import NewsAnalyzer

__all__ = ["NewsAnalyzer"]
---------- END __init__.py ----------


---------- ai_analyzer.py ----------
# services/news_analyzer/ai_analyzer.py
import time
from core.logging_setup import logger
from core.config import NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION


def _parse_ai_section_from_news_analysis(ai_text, section_header_keywords):
    """
    Parses a specific section from a larger AI-generated text block for news analysis.
    """
    if not ai_text or ai_text.startswith("Error:"):
        return "AI Error or No Text"

    keywords_to_check = [k.lower().strip() for k in (
        [section_header_keywords] if isinstance(section_header_keywords, str) else section_header_keywords)]
    lines = ai_text.split('\n')
    capture_content = False
    section_content_lines = []

    all_known_headers_lower_prefixes = [
        "news summary:", "affected entities:", "affected companies:", "affected stocks/sectors:",
        "mechanism of impact:", "estimated timing & duration:", "estimated timing:",
        "estimated magnitude & direction:", "estimated magnitude/direction:",
        "confidence level:", "investor summary:", "final summary for investor:"
    ]  # Ensure this list is comprehensive for your Gemini prompts

    for line_original in lines:
        line_stripped_lower = line_original.strip().lower()

        matched_current_keyword = next((kw_lower for kw_lower in keywords_to_check if
                                        line_stripped_lower.startswith(kw_lower) or line_stripped_lower == kw_lower),
                                       None)

        if matched_current_keyword:
            capture_content = True
            # Extract content on the same line after the keyword and colon
            content_on_header_line = line_original.strip()
            # Remove the keyword part
            for kw in keywords_to_check:  # Check all variations
                if content_on_header_line.lower().startswith(kw):
                    content_on_header_line = content_on_header_line[len(kw):].strip()
                    break
            if content_on_header_line.startswith(":"):
                content_on_header_line = content_on_header_line[1:].strip()

            if content_on_header_line:
                section_content_lines.append(content_on_header_line)
            continue

        if capture_content:
            # Stop if another known header (not one of the target ones) is encountered
            is_another_known_header = any(line_stripped_lower.startswith(known_header_prefix) for known_header_prefix in
                                          all_known_headers_lower_prefixes if
                                          known_header_prefix not in keywords_to_check)
            if is_another_known_header:
                break
            section_content_lines.append(line_original)  # Append the original line to preserve formatting

    return "\n".join(section_content_lines).strip() if section_content_lines else "Section not found or empty."


def perform_ai_analysis_for_news_item(analyzer_instance, news_event_db_obj):
    """
    Performs AI-driven analysis on the content of a news event.
    """
    headline = news_event_db_obj.event_title
    content_for_analysis = news_event_db_obj.full_article_text
    analysis_source_type = "full article"

    if not content_for_analysis:
        content_for_analysis = headline  # Fallback to headline if no full text
        analysis_source_type = "headline only"
        logger.warning(f"No full article text for '{headline}'. Analyzing based on headline only.")

    # Truncate if content is too long for Gemini
    if len(content_for_analysis) > NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION:
        content_for_analysis = content_for_analysis[
                               :NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION] + "\n... [CONTENT TRUNCATED FOR AI ANALYSIS] ..."
        logger.info(
            f"Truncated news content for '{headline}' to {NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION} chars for Gemini.")
        analysis_source_type += " (truncated)"

    logger.info(f"Analyzing news: '{headline[:70]}...' (using {analysis_source_type})")

    analysis_payload = {
        "key_news_snippets": {"headline": headline, "source_type_used": analysis_source_type}
    }

    # 1. Sentiment Analysis
    sentiment_response = analyzer_instance.gemini.analyze_sentiment_with_reasoning(
        content_for_analysis,
        context=f"News headline for context: {headline}"
    )
    time.sleep(2)  # API delay

    if sentiment_response and not sentiment_response.startswith("Error:"):
        try:
            # Example parsing logic (adjust based on actual Gemini output format)
            # Sentiment: [Positive/Negative/Neutral]\nReasoning: [Explanation]
            parts = sentiment_response.split("Reasoning:", 1)
            sentiment_part = parts[0].split(":", 1)[1].strip() if ":" in parts[0] else parts[0].strip()
            # Take the first word of the sentiment part as the sentiment
            analysis_payload["sentiment"] = sentiment_part.split(' ')[0].split('.')[0].split(',')[0].strip()
            analysis_payload["sentiment_reasoning"] = parts[1].strip() if len(parts) > 1 else sentiment_response
        except Exception as e_parse_sent:
            logger.warning(
                f"Could not parse sentiment response for '{headline}': {sentiment_response}. Error: {e_parse_sent}. Storing raw.")
            analysis_payload["sentiment"] = "Error Parsing"
            analysis_payload["sentiment_reasoning"] = sentiment_response
    else:
        analysis_payload["sentiment"] = "AI Error"
        analysis_payload["sentiment_reasoning"] = sentiment_response or "AI Error: Empty sentiment response."

    # 2. Detailed Impact Analysis
    prompt_detailed_analysis = (
        f"News Headline: \"{headline}\"\n"
        f"News Content (may be truncated or headline only): \"\"\"\n{content_for_analysis}\n\"\"\"\n\n"
        f"Instructions for Analysis:\n"
        f"1. News Summary: Provide a comprehensive yet concise summary of this news article (3-5 key sentences).\n"
        f"2. Affected Entities: Identify specific companies (with ticker symbols if known and highly relevant) and/or specific industry sectors directly or significantly indirectly affected by this news. Explain why briefly for each.\n"
        f"3. Mechanism of Impact: For the primary affected entities, describe how this news will likely affect their fundamentals (e.g., revenue, costs, market share, customer sentiment) or market perception.\n"
        f"4. Estimated Timing & Duration: Estimate the likely timing (e.g., Immediate, Short-term <3mo, Medium-term 3-12mo, Long-term >1yr) and duration of the impact.\n"
        f"5. Estimated Magnitude & Direction: Estimate the potential magnitude (e.g., Low, Medium, High) and direction (e.g., Positive, Negative, Neutral/Mixed) of the impact on the primary affected entities.\n"
        f"6. Confidence Level: State your confidence (High, Medium, Low) in this overall impact assessment, briefly justifying it (e.g., based on clarity of news, directness of impact).\n"
        f"7. Investor Summary: Provide a final 2-sentence summary specifically for an investor, highlighting the most critical implication or takeaway.\n\n"
        f"Structure your response clearly with headings for each point (e.g., 'News Summary:', 'Affected Entities:', etc.)."
    )
    impact_analysis_response = analyzer_instance.gemini.generate_text(prompt_detailed_analysis)
    time.sleep(2)  # API delay

    if impact_analysis_response and not impact_analysis_response.startswith("Error:"):
        analysis_payload["news_summary_detailed"] = _parse_ai_section_from_news_analysis(impact_analysis_response,
                                                                                         "News Summary:")
        # Affected Entities might contain both companies and sectors.
        affected_entities_text = _parse_ai_section_from_news_analysis(impact_analysis_response,
                                                                      ["Affected Entities:", "Affected Companies:",
                                                                       "Affected Stocks/Sectors:"])
        analysis_payload["potential_impact_on_companies"] = affected_entities_text  # Store the full text here

        # Try to parse sectors specifically if "Affected Sectors:" header is used by AI
        sectors_text = _parse_ai_section_from_news_analysis(impact_analysis_response, "Affected Sectors:")
        if sectors_text and not sectors_text.startswith("Section not found") and not sectors_text.startswith(
                "AI Error"):
            analysis_payload["potential_impact_on_sectors"] = sectors_text
        else:  # Fallback if specific sector parsing fails or not present
            analysis_payload[
                "potential_impact_on_sectors"] = affected_entities_text  # It might be mixed in "Affected Entities"

        analysis_payload["mechanism_of_impact"] = _parse_ai_section_from_news_analysis(impact_analysis_response,
                                                                                       "Mechanism of Impact:")
        analysis_payload["estimated_timing_duration"] = _parse_ai_section_from_news_analysis(impact_analysis_response, [
            "Estimated Timing & Duration:", "Estimated Timing:"])
        analysis_payload["estimated_magnitude_direction"] = _parse_ai_section_from_news_analysis(
            impact_analysis_response, ["Estimated Magnitude & Direction:", "Estimated Magnitude/Direction:"])
        analysis_payload["confidence_of_assessment"] = _parse_ai_section_from_news_analysis(impact_analysis_response,
                                                                                            "Confidence Level:")
        analysis_payload["summary_for_email"] = _parse_ai_section_from_news_analysis(impact_analysis_response,
                                                                                     ["Investor Summary:",
                                                                                      "Final Summary for Investor:"])
    else:
        logger.error(f"Gemini failed to provide detailed impact analysis for '{headline}': {impact_analysis_response}")
        analysis_payload[
            "news_summary_detailed"] = impact_analysis_response or "AI Error: Empty impact analysis response."
        # Set other fields to AI Error or similar indication
        error_indicator = analysis_payload["news_summary_detailed"]
        analysis_payload["potential_impact_on_companies"] = error_indicator
        analysis_payload["potential_impact_on_sectors"] = error_indicator
        analysis_payload["mechanism_of_impact"] = error_indicator
        analysis_payload["estimated_timing_duration"] = error_indicator
        analysis_payload["estimated_magnitude_direction"] = error_indicator
        analysis_payload["confidence_of_assessment"] = error_indicator
        analysis_payload["summary_for_email"] = error_indicator

    return analysis_payload
---------- END ai_analyzer.py ----------


---------- data_fetcher.py ----------
# services/news_analyzer/data_fetcher.py
import time
from api_clients import scrape_article_content  # scrape_article_content is generic
from core.logging_setup import logger
from core.config import MAX_NEWS_ARTICLES_PER_QUERY  # Use default from config if not overridden


def fetch_market_news_from_api(analyzer_instance, category="general",
                               count_to_fetch_from_api=MAX_NEWS_ARTICLES_PER_QUERY):
    """Fetches market news from the Finnhub API."""
    logger.info(f"Fetching latest market news for category: {category} (max {count_to_fetch_from_api} from API)...")
    news_items = analyzer_instance.finnhub.get_market_news(
        category=category)  # Finnhub `get_market_news` doesn't have a count param in current client

    if news_items and isinstance(news_items, list):
        logger.info(f"Fetched {len(news_items)} news items from Finnhub.")
        # The Finnhub client's get_market_news might already limit, but we can slice here if needed
        return news_items[:count_to_fetch_from_api]
    else:
        logger.warning(f"Failed to fetch news or received unexpected format from Finnhub: {news_items}")
        return []


def scrape_news_article_content(news_url):
    """Scrapes content for a given news URL."""
    if not news_url:
        return None
    logger.info(f"Attempting to scrape full article for: {news_url}")
    full_article_text = scrape_article_content(news_url)  # This is the generic scraper
    time.sleep(1)  # Small delay after scraping
    if full_article_text:
        logger.info(f"Scraped ~{len(full_article_text)} chars for {news_url}")
    else:
        logger.warning(f"Failed to scrape full article for {news_url}.")
    return full_article_text
---------- END data_fetcher.py ----------


---------- db_handler.py ----------
# services/news_analyzer/db_handler.py
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timezone
from database import NewsEvent
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError
from .data_fetcher import scrape_news_article_content  # For scraping if needed during get/create


def _ensure_news_event_session_is_active(analyzer_instance, news_identifier_for_log="Unknown News"):
    """Ensures the NewsAnalyzer's DB session is active, re-establishing if necessary."""
    if not analyzer_instance.db_session or not analyzer_instance.db_session.is_active:
        logger.warning(f"Session for News '{news_identifier_for_log}' was inactive/closed. Re-establishing.")
        analyzer_instance._close_session_if_active()  # Call the NewsAnalyzer's close method
        analyzer_instance.db_session = next(
            analyzer_instance.get_new_db_session_generator())  # Get a fresh session via NewsAnalyzer's method
        logger.info(f"Re-established DB session for NewsAnalyzer processing '{news_identifier_for_log}'.")


def _ensure_news_event_is_bound_to_session(analyzer_instance, news_event_db_obj):
    """Ensures a NewsEvent DB object is bound to the NewsAnalyzer's current session."""
    if not news_event_db_obj:
        return None

    news_title_for_log = news_event_db_obj.event_title[:50] if news_event_db_obj.event_title else 'Unknown News'
    _ensure_news_event_session_is_active(analyzer_instance, news_title_for_log)

    instance_state = sa_inspect(news_event_db_obj)

    # If object is not in any session OR in a different session, merge it.
    if not instance_state.session or instance_state.session is not analyzer_instance.db_session:
        obj_id_log = news_event_db_obj.id if instance_state.has_identity else 'Transient'
        logger.warning(
            f"NewsEvent DB entry '{news_title_for_log}...' (ID: {obj_id_log}) not bound to current session. Merging."
        )
        try:
            # If it's a new object not yet persisted (no ID) but we found an existing one by URL, use that one.
            if not instance_state.has_identity and news_event_db_obj.id is None and news_event_db_obj.source_url:
                existing_in_session_by_url = analyzer_instance.db_session.query(NewsEvent).filter_by(
                    source_url=news_event_db_obj.source_url).first()
                if existing_in_session_by_url:
                    logger.info(
                        f"Replaced transient NewsEvent for '{news_event_db_obj.source_url}' with instance (ID: {existing_in_session_by_url.id}) from session during binding."
                    )
                    return existing_in_session_by_url  # Return the one already in session

            # Proceed with merge for objects with identity or new objects not found by URL
            merged_event = analyzer_instance.db_session.merge(news_event_db_obj)
            logger.info(
                f"Successfully merged NewsEvent '{merged_event.event_title[:50]}...' (ID: {merged_event.id}) into session."
            )
            return merged_event
        except Exception as e_merge:
            logger.error(
                f"Failed to merge NewsEvent '{news_title_for_log}...' into session: {e_merge}. Attempting re-fetch.",
                exc_info=True
            )
            # Fallback: Try to re-fetch the object using its ID or a unique key if merge fails
            fallback_event = None
            if instance_state.has_identity and news_event_db_obj.id:  # If it had an ID
                fallback_event = analyzer_instance.db_session.query(NewsEvent).get(news_event_db_obj.id)
            elif news_event_db_obj.source_url:  # If it had a URL (unique constraint)
                fallback_event = analyzer_instance.db_session.query(NewsEvent).filter_by(
                    source_url=news_event_db_obj.source_url).first()

            if not fallback_event:
                logger.critical(
                    f"CRITICAL: Failed to re-associate NewsEvent '{news_title_for_log}...' with session after merge failure and re-fetch attempt.");
                return None  # Critical failure
            logger.info(
                f"Successfully re-fetched NewsEvent '{fallback_event.event_title[:50]}...' (ID: {fallback_event.id}) into session after merge failure."
            )
            return fallback_event

    return news_event_db_obj  # Object is already bound correctly


def get_or_create_news_event_db_entry(analyzer_instance, news_item_from_api):
    """
    Gets an existing NewsEvent from the DB or creates a new one.
    Manages scraping of full article text if not already present.
    Uses the NewsAnalyzer's managed db_session.
    """
    session_log_id = news_item_from_api.get('headline', 'Unknown News API Item')
    _ensure_news_event_session_is_active(analyzer_instance, session_log_id)

    source_url = news_item_from_api.get("url")
    if not source_url:
        logger.warning(f"News item missing URL, cannot process: {news_item_from_api.get('headline')}")
        return None

    event = analyzer_instance.db_session.query(NewsEvent).filter_by(source_url=source_url).first()

    full_article_text_scraped_this_time = None
    # Scrape if event doesn't exist, or if it exists but has no full_article_text
    if not event or (event and not event.full_article_text):
        full_article_text_scraped_this_time = scrape_news_article_content(source_url)

    current_time_utc = datetime.now(timezone.utc)
    if event:
        logger.debug(f"News event '{event.event_title[:70]}...' (URL: {source_url}) already in DB.")
        # If we scraped text now and the existing event didn't have it, update the event
        if full_article_text_scraped_this_time and not event.full_article_text:
            logger.info(f"Updating existing event {event.id} with newly scraped full article text.")
            event.full_article_text = full_article_text_scraped_this_time
            event.processed_date = current_time_utc  # Update processed date as we added content
            try:
                analyzer_instance.db_session.commit()
            except SQLAlchemyError as e:
                analyzer_instance.db_session.rollback()
                logger.error(f"Error updating full_article_text for existing event {source_url}: {e}")
        return event  # Return existing event (possibly updated)

    # If event does not exist, create a new one
    event_timestamp = news_item_from_api.get("datetime")  # Finnhub provides UNIX timestamp
    event_datetime_utc = datetime.fromtimestamp(event_timestamp, timezone.utc) if event_timestamp else current_time_utc

    new_event = NewsEvent(
        event_title=news_item_from_api.get("headline"),
        event_date=event_datetime_utc,
        source_url=source_url,
        source_name=news_item_from_api.get("source"),
        category=news_item_from_api.get("category"),
        full_article_text=full_article_text_scraped_this_time,  # Use newly scraped text
        processed_date=current_time_utc
    )
    analyzer_instance.db_session.add(new_event)
    try:
        analyzer_instance.db_session.commit()
        analyzer_instance.db_session.refresh(new_event)  # Get ID and other defaults
        logger.info(f"Stored new news event: {new_event.event_title[:70]}... (ID: {new_event.id})")
        return new_event
    except SQLAlchemyError as e:
        analyzer_instance.db_session.rollback()
        logger.error(f"Database error storing new news event '{news_item_from_api.get('headline')}': {e}",
                     exc_info=True)
        # Attempt to find if it was created by a concurrent process due to unique constraint
        # This can happen if pipeline runs in parallel or if error handling is complex
        existing_after_error = analyzer_instance.db_session.query(NewsEvent).filter_by(source_url=source_url).first()
        if existing_after_error:
            logger.warning(
                f"Found existing event for {source_url} after commit error, likely due to race condition. Using existing.")
            return existing_after_error
        return None
---------- END db_handler.py ----------


---------- news_analyzer.py ----------
# services/news_analyzer/news_analyzer.py
import time
from datetime import datetime, timezone, timedelta
from database import SessionLocal, get_db_session, NewsEventAnalysis
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError
from core.config import MAX_NEWS_ARTICLES_PER_QUERY, MAX_NEWS_TO_ANALYZE_PER_RUN

from api_clients import FinnhubClient, GeminiAPIClient  # No scrape_article_content directly needed here

# Import from submodules
from .data_fetcher import fetch_market_news_from_api
from .db_handler import (
    get_or_create_news_event_db_entry,
    _ensure_news_event_is_bound_to_session,
    _ensure_news_event_session_is_active
)
from .ai_analyzer import perform_ai_analysis_for_news_item


class NewsAnalyzer:
    def __init__(self):
        self.finnhub = FinnhubClient()
        self.gemini = GeminiAPIClient()
        # NewsAnalyzer manages its own DB session for the duration of its pipeline run or instance lifetime.
        self.db_session_generator = get_db_session()  # Store the generator
        self.db_session = next(self.db_session_generator)  # Get the initial session
        logger.debug("NewsAnalyzer initialized with a new DB session.")

    def get_new_db_session_generator(self):
        """Allows db_handler to get a new session generator if needed."""
        return get_db_session()

    def _close_session_if_active(self):
        """Closes the NewsAnalyzer's current DB session if it's active."""
        if self.db_session and self.db_session.is_active:
            try:
                self.db_session.close()
                logger.debug("DB session closed in NewsAnalyzer.")
            except Exception as e_close:
                logger.warning(f"Error closing session in NewsAnalyzer: {e_close}")
        # db_session_generator does not need explicit closing here.
        # SessionLocal.remove() will be called by the context manager of get_db_session if it was used that way.
        # Or if SessionLocal() was directly used, it must be managed.
        # For now, we assume SessionLocal() created by get_db_session handles its own removal/closing when the generator is exhausted or via its yield finally block.

    def analyze_single_news_item_and_save(self, news_event_db_obj):
        """
        Orchestrates AI analysis for a single news item and saves the analysis.
        Uses the NewsAnalyzer's managed db_session.
        """
        if not news_event_db_obj:
            logger.error("analyze_single_news_item_and_save called with no NewsEvent DB object.")
            return None

        # Ensure the object is bound to the current session
        bound_news_event_db_obj = _ensure_news_event_is_bound_to_session(self, news_event_db_obj)
        if not bound_news_event_db_obj:
            logger.error(
                f"Failed to bind news event (ID: {news_event_db_obj.id if news_event_db_obj.id else 'N/A'}) to session. Cannot analyze.")
            return None

        news_event_to_analyze = bound_news_event_db_obj  # Use the (potentially merged/re-fetched) object

        analysis_payload = perform_ai_analysis_for_news_item(self, news_event_to_analyze)

        current_analysis_time = datetime.now(timezone.utc)
        news_analysis_entry = NewsEventAnalysis(
            news_event_id=news_event_to_analyze.id,
            analysis_date=current_analysis_time,
            **analysis_payload
        )

        self.db_session.add(news_analysis_entry)
        news_event_to_analyze.last_analyzed_date = current_analysis_time  # Update parent event

        try:
            self.db_session.commit()
            logger.info(
                f"Successfully analyzed and saved news: '{news_event_to_analyze.event_title[:70]}...' (Analysis ID: {news_analysis_entry.id})")
        except SQLAlchemyError as e:
            self.db_session.rollback()
            logger.error(f"Database error saving news analysis for '{news_event_to_analyze.event_title[:70]}...': {e}",
                         exc_info=True)
            return None  # Indicate failure

        return news_analysis_entry

    def run_news_analysis_pipeline(self, category="general", count_to_fetch_from_api=MAX_NEWS_ARTICLES_PER_QUERY,
                                   count_to_analyze_this_run=MAX_NEWS_TO_ANALYZE_PER_RUN):
        try:
            _ensure_news_event_session_is_active(self,
                                                 f"Pipeline Start: Category {category}")  # Ensure session is good at start

            fetched_news_items_api = fetch_market_news_from_api(self, category=category,
                                                                count_to_fetch_from_api=count_to_fetch_from_api)
            if not fetched_news_items_api:
                logger.info("No news items fetched from API for analysis.")
                return []

            analyzed_news_results = []
            newly_analyzed_count_this_run = 0

            # Define how old an analysis can be before re-analysis is considered
            reanalyze_older_than_days = 2
            reanalyze_threshold_date = datetime.now(timezone.utc) - timedelta(days=reanalyze_older_than_days)

            for news_item_api_data in fetched_news_items_api:
                if newly_analyzed_count_this_run >= count_to_analyze_this_run:
                    logger.info(
                        f"Reached analysis limit of {count_to_analyze_this_run} new/re-analyzed items for this run.")
                    break

                try:
                    # Get or create the NewsEvent DB entry. This also handles scraping.
                    news_event_db = get_or_create_news_event_db_entry(self, news_item_api_data)
                    if not news_event_db:
                        logger.warning(
                            f"Skipping news item (could not get/create in DB): {news_item_api_data.get('headline')}")
                        continue

                    # Ensure this db object (which might be new or existing) is bound to the session correctly
                    news_event_db = _ensure_news_event_is_bound_to_session(self, news_event_db)
                    if not news_event_db:  # If binding failed critically
                        logger.error(
                            f"Critical: Failed to bind news event {news_item_api_data.get('headline')} to session. Skipping.")
                        continue

                    analysis_needed = False
                    latest_analysis = None

                    # Check existing analyses for this news event
                    # Query sorted by date to get the most recent one
                    if news_event_db.id:  # Ensure event has an ID (i.e., it's persisted or merged)
                        latest_analysis = self.db_session.query(NewsEventAnalysis) \
                            .filter(NewsEventAnalysis.news_event_id == news_event_db.id) \
                            .order_by(NewsEventAnalysis.analysis_date.desc()) \
                            .first()

                    if not latest_analysis:
                        analysis_needed = True
                        logger.info(f"News '{news_event_db.event_title[:50]}...' requires new analysis.")
                    elif latest_analysis.analysis_date < reanalyze_threshold_date:
                        analysis_needed = True
                        logger.info(
                            f"News '{news_event_db.event_title[:50]}...' requires re-analysis (last analyzed {latest_analysis.analysis_date}, older than {reanalyze_older_than_days} days).")
                    # Check if full text became available since last analysis (if last analysis was headline-only)
                    elif news_event_db.full_article_text and latest_analysis and latest_analysis.key_news_snippets:
                        source_type_used_last_time = latest_analysis.key_news_snippets.get("source_type_used", "")
                        if "full article" not in source_type_used_last_time.lower():
                            analysis_needed = True
                            logger.info(
                                f"News '{news_event_db.event_title[:50]}...' re-analyzing with newly available/confirmed full text (was: {source_type_used_last_time}).")
                    else:
                        logger.info(
                            f"News '{news_event_db.event_title[:50]}...' already recently analyzed with available text. Skipping.")

                    if analysis_needed:
                        analysis_result = self.analyze_single_news_item_and_save(news_event_db)
                        if analysis_result:
                            analyzed_news_results.append(analysis_result)
                            newly_analyzed_count_this_run += 1
                        time.sleep(3)  # Delay between AI calls if multiple items are analyzed

                except Exception as e_item:  # Catch errors for a single item processing
                    logger.error(
                        f"Failed to process or analyze news item '{news_item_api_data.get('headline')}': {e_item}",
                        exc_info=True)
                    _ensure_news_event_session_is_active(self,
                                                         f"Error Recovery for {news_item_api_data.get('headline')}")  # Ensure session is active for next item
                    if self.db_session and self.db_session.is_active:
                        self.db_session.rollback()  # Rollback any partial transaction for this item

            logger.info(
                f"News analysis pipeline completed. Newly analyzed/re-analyzed {newly_analyzed_count_this_run} items.")
            return analyzed_news_results

        finally:
            self._close_session_if_active()  # Close session at the end of the pipeline run
---------- END news_analyzer.py ----------


---------- __init__.py ----------
# services/stock_analyzer/__init__.py
from .stock_analyzer import StockAnalyzer

__all__ = ["StockAnalyzer"]
---------- END __init__.py ----------


---------- ai_synthesis.py ----------
# services/stock_analyzer/ai_synthesis.py
import re
from core.logging_setup import logger
from .helpers import safe_get_float


def _parse_ai_investment_thesis_response(ticker_for_log, ai_response_text):
    parsed_data = {
        "investment_thesis_full": "AI response not fully processed or 'Investment Thesis:' section missing.",
        "investment_decision": "Review AI Output",
        "strategy_type": "Not Specified by AI",
        "confidence_level": "Not Specified by AI",
        "reasoning": "AI response not fully processed or 'Key Reasoning Points:' section missing."
    }

    if not ai_response_text or ai_response_text.startswith("Error:"):
        error_message = ai_response_text if ai_response_text else "Error: Empty response from AI for thesis."
        parsed_data["investment_thesis_full"] = error_message
        parsed_data["reasoning"] = error_message
        parsed_data["investment_decision"] = "AI Error"
        parsed_data["strategy_type"] = "AI Error"
        parsed_data["confidence_level"] = "AI Error"
        return parsed_data

    text_content = ai_response_text.replace('\r\n', '\n').strip()

    # Define patterns for each section
    # Using re.DOTALL to make '.' match newlines, and re.MULTILINE for '^'
    # Lookahead assertions ensure non-greedy matching up to the next known header or end of string
    patterns = {
        "investment_thesis_full": re.compile(
            r"^\s*Investment Thesis:\s*\n?(.*?)(?=\n\s*(?:Investment Decision:|Strategy Type:|Confidence Level:|Key Reasoning Points:)|^\s*$|\Z)",
            re.IGNORECASE | re.MULTILINE | re.DOTALL
        ),
        "investment_decision": re.compile(
            r"^\s*Investment Decision:\s*\n?(.*?)(?=\n\s*(?:Investment Thesis:|Strategy Type:|Confidence Level:|Key Reasoning Points:)|^\s*$|\Z)",
            re.IGNORECASE | re.MULTILINE | re.DOTALL
        ),
        "strategy_type": re.compile(
            r"^\s*Strategy Type:\s*\n?(.*?)(?=\n\s*(?:Investment Thesis:|Investment Decision:|Confidence Level:|Key Reasoning Points:)|^\s*$|\Z)",
            re.IGNORECASE | re.MULTILINE | re.DOTALL
        ),
        "confidence_level": re.compile(
            r"^\s*Confidence Level:\s*\n?(.*?)(?=\n\s*(?:Investment Thesis:|Investment Decision:|Strategy Type:|Key Reasoning Points:)|^\s*$|\Z)",
            re.IGNORECASE | re.MULTILINE | re.DOTALL
        ),
        "reasoning": re.compile(
            r"^\s*Key Reasoning Points:\s*\n?(.*?)(?=\n\s*(?:Investment Thesis:|Investment Decision:|Strategy Type:|Confidence Level:)|^\s*$|\Z)",
            re.IGNORECASE | re.MULTILINE | re.DOTALL
        )
    }

    found_any_section = False
    for key, pattern in patterns.items():
        match = pattern.search(text_content)
        if match:
            content = match.group(1).strip()
            if content:
                # For single-line answers, take the first line after stripping
                if key in ["investment_decision", "strategy_type", "confidence_level"]:
                    parsed_data[key] = content.split('\n')[0].strip()
                else:  # For multi-line answers like thesis and reasoning
                    parsed_data[key] = content
                found_any_section = True
            else:  # Header found but content is empty
                parsed_data[key] = f"'{key.replace('_', ' ').title()}:' section found but content empty."

    if not found_any_section and not ai_response_text.startswith("Error:"):
        # If no sections are parsed but there is AI text, put all of it in the thesis.
        logger.warning(f"Could not parse distinct sections from AI thesis response for {ticker_for_log}. "
                       f"Full response will be in 'investment_thesis_full'.")
        parsed_data["investment_thesis_full"] = text_content
        # Other fields remain as "Review AI Output" or "Not Specified by AI"

    return parsed_data


def synthesize_investment_thesis(analyzer_instance):
    ticker = analyzer_instance.ticker
    logger.info(f"Synthesizing investment thesis for {ticker}...")

    # Retrieve all necessary data from the analyzer instance's cache
    metrics = analyzer_instance._financial_data_cache.get('calculated_metrics', {})
    qual_summaries = analyzer_instance._financial_data_cache.get('10k_summaries', {})
    dcf_results = analyzer_instance._financial_data_cache.get('dcf_results', {})
    profile = analyzer_instance._financial_data_cache.get('profile_fmp', {})
    competitor_analysis_summary = analyzer_instance._financial_data_cache.get('competitor_analysis', {}).get("summary",
                                                                                                             "N/A")

    company_name = analyzer_instance.stock_db_entry.company_name or ticker
    industry = analyzer_instance.stock_db_entry.industry or "N/A"
    sector = analyzer_instance.stock_db_entry.sector or "N/A"

    prompt = f"Company: {company_name} ({ticker})\nIndustry: {industry}, Sector: {sector}\n\n"
    prompt += "Key Financial Metrics & Data:\n"

    metrics_for_prompt = {
        "P/E Ratio": metrics.get("pe_ratio"), "P/B Ratio": metrics.get("pb_ratio"),
        "P/S Ratio": metrics.get("ps_ratio"), "Dividend Yield": metrics.get("dividend_yield"),
        "ROE": metrics.get("roe"), "ROIC": metrics.get("roic"),
        "Debt-to-Equity": metrics.get("debt_to_equity"), "Debt-to-EBITDA": metrics.get("debt_to_ebitda"),
        "Revenue Growth YoY": metrics.get("revenue_growth_yoy"),
        "Revenue Growth QoQ": metrics.get("revenue_growth_qoq"),
        f"Latest Quarterly Revenue (Source: {metrics.get('key_metrics_snapshot', {}).get('q_revenue_source', 'N/A')})": metrics.get(
            'key_metrics_snapshot', {}).get('latest_q_revenue'),
        "EPS Growth YoY": metrics.get("eps_growth_yoy"),
        "Net Profit Margin": metrics.get("net_profit_margin"),
        "Operating Profit Margin": metrics.get("operating_profit_margin"),
        "Free Cash Flow Yield": metrics.get("free_cash_flow_yield"),
        "FCF Trend (3yr)": metrics.get("free_cash_flow_trend"),
        "Retained Earnings Trend (3yr)": metrics.get("retained_earnings_trend"),
    }

    for name, val in metrics_for_prompt.items():
        if val is not None:
            formatted_val = val
            if isinstance(val, float):
                if any(kw in name.lower() for kw in ['yield', 'growth', 'margin', 'roe', 'roic']):
                    formatted_val = f'{val:.2%}'
                elif 'revenue' in name.lower() and 'growth' not in name.lower():
                    formatted_val = f'{val:,.0f}'
                else:
                    formatted_val = f'{val:.2f}'
            prompt += f"- {name}: {formatted_val}\n"

    current_stock_price = safe_get_float(profile, "price")
    dcf_iv = dcf_results.get("dcf_intrinsic_value")
    dcf_upside = dcf_results.get("dcf_upside_percentage")

    if current_stock_price is not None:
        prompt += f"- Current Stock Price: {current_stock_price:.2f}\n"
    if dcf_iv is not None:
        prompt += f"- DCF Intrinsic Value/Share (Base Case): {dcf_iv:.2f}\n"
    if dcf_upside is not None:
        prompt += f"- DCF Upside/Downside (Base Case): {dcf_upside:.2%}\n"

    if dcf_results.get("dcf_assumptions", {}).get("sensitivity_analysis"):
        prompt += "- DCF Sensitivity Highlights:\n"
        for s_idx, s_data in enumerate(dcf_results["dcf_assumptions"]["sensitivity_analysis"]):
            if s_idx < 2:  # Limit to a few examples for brevity
                upside_str = f"{s_data['upside']:.2%}" if s_data['upside'] is not None else "N/A"
                prompt += f"  - {s_data['scenario']}: IV {s_data['intrinsic_value']:.2f} (Upside: {upside_str})\n"

    prompt += "\nQualitative Summaries (from 10-K & AI analysis):\n"
    qual_for_prompt = {
        "Business Model": qual_summaries.get("business_summary"),
        "Economic Moat": qual_summaries.get("economic_moat_summary"),
        "Industry Trends & Positioning": qual_summaries.get("industry_trends_summary"),
        "Competitive Landscape": competitor_analysis_summary,  # Use the summary string
        "Management Discussion Highlights (MD&A)": qual_summaries.get("management_assessment_summary"),
        # Renamed from mda_summary
        "Key Risk Factors (from 10-K)": qual_summaries.get("risk_factors_summary"),
    }
    for name, text_val in qual_for_prompt.items():
        if text_val and isinstance(text_val, str) and not text_val.startswith(
                ("AI analysis", "Section not found", "Insufficient input")):
            prompt += f"- {name}:\n{text_val[:500].replace('...', '').strip()}...\n\n"  # Truncate for prompt
        elif text_val:  # Catch-all for other cases, like "N/A" or short error messages
            prompt += f"- {name}: {text_val}\n\n"

    if analyzer_instance.data_quality_warnings:
        prompt += "IMPORTANT DATA QUALITY CONSIDERATIONS:\n"
        for i, warn_msg in enumerate(analyzer_instance.data_quality_warnings):
            prompt += f"- WARNING {i + 1}: {warn_msg}\n"
        prompt += "Acknowledge these warnings in your risk assessment or confidence level.\n\n"

    prompt += (
        "Instructions for AI: Based on ALL the above information (quantitative, qualitative, DCF, competitor data, and data quality warnings), "
        "provide a detailed financial analysis and investment thesis. "
        "Structure your response *EXACTLY* as follows, using these specific headings on separate lines:\n\n"
        "Investment Thesis:\n"
        "[Comprehensive thesis (2-4 paragraphs) synthesizing all data. Discuss positives, negatives, outlook. "
        "If revenue growth is stagnant/negative but EPS growth is positive, explain the drivers (e.g., buybacks, margin expansion) and sustainability. "
        "Address any points on margin pressures (e.g., in DTC if mentioned in MD&A) or changes in segment profitability.]\n\n"
        "Investment Decision:\n"
        "[Choose ONE: Strong Buy, Buy, Hold, Monitor, Reduce, Sell, Avoid. Base this on the overall analysis.]\n\n"
        "Strategy Type:\n"
        "[Choose ONE that best fits: Value, GARP (Growth At a Reasonable Price), Growth, Income, Speculative, Special Situation, Turnaround.]\n\n"
        "Confidence Level:\n"
        "[Choose ONE: High, Medium, Low. This reflects confidence in YOUR analysis and decision, considering data quality and completeness.]\n\n"
        "Key Reasoning Points:\n"
        "[3-7 bullet points. Each point should be a concise summary of a key factor supporting your decision. "
        "Cover: Valuation (DCF, comparables if any), Financial Health & Profitability, Growth Prospects (Revenue & EPS), "
        "Economic Moat, Competitive Position, Key Risks (including data quality issues if significant), Management & Strategy (if inferable).]\n"
    )

    ai_response_text = analyzer_instance.gemini.generate_text(prompt)
    parsed_thesis_data = _parse_ai_investment_thesis_response(ticker, ai_response_text)

    # Adjust confidence based on data quality warnings
    if any("CRITICAL:" in warn for warn in analyzer_instance.data_quality_warnings) or \
            any("DATA QUALITY WARNING:" in warn for warn in analyzer_instance.data_quality_warnings if
                "revenue" in warn.lower()):
        current_confidence = parsed_thesis_data.get("confidence_level", "").lower()
        if current_confidence == "high":
            logger.warning(
                f"Downgrading AI confidence from High to Medium for {ticker} due to critical data quality warnings.")
            parsed_thesis_data["confidence_level"] = "Medium"
        elif current_confidence == "medium":
            logger.warning(
                f"Downgrading AI confidence from Medium to Low for {ticker} due to critical data quality warnings.")
            parsed_thesis_data["confidence_level"] = "Low"

    logger.info(f"Generated thesis for {ticker}. Decision: {parsed_thesis_data.get('investment_decision')}, "
                f"Strategy: {parsed_thesis_data.get('strategy_type')}, Confidence: {parsed_thesis_data.get('confidence_level')}")

    return parsed_thesis_data
---------- END ai_synthesis.py ----------


---------- data_fetcher.py ----------
# services/stock_analyzer/data_fetcher.py
import time
from core.logging_setup import logger
from core.config import STOCK_FINANCIAL_YEARS

def fetch_financial_statements_data(analyzer_instance):
    """Fetches all necessary financial statements and stores them in analyzer_instance._financial_data_cache."""
    ticker = analyzer_instance.ticker
    logger.info(f"Fetching financial statements for {ticker}...")
    statements_cache = {
        "fmp_income_annual": [], "fmp_balance_annual": [], "fmp_cashflow_annual": [],
        "fmp_income_quarterly": [],
        "finnhub_financials_quarterly_reported": {"data": []},
        "alphavantage_income_quarterly": {"quarterlyReports": []},
        "alphavantage_balance_quarterly": {"quarterlyReports": []},
        "alphavantage_cashflow_quarterly": {"quarterlyReports": []}
    }
    try:
        # FMP Annuals
        statements_cache["fmp_income_annual"] = analyzer_instance.fmp.get_financial_statements(ticker, "income-statement", "annual", STOCK_FINANCIAL_YEARS) or []
        time.sleep(1.5)
        statements_cache["fmp_balance_annual"] = analyzer_instance.fmp.get_financial_statements(ticker, "balance-sheet-statement", "annual", STOCK_FINANCIAL_YEARS) or []
        time.sleep(1.5)
        statements_cache["fmp_cashflow_annual"] = analyzer_instance.fmp.get_financial_statements(ticker, "cash-flow-statement", "annual", STOCK_FINANCIAL_YEARS) or []
        time.sleep(1.5)
        logger.info(f"FMP Annuals for {ticker}: Income({len(statements_cache['fmp_income_annual'])}), Balance({len(statements_cache['fmp_balance_annual'])}), Cashflow({len(statements_cache['fmp_cashflow_annual'])}).")

        # FMP Quarterlies
        statements_cache["fmp_income_quarterly"] = analyzer_instance.fmp.get_financial_statements(ticker, "income-statement", "quarter", 8) or []
        time.sleep(1.5)
        logger.info(f"FMP Quarterly Income for {ticker}: {len(statements_cache['fmp_income_quarterly'])} records.")

        # Finnhub Quarterlies
        fh_q_data = analyzer_instance.finnhub.get_financials_reported(ticker, freq="quarterly", count=8)
        time.sleep(1.5)
        if fh_q_data and isinstance(fh_q_data, dict) and fh_q_data.get("data"):
            statements_cache["finnhub_financials_quarterly_reported"] = fh_q_data
            logger.info(f"Fetched {len(fh_q_data['data'])} quarterly reports from Finnhub for {ticker}.")
        else:
            logger.warning(f"Finnhub quarterly financials reported data missing or malformed for {ticker}.")

        # Alpha Vantage Quarterlies
        av_income_q = analyzer_instance.alphavantage.get_income_statement_quarterly(ticker)
        time.sleep(15) # Alpha Vantage free tier has strict rate limits
        if av_income_q and isinstance(av_income_q, dict) and av_income_q.get("quarterlyReports"):
            statements_cache["alphavantage_income_quarterly"] = av_income_q
            logger.info(f"Fetched {len(av_income_q['quarterlyReports'])} quarterly income reports from Alpha Vantage for {ticker}.")
        else:
            logger.warning(f"Alpha Vantage quarterly income reports missing or malformed for {ticker}.")

        av_balance_q = analyzer_instance.alphavantage.get_balance_sheet_quarterly(ticker)
        time.sleep(15)
        if av_balance_q and isinstance(av_balance_q, dict) and av_balance_q.get("quarterlyReports"):
            statements_cache["alphavantage_balance_quarterly"] = av_balance_q
            logger.info(f"Fetched {len(av_balance_q['quarterlyReports'])} quarterly balance reports from Alpha Vantage for {ticker}.")
        else:
            logger.warning(f"Alpha Vantage quarterly balance reports missing or malformed for {ticker}.")

        av_cashflow_q = analyzer_instance.alphavantage.get_cash_flow_quarterly(ticker)
        time.sleep(15)
        if av_cashflow_q and isinstance(av_cashflow_q, dict) and av_cashflow_q.get("quarterlyReports"):
            statements_cache["alphavantage_cashflow_quarterly"] = av_cashflow_q
            logger.info(f"Fetched {len(av_cashflow_q['quarterlyReports'])} quarterly cash flow reports from Alpha Vantage for {ticker}.")
        else:
            logger.warning(f"Alpha Vantage quarterly cash flow reports missing or malformed for {ticker}.")

    except Exception as e:
        logger.warning(f"Error during financial statements fetch for {ticker}: {e}.", exc_info=True)

    analyzer_instance._financial_data_cache['financial_statements'] = statements_cache


def fetch_key_metrics_and_profile_data(analyzer_instance):
    """Fetches key metrics and profile data, storing them in analyzer_instance._financial_data_cache."""
    ticker = analyzer_instance.ticker
    logger.info(f"Fetching key metrics and profile for {ticker}.")

    # FMP Key Metrics (Annual & Quarterly)
    analyzer_instance._financial_data_cache['key_metrics_annual_fmp'] = analyzer_instance.fmp.get_key_metrics(ticker, "annual", STOCK_FINANCIAL_YEARS + 2) or []
    time.sleep(1.5)
    key_metrics_quarterly_fmp = analyzer_instance.fmp.get_key_metrics(ticker, "quarterly", 8)
    time.sleep(1.5)
    analyzer_instance._financial_data_cache['key_metrics_quarterly_fmp'] = key_metrics_quarterly_fmp if key_metrics_quarterly_fmp is not None else []

    # Finnhub Basic Financials
    analyzer_instance._financial_data_cache['basic_financials_finnhub'] = analyzer_instance.finnhub.get_basic_financials(ticker) or {}
    time.sleep(1.5)

    # FMP Profile (if not already fetched during _get_or_create_stock_entry)
    if 'profile_fmp' not in analyzer_instance._financial_data_cache or not analyzer_instance._financial_data_cache.get('profile_fmp'):
        profile_fmp_list = analyzer_instance.fmp.get_company_profile(ticker)
        time.sleep(1.5)
        analyzer_instance._financial_data_cache['profile_fmp'] = profile_fmp_list[0] if profile_fmp_list and isinstance(profile_fmp_list, list) and profile_fmp_list[0] else {}

    logger.info(f"FMP KM Annual for {ticker}: {len(analyzer_instance._financial_data_cache['key_metrics_annual_fmp'])}. "
                f"FMP KM Quarterly for {ticker}: {len(analyzer_instance._financial_data_cache['key_metrics_quarterly_fmp'])}. "
                f"Finnhub Basic Financials for {ticker}: {'OK' if analyzer_instance._financial_data_cache.get('basic_financials_finnhub', {}).get('metric') else 'Data missing'}.")
---------- END data_fetcher.py ----------


---------- dcf_analyzer.py ----------
# services/stock_analyzer/dcf_analyzer.py
from core.logging_setup import logger
from .helpers import safe_get_float, get_value_from_statement_list, calculate_cagr
from core.config import (
    DEFAULT_DISCOUNT_RATE, DEFAULT_PERPETUAL_GROWTH_RATE,
    DEFAULT_FCF_PROJECTION_YEARS
)


def _calculate_dcf_value_internal(ticker_for_log, start_fcf, initial_growth, discount_rate, perpetual_growth,
                                  proj_years, shares_outstanding_val):
    projected_fcfs = []
    last_projected_fcf = start_fcf
    current_year_growth_rates = []

    # Linear decline in growth rate from initial_growth to perpetual_growth over proj_years
    growth_rate_decline_per_year = (initial_growth - perpetual_growth) / float(proj_years) if proj_years > 0 else 0

    for i in range(proj_years):
        current_year_growth_rate = max(initial_growth - (growth_rate_decline_per_year * i), perpetual_growth)
        projected_fcf = last_projected_fcf * (1 + current_year_growth_rate)
        projected_fcfs.append(projected_fcf)
        last_projected_fcf = projected_fcf
        current_year_growth_rates.append(round(current_year_growth_rate, 4))  # Store for assumptions

    if not projected_fcfs:  # Should not happen if proj_years > 0
        return None, []

    # Terminal Value Calculation
    terminal_year_fcf_for_tv = projected_fcfs[-1] * (1 + perpetual_growth)
    terminal_value_denominator = discount_rate - perpetual_growth

    terminal_value = 0
    if terminal_value_denominator <= 1e-6:  # Avoid division by zero or very small numbers, or negative if pgr > dr
        logger.warning(f"DCF for {ticker_for_log}: Discount rate ({discount_rate:.3f}) is too close to or less than "
                       f"perpetual growth rate ({perpetual_growth:.3f}). Terminal Value may be unreliable or infinite. Setting TV to 0.")
    else:
        terminal_value = terminal_year_fcf_for_tv / terminal_value_denominator

    # Discount FCFs and Terminal Value
    sum_discounted_fcf = sum(fcf / ((1 + discount_rate) ** (i + 1)) for i, fcf in enumerate(projected_fcfs))
    discounted_terminal_value = terminal_value / ((1 + discount_rate) ** proj_years)

    intrinsic_equity_value = sum_discounted_fcf + discounted_terminal_value

    if shares_outstanding_val is None or shares_outstanding_val == 0:
        logger.error(f"DCF for {ticker_for_log}: Shares outstanding is zero or None. Cannot calculate per share value.")
        return None, current_year_growth_rates

    return intrinsic_equity_value / shares_outstanding_val, current_year_growth_rates


def perform_dcf_analysis(analyzer_instance):
    ticker = analyzer_instance.ticker
    logger.info(f"Performing simplified DCF analysis for {ticker}...")

    dcf_results = {
        "dcf_intrinsic_value": None,
        "dcf_upside_percentage": None,
        "dcf_assumptions": {
            "discount_rate": DEFAULT_DISCOUNT_RATE,
            "perpetual_growth_rate": DEFAULT_PERPETUAL_GROWTH_RATE,
            "projection_years": DEFAULT_FCF_PROJECTION_YEARS,
            "start_fcf": None,
            "start_fcf_basis": "N/A",
            "fcf_growth_rates_projection": [],
            "initial_fcf_growth_rate_used": None,
            "initial_fcf_growth_rate_basis": "N/A",
            "sensitivity_analysis": []
        }
    }
    assumptions = dcf_results["dcf_assumptions"]

    # Retrieve necessary data from cache
    cashflow_annual_fmp = analyzer_instance._financial_data_cache.get('financial_statements', {}).get(
        'fmp_cashflow_annual', [])
    profile_fmp = analyzer_instance._financial_data_cache.get('profile_fmp', {})
    calculated_metrics = analyzer_instance._financial_data_cache.get('calculated_metrics', {})

    current_price = safe_get_float(profile_fmp, "price")
    shares_outstanding_profile = safe_get_float(profile_fmp, "sharesOutstanding")
    mkt_cap_profile = safe_get_float(profile_fmp, "mktCap")

    shares_outstanding_calc = (
                mkt_cap_profile / current_price) if mkt_cap_profile and current_price and current_price != 0 else None
    shares_outstanding = shares_outstanding_profile if shares_outstanding_profile is not None and shares_outstanding_profile > 0 else shares_outstanding_calc

    if not cashflow_annual_fmp or not profile_fmp or current_price is None or shares_outstanding is None or shares_outstanding == 0:
        logger.warning(
            f"Insufficient data for DCF for {ticker} (FCF statements, profile, price, or shares missing/zero).")
        analyzer_instance._financial_data_cache['dcf_results'] = dcf_results
        return dcf_results

    current_fcf_annual = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 0)
    if current_fcf_annual is None or current_fcf_annual <= 10000:  # Arbitrary small positive FCF threshold
        logger.warning(
            f"Current annual FCF for {ticker} is {current_fcf_annual}. DCF requires substantial positive FCF. Skipping DCF.")
        analyzer_instance._financial_data_cache['dcf_results'] = dcf_results
        return dcf_results

    assumptions["start_fcf"] = current_fcf_annual
    assumptions[
        "start_fcf_basis"] = f"Latest Annual FCF ({cashflow_annual_fmp[0].get('date') if cashflow_annual_fmp and cashflow_annual_fmp[0] else 'N/A'})"

    # Determine initial FCF growth rate
    fcf_growth_rate_3yr_cagr = None
    if len(cashflow_annual_fmp) >= 4:  # Need 4 data points for 3-year CAGR (current and 3 years prior)
        fcf_start_for_cagr = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 3)  # 3 years prior
        if fcf_start_for_cagr and fcf_start_for_cagr > 0 and current_fcf_annual > 0:  # Both positive for meaningful CAGR
            fcf_growth_rate_3yr_cagr = calculate_cagr(current_fcf_annual, fcf_start_for_cagr, 3)

    initial_fcf_growth_rate = DEFAULT_PERPETUAL_GROWTH_RATE  # Default
    assumptions["initial_fcf_growth_rate_basis"] = "Default (Perpetual Growth Rate)"

    if fcf_growth_rate_3yr_cagr is not None:
        initial_fcf_growth_rate = fcf_growth_rate_3yr_cagr
        assumptions["initial_fcf_growth_rate_basis"] = "Historical 3yr FCF CAGR"
    elif calculated_metrics.get("revenue_growth_cagr_3yr") is not None:
        initial_fcf_growth_rate = calculated_metrics["revenue_growth_cagr_3yr"]
        assumptions["initial_fcf_growth_rate_basis"] = "Proxy: Revenue Growth CAGR (3yr)"
    elif calculated_metrics.get("revenue_growth_yoy") is not None:
        initial_fcf_growth_rate = calculated_metrics["revenue_growth_yoy"]
        assumptions["initial_fcf_growth_rate_basis"] = "Proxy: Revenue Growth YoY"

    if not isinstance(initial_fcf_growth_rate, (int, float)):  # Ensure it's a number
        initial_fcf_growth_rate = DEFAULT_PERPETUAL_GROWTH_RATE

    # Cap and floor the initial growth rate to reasonable bounds
    initial_fcf_growth_rate = min(max(initial_fcf_growth_rate, -0.05), 0.15)  # e.g., -5% to 15%
    assumptions["initial_fcf_growth_rate_used"] = initial_fcf_growth_rate

    # Base Case DCF
    base_iv_per_share, base_fcf_growth_rates = _calculate_dcf_value_internal(
        ticker, assumptions["start_fcf"], assumptions["initial_fcf_growth_rate_used"],
        assumptions["discount_rate"], assumptions["perpetual_growth_rate"],
        assumptions["projection_years"], shares_outstanding
    )

    if base_iv_per_share is not None:
        dcf_results["dcf_intrinsic_value"] = base_iv_per_share
        assumptions["fcf_growth_rates_projection"] = base_fcf_growth_rates
        if current_price and current_price != 0:
            dcf_results["dcf_upside_percentage"] = (base_iv_per_share - current_price) / current_price
    else:
        logger.error(f"DCF base case calculation failed for {ticker}.")
        analyzer_instance._financial_data_cache['dcf_results'] = dcf_results
        return dcf_results  # Exit if base case fails

    # Sensitivity Analysis
    sensitivity_scenarios = [
        {"dr_adj": -0.005, "pgr_adj": 0.0, "label": "Discount Rate -0.5%"},
        {"dr_adj": +0.005, "pgr_adj": 0.0, "label": "Discount Rate +0.5%"},
        {"dr_adj": 0.0, "pgr_adj": -0.0025, "label": "Perp. Growth -0.25%"},
        {"dr_adj": 0.0, "pgr_adj": +0.0025, "label": "Perp. Growth +0.25%"}
    ]

    for scenario in sensitivity_scenarios:
        sens_dr = assumptions["discount_rate"] + scenario["dr_adj"]
        sens_pgr = assumptions["perpetual_growth_rate"] + scenario["pgr_adj"]

        # Ensure perpetual growth is less than discount rate for stable model
        if sens_pgr >= sens_dr - 0.001:  # Small margin
            logger.debug(
                f"Skipping DCF sensitivity scenario '{scenario['label']}' for {ticker} as PGR ({sens_pgr:.3f}) >= DR ({sens_dr:.3f}).")
            continue

        iv_sens, _ = _calculate_dcf_value_internal(
            ticker, assumptions["start_fcf"], assumptions["initial_fcf_growth_rate_used"],
            sens_dr, sens_pgr, assumptions["projection_years"], shares_outstanding
        )
        if iv_sens is not None:
            upside_sens = (iv_sens - current_price) / current_price if current_price and current_price != 0 else None
            assumptions["sensitivity_analysis"].append({
                "scenario": scenario["label"],
                "discount_rate": sens_dr,
                "perpetual_growth_rate": sens_pgr,
                "intrinsic_value": iv_sens,
                "upside": upside_sens
            })

    logger.info(f"DCF for {ticker}: Base IV/Share: {dcf_results.get('dcf_intrinsic_value', 'N/A'):.2f}, "
                f"Upside: {dcf_results.get('dcf_upside_percentage', 'N/A') * 100 if dcf_results.get('dcf_upside_percentage') is not None else 'N/A':.2f}%")

    analyzer_instance._financial_data_cache['dcf_results'] = dcf_results
    return dcf_results
---------- END dcf_analyzer.py ----------


---------- helpers.py ----------
# services/stock_analyzer/helpers.py
import math
from core.logging_setup import logger

def safe_get_float(data_dict, key, default=None):
    if data_dict is None or not isinstance(data_dict, dict): return default
    val = data_dict.get(key)
    if val is None or val == "None" or val == "" or str(val).lower() == "n/a" or str(val).lower() == "-": return default
    try: return float(val)
    except (ValueError, TypeError): return default

def calculate_cagr(end_value, start_value, years):
    if start_value is None or end_value is None or not isinstance(years, (int, float)) or years <= 0: return None
    if start_value == 0: return None
    if (start_value < 0 and end_value > 0) or (start_value > 0 and end_value < 0): return None
    if start_value < 0 and end_value < 0: return None # Or handle appropriately if CAGR for negative values is desired
    if end_value == 0 and start_value > 0: return -1.0 # Total loss
    try:
        return ((float(end_value) / float(start_value)) ** (1 / float(years))) - 1
    except (ValueError, TypeError, ZeroDivisionError): # ZeroDivisionError for years = 0, though already checked
        return None


def calculate_growth(current_value, previous_value):
    if previous_value is None or current_value is None: return None
    try:
        current_value_float = float(current_value)
        previous_value_float = float(previous_value)
        if previous_value_float == 0:
            return None if current_value_float == 0 else (float('inf') if current_value_float > 0 else float('-inf'))
        return (current_value_float - previous_value_float) / abs(previous_value_float)
    except (ValueError, TypeError):
        return None

def get_value_from_statement_list(data_list, field, year_offset=0, report_date_for_log=None):
    if data_list and isinstance(data_list, list) and len(data_list) > year_offset:
        report = data_list[year_offset]
        if report and isinstance(report, dict):
            val = safe_get_float(report, field)
            # Optional: detailed logging if value is None
            # if val is None:
            #     date_info = report_date_for_log or report.get('date', 'Unknown Date')
            #     logger.debug(f"Field '{field}' not found or invalid in report for {date_info} (offset {year_offset}).")
            return val
    return None

def get_finnhub_concept_value(finnhub_quarterly_reports_data, report_section_key, concept_names_list, quarter_offset=0):
    if not finnhub_quarterly_reports_data or len(finnhub_quarterly_reports_data) <= quarter_offset: return None
    report_data = finnhub_quarterly_reports_data[quarter_offset]
    if 'report' not in report_data or report_section_key not in report_data['report']: return None
    section_items = report_data['report'][report_section_key]
    if not section_items: return None
    for item in section_items:
        if item.get('concept') in concept_names_list or item.get('label') in concept_names_list:
            return safe_get_float(item, 'value')
    return None

def get_alphavantage_value(av_quarterly_reports, field_name, quarter_offset_from_latest=0):
    if not av_quarterly_reports or len(av_quarterly_reports) <= quarter_offset_from_latest: return None
    report = av_quarterly_reports[quarter_offset_from_latest]
    return safe_get_float(report, field_name)

def get_fmp_value(fmp_quarterly_reports, field_name, quarter_offset_from_latest=0):
    if not fmp_quarterly_reports or len(fmp_quarterly_reports) <= quarter_offset_from_latest: return None
    report = fmp_quarterly_reports[quarter_offset_from_latest]
    return safe_get_float(report, field_name)
---------- END helpers.py ----------


---------- metrics_calculator.py ----------
# services/stock_analyzer/metrics_calculator.py
import math
import json
from core.logging_setup import logger
from .helpers import (
    safe_get_float, calculate_cagr, calculate_growth,
    get_value_from_statement_list, get_fmp_value,
    get_alphavantage_value, get_finnhub_concept_value
)
from core.config import (
    Q_REVENUE_SANITY_CHECK_DEVIATION_THRESHOLD,
    PRIORITY_REVENUE_SOURCES
)


def _calculate_valuation_ratios(latest_km_q_fmp, latest_km_a_fmp, basic_fin_fh_metric, overview_av):
    ratios = {}
    ratios["pe_ratio"] = safe_get_float(latest_km_q_fmp, "peRatioTTM") or \
                         safe_get_float(latest_km_a_fmp, "peRatio") or \
                         safe_get_float(basic_fin_fh_metric, "peTTM")
    ratios["pb_ratio"] = safe_get_float(latest_km_q_fmp, "priceToBookRatioTTM") or \
                         safe_get_float(latest_km_a_fmp, "pbRatio") or \
                         safe_get_float(basic_fin_fh_metric, "pbAnnual")
    ratios["ps_ratio"] = safe_get_float(latest_km_q_fmp, "priceToSalesRatioTTM") or \
                         safe_get_float(latest_km_a_fmp, "priceSalesRatio") or \
                         safe_get_float(basic_fin_fh_metric, "psTTM")

    ratios["ev_to_sales"] = safe_get_float(latest_km_q_fmp, "enterpriseValueOverRevenueTTM") or \
                            safe_get_float(latest_km_a_fmp, "enterpriseValueOverRevenue") or \
                            safe_get_float(overview_av, "EVToRevenue")

    ratios["ev_to_ebitda"] = safe_get_float(latest_km_q_fmp, "evToEbitdaTTM") or \
                             safe_get_float(latest_km_a_fmp, "evToEbitda") or \
                             safe_get_float(overview_av, "EVToEBITDA")

    div_yield_fmp_q = safe_get_float(latest_km_q_fmp, "dividendYieldTTM")
    div_yield_fmp_a = safe_get_float(latest_km_a_fmp, "dividendYield")
    div_yield_fh_raw = safe_get_float(basic_fin_fh_metric, "dividendYieldAnnual")
    div_yield_fh = div_yield_fh_raw / 100.0 if div_yield_fh_raw is not None else None
    div_yield_av = safe_get_float(overview_av, "DividendYield")

    ratios["dividend_yield"] = div_yield_fmp_q if div_yield_fmp_q is not None else \
        (div_yield_fmp_a if div_yield_fmp_a is not None else \
             (div_yield_fh if div_yield_fh is not None else div_yield_av))
    return ratios


def _calculate_profitability_metrics(analyzer_instance, income_annual_fmp, balance_annual_fmp, latest_km_a_fmp,
                                     overview_av):
    metrics = {}
    ticker = analyzer_instance.ticker

    # From FMP Annual Income Statement
    latest_ia_fmp = income_annual_fmp[0] if income_annual_fmp else {}

    metrics["eps"] = safe_get_float(latest_ia_fmp, "eps") or \
                     safe_get_float(latest_km_a_fmp, "eps") or \
                     safe_get_float(overview_av, "EPS")

    metrics["net_profit_margin"] = safe_get_float(latest_ia_fmp, "netProfitMargin") or \
                                   safe_get_float(overview_av, "ProfitMargin")

    # Gross Profit Margin: FMP or (AV GrossProfitTTM / AV RevenueTTM)
    fmp_gross_margin = safe_get_float(latest_ia_fmp, "grossProfitMargin")
    if fmp_gross_margin is not None:
        metrics["gross_profit_margin"] = fmp_gross_margin
    else:
        av_gross_profit_ttm = safe_get_float(overview_av, "GrossProfitTTM")
        av_revenue_ttm = safe_get_float(overview_av, "RevenueTTM")
        if av_gross_profit_ttm is not None and av_revenue_ttm is not None and av_revenue_ttm != 0:
            metrics["gross_profit_margin"] = av_gross_profit_ttm / av_revenue_ttm
        else:
            metrics["gross_profit_margin"] = None

    metrics["operating_profit_margin"] = safe_get_float(latest_ia_fmp,
                                                        "operatingIncomeRatio")  # FMP specific for op margin
    # AlphaVantage overview_av also has "OperatingMarginTTM"
    if metrics["operating_profit_margin"] is None:
        metrics["operating_profit_margin"] = safe_get_float(overview_av, "OperatingMarginTTM")

    ebit_fmp = safe_get_float(latest_ia_fmp, "operatingIncome")
    interest_expense_fmp = safe_get_float(latest_ia_fmp, "interestExpense")
    if ebit_fmp is not None and interest_expense_fmp is not None and abs(interest_expense_fmp) > 1e-6:
        metrics["interest_coverage_ratio"] = ebit_fmp / abs(interest_expense_fmp)
    else:
        metrics["interest_coverage_ratio"] = None

    # ROE, ROA from various sources
    # Priority: FMP calculations > AlphaVantage direct > Finnhub direct
    # FMP calculation parts:
    total_equity_fmp = get_value_from_statement_list(balance_annual_fmp, "totalStockholdersEquity", 0)
    total_assets_fmp = get_value_from_statement_list(balance_annual_fmp, "totalAssets", 0)
    latest_net_income_fmp = get_value_from_statement_list(income_annual_fmp, "netIncome", 0)

    roe_fmp_calc = None
    if total_equity_fmp and total_equity_fmp != 0 and latest_net_income_fmp is not None:
        roe_fmp_calc = latest_net_income_fmp / total_equity_fmp

    roa_fmp_calc = None
    if total_assets_fmp and total_assets_fmp != 0 and latest_net_income_fmp is not None:
        roa_fmp_calc = latest_net_income_fmp / total_assets_fmp

    metrics["roe"] = roe_fmp_calc if roe_fmp_calc is not None else safe_get_float(overview_av, "ReturnOnEquityTTM")
    metrics["roa"] = roa_fmp_calc if roa_fmp_calc is not None else safe_get_float(overview_av, "ReturnOnAssetsTTM")

    # ROIC Calculation (Primarily FMP based due to detail needed)
    ebit_roic_fmp = get_value_from_statement_list(income_annual_fmp, "operatingIncome", 0)
    income_tax_expense_roic_fmp = get_value_from_statement_list(income_annual_fmp, "incomeTaxExpense", 0)
    income_before_tax_roic_fmp = get_value_from_statement_list(income_annual_fmp, "incomeBeforeTax", 0)

    effective_tax_rate = 0.21  # Default
    if income_tax_expense_roic_fmp is not None and income_before_tax_roic_fmp is not None and income_before_tax_roic_fmp != 0:
        calculated_tax_rate = income_tax_expense_roic_fmp / income_before_tax_roic_fmp
        if 0 <= calculated_tax_rate <= 0.50:
            effective_tax_rate = calculated_tax_rate
        else:
            logger.debug(
                f"Calculated tax rate {calculated_tax_rate:.2%} for {ticker} is unusual. Using default {effective_tax_rate:.2%}.")

    nopat_fmp = ebit_roic_fmp * (1 - effective_tax_rate) if ebit_roic_fmp is not None else None

    total_debt_roic_fmp = get_value_from_statement_list(balance_annual_fmp, "totalDebt", 0)
    cash_equivalents_roic_fmp = get_value_from_statement_list(balance_annual_fmp, "cashAndCashEquivalents", 0) or 0

    if total_debt_roic_fmp is not None and total_equity_fmp is not None:  # total_equity_fmp defined above
        invested_capital_fmp = total_debt_roic_fmp + total_equity_fmp - cash_equivalents_roic_fmp
        if nopat_fmp is not None and invested_capital_fmp is not None and invested_capital_fmp != 0:
            metrics["roic"] = nopat_fmp / invested_capital_fmp
        else:
            metrics["roic"] = None  # FMP ROIC calc failed
    else:
        metrics["roic"] = None  # FMP ROIC calc failed

    # AlphaVantage overview_av does not have ROIC directly.
    # Finnhub basic_financials might have roicAnnual, roicTTM under 'metric'
    if metrics["roic"] is None:
        fh_metrics = analyzer_instance._financial_data_cache.get('basic_financials_finnhub', {}).get('metric', {})
        metrics["roic"] = safe_get_float(fh_metrics, "roicTTM") or safe_get_float(fh_metrics, "roicAnnual")

    return metrics


def _calculate_financial_health_metrics(balance_annual_fmp, income_annual_fmp, latest_km_a_fmp, overview_av):
    metrics = {}
    latest_ba_fmp = balance_annual_fmp[0] if balance_annual_fmp else {}
    total_equity_fmp = safe_get_float(latest_ba_fmp, "totalStockholdersEquity")

    # Debt-to-Equity: FMP Key Metric > FMP Balance Sheet Calc > AlphaVantage Overview
    metrics["debt_to_equity"] = safe_get_float(latest_km_a_fmp, "debtToEquity")
    if metrics["debt_to_equity"] is None:
        total_debt_ba_fmp = safe_get_float(latest_ba_fmp, "totalDebt")
        if total_debt_ba_fmp is not None and total_equity_fmp and total_equity_fmp != 0:
            metrics["debt_to_equity"] = total_debt_ba_fmp / total_equity_fmp
    if metrics["debt_to_equity"] is None:
        # AlphaVantage has total debt and total equity in quarterly balance sheets, not directly in overview.
        # And DebtToEquityRatio is usually a TTM or annual metric. For now, stick to FMP.
        pass

    current_assets_fmp = safe_get_float(latest_ba_fmp, "totalCurrentAssets")
    current_liabilities_fmp = safe_get_float(latest_ba_fmp, "totalCurrentLiabilities")
    if current_assets_fmp is not None and current_liabilities_fmp is not None and current_liabilities_fmp != 0:
        metrics["current_ratio"] = current_assets_fmp / current_liabilities_fmp
    else:  # Fallback to AlphaVantage if FMP fails
        metrics["current_ratio"] = safe_get_float(overview_av, "CurrentRatio")

    cash_equivalents_fmp = safe_get_float(latest_ba_fmp, "cashAndCashEquivalents", 0)
    short_term_investments_fmp = safe_get_float(latest_ba_fmp, "shortTermInvestments", 0)
    net_receivables_fmp = safe_get_float(latest_ba_fmp, "netReceivables", 0)
    if current_liabilities_fmp is not None and current_liabilities_fmp != 0:  # Requires FMP current_liabilities
        metrics["quick_ratio"] = (
                                             cash_equivalents_fmp + short_term_investments_fmp + net_receivables_fmp) / current_liabilities_fmp
    else:  # Fallback to AlphaVantage if FMP fails
        # AlphaVantage overview_av does not directly provide quick ratio components in a simple way.
        # It might be in the full balance sheet. For now, if FMP fails, quick_ratio might be None.
        metrics["quick_ratio"] = None

    # Debt-to-EBITDA
    # Priority: FMP Key Metric > FMP Calc (Total Debt / EBITDA from Income Statement)
    latest_annual_ebitda_km_fmp = safe_get_float(latest_km_a_fmp, "ebitda")
    latest_annual_ebitda_is_fmp = get_value_from_statement_list(income_annual_fmp, "ebitda", 0)
    latest_annual_ebitda_fmp = latest_annual_ebitda_km_fmp if latest_annual_ebitda_km_fmp is not None else latest_annual_ebitda_is_fmp

    if latest_annual_ebitda_fmp and latest_annual_ebitda_fmp != 0:
        total_debt_val_fmp = get_value_from_statement_list(balance_annual_fmp, "totalDebt", 0)
        if total_debt_val_fmp is not None:
            metrics["debt_to_ebitda"] = total_debt_val_fmp / latest_annual_ebitda_fmp
        else:
            metrics["debt_to_ebitda"] = None
    else:
        metrics["debt_to_ebitda"] = None

    # AlphaVantage overview_av doesn't have DebtToEBITDA.

    return metrics


def _get_cross_validated_quarterly_revenue(analyzer_instance, statements_cache):
    ticker = analyzer_instance.ticker
    latest_q_revenue, previous_q_revenue, source_name, historical_revenues = None, None, None, []

    # Define revenue field names for each source
    revenue_fields = {
        "fmp_quarterly": "revenue",
        "alphavantage_quarterly": "totalRevenue",
        "finnhub_quarterly": ["Revenues", "RevenueFromContractWithCustomerExcludingAssessedTax", "TotalRevenues",
                              "NetSales"]  # List of possible concepts
    }

    for src_key in PRIORITY_REVENUE_SOURCES:
        try:
            if src_key == "fmp_quarterly" and statements_cache.get('fmp_income_quarterly'):
                reports = statements_cache['fmp_income_quarterly']
                if not reports: continue
                latest_val = get_fmp_value(reports, revenue_fields[src_key], 0)
                prev_val = get_fmp_value(reports, revenue_fields[src_key], 1) if len(reports) > 1 else None
                if latest_val is not None:
                    latest_q_revenue, previous_q_revenue, source_name = latest_val, prev_val, "FMP"
                    for i in range(min(len(reports), 5)):  # Get up to 5 historical points
                        h_val = get_fmp_value(reports, revenue_fields[src_key], i)
                        if h_val is not None: historical_revenues.append(h_val)
                    break
            elif src_key == "alphavantage_quarterly" and statements_cache.get('alphavantage_income_quarterly', {}).get(
                    'quarterlyReports'):
                reports = statements_cache['alphavantage_income_quarterly']['quarterlyReports']
                if not reports: continue
                latest_val = get_alphavantage_value(reports, revenue_fields[src_key], 0)
                prev_val = get_alphavantage_value(reports, revenue_fields[src_key], 1) if len(reports) > 1 else None
                if latest_val is not None:
                    latest_q_revenue, previous_q_revenue, source_name = latest_val, prev_val, "AlphaVantage"
                    for i in range(min(len(reports), 5)):
                        h_val = get_alphavantage_value(reports, revenue_fields[src_key], i)
                        if h_val is not None: historical_revenues.append(h_val)
                    break
            elif src_key == "finnhub_quarterly" and statements_cache.get('finnhub_financials_quarterly_reported',
                                                                         {}).get('data'):
                reports = statements_cache['finnhub_financials_quarterly_reported']['data']
                if not reports: continue
                latest_val = get_finnhub_concept_value(reports, 'ic', revenue_fields[src_key], 0)
                prev_val = get_finnhub_concept_value(reports, 'ic', revenue_fields[src_key], 1) if len(
                    reports) > 1 else None
                if latest_val is not None:
                    latest_q_revenue, previous_q_revenue, source_name = latest_val, prev_val, "Finnhub"
                    for i in range(min(len(reports), 5)):
                        h_val = get_finnhub_concept_value(reports, 'ic', revenue_fields[src_key], i)
                        if h_val is not None: historical_revenues.append(h_val)
                    break
        except Exception as e:
            logger.warning(f"Error processing quarterly revenue from {src_key} for {ticker}: {e}")
            continue

    avg_historical_q_revenue = None
    if historical_revenues:
        points_for_avg = [r for r in historical_revenues if r is not None and r > 0]  # Use only positive values
        # If latest_q_revenue is the first in historical_revenues, exclude it for avg calculation to compare against prior periods
        avg_base_points = points_for_avg[1:] if points_for_avg and points_for_avg[0] == latest_q_revenue and len(
            points_for_avg) > 1 else points_for_avg

        if len(avg_base_points) > 1:  # Need at least two points for a meaningful average
            avg_historical_q_revenue = sum(avg_base_points) / len(avg_base_points)
            if latest_q_revenue is not None and avg_historical_q_revenue > 0:  # Ensure avg is positive for deviation calc
                deviation = abs(latest_q_revenue - avg_historical_q_revenue) / avg_historical_q_revenue
                if deviation > Q_REVENUE_SANITY_CHECK_DEVIATION_THRESHOLD:
                    warning_msg = (
                        f"DATA QUALITY WARNING: Latest quarterly revenue ({latest_q_revenue:,.0f} from {source_name}) "
                        f"deviates by {deviation:.2%} from avg of recent historical quarters ({avg_historical_q_revenue:,.0f}). "
                        f"Review data accuracy.")
                    logger.warning(warning_msg)
                    analyzer_instance.data_quality_warnings.append(warning_msg)
        else:
            logger.info(
                f"Not enough historical quarterly revenue data (after filtering for positive values and excluding current if present) to perform sanity check for {ticker}.")
    else:
        logger.info(f"No historical quarterly revenue data found for sanity check for {ticker}.")

    if latest_q_revenue is None:
        logger.error(f"Could not determine latest quarterly revenue for {ticker} from any source.")
        analyzer_instance.data_quality_warnings.append("CRITICAL: Latest quarterly revenue could not be determined.")
    else:
        logger.info(f"Using latest quarterly revenue: {latest_q_revenue:,.0f} (Source: {source_name}) for {ticker}.")

    return latest_q_revenue, previous_q_revenue, source_name, avg_historical_q_revenue


def _calculate_growth_metrics(analyzer_instance, income_annual_fmp, statements_cache, overview_av):
    metrics = {"key_metrics_snapshot": {}}  # Initialize snapshot dict
    ticker = analyzer_instance.ticker

    # YoY Growth
    # FMP Annual Revenue
    fmp_revenue_y0 = get_value_from_statement_list(income_annual_fmp, "revenue", 0)
    fmp_revenue_y1 = get_value_from_statement_list(income_annual_fmp, "revenue", 1)

    # FMP Annual EPS
    fmp_eps_y0 = get_value_from_statement_list(income_annual_fmp, "eps", 0)
    fmp_eps_y1 = get_value_from_statement_list(income_annual_fmp, "eps", 1)

    metrics["revenue_growth_yoy"] = calculate_growth(fmp_revenue_y0, fmp_revenue_y1)
    metrics["eps_growth_yoy"] = calculate_growth(fmp_eps_y0, fmp_eps_y1)

    # AlphaVantage has "QuarterlyRevenueGrowthYOY", "QuarterlyEarningsGrowthYOY"
    # These are quarterly YoY. We are calculating annual YoY above.
    # We can add AV's TTM RevenueGrowth and EPSGrowth if available as fallbacks for YoY.
    # Overview_av has "RevenueGrowth" but it's usually for TTM or MRQ.
    # Let's stick to FMP for annual YoY growth for now due to clarity of period.

    # CAGR 3-year
    if len(income_annual_fmp) >= 3:
        metrics["revenue_growth_cagr_3yr"] = calculate_cagr(
            get_value_from_statement_list(income_annual_fmp, "revenue", 0),
            get_value_from_statement_list(income_annual_fmp, "revenue", 2), 2
        )
        metrics["eps_growth_cagr_3yr"] = calculate_cagr(
            get_value_from_statement_list(income_annual_fmp, "eps", 0),
            get_value_from_statement_list(income_annual_fmp, "eps", 2), 2
        )
    else:
        metrics["revenue_growth_cagr_3yr"] = None
        metrics["eps_growth_cagr_3yr"] = None

    # CAGR 5-year
    if len(income_annual_fmp) >= 5:
        metrics["revenue_growth_cagr_5yr"] = calculate_cagr(
            get_value_from_statement_list(income_annual_fmp, "revenue", 0),
            get_value_from_statement_list(income_annual_fmp, "revenue", 4), 4
        )
        metrics["eps_growth_cagr_5yr"] = calculate_cagr(
            get_value_from_statement_list(income_annual_fmp, "eps", 0),
            get_value_from_statement_list(income_annual_fmp, "eps", 4), 4
        )
    else:
        metrics["revenue_growth_cagr_5yr"] = None
        metrics["eps_growth_cagr_5yr"] = None

    # QoQ Revenue Growth
    latest_q_rev, prev_q_rev, rev_src_name, avg_hist_q_rev = _get_cross_validated_quarterly_revenue(analyzer_instance,
                                                                                                    statements_cache)

    if latest_q_rev is not None:
        metrics["key_metrics_snapshot"]["q_revenue_source"] = rev_src_name
        metrics["key_metrics_snapshot"]["latest_q_revenue"] = latest_q_rev
        metrics["key_metrics_snapshot"]["avg_historical_q_revenue_for_check"] = avg_hist_q_rev
        if prev_q_rev is not None:
            metrics["revenue_growth_qoq"] = calculate_growth(latest_q_rev, prev_q_rev)
        else:
            logger.info(
                f"Previous quarter revenue not available from source {rev_src_name} for {ticker}. Cannot calculate QoQ revenue growth.")
            metrics["revenue_growth_qoq"] = None
    else:  # Fallback to AlphaVantage QuarterlyRevenueGrowthYOY as a proxy if direct QoQ fails
        metrics["revenue_growth_qoq"] = safe_get_float(overview_av,
                                                       "QuarterlyRevenueGrowthYOY")  # Note: This is YOY not QOQ.
        metrics["key_metrics_snapshot"]["q_revenue_source"] = "N/A (or AV YOY as proxy)" if metrics[
                                                                                                "revenue_growth_qoq"] is None else "AlphaVantage (QuarterlyYoY as QoQ proxy)"
        metrics["key_metrics_snapshot"]["latest_q_revenue"] = None  # Can't determine specific latest Q revenue
        metrics["key_metrics_snapshot"]["avg_historical_q_revenue_for_check"] = None

    return metrics


def _calculate_cash_flow_and_trend_metrics(cashflow_annual_fmp, balance_annual_fmp, profile_fmp, overview_av):
    metrics = {}

    # FCF per Share & FCF Yield
    fcf_latest_annual_fmp = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 0)

    shares_outstanding_profile_fmp = safe_get_float(profile_fmp, "sharesOutstanding")
    mkt_cap_profile_fmp = safe_get_float(profile_fmp, "mktCap")
    price_profile_fmp = safe_get_float(profile_fmp, "price")

    # Use AlphaVantage SharesOutstanding if FMP's is missing
    shares_outstanding_av = safe_get_float(overview_av, "SharesOutstanding")
    shares_outstanding = shares_outstanding_profile_fmp if shares_outstanding_profile_fmp is not None and shares_outstanding_profile_fmp > 0 else shares_outstanding_av

    # Calculate shares outstanding if direct value is missing or zero, using mktCap and price
    if (
            shares_outstanding is None or shares_outstanding == 0) and mkt_cap_profile_fmp and price_profile_fmp and price_profile_fmp != 0:
        shares_outstanding = mkt_cap_profile_fmp / price_profile_fmp

    if fcf_latest_annual_fmp is not None and shares_outstanding and shares_outstanding != 0:
        metrics["free_cash_flow_per_share"] = fcf_latest_annual_fmp / shares_outstanding
        # Use MktCap from FMP profile first, then AV overview for FCF Yield
        mkt_cap_for_yield = mkt_cap_profile_fmp if mkt_cap_profile_fmp else safe_get_float(overview_av,
                                                                                           "MarketCapitalization")
        if mkt_cap_for_yield and mkt_cap_for_yield != 0:
            metrics["free_cash_flow_yield"] = fcf_latest_annual_fmp / mkt_cap_for_yield
        else:
            metrics["free_cash_flow_yield"] = None
    else:
        metrics["free_cash_flow_per_share"] = None
        metrics["free_cash_flow_yield"] = None

    # FCF Trend (3-year simple trend from FMP annual data)
    if len(cashflow_annual_fmp) >= 3:
        fcf0 = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 0)
        fcf1 = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 1)
        fcf2 = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 2)

        if all(isinstance(x, (int, float)) for x in [fcf0, fcf1, fcf2] if x is not None) and \
                all(x is not None for x in [fcf0, fcf1, fcf2]):
            if fcf0 > fcf1 > fcf2:
                metrics["free_cash_flow_trend"] = "Growing"
            elif fcf0 < fcf1 < fcf2:
                metrics["free_cash_flow_trend"] = "Declining"
            elif fcf0 > fcf1 and fcf1 < fcf2:
                metrics["free_cash_flow_trend"] = "Volatile (Dip then Rise)"
            elif fcf0 < fcf1 and fcf1 > fcf2:
                metrics["free_cash_flow_trend"] = "Volatile (Rise then Dip)"
            else:
                metrics["free_cash_flow_trend"] = "Mixed/Stable"
        else:
            metrics["free_cash_flow_trend"] = "Data Incomplete/Non-Numeric"
    else:
        metrics["free_cash_flow_trend"] = "Data N/A (<3 yrs)"

    # Retained Earnings Trend (3-year simple trend from FMP annual data)
    if len(balance_annual_fmp) >= 3:
        re0 = get_value_from_statement_list(balance_annual_fmp, "retainedEarnings", 0)
        re1 = get_value_from_statement_list(balance_annual_fmp, "retainedEarnings", 1)
        re2 = get_value_from_statement_list(balance_annual_fmp, "retainedEarnings", 2)

        if all(isinstance(x, (int, float)) for x in [re0, re1, re2] if x is not None) and \
                all(x is not None for x in [re0, re1, re2]):
            if re0 > re1 > re2:
                metrics["retained_earnings_trend"] = "Growing"
            elif re0 < re1 < re2:
                metrics["retained_earnings_trend"] = "Declining"
            else:
                metrics["retained_earnings_trend"] = "Mixed/Stable"
        else:
            metrics["retained_earnings_trend"] = "Data Incomplete/Non-Numeric"
    else:
        metrics["retained_earnings_trend"] = "Data N/A (<3 yrs)"

    return metrics


def calculate_all_derived_metrics(analyzer_instance):
    logger.info(f"Calculating derived metrics for {analyzer_instance.ticker}...")
    all_metrics_temp = {}

    # Retrieve cached data
    statements = analyzer_instance._financial_data_cache.get('financial_statements', {})
    income_annual_fmp = statements.get('fmp_income_annual', [])
    balance_annual_fmp = statements.get('fmp_balance_annual', [])
    cashflow_annual_fmp = statements.get('fmp_cashflow_annual', [])

    key_metrics_annual_fmp = analyzer_instance._financial_data_cache.get('key_metrics_annual_fmp', [])
    key_metrics_quarterly_fmp = analyzer_instance._financial_data_cache.get('key_metrics_quarterly_fmp', [])

    basic_fin_fh_metric = analyzer_instance._financial_data_cache.get('basic_financials_finnhub', {}).get('metric', {})
    profile_fmp = analyzer_instance._financial_data_cache.get('profile_fmp', {})
    # Ensure AlphaVantage overview is available
    overview_av = analyzer_instance._financial_data_cache.get('overview_alphavantage', {})
    if not overview_av:  # If somehow not fetched by stock_analyzer's init
        logger.warning(f"AlphaVantage overview data not found in cache for {analyzer_instance.ticker}. Fetching now.")
        overview_av = analyzer_instance.alphavantage.get_company_overview(analyzer_instance.ticker)
        analyzer_instance._financial_data_cache['overview_alphavantage'] = overview_av if overview_av else {}

    latest_km_q_fmp = key_metrics_quarterly_fmp[0] if key_metrics_quarterly_fmp else {}
    latest_km_a_fmp = key_metrics_annual_fmp[0] if key_metrics_annual_fmp else {}

    all_metrics_temp.update(
        _calculate_valuation_ratios(latest_km_q_fmp, latest_km_a_fmp, basic_fin_fh_metric, overview_av))
    all_metrics_temp.update(
        _calculate_profitability_metrics(analyzer_instance, income_annual_fmp, balance_annual_fmp, latest_km_a_fmp,
                                         overview_av))
    all_metrics_temp.update(
        _calculate_financial_health_metrics(balance_annual_fmp, income_annual_fmp, latest_km_a_fmp, overview_av))

    growth_metrics_result = _calculate_growth_metrics(analyzer_instance, income_annual_fmp, statements, overview_av)
    all_metrics_temp.update(growth_metrics_result)

    all_metrics_temp.update(
        _calculate_cash_flow_and_trend_metrics(cashflow_annual_fmp, balance_annual_fmp, profile_fmp, overview_av))

    final_metrics_cleaned = {}
    key_metrics_snapshot_data = all_metrics_temp.pop("key_metrics_snapshot", {})

    for k, v in all_metrics_temp.items():
        if isinstance(v, float):
            final_metrics_cleaned[k] = v if not (math.isnan(v) or math.isinf(v)) else None
        elif v is not None:
            final_metrics_cleaned[k] = v
        else:
            final_metrics_cleaned[k] = None

    final_metrics_cleaned["key_metrics_snapshot"] = {
        sk: sv for sk, sv in key_metrics_snapshot_data.items()
        if sv is not None and not (isinstance(sv, float) and (math.isnan(sv) or math.isinf(sv)))
    }

    log_metrics_display = {k: v for k, v in final_metrics_cleaned.items() if k != "key_metrics_snapshot"}
    logger.info(
        f"Calculated metrics for {analyzer_instance.ticker}: {json.dumps(log_metrics_display, indent=2, default=str)}")
    if final_metrics_cleaned["key_metrics_snapshot"]:
        logger.info(
            f"Key metrics snapshot for {analyzer_instance.ticker}: {json.dumps(final_metrics_cleaned['key_metrics_snapshot'], indent=2, default=str)}")

    analyzer_instance._financial_data_cache['calculated_metrics'] = final_metrics_cleaned
    return final_metrics_cleaned
---------- END metrics_calculator.py ----------


---------- qualitative_analyzer.py ----------
# services/stock_analyzer/qualitative_analyzer.py
import time
from core.logging_setup import logger
from api_clients import extract_S1_text_sections  # Re-check if S1 or TEN_K version
from core.config import (
    TEN_K_KEY_SECTIONS, SUMMARIZATION_CHUNK_SIZE_CHARS,
    SUMMARIZATION_CHUNK_OVERLAP_CHARS, SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS,
    MAX_COMPETITORS_TO_ANALYZE
)
from .helpers import safe_get_float


def _summarize_text_chunked(analyzer_instance, text_to_summarize, base_context, section_specific_instruction,
                            company_name_ticker_prompt):
    gemini_client = analyzer_instance.gemini
    if not text_to_summarize or not isinstance(text_to_summarize, str) or not text_to_summarize.strip():
        return "No text provided for summarization.", 0

    text_len = len(text_to_summarize)
    logger.info(f"Summarizing '{base_context}' for {company_name_ticker_prompt}, original length: {text_len} chars.")

    if text_len < SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS:  # If short enough, summarize directly
        logger.info(
            f"Section length {text_len} is within single-pass limit ({SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS}). Summarizing directly.")
        summary = gemini_client.summarize_text_with_context(
            text_to_summarize,
            f"{base_context} for {company_name_ticker_prompt}.",
            section_specific_instruction
        )
        time.sleep(2)  # API call delay
        return (summary if summary and not summary.startswith(
            "Error:") else f"AI summary error or no content for '{base_context}'."), text_len

    # Chunked summarization
    logger.info(f"Section length {text_len} exceeds single-pass limit. Applying chunked summarization "
                f"(Chunk size: {SUMMARIZATION_CHUNK_SIZE_CHARS}, Overlap: {SUMMARIZATION_CHUNK_OVERLAP_CHARS}).")
    chunks = []
    start = 0
    while start < text_len:
        end = start + SUMMARIZATION_CHUNK_SIZE_CHARS
        chunks.append(text_to_summarize[start:end])
        if end >= text_len or SUMMARIZATION_CHUNK_OVERLAP_CHARS >= SUMMARIZATION_CHUNK_SIZE_CHARS:  # Prevent negative start
            start = end
        else:
            start = end - SUMMARIZATION_CHUNK_OVERLAP_CHARS

    chunk_summaries = []
    for i, chunk in enumerate(chunks):
        logger.info(
            f"Summarizing chunk {i + 1}/{len(chunks)} for '{base_context}' of {company_name_ticker_prompt} (length: {len(chunk)} chars).")
        summary = gemini_client.summarize_text_with_context(
            chunk,
            f"This is chunk {i + 1} of {len(chunks)} from the '{base_context}' section for {company_name_ticker_prompt}.",
            f"Summarize this chunk. Focus on key facts and figures relevant to: {section_specific_instruction}"
        )
        time.sleep(2)  # API call delay
        chunk_summaries.append(summary if summary and not summary.startswith(
            "Error:") else f"[AI error or no content for chunk {i + 1} of '{base_context}']")

    if not chunk_summaries:
        return f"No summaries generated from chunks for '{base_context}'.", text_len

    concatenated_summaries = "\n\n---\n\n".join(chunk_summaries)
    logger.info(f"Concatenated chunk summaries length for '{base_context}': {len(concatenated_summaries)} chars.")

    if not concatenated_summaries.strip() or all("[AI error" in s for s in chunk_summaries):
        return f"Failed to generate summaries for any chunk of '{base_context}'.", text_len

    # If concatenated summaries are too long, summarize the summaries
    if len(concatenated_summaries) > SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS:
        logger.info(
            f"Concatenated summaries for '{base_context}' too long. Performing a final 'summary of summaries' pass.")
        final_summary = gemini_client.summarize_text_with_context(
            concatenated_summaries,
            f"The following are collated summaries from different parts of the '{base_context}' section for {company_name_ticker_prompt}.",
            f"Synthesize these individual chunk summaries into a single, cohesive overview of the '{base_context}', "
            f"maintaining factual accuracy and addressing the original goal: {section_specific_instruction}."
        )
        time.sleep(2)  # API call delay
        return (final_summary if final_summary and not final_summary.startswith(
            "Error:") else f"AI error in final summary pass for '{base_context}'."), text_len
    else:
        return concatenated_summaries, text_len


def fetch_and_summarize_10k_data(analyzer_instance):
    ticker = analyzer_instance.ticker
    logger.info(f"Fetching and attempting to summarize latest 10-K for {ticker}")
    summary_results = {"qualitative_sources_summary": {}}  # Initialize the sub-dictionary

    if not analyzer_instance.stock_db_entry or not analyzer_instance.stock_db_entry.cik:
        logger.warning(f"No CIK for {ticker}. Cannot fetch 10-K.")
        return summary_results

    # Try 10-K first, then 10-K/A
    filing_url = analyzer_instance.sec_edgar.get_filing_document_url(analyzer_instance.stock_db_entry.cik, "10-K")
    time.sleep(0.5)
    if not filing_url:
        logger.info(f"No recent 10-K found for {ticker}, trying 10-K/A.")
        filing_url = analyzer_instance.sec_edgar.get_filing_document_url(analyzer_instance.stock_db_entry.cik, "10-K/A")
        time.sleep(0.5)

    if not filing_url:
        logger.warning(f"No 10-K or 10-K/A URL found for {ticker} (CIK: {analyzer_instance.stock_db_entry.cik})")
        return summary_results

    summary_results["qualitative_sources_summary"]["10k_filing_url_used"] = filing_url
    text_content = analyzer_instance.sec_edgar.get_filing_text(filing_url)

    if not text_content:
        logger.warning(f"Failed to fetch/load 10-K text from {filing_url}")
        return summary_results

    logger.info(f"Fetched 10-K text (length: {len(text_content)}) for {ticker}. Extracting and summarizing sections.")
    sections = extract_S1_text_sections(text_content,
                                        TEN_K_KEY_SECTIONS)  # TEN_K_KEY_SECTIONS is alias for S1_KEY_SECTIONS

    company_name_for_prompt = analyzer_instance.stock_db_entry.company_name or ticker

    section_details = {
        "business": ("Business (Item 1)",
                     "Summarize the company's core business operations, primary products/services, revenue generation model, key customer segments, and primary markets. Highlight any recent strategic shifts mentioned."),
        "risk_factors": ("Risk Factors (Item 1A)",
                         "Identify and summarize the 3-5 most significant and company-specific risk factors disclosed. Focus on operational and strategic risks rather than generic market risks. Briefly explain the potential impact of each."),
        "mda": ("Management's Discussion and Analysis (Item 7)",
                "Summarize key insights into financial performance drivers (revenue, costs, profitability), financial condition (liquidity, capital resources), and management's outlook or significant focus areas. Note any discussion on margin pressures or segment performance changes.")
    }

    for section_key, (prompt_section_name, specific_instruction) in section_details.items():
        section_text = sections.get(section_key)
        if not section_text:
            logger.warning(f"Section '{prompt_section_name}' not found in 10-K for {ticker}.")
            summary_results[f"{section_key}_summary"] = "Section not found in 10-K document."
            summary_results["qualitative_sources_summary"][f"{section_key}_10k_source_length"] = 0
            continue

        summary, source_len = _summarize_text_chunked(analyzer_instance, section_text, prompt_section_name,
                                                      specific_instruction, f"{company_name_for_prompt} ({ticker})")
        summary_results[f"{section_key}_summary"] = summary
        summary_results["qualitative_sources_summary"][f"{section_key}_10k_source_length"] = source_len
        logger.info(
            f"Summary for '{prompt_section_name}' (source length {source_len}): {summary[:150].replace(chr(10), ' ')}...")

    # Economic Moat Analysis (derived from business and risk summaries)
    biz_summary_str = summary_results.get("business_summary", "")
    mda_summary_str = summary_results.get("mda_summary", "")  # For industry trends
    risk_summary_str = summary_results.get("risk_factors_summary", "")

    # Clean up potentially errored summaries for concatenation
    if biz_summary_str.startswith(("Section not found", "AI summary error")): biz_summary_str = ""
    if mda_summary_str.startswith(("Section not found", "AI summary error")): mda_summary_str = ""
    if risk_summary_str.startswith(("Section not found", "AI summary error")): risk_summary_str = ""

    moat_input_text = (f"Business Summary:\n{biz_summary_str}\n\nRisk Factors Summary:\n{risk_summary_str}").strip()
    if moat_input_text and (biz_summary_str or risk_summary_str):  # Check if there's meaningful input
        moat_prompt = (
            f"Analyze the primary economic moats (e.g., brand strength, network effects, switching costs, intangible assets like patents/IP, cost advantages from scale/process) "
            f"for {company_name_for_prompt} ({ticker}), based on the following summaries from its 10-K:\n\n{moat_input_text}\n\n"
            f"Provide a concise analysis of its key economic moats. For each identified moat, briefly explain the evidence from the text and assess its perceived strength (e.g., Very Strong, Strong, Moderate, Weak). If certain moats are not strongly evident, state that."
        )
        moat_summary = analyzer_instance.gemini.generate_text(moat_prompt)
        time.sleep(3)  # API call delay
        summary_results["economic_moat_summary"] = moat_summary if moat_summary and not moat_summary.startswith(
            "Error:") else "AI analysis for economic moat failed or no input."
    else:
        summary_results["economic_moat_summary"] = "Insufficient input from 10-K summaries for economic moat analysis."

    # Industry Trends Analysis
    industry_context_text = (
        f"Company: {company_name_for_prompt} ({ticker})\n"
        f"Industry: {analyzer_instance.stock_db_entry.industry or 'Not Specified'}\n"
        f"Sector: {analyzer_instance.stock_db_entry.sector or 'Not Specified'}\n\n"
        f"Business Summary (from 10-K):\n{biz_summary_str}\n\n"
        f"MD&A Highlights (from 10-K):\n{mda_summary_str}"
    ).strip()

    if biz_summary_str:  # Requires business summary at least
        industry_prompt = (
            f"Based on the provided information for {company_name_for_prompt} ({ticker}):\n\n{industry_context_text}\n\n"
            f"Analyze key industry trends relevant to this company. Discuss significant opportunities and challenges within this industry context. "
            f"How does the company appear to be positioned to capitalize on opportunities and mitigate challenges, based on its business summary and MD&A highlights? Be specific and use information from the text."
        )
        industry_summary = analyzer_instance.gemini.generate_text(industry_prompt)
        time.sleep(3)  # API call delay
        summary_results[
            "industry_trends_summary"] = industry_summary if industry_summary and not industry_summary.startswith(
            "Error:") else "AI analysis for industry trends failed or no input."
    else:
        summary_results[
            "industry_trends_summary"] = "Insufficient input from 10-K (Business Summary missing) for industry analysis."

    # Rename mda_summary to management_assessment_summary for consistency with DB model
    if "mda_summary" in summary_results:
        summary_results["management_assessment_summary"] = summary_results.pop("mda_summary")
        if "mda_10k_source_length" in summary_results["qualitative_sources_summary"]:
            summary_results["qualitative_sources_summary"]["management_assessment_10k_source_length"] = summary_results[
                "qualitative_sources_summary"].pop("mda_10k_source_length")

    logger.info(f"10-K qualitative summaries and AI interpretations generated for {ticker}.")
    analyzer_instance._financial_data_cache['10k_summaries'] = summary_results
    return summary_results


def fetch_and_analyze_competitors(analyzer_instance):
    ticker = analyzer_instance.ticker
    logger.info(f"Fetching and analyzing competitor data for {ticker}...")
    competitor_analysis_summary_text = "Competitor analysis not performed or failed."

    peers_data_finnhub = analyzer_instance.finnhub.get_company_peers(ticker)
    time.sleep(1.5)

    if not peers_data_finnhub or not isinstance(peers_data_finnhub, list) or not peers_data_finnhub[0]:
        logger.warning(f"No direct peer data found from Finnhub for {ticker}.")
        analyzer_instance._financial_data_cache['competitor_analysis'] = {
            "summary": "No peer data found from primary source (Finnhub).", "peers_data": []}
        return "No peer data found from primary source (Finnhub)."

    # Finnhub might return a list containing a list of peers, or just a list of peers
    if isinstance(peers_data_finnhub[0], list):
        peers_data_finnhub = peers_data_finnhub[0]

    # Filter out the current ticker and limit number of peers
    peer_tickers = [p for p in peers_data_finnhub if p and p.upper() != ticker.upper()][:MAX_COMPETITORS_TO_ANALYZE]

    if not peer_tickers:
        logger.info(f"No distinct competitor tickers found after filtering for {ticker}.")
        analyzer_instance._financial_data_cache['competitor_analysis'] = {
            "summary": "No distinct competitor tickers identified.", "peers_data": []}
        return "No distinct competitor tickers identified."

    logger.info(f"Identified peers for {ticker}: {peer_tickers}. Fetching basic data for comparison.")
    peer_details_list = []
    for peer_ticker_symbol in peer_tickers:
        try:
            logger.debug(f"Fetching basic data for peer: {peer_ticker_symbol}")
            # FMP Profile for name and market cap
            peer_profile_fmp_list = analyzer_instance.fmp.get_company_profile(peer_ticker_symbol)
            time.sleep(1.5)
            peer_profile_fmp = peer_profile_fmp_list[0] if peer_profile_fmp_list and isinstance(peer_profile_fmp_list,
                                                                                                list) and \
                                                           peer_profile_fmp_list[0] else {}

            # FMP Key Metrics for P/E, P/S
            peer_metrics_fmp_list = analyzer_instance.fmp.get_key_metrics(peer_ticker_symbol, period="annual",
                                                                          limit=1)  # TTM might be better if available
            time.sleep(1.5)
            peer_metrics_fmp = peer_metrics_fmp_list[0] if peer_metrics_fmp_list and isinstance(peer_metrics_fmp_list,
                                                                                                list) and \
                                                           peer_metrics_fmp_list[0] else {}

            peer_fh_basics = {}
            # Fallback to Finnhub if FMP metrics are missing
            if not peer_metrics_fmp.get("peRatio") or not peer_metrics_fmp.get("priceSalesRatio"):
                peer_fh_basics_data = analyzer_instance.finnhub.get_basic_financials(peer_ticker_symbol)
                time.sleep(1.5)
                peer_fh_basics = peer_fh_basics_data.get("metric", {}) if peer_fh_basics_data else {}

            peer_name = peer_profile_fmp.get("companyName", peer_ticker_symbol)
            market_cap = safe_get_float(peer_profile_fmp, "mktCap")
            pe_ratio = safe_get_float(peer_metrics_fmp, "peRatio") or safe_get_float(peer_fh_basics, "peTTM")
            ps_ratio = safe_get_float(peer_metrics_fmp, "priceSalesRatio") or safe_get_float(peer_fh_basics, "psTTM")

            peer_info = {"ticker": peer_ticker_symbol, "name": peer_name, "market_cap": market_cap,
                         "pe_ratio": pe_ratio, "ps_ratio": ps_ratio}
            if peer_name != peer_ticker_symbol or market_cap or pe_ratio or ps_ratio:  # Add if any meaningful data was found
                peer_details_list.append(peer_info)
        except Exception as e:
            logger.warning(f"Error fetching data for peer {peer_ticker_symbol}: {e}", exc_info=True)
        if len(peer_details_list) >= MAX_COMPETITORS_TO_ANALYZE:
            break

    if not peer_details_list:
        competitor_analysis_summary_text = "Could not fetch sufficient data for identified competitors."
        analyzer_instance._financial_data_cache['competitor_analysis'] = {"summary": competitor_analysis_summary_text,
                                                                          "peers_data": []}
        return competitor_analysis_summary_text

    # AI Synthesis of Competitor Data
    company_name_for_prompt = analyzer_instance.stock_db_entry.company_name or ticker
    k_summaries = analyzer_instance._financial_data_cache.get('10k_summaries', {})
    biz_summary_10k = k_summaries.get('business_summary', 'N/A')
    if biz_summary_10k.startswith("Section not found") or biz_summary_10k.startswith("AI summary error"):
        biz_summary_10k = "Business summary from 10-K not available or failed."

    prompt_context = (
        f"Company being analyzed: {company_name_for_prompt} ({ticker}).\n"
        f"Its 10-K Business Summary: {biz_summary_10k}\n\n"
        f"Identified Competitors and their basic data:\n"
    )
    for peer in peer_details_list:
        mc_str = f"{peer['market_cap']:,.0f}" if peer['market_cap'] else "N/A"
        pe_str = f"{peer['pe_ratio']:.2f}" if peer['pe_ratio'] is not None else "N/A"
        ps_str = f"{peer['ps_ratio']:.2f}" if peer['ps_ratio'] is not None else "N/A"
        prompt_context += f"- {peer['name']} ({peer['ticker']}): Market Cap: {mc_str}, P/E: {pe_str}, P/S: {ps_str}\n"

    comp_prompt = (
        f"{prompt_context}\n\n"
        f"Instruction: Based on the business summary of {company_name_for_prompt} and the list of its competitors with their financial metrics, "
        f"provide a concise analysis of the competitive landscape. Discuss {company_name_for_prompt}'s market positioning relative to these competitors. "
        f"Highlight any key differences in scale (market cap) or valuation (P/E, P/S) that stand out. Address the intensity of competition. "
        f"Do not invent information not present. If competitor data is sparse, acknowledge that. This summary should complement, not merely repeat, the 10-K business description."
    )

    comp_summary_ai = analyzer_instance.gemini.generate_text(comp_prompt)
    time.sleep(3)  # API call delay

    if comp_summary_ai and not comp_summary_ai.startswith("Error:"):
        competitor_analysis_summary_text = comp_summary_ai
    else:
        competitor_analysis_summary_text = "AI synthesis of competitor data failed. Basic peer data might be available in snapshot."
        analyzer_instance.data_quality_warnings.append("Competitor analysis AI synthesis failed.")

    analyzer_instance._financial_data_cache['competitor_analysis'] = {
        "summary": competitor_analysis_summary_text,
        "peers_data": peer_details_list
    }
    logger.info(f"Competitor analysis summary generated for {ticker}.")
    return competitor_analysis_summary_text
---------- END qualitative_analyzer.py ----------


---------- stock_analyzer.py ----------
# services/stock_analyzer/stock_analyzer.py
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timezone
import math
import time
import warnings
from bs4 import XMLParsedAsHTMLWarning
import json

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

from api_clients import (
    FinnhubClient, FinancialModelingPrepClient, AlphaVantageClient,
    EODHDClient, GeminiAPIClient, SECEDGARClient
)
from database import SessionLocal, get_db_session, Stock, StockAnalysis
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError

# Import functions from submodules
from .data_fetcher import fetch_financial_statements_data, fetch_key_metrics_and_profile_data
from .metrics_calculator import calculate_all_derived_metrics
from .dcf_analyzer import perform_dcf_analysis
from .qualitative_analyzer import fetch_and_summarize_10k_data, fetch_and_analyze_competitors
from .ai_synthesis import synthesize_investment_thesis


class StockAnalyzer:
    def __init__(self, ticker):
        self.ticker = ticker.upper()
        self.finnhub = FinnhubClient()
        self.fmp = FinancialModelingPrepClient()
        self.alphavantage = AlphaVantageClient()
        self.eodhd = EODHDClient()  # Currently unused but kept for potential future use
        self.gemini = GeminiAPIClient()
        self.sec_edgar = SECEDGARClient()

        self.db_session = next(get_db_session())
        self.stock_db_entry = None
        self._financial_data_cache = {}  # Holds all fetched and calculated data for the analysis run
        self.data_quality_warnings = []  # Collects warnings during data processing

        try:
            self._get_or_create_stock_entry()
        except Exception as e:
            logger.error(f"CRITICAL: Failed during _get_or_create_stock_entry for {self.ticker}: {e}", exc_info=True)
            self._close_session_if_active()  # Ensure session is closed on error
            # Re-raise as a more specific error or allow main handler to catch
            raise RuntimeError(
                f"StockAnalyzer for {self.ticker} could not be initialized due to DB/API issues during stock entry setup.") from e

    def _close_session_if_active(self):
        if self.db_session and self.db_session.is_active:
            try:
                self.db_session.close()
                logger.debug(f"DB session closed for {self.ticker}.")
            except Exception as e_close:
                logger.warning(f"Error closing session for {self.ticker}: {e_close}")

    def _get_or_create_stock_entry(self):
        # Ensure session is active before starting
        if not self.db_session.is_active:
            logger.warning(f"Session for {self.ticker} inactive in _get_or_create. Re-establishing.")
            self._close_session_if_active()  # Close old one just in case
            self.db_session = next(get_db_session())

        self.stock_db_entry = self.db_session.query(Stock).filter_by(ticker=self.ticker).first()

        company_name, industry, sector, cik = None, None, None, None

        # Try FMP first for profile data
        profile_fmp_list = self.fmp.get_company_profile(self.ticker)
        time.sleep(1.5)  # API call delay
        if profile_fmp_list and isinstance(profile_fmp_list, list) and len(profile_fmp_list) > 0 and profile_fmp_list[
            0]:
            data = profile_fmp_list[0]
            self._financial_data_cache['profile_fmp'] = data  # Cache for later use
            company_name = data.get('companyName')
            industry = data.get('industry')
            sector = data.get('sector')
            cik_val = data.get('cik')
            if cik_val: cik = str(cik_val).zfill(10)  # Pad CIK
            logger.info(f"Fetched profile from FMP for {self.ticker}.")
        else:
            logger.warning(f"FMP profile failed or empty for {self.ticker}. Trying Finnhub.")
            profile_fh = self.finnhub.get_company_profile2(self.ticker)
            time.sleep(1.5)
            if profile_fh:
                self._financial_data_cache['profile_finnhub'] = profile_fh
                company_name = profile_fh.get('name')
                industry = profile_fh.get('finnhubIndustry')
                # Finnhub doesn't typically provide sector or CIK directly in profile2
                logger.info(f"Fetched profile from Finnhub for {self.ticker}.")
            else:
                logger.warning(f"Finnhub profile failed for {self.ticker}. Trying Alpha Vantage Overview.")
                overview_av = self.alphavantage.get_company_overview(self.ticker)
                time.sleep(2)  # AV can be slow/rate-limited
                if overview_av and overview_av.get("Symbol") == self.ticker:  # Ensure it's the correct ticker
                    self._financial_data_cache['overview_alphavantage'] = overview_av
                    company_name = overview_av.get('Name')
                    industry = overview_av.get('Industry')
                    sector = overview_av.get('Sector')
                    cik_val = overview_av.get('CIK')
                    if cik_val: cik = str(cik_val).zfill(10)
                    logger.info(f"Fetched overview from Alpha Vantage for {self.ticker}.")
                else:
                    logger.warning(
                        f"All primary profile fetches (FMP, Finnhub, AV) failed or incomplete for {self.ticker}.")

        if not company_name: company_name = self.ticker  # Fallback if no name found

        # If CIK is still missing, try SEC EDGAR map
        if not cik and self.ticker:
            logger.info(f"CIK not found from profiles for {self.ticker}. Querying SEC EDGAR CIK map.")
            cik_from_edgar = self.sec_edgar.get_cik_by_ticker(self.ticker)
            time.sleep(0.5)
            if cik_from_edgar:
                cik = str(cik_from_edgar).zfill(10)
                logger.info(f"Fetched CIK {cik} from SEC EDGAR CIK map for {self.ticker}.")
            else:
                logger.warning(f"Could not fetch CIK from SEC EDGAR CIK map for {self.ticker}.")

        if not self.stock_db_entry:
            logger.info(f"Stock {self.ticker} not found in DB, creating new entry.")
            self.stock_db_entry = Stock(
                ticker=self.ticker,
                company_name=company_name,
                industry=industry,
                sector=sector,
                cik=cik
            )
            self.db_session.add(self.stock_db_entry)
            try:
                self.db_session.commit()
                self.db_session.refresh(self.stock_db_entry)
            except SQLAlchemyError as e:
                self.db_session.rollback()
                logger.error(f"Error creating stock entry for {self.ticker}: {e}", exc_info=True)
                raise  # Re-raise to indicate critical failure
        else:
            # Update existing entry if new data is available and different
            updated = False
            if company_name and self.stock_db_entry.company_name != company_name:
                self.stock_db_entry.company_name = company_name;
                updated = True
            if industry and self.stock_db_entry.industry != industry:
                self.stock_db_entry.industry = industry;
                updated = True
            if sector and self.stock_db_entry.sector != sector:
                self.stock_db_entry.sector = sector;
                updated = True
            if cik and self.stock_db_entry.cik != cik:  # CIK found and is different
                self.stock_db_entry.cik = cik;
                updated = True
            elif not self.stock_db_entry.cik and cik:  # CIK was missing, now found
                self.stock_db_entry.cik = cik;
                updated = True

            if updated:
                try:
                    self.db_session.commit()
                    self.db_session.refresh(self.stock_db_entry)
                    logger.info(f"Updated stock entry for {self.ticker} with new profile data.")
                except SQLAlchemyError as e:
                    self.db_session.rollback()
                    logger.error(f"Error updating stock entry for {self.ticker}: {e}")

        logger.info(
            f"Stock entry for {self.ticker} (ID: {self.stock_db_entry.id if self.stock_db_entry else 'N/A'}, CIK: {self.stock_db_entry.cik if self.stock_db_entry and self.stock_db_entry.cik else 'N/A'}) ready.")

    def _ensure_stock_db_entry_is_bound(self):
        if not self.stock_db_entry:
            raise RuntimeError(
                f"Stock entry for {self.ticker} is None during binding check. Prior initialization failure.")

        if not self.db_session.is_active:
            logger.warning(f"DB Session for {self.ticker} was INACTIVE before operation. Re-establishing.")
            self._close_session_if_active()  # Close old session
            self.db_session = next(get_db_session())  # Get a new session

            # Re-fetch the stock entry with the new session
            re_fetched_stock = self.db_session.query(Stock).filter(Stock.ticker == self.ticker).first()
            if not re_fetched_stock:
                # This is a critical state, as the stock entry should exist
                raise RuntimeError(
                    f"Failed to re-fetch stock {self.ticker} for new session after inactivity. Critical state.")
            self.stock_db_entry = re_fetched_stock
            logger.info(
                f"Re-fetched and bound stock {self.ticker} (ID: {self.stock_db_entry.id}) to new active session.")
            return  # Successfully re-bound

        # Check if the current stock_db_entry is associated with the current db_session
        instance_state = sa_inspect(self.stock_db_entry)
        if not instance_state.session or instance_state.session is not self.db_session:
            obj_id_log = self.stock_db_entry.id if instance_state.has_identity else 'Transient/No ID'
            logger.warning(
                f"Stock {self.ticker} (ID: {obj_id_log}) DETACHED or bound to DIFFERENT session. Attempting to merge.")
            try:
                # Merge the detached instance into the current session
                self.stock_db_entry = self.db_session.merge(self.stock_db_entry)
                self.db_session.flush()  # Ensure it's actually in the session's identity map
                logger.info(
                    f"Successfully merged stock {self.ticker} (ID: {self.stock_db_entry.id}) into current session.")
            except Exception as e_merge:
                logger.error(f"Failed to merge stock {self.ticker} into session: {e_merge}. Re-fetching as a fallback.",
                             exc_info=True)
                # Fallback: try to load it directly from the DB with the current session
                re_fetched_from_db_after_merge_fail = self.db_session.query(Stock).filter(
                    Stock.ticker == self.ticker).first()
                if re_fetched_from_db_after_merge_fail:
                    self.stock_db_entry = re_fetched_from_db_after_merge_fail
                    logger.info(
                        f"Successfully re-fetched stock {self.ticker} (ID: {self.stock_db_entry.id}) after merge failure.")
                else:
                    # This is a critical failure if the stock cannot be associated with the session
                    raise RuntimeError(
                        f"Failed to bind stock {self.ticker} to session after merge failure and re-fetch attempt. Analysis cannot proceed.")

    def analyze(self):
        logger.info(f"Full analysis pipeline started for {self.ticker}...")
        final_data_for_db = {}
        try:
            if not self.stock_db_entry:
                logger.error(f"Stock DB entry for {self.ticker} not initialized properly. Aborting analysis.")
                return None

            self._ensure_stock_db_entry_is_bound()  # Ensure stock object is session-bound

            # Step 1: Fetch all raw data
            fetch_financial_statements_data(self)
            fetch_key_metrics_and_profile_data(self)

            # Step 2: Calculate quantitative metrics
            final_data_for_db.update(calculate_all_derived_metrics(self))
            final_data_for_db.update(perform_dcf_analysis(self))

            # Step 3: Perform qualitative analysis (10-K, Competitors)
            qual_summaries = fetch_and_summarize_10k_data(self)
            final_data_for_db.update(qual_summaries)  # This includes business_summary, risk_factors_summary etc.

            # fetch_and_analyze_competitors returns the summary string directly
            final_data_for_db["competitive_landscape_summary"] = fetch_and_analyze_competitors(self)

            # Step 4: AI Synthesis for Investment Thesis
            final_data_for_db.update(synthesize_investment_thesis(self))

            # Step 5: Save to Database
            analysis_entry = StockAnalysis(stock_id=self.stock_db_entry.id, analysis_date=datetime.now(timezone.utc))

            model_fields = [c.key for c in StockAnalysis.__table__.columns if
                            c.key not in ['id', 'stock_id', 'analysis_date']]

            for field_name in model_fields:
                if field_name in final_data_for_db:
                    value_to_set = final_data_for_db[field_name]
                    target_column_type = getattr(StockAnalysis, field_name).type.python_type

                    # Type checking and conversion for float
                    if target_column_type == float:
                        if isinstance(value_to_set, str):
                            try:
                                value_to_set = float(value_to_set)
                            except ValueError:
                                value_to_set = None  # Cannot convert string to float
                        if isinstance(value_to_set, float) and (math.isnan(value_to_set) or math.isinf(value_to_set)):
                            value_to_set = None  # SQLAlchemy typically handles None for nullable float fields
                    elif target_column_type == dict and not isinstance(value_to_set, dict):
                        # If it's supposed to be JSON/dict but isn't, set to None or handle as error
                        logger.warning(f"Field {field_name} expected dict, got {type(value_to_set)}. Setting to None.")
                        value_to_set = None
                    elif target_column_type == str and not isinstance(value_to_set, str):
                        value_to_set = str(value_to_set) if value_to_set is not None else None

                    setattr(analysis_entry, field_name, value_to_set)

            self.db_session.add(analysis_entry)
            self.stock_db_entry.last_analysis_date = analysis_entry.analysis_date  # Update parent stock
            self.db_session.commit()
            logger.info(f"Successfully analyzed and saved stock data: {self.ticker} (Analysis ID: {analysis_entry.id})")
            return analysis_entry

        except RuntimeError as rt_err:  # Catch specific init error or binding error
            logger.critical(f"Runtime error during full analysis for {self.ticker}: {rt_err}", exc_info=True)
            # Session might already be closed by the raiser or needs to be handled by caller
            return None
        except Exception as e:
            logger.error(f"CRITICAL error in full analysis pipeline for {self.ticker}: {e}", exc_info=True)
            if self.db_session and self.db_session.is_active:
                try:
                    self.db_session.rollback()
                    logger.info(f"Rolled back DB transaction for {self.ticker} due to error.")
                except Exception as e_rb:
                    logger.error(f"Rollback error for {self.ticker}: {e_rb}")
            return None
        finally:
            self._close_session_if_active()  # Ensure session is closed at the end of analysis
---------- END stock_analyzer.py ----------


---------- __init__.py ----------
# services/__init__.py
from .stock_analyzer.stock_analyzer import StockAnalyzer
from .ipo_analyzer.ipo_analyzer import IPOAnalyzer
from .news_analyzer.news_analyzer import NewsAnalyzer
from .email_service import EmailService

__all__ = [
    "StockAnalyzer",
    "IPOAnalyzer",
    "NewsAnalyzer",
    "EmailService",
]
---------- END __init__.py ----------


---------- email_service.py ----------
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from datetime import datetime, timezone
import json
import math
from markdown2 import Markdown

from core.config import (
    EMAIL_HOST, EMAIL_PORT, EMAIL_USE_TLS, EMAIL_HOST_USER,
    EMAIL_HOST_PASSWORD, EMAIL_SENDER, EMAIL_RECIPIENT
)
from core.logging_setup import logger
from database.models import StockAnalysis, IPOAnalysis, NewsEventAnalysis


class EmailService:
    def __init__(self):
        self.markdowner = Markdown(extras=["tables", "fenced-code-blocks", "break-on-newline"])

    def _md_to_html(self, md_text):
        if md_text is None: return "<p>N/A</p>"
        if isinstance(md_text, (dict, list)):
            return f"<pre>{json.dumps(md_text, indent=2)}</pre>"
        if not isinstance(md_text, str): md_text = str(md_text)
        if "<" in md_text and ">" in md_text and ("<p>" in md_text.lower() or "<div>" in md_text.lower()):
            return md_text
        return self.markdowner.convert(md_text)

    def _format_stock_analysis_html(self, analysis: StockAnalysis):
        if not analysis: return ""
        stock = analysis.stock

        def fmt_num(val, type="decimal", na_val="N/A"):
            if val is None or (isinstance(val, float) and (math.isnan(val) or math.isinf(val))): return na_val
            if type == "percent": return f"{val * 100:.2f}%"
            if type == "decimal": return f"{val:.2f}"
            return str(val)

        business_summary_html = self._md_to_html(analysis.business_summary)
        economic_moat_html = self._md_to_html(analysis.economic_moat_summary)
        industry_trends_html = self._md_to_html(analysis.industry_trends_summary)
        competitive_landscape_html = self._md_to_html(analysis.competitive_landscape_summary)
        management_assessment_html = self._md_to_html(analysis.management_assessment_summary)
        risk_factors_html = self._md_to_html(analysis.risk_factors_summary)
        investment_thesis_html = self._md_to_html(analysis.investment_thesis_full)
        reasoning_points_html = self._md_to_html(analysis.reasoning)

        dcf_assumptions_html = "<ul>"
        if analysis.dcf_assumptions and isinstance(analysis.dcf_assumptions, dict):
            assumptions_data = analysis.dcf_assumptions
            dcf_assumptions_html += f"<li>Discount Rate: {fmt_num(assumptions_data.get('discount_rate'), 'percent')}</li>"
            dcf_assumptions_html += f"<li>Perpetual Growth Rate: {fmt_num(assumptions_data.get('perpetual_growth_rate'), 'percent')}</li>"
            dcf_assumptions_html += f"<li>FCF Projection Years: {assumptions_data.get('projection_years', 'N/A')}</li>"
            dcf_assumptions_html += f"<li>Starting FCF: {fmt_num(assumptions_data.get('start_fcf'))}</li>"
            fcf_growth_proj = assumptions_data.get('fcf_growth_rates_projection')
            if fcf_growth_proj and isinstance(fcf_growth_proj, list):
                 dcf_assumptions_html += f"<li>Projected FCF Growth Rates: {', '.join([fmt_num(r, 'percent') for r in fcf_growth_proj])}</li>"
        else:
            dcf_assumptions_html += "<li>N/A</li>"
        dcf_assumptions_html += "</ul>"

        html = f"""
        <div class="analysis-block stock-analysis">
            <h2>Stock Analysis: {stock.company_name} ({stock.ticker})</h2>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Industry:</strong> {stock.industry or 'N/A'}, <strong>Sector:</strong> {stock.sector or 'N/A'}</p>
            <p><strong>Investment Decision:</strong> {analysis.investment_decision or 'N/A'}</p>
            <p><strong>Strategy Type:</strong> {analysis.strategy_type or 'N/A'}</p>
            <p><strong>Confidence Level:</strong> {analysis.confidence_level or 'N/A'}</p>
            <details>
                <summary><strong>Investment Thesis & Reasoning (Click to expand)</strong></summary>
                <h4>Full Thesis:</h4><div class="markdown-content">{investment_thesis_html}</div>
                <h4>Key Reasoning Points:</h4><div class="markdown-content">{reasoning_points_html}</div>
            </details>
            <details>
                <summary><strong>Key Financial Metrics (Click to expand)</strong></summary>
                <ul>
                    <li>P/E Ratio: {fmt_num(analysis.pe_ratio)}</li><li>P/B Ratio: {fmt_num(analysis.pb_ratio)}</li>
                    <li>P/S Ratio: {fmt_num(analysis.ps_ratio)}</li><li>EV/Sales: {fmt_num(analysis.ev_to_sales)}</li>
                    <li>EV/EBITDA: {fmt_num(analysis.ev_to_ebitda)}</li><li>EPS: {fmt_num(analysis.eps)}</li>
                    <li>ROE: {fmt_num(analysis.roe, 'percent')}</li><li>ROA: {fmt_num(analysis.roa, 'percent')}</li>
                    <li>ROIC: {fmt_num(analysis.roic, 'percent')}</li><li>Dividend Yield: {fmt_num(analysis.dividend_yield, 'percent')}</li>
                    <li>Debt-to-Equity: {fmt_num(analysis.debt_to_equity)}</li><li>Debt-to-EBITDA: {fmt_num(analysis.debt_to_ebitda)}</li>
                    <li>Interest Coverage: {fmt_num(analysis.interest_coverage_ratio)}x</li><li>Current Ratio: {fmt_num(analysis.current_ratio)}</li>
                    <li>Quick Ratio: {fmt_num(analysis.quick_ratio)}</li>
                    <li>Gross Profit Margin: {fmt_num(analysis.gross_profit_margin, 'percent')}</li>
                    <li>Operating Profit Margin: {fmt_num(analysis.operating_profit_margin, 'percent')}</li>
                    <li>Net Profit Margin: {fmt_num(analysis.net_profit_margin, 'percent')}</li>
                    <li>Revenue Growth YoY: {fmt_num(analysis.revenue_growth_yoy, 'percent')} (QoQ: {fmt_num(analysis.revenue_growth_qoq, 'percent')})</li>
                    <li>Revenue Growth CAGR (3yr/5yr): {fmt_num(analysis.revenue_growth_cagr_3yr, 'percent')} / {fmt_num(analysis.revenue_growth_cagr_5yr, 'percent')}</li>
                    <li>EPS Growth YoY: {fmt_num(analysis.eps_growth_yoy, 'percent')}</li>
                    <li>EPS Growth CAGR (3yr/5yr): {fmt_num(analysis.eps_growth_cagr_3yr, 'percent')} / {fmt_num(analysis.eps_growth_cagr_5yr, 'percent')}</li>
                    <li>FCF per Share: {fmt_num(analysis.free_cash_flow_per_share)}</li><li>FCF Yield: {fmt_num(analysis.free_cash_flow_yield, 'percent')}</li>
                    <li>FCF Trend: {analysis.free_cash_flow_trend or 'N/A'}</li><li>Retained Earnings Trend: {analysis.retained_earnings_trend or 'N/A'}</li>
                </ul>
            </details>
            <details>
                <summary><strong>DCF Analysis (Simplified) (Click to expand)</strong></summary>
                <ul>
                    <li>Intrinsic Value per Share: {fmt_num(analysis.dcf_intrinsic_value)}</li>
                    <li>Upside/Downside: {fmt_num(analysis.dcf_upside_percentage, 'percent')}</li>
                </ul>
                <p><em>Key Assumptions Used:</em></p>
                {dcf_assumptions_html}
            </details>
            <details>
                <summary><strong>Qualitative Analysis (from 10-K/Profile & AI) (Click to expand)</strong></summary>
                <p><strong>Business Summary:</strong></p><div class="markdown-content">{business_summary_html}</div>
                <p><strong>Economic Moat:</strong></p><div class="markdown-content">{economic_moat_html}</div>
                <p><strong>Industry Trends & Position:</strong></p><div class="markdown-content">{industry_trends_html}</div>
                <p><strong>Competitive Landscape:</strong></p><div class="markdown-content">{competitive_landscape_html}</div>
                <p><strong>Management Discussion Highlights (MD&A/Assessment):</strong></p><div class="markdown-content">{management_assessment_html}</div>
                <p><strong>Key Risk Factors:</strong></p><div class="markdown-content">{risk_factors_html}</div>
            </details>
            <details>
                <summary><strong>Supporting Data Snapshots (Click to expand)</strong></summary>
                <p><em>Key Metrics Data Points Used:</em></p><div class="markdown-content">{self._md_to_html(analysis.key_metrics_snapshot)}</div>
                <p><em>Qualitative Analysis Sources Summary:</em></p><div class="markdown-content">{self._md_to_html(analysis.qualitative_sources_summary)}</div>
            </details>
        </div>
        """
        return html

    def _format_ipo_analysis_html(self, analysis: IPOAnalysis):
        if not analysis: return ""
        ipo = analysis.ipo
        def fmt_price(val_low, val_high, currency="USD"):
            if val_low is None and val_high is None: return "N/A"
            if val_low is not None and val_high is not None:
                if val_low == val_high: return f"{val_low:.2f} {currency}"
                return f"{val_low:.2f} - {val_high:.2f} {currency}"
            if val_low is not None: return f"{val_low:.2f} {currency}"
            if val_high is not None: return f"{val_high:.2f} {currency}"
            return "N/A"
        reasoning_html = self._md_to_html(analysis.reasoning)
        s1_business_summary_html = self._md_to_html(analysis.s1_business_summary or analysis.business_model_summary)
        s1_risk_factors_summary_html = self._md_to_html(analysis.s1_risk_factors_summary or analysis.risk_factors_summary)
        s1_mda_summary_html = self._md_to_html(analysis.s1_mda_summary)
        s1_financial_health_summary_html = self._md_to_html(analysis.s1_financial_health_summary or analysis.pre_ipo_financials_summary)
        competitive_landscape_html = self._md_to_html(analysis.competitive_landscape_summary)
        industry_outlook_html = self._md_to_html(analysis.industry_outlook_summary)
        use_of_proceeds_html = self._md_to_html(analysis.use_of_proceeds_summary)
        management_team_html = self._md_to_html(analysis.management_team_assessment)
        underwriter_html = self._md_to_html(analysis.underwriter_quality_assessment)
        valuation_html = self._md_to_html(analysis.valuation_comparison_summary)
        html = f"""
        <div class="analysis-block ipo-analysis">
            <h2>IPO Analysis: {ipo.company_name} ({ipo.symbol or 'N/A'})</h2>
            <p><strong>Expected IPO Date:</strong> {ipo.ipo_date.strftime('%Y-%m-%d') if ipo.ipo_date else ipo.ipo_date_str or 'N/A'}</p>
            <p><strong>Expected Price Range:</strong> {fmt_price(ipo.expected_price_range_low, ipo.expected_price_range_high, ipo.expected_price_currency)}</p>
            <p><strong>Exchange:</strong> {ipo.exchange or 'N/A'}, <strong>Status:</strong> {ipo.status or 'N/A'}</p>
            <p><strong>S-1 Filing URL:</strong> {f'<a href="{ipo.s1_filing_url}">{ipo.s1_filing_url}</a>' if ipo.s1_filing_url else 'Not Found'}</p>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Preliminary Stance:</strong> {analysis.investment_decision or 'N/A'}</p>
            <details><summary><strong>AI Synthesized Reasoning & Critical Verification Points (Click to expand)</strong></summary><div class="markdown-content">{reasoning_html}</div></details>
            <details>
                <summary><strong>S-1 Based Summaries (if available) & AI Analysis (Click to expand)</strong></summary>
                <p><strong>Business Summary (S-1/inferred):</strong></p><div class="markdown-content">{s1_business_summary_html}</div>
                <p><strong>Competitive Landscape:</strong></p><div class="markdown-content">{competitive_landscape_html}</div>
                <p><strong>Industry Outlook:</strong></p><div class="markdown-content">{industry_outlook_html}</div>
                <p><strong>Risk Factors Summary (S-1/inferred):</strong></p><div class="markdown-content">{s1_risk_factors_summary_html}</div>
                <p><strong>Use of Proceeds (S-1/inferred):</strong></p><div class="markdown-content">{use_of_proceeds_html}</div>
                <p><strong>MD&A / Financial Health Summary (S-1/inferred):</strong></p><div class="markdown-content">{s1_mda_summary_html if s1_mda_summary_html and not s1_mda_summary_html.startswith("Section not found") else s1_financial_health_summary_html}</div>
                <p><strong>Management Team Assessment:</strong></p><div class="markdown-content">{management_team_html}</div>
                <p><strong>Underwriter Quality Assessment:</strong></p><div class="markdown-content">{underwriter_html}</div>
                <p><strong>Valuation Comparison Guidance:</strong></p><div class="markdown-content">{valuation_html}</div>
            </details>
            <details><summary><strong>Supporting Data (Click to expand)</strong></summary>
                <p><em>Raw IPO calendar API data:</em></p><div class="markdown-content">{self._md_to_html(analysis.key_data_snapshot)}</div>
                <p><em>S-1 Sections Used (True if found & used):</em></p><div class="markdown-content">{self._md_to_html(analysis.s1_sections_used)}</div>
            </details>
        </div>"""
        return html

    def _format_news_event_analysis_html(self, analysis: NewsEventAnalysis):
        if not analysis: return ""
        news_event = analysis.news_event
        sentiment_html = self._md_to_html(f"**Sentiment:** {analysis.sentiment or 'N/A'}\n**Reasoning:** {analysis.sentiment_reasoning or 'N/A'}")
        news_summary_detailed_html = self._md_to_html(analysis.news_summary_detailed)
        impact_companies_html = self._md_to_html(analysis.potential_impact_on_companies)
        impact_sectors_html = self._md_to_html(analysis.potential_impact_on_sectors)
        mechanism_html = self._md_to_html(analysis.mechanism_of_impact)
        timing_duration_html = self._md_to_html(analysis.estimated_timing_duration)
        magnitude_direction_html = self._md_to_html(analysis.estimated_magnitude_direction)
        confidence_html = self._md_to_html(analysis.confidence_of_assessment)
        investor_summary_html = self._md_to_html(analysis.summary_for_email)
        html = f"""
        <div class="analysis-block news-analysis">
            <h2>News/Event Analysis: {news_event.event_title}</h2>
            <p><strong>Event Date:</strong> {news_event.event_date.strftime('%Y-%m-%d %H:%M %Z') if news_event.event_date else 'N/A'}</p>
            <p><strong>Source:</strong> <a href="{news_event.source_url}">{news_event.source_name or news_event.source_url}</a></p>
            <p><strong>Full Article Scraped:</strong> {'Yes' if news_event.full_article_text else 'No (Analysis based on headline/summary if available)'}</p>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Investor Summary:</strong></p><div class="markdown-content">{investor_summary_html}</div>
            <details><summary><strong>Detailed AI Analysis (Click to expand)</strong></summary>
                <p><strong>Sentiment Analysis:</strong></p><div class="markdown-content">{sentiment_html}</div>
                <p><strong>Detailed News Summary:</strong></p><div class="markdown-content">{news_summary_detailed_html}</div>
                <p><strong>Potentially Affected Companies/Stocks:</strong></p><div class="markdown-content">{impact_companies_html}</div>
                <p><strong>Potentially Affected Sectors:</strong></p><div class="markdown-content">{impact_sectors_html}</div>
                <p><strong>Mechanism of Impact:</strong></p><div class="markdown-content">{mechanism_html}</div>
                <p><strong>Estimated Timing & Duration:</strong></p><div class="markdown-content">{timing_duration_html}</div>
                <p><strong>Estimated Magnitude & Direction:</strong></p><div class="markdown-content">{magnitude_direction_html}</div>
                <p><strong>Confidence of Assessment:</strong></p><div class="markdown-content">{confidence_html}</div>
            </details>
            <details><summary><strong>Key Snippets Used for Analysis (Click to expand)</strong></summary><div class="markdown-content">{self._md_to_html(analysis.key_news_snippets)}</div></details>
        </div>"""
        return html

    def create_summary_email(self, stock_analyses=None, ipo_analyses=None, news_analyses=None):
        if not any([stock_analyses, ipo_analyses, news_analyses]):
            logger.info("No analyses provided to create an email."); return None
        subject_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        subject = f"Financial Analysis Summary - {subject_date}"
        html_body = f"""
        <html><head><style>
            body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; background-color: #f4f4f4; line-height: 1.6; color: #333; }}
            .container {{ background-color: #ffffff; padding: 20px; border-radius: 8px; box-shadow: 0 0 15px rgba(0,0,0,0.1); max-width: 900px; margin: auto; }}
            .analysis-block {{ border: 1px solid #ddd; padding: 15px; margin-bottom: 25px; border-radius: 5px; background-color: #fdfdfd; box-shadow: 0 2px 4px rgba(0,0,0,0.05);}}
            .stock-analysis {{ border-left: 5px solid #4CAF50; }} .ipo-analysis {{ border-left: 5px solid #2196F3; }} .news-analysis {{ border-left: 5px solid #FFC107; }}
            h1 {{ color: #2c3e50; text-align: center; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
            h2 {{ color: #34495e; border-bottom: 1px solid #eee; padding-bottom: 5px; margin-top: 0; }}
            h4 {{ color: #555; margin-top: 15px; margin-bottom: 5px; }}
            details > summary {{ cursor: pointer; font-weight: bold; margin-bottom: 10px; color: #2980b9; padding: 5px; background-color: #ecf0f1; border-radius:3px; }}
            details[open] > summary {{ background-color: #dde5e8; }}
            pre {{ background-color: #eee; padding: 10px; border-radius: 4px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; font-size: 0.85em; border: 1px solid #ccc; }}
            ul {{ list-style-type: disc; margin-left: 20px; padding-left: 5px; }} li {{ margin-bottom: 8px; }}
            .markdown-content p {{ margin: 0.5em 0; }} .markdown-content ul, .markdown-content ol {{ margin-left: 20px; }}
            .markdown-content table {{ border-collapse: collapse; width: 100%; margin-bottom: 1em;}}
            .markdown-content th, .markdown-content td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }} .markdown-content th {{ background-color: #f2f2f2; }}
            .report-footer {{ text-align: center; font-size: 0.9em; color: #777; margin-top: 30px; }}
        </style></head><body><div class="container">
            <h1>Financial Analysis Report</h1>
            <p style="text-align:center; font-style:italic; color:#555;">Generated: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z')}</p>
            <p style="text-align:center; font-style:italic; color:#7f8c8d;"><em>This email contains automated analysis. Always do your own research before making investment decisions.</em></p>"""
        if stock_analyses: html_body += "<h2>Individual Stock Analyses</h2>"; [html_body := html_body + self._format_stock_analysis_html(sa) for sa in stock_analyses]
        if ipo_analyses: html_body += "<h2>Upcoming IPO Analyses</h2>"; [html_body := html_body + self._format_ipo_analysis_html(ia) for ia in ipo_analyses]
        if news_analyses: html_body += "<h2>Recent News & Event Analyses</h2>"; [html_body := html_body + self._format_news_event_analysis_html(na) for na in news_analyses]
        html_body += """<div class="report-footer"><p>© Automated Financial Analysis System</p></div></div></body></html>"""
        msg = MIMEMultipart('alternative'); msg['Subject'], msg['From'], msg['To'] = subject, EMAIL_SENDER, EMAIL_RECIPIENT
        msg.attach(MIMEText(html_body, 'html', 'utf-8')); return msg

    def send_email(self, message: MIMEMultipart):
        if not message: logger.error("No message object provided to send_email."); return False
        try:
            smtp_server = smtplib.SMTP(EMAIL_HOST, EMAIL_PORT, timeout=20)
            if EMAIL_USE_TLS: smtp_server.starttls()
            smtp_server.login(EMAIL_HOST_USER, EMAIL_HOST_PASSWORD)
            smtp_server.sendmail(EMAIL_SENDER, EMAIL_RECIPIENT, message.as_string())
            smtp_server.quit(); logger.info(f"Email sent successfully to {EMAIL_RECIPIENT}"); return True
        except smtplib.SMTPException as e_smtp: logger.error(f"SMTP error sending email: {e_smtp}", exc_info=True); return False
        except Exception as e: logger.error(f"General error sending email: {e}", exc_info=True); return False

if __name__ == '__main__':
    logger.info("Starting email service test...")
    class MockStock: __init__ = lambda self, ticker, company_name, industry="Tech", sector="Software": setattr(self, 'ticker', ticker) or setattr(self, 'company_name', company_name) or setattr(self, 'industry', industry) or setattr(self, 'sector', sector)
    class MockIPO: __init__ = lambda self, company_name, symbol, ipo_date_str="2025-07-15": setattr(self, 'company_name', company_name) or setattr(self, 'symbol', symbol) or setattr(self, 'ipo_date_str', ipo_date_str) or setattr(self, 'ipo_date', datetime.strptime(ipo_date_str, "%Y-%m-%d").date() if ipo_date_str else None) or setattr(self, 'expected_price_range_low', 18.00) or setattr(self, 'expected_price_range_high', 22.00) or setattr(self, 'expected_price_currency', "USD") or setattr(self, 'exchange', "NASDAQ") or setattr(self, 'status', "Filed") or setattr(self, 's1_filing_url', "http://example.com/s1")
    class MockNewsEvent: __init__ = lambda self, title, url, event_date_str="2025-05-25 10:00:00": setattr(self, 'event_title', title) or setattr(self, 'source_url', url) or setattr(self, 'source_name', "Mock News") or setattr(self, 'event_date', datetime.strptime(event_date_str, "%Y-%m-%d %H:%M:%S").replace(tzinfo=timezone.utc)) or setattr(self, 'full_article_text', "Full article text...")
    class MockStockAnalysis:
        def __init__(self, stock): self.stock, self.analysis_date, self.investment_decision, self.strategy_type, self.confidence_level, self.investment_thesis_full, self.reasoning = stock, datetime.now(timezone.utc), "Buy", "GARP", "Medium", "Thesis...", "Reasons..."
        self.dcf_assumptions = {"discount_rate": 0.095, "perpetual_growth_rate": 0.025, "projection_years": 5, "start_fcf": 1.2e9, "fcf_growth_rates_projection": [0.08, 0.07, 0.06, 0.05, 0.04]}
        self.pe_ratio, self.pb_ratio, self.ps_ratio, self.ev_to_sales, self.ev_to_ebitda, self.eps, self.roe, self.roa, self.roic, self.dividend_yield, self.debt_to_equity, self.debt_to_ebitda, self.interest_coverage_ratio, self.current_ratio, self.quick_ratio, self.gross_profit_margin, self.operating_profit_margin, self.net_profit_margin, self.revenue_growth_yoy, self.revenue_growth_qoq, self.revenue_growth_cagr_3yr, self.revenue_growth_cagr_5yr, self.eps_growth_yoy, self.eps_growth_cagr_3yr, self.eps_growth_cagr_5yr, self.free_cash_flow_per_share, self.free_cash_flow_yield, self.free_cash_flow_trend, self.retained_earnings_trend, self.dcf_intrinsic_value, self.dcf_upside_percentage, self.business_summary, self.economic_moat_summary, self.industry_trends_summary, self.competitive_landscape_summary, self.management_assessment_summary, self.risk_factors_summary, self.key_metrics_snapshot, self.qualitative_sources_summary = (18.5, 3.2, 2.5, 2.8, 12.0, 2.50, 0.22, 0.10, 0.15, 0.015, 0.5, 2.1, 8.0, 1.8, 1.2, 0.60, 0.20, 0.12, 0.15, 0.04, 0.12, 0.10, 0.20, 0.18, 0.15, 1.80, 0.05, "Growing", "Growing", 120.50, 0.205, "Biz Sum.", "Moat Sum.", "Ind Sum.", "Comp Sum.", "Mgmt Sum.", "Risk Sum.", {"price": 100}, {"10k_url": "url"})
    class MockIPOAnalysis: __init__ = lambda self, ipo: setattr(self, 'ipo', ipo) or setattr(self, 'analysis_date', datetime.now(timezone.utc))
    class MockNewsEventAnalysis: __init__ = lambda self, news_event: setattr(self, 'news_event', news_event) or setattr(self, 'analysis_date', datetime.now(timezone.utc))
    mock_sa = MockStockAnalysis(MockStock("MOCK", "MockCorp Inc."))
    mock_ipo_a = MockIPOAnalysis(MockIPO("NewIPO Inc.", "NIPO"))
    mock_news_a = MockNewsEventAnalysis(MockNewsEvent("Major Tech Breakthrough", "http://example.com/news1"))
    email_svc = EmailService()
    email_message = email_svc.create_summary_email(stock_analyses=[mock_sa], ipo_analyses=[mock_ipo_a], news_analyses=[mock_news_a])
    if email_message:
        logger.info("Email message created successfully.")
        output_filename = f"test_email_summary_refactored_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(output_filename, "w", encoding="utf-8") as f:
            payload_html = "";
            if email_message.is_multipart():
                for part in email_message.get_payload():
                    if part.get_content_type() == "text/html": payload_html = part.get_payload(decode=True).decode(part.get_content_charset() or 'utf-8'); break
            else: payload_html = email_message.get_payload(decode=True).decode(email_message.get_content_charset() or 'utf-8')
            if payload_html: f.write(payload_html); logger.info(f"Test email HTML saved to {output_filename}")
            else: logger.error("Could not extract HTML payload.")
        # if email_svc.send_email(email_message): logger.info("Test email sent.") else: logger.error("Failed to send test email.")
    else: logger.error("Failed to create email message.")
---------- END email_service.py ----------


---------- .gitignore ----------
venv
/__pycache__

---------- END .gitignore ----------


---------- app_analysis.log ----------
2025-05-26 10:43:56,558 - root - INFO - main:113 - ===================================================================
2025-05-26 10:43:56,559 - root - INFO - main:114 - Starting Financial Analysis Script at 2025-05-26 07:43:56 UTC
2025-05-26 10:43:56,559 - root - INFO - main:115 - ===================================================================
2025-05-26 10:43:56,561 - root - INFO - main:89 - Initializing database as per command line argument...
2025-05-26 10:43:56,562 - root - INFO - connection:19 - Initializing database and creating tables...
2025-05-26 10:44:00,488 - root - INFO - connection:23 - Database tables created successfully (if they didn't exist).
2025-05-26 10:44:00,491 - root - INFO - main:90 - Database initialization complete.
2025-05-26 10:44:00,493 - root - INFO - main:108 - --- Main script execution finished. ---
2025-05-26 10:44:00,494 - root - INFO - main:118 - Financial Analysis Script finished at 2025-05-26 07:44:00 UTC
2025-05-26 10:44:00,495 - root - INFO - main:119 - Total execution time: 0:00:03.935530
2025-05-26 10:44:00,496 - root - INFO - main:120 - ===================================================================
2025-05-26 10:44:05,237 - root - INFO - main:113 - ===================================================================
2025-05-26 10:44:05,238 - root - INFO - main:114 - Starting Financial Analysis Script at 2025-05-26 07:44:05 UTC
2025-05-26 10:44:05,238 - root - INFO - main:115 - ===================================================================
2025-05-26 10:44:05,240 - root - INFO - main:16 - --- Starting Individual Stock Analysis for: ['NKE'] ---
2025-05-26 10:44:07,603 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:44:09,104 - root - INFO - stock_analyzer:83 - Fetched profile from FMP for NKE.
2025-05-26 10:44:09,105 - root - INFO - stock_analyzer:124 - Stock NKE not found in DB, creating new entry.
2025-05-26 10:44:09,578 - root - INFO - stock_analyzer:168 - Stock entry for NKE (ID: 1, CIK: 0000320187) ready.
2025-05-26 10:44:09,578 - root - INFO - stock_analyzer:220 - Full analysis pipeline started for NKE...
2025-05-26 10:44:09,579 - root - INFO - data_fetcher:9 - Fetching financial statements for NKE...
2025-05-26 10:44:10,661 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91TVzf...
2025-05-26 10:44:13,380 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/NKE?apikey=62ERGmJoqQgGD0nSGxRZ...
2025-05-26 10:44:16,086 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91T...
2025-05-26 10:44:17,588 - root - INFO - data_fetcher:26 - FMP Annuals for NKE: Income(5), Balance(5), Cashflow(5).
2025-05-26 10:44:18,462 - root - WARNING - base_client:139 - HTTP error on attempt 1/3 for GET https://financialmodelingprep.com/api/v3/income-statement/NKE (Params: {'apikey': '62ER******61uB', 'period': 'quarter', 'limit': 8}, Headers: {}): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-26 10:44:18,464 - root - ERROR - base_client:158 - Client error 403 (Unauthorized/Forbidden) for https://financialmodelingprep.com/api/v3/income-statement/NKE. API key may be invalid or permissions lacking. No retry. Params: {'apikey': '62ER******61uB', 'period': 'quarter', 'limit': 8}
2025-05-26 10:44:19,968 - root - INFO - data_fetcher:31 - FMP Quarterly Income for NKE: 0 records.
2025-05-26 10:44:21,708 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/financials-reported?count=8&freq=quarterly&symbol=NKE&token=d0o7...
2025-05-26 10:44:23,211 - root - INFO - data_fetcher:38 - Fetched 45 quarterly reports from Finnhub for NKE.
2025-05-26 10:44:24,664 - root - INFO - base_client:65 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=INCOME_STATEMENT&symbol=NKE...
2025-05-26 10:44:39,666 - root - INFO - data_fetcher:47 - Fetched 81 quarterly income reports from Alpha Vantage for NKE.
2025-05-26 10:44:41,188 - root - INFO - base_client:65 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=BALANCE_SHEET&symbol=NKE...
2025-05-26 10:44:56,190 - root - INFO - data_fetcher:55 - Fetched 81 quarterly balance reports from Alpha Vantage for NKE.
2025-05-26 10:44:57,604 - root - INFO - base_client:65 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=CASH_FLOW&symbol=NKE...
2025-05-26 10:45:12,606 - root - INFO - data_fetcher:63 - Fetched 81 quarterly cash flow reports from Alpha Vantage for NKE.
2025-05-26 10:45:12,606 - root - INFO - data_fetcher:76 - Fetching key metrics and profile for NKE.
2025-05-26 10:45:13,949 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:45:16,324 - root - WARNING - base_client:139 - HTTP error on attempt 1/3 for GET https://financialmodelingprep.com/api/v3/key-metrics/NKE (Params: {'apikey': '62ER******61uB', 'period': 'quarterly', 'limit': 8}, Headers: {}): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-26 10:45:16,325 - root - ERROR - base_client:158 - Client error 403 (Unauthorized/Forbidden) for https://financialmodelingprep.com/api/v3/key-metrics/NKE. API key may be invalid or permissions lacking. No retry. Params: {'apikey': '62ER******61uB', 'period': 'quarterly', 'limit': 8}
2025-05-26 10:45:18,952 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=NKE&token=d0o7hphr01qqr9alj38gd0o7hphr0...
2025-05-26 10:45:20,454 - root - INFO - data_fetcher:95 - FMP KM Annual for NKE: 5. FMP KM Quarterly for NKE: 0. Finnhub Basic Financials for NKE: OK.
2025-05-26 10:45:20,454 - root - INFO - metrics_calculator:437 - Calculating derived metrics for NKE...
2025-05-26 10:45:20,455 - root - WARNING - metrics_calculator:454 - AlphaVantage overview data not found in cache for NKE. Fetching now.
2025-05-26 10:45:21,577 - root - INFO - base_client:65 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=OVERVIEW&symbol=NKE...
2025-05-26 10:45:21,578 - root - INFO - metrics_calculator:279 - Using latest quarterly revenue: 35,212,000,000 (Source: Finnhub) for NKE.
2025-05-26 10:45:21,579 - root - INFO - metrics_calculator:492 - Calculated metrics for NKE: {
  "pe_ratio": 25.306645614035087,
  "pb_ratio": 9.996388080388082,
  "ps_ratio": 1.8526,
  "ev_to_sales": 1.884,
  "ev_to_ebitda": 15.69,
  "dividend_yield": 0.015036616136056905,
  "eps": 3.76,
  "net_profit_margin": 0.0943,
  "gross_profit_margin": 0.43824503066815973,
  "operating_profit_margin": 0.1302519372,
  "interest_coverage_ratio": null,
  "roe": 0.39501039501039503,
  "roa": 0.14956704277092628,
  "roic": 0.3444796354747266,
  "debt_to_equity": 0.8282744282744283,
  "current_ratio": 2.3961106391012934,
  "quick_ratio": 1.5112810346455205,
  "debt_to_ebitda": 1.6704402515723271,
  "revenue_growth_yoy": 0.0028310912392369722,
  "eps_growth_yoy": 0.14984709480122316,
  "revenue_growth_cagr_3yr": 0.04861491256483208,
  "eps_growth_cagr_3yr": -0.009180522194930263,
  "revenue_growth_cagr_5yr": 0.08251487385466705,
  "eps_growth_cagr_5yr": 0.23239537592729897,
  "revenue_growth_qoq": 0.47065948293864596,
  "free_cash_flow_per_share": 5.6166709107885575,
  "free_cash_flow_yield": 0.07456847629026464,
  "free_cash_flow_trend": "Growing",
  "retained_earnings_trend": "Declining"
}
2025-05-26 10:45:21,586 - root - INFO - metrics_calculator:495 - Key metrics snapshot for NKE: {
  "q_revenue_source": "Finnhub",
  "latest_q_revenue": 35212000000.0,
  "avg_historical_q_revenue_for_check": 25153750000.0
}
2025-05-26 10:45:21,587 - root - INFO - dcf_analyzer:55 - Performing simplified DCF analysis for NKE...
2025-05-26 10:45:21,588 - root - INFO - dcf_analyzer:181 - DCF for NKE: Base IV/Share: 72.57, Upside: 20.92%
2025-05-26 10:45:21,588 - root - INFO - qualitative_analyzer:88 - Fetching and attempting to summarize latest 10-K for NKE
2025-05-26 10:45:22,878 - root - INFO - base_client:65 - Cached response for: GET:https://data.sec.gov/submissions/CIK0000320187.json?...
2025-05-26 10:45:23,382 - root - INFO - sec_edgar_client:127 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/320187/000032018724000044/nke-20240531.htm
2025-05-26 10:45:24,697 - root - INFO - base_client:65 - Cached response for: GET:https://www.sec.gov/Archives/edgar/data/320187/000032018724000044/nke-20240531.htm?...
2025-05-26 10:45:24,699 - root - INFO - qualitative_analyzer:114 - Fetched 10-K text (length: 2651098) for NKE. Extracting and summarizing sections.
2025-05-26 10:45:26,033 - root - INFO - qualitative_analyzer:20 - Summarizing 'Business (Item 1)' for NIKE, Inc. (NKE), original length: 10577 chars.
2025-05-26 10:45:26,033 - root - INFO - qualitative_analyzer:23 - Section length 10577 is within single-pass limit (100000). Summarizing directly.
2025-05-26 10:45:38,111 - root - INFO - qualitative_analyzer:141 - Summary for 'Business (Item 1)' (source length 10577): NIKE, Inc.'s core business operation is the design, development, and worldwide marketing and selling of athletic footwear, apparel, equipment, accesso...
2025-05-26 10:45:38,112 - root - INFO - qualitative_analyzer:20 - Summarizing 'Risk Factors (Item 1A)' for NIKE, Inc. (NKE), original length: 310235 chars.
2025-05-26 10:45:38,113 - root - INFO - qualitative_analyzer:35 - Section length 310235 exceeds single-pass limit. Applying chunked summarization (Chunk size: 80000, Overlap: 5000).
2025-05-26 10:45:38,115 - root - INFO - qualitative_analyzer:49 - Summarizing chunk 1/5 for 'Risk Factors (Item 1A)' of NIKE, Inc. (NKE) (length: 80000 chars).
2025-05-26 10:45:56,624 - root - INFO - qualitative_analyzer:49 - Summarizing chunk 2/5 for 'Risk Factors (Item 1A)' of NIKE, Inc. (NKE) (length: 80000 chars).
2025-05-26 10:46:09,976 - root - INFO - qualitative_analyzer:49 - Summarizing chunk 3/5 for 'Risk Factors (Item 1A)' of NIKE, Inc. (NKE) (length: 80000 chars).
2025-05-26 10:46:43,487 - root - INFO - qualitative_analyzer:49 - Summarizing chunk 4/5 for 'Risk Factors (Item 1A)' of NIKE, Inc. (NKE) (length: 80000 chars).
2025-05-26 10:47:08,181 - root - INFO - qualitative_analyzer:49 - Summarizing chunk 5/5 for 'Risk Factors (Item 1A)' of NIKE, Inc. (NKE) (length: 10235 chars).
2025-05-26 10:47:20,421 - root - INFO - qualitative_analyzer:64 - Concatenated chunk summaries length for 'Risk Factors (Item 1A)': 10147 chars.
2025-05-26 10:47:20,423 - root - INFO - qualitative_analyzer:141 - Summary for 'Risk Factors (Item 1A)' (source length 310235): NIKE, Inc. faces several significant operational and strategic risk factors:  1.  **Changing Consumer Preferences and Design Trends:** Demand for prod...
2025-05-26 10:47:20,424 - root - INFO - qualitative_analyzer:20 - Summarizing 'Management's Discussion and Analysis (Item 7)' for NIKE, Inc. (NKE), original length: 11209 chars.
2025-05-26 10:47:20,425 - root - INFO - qualitative_analyzer:23 - Section length 11209 is within single-pass limit (100000). Summarizing directly.
2025-05-26 10:47:33,934 - root - INFO - qualitative_analyzer:141 - Summary for 'Management's Discussion and Analysis (Item 7)' (source length 11209): NIKE, Inc. reported a slight increase in fiscal 2024 revenues to $51.4 billion, up from $51.2 billion in fiscal 2023. This growth was supported by a 1...
2025-05-26 10:48:17,633 - root - INFO - qualitative_analyzer:199 - 10-K qualitative summaries and AI interpretations generated for NKE.
2025-05-26 10:48:17,636 - root - INFO - qualitative_analyzer:206 - Fetching and analyzing competitor data for NKE...
2025-05-26 10:48:18,622 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/peers?symbol=NKE&token=d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390...
2025-05-26 10:48:20,125 - root - INFO - qualitative_analyzer:231 - Identified peers for NKE: ['DECK', 'SKX', 'CROX', 'SHOO', 'WWW']. Fetching basic data for comparison.
2025-05-26 10:48:21,389 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/DECK?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:48:24,099 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/DECK?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61u...
2025-05-26 10:48:27,016 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=DECK&token=d0o7hphr01qqr9alj38gd0o7hphr...
2025-05-26 10:48:29,728 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/SKX?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:48:32,428 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/SKX?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:48:35,131 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=SKX&token=d0o7hphr01qqr9alj38gd0o7hphr0...
2025-05-26 10:48:37,838 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/CROX?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:48:40,575 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/CROX?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61u...
2025-05-26 10:48:43,252 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=CROX&token=d0o7hphr01qqr9alj38gd0o7hphr...
2025-05-26 10:48:45,966 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/SHOO?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:48:48,658 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/SHOO?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61u...
2025-05-26 10:48:51,349 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=SHOO&token=d0o7hphr01qqr9alj38gd0o7hphr...
2025-05-26 10:48:54,091 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/WWW?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:48:56,837 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/WWW?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:48:59,759 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=WWW&token=d0o7hphr01qqr9alj38gd0o7hphr0...
2025-05-26 10:49:18,288 - root - INFO - qualitative_analyzer:317 - Competitor analysis summary generated for NKE.
2025-05-26 10:49:18,289 - root - INFO - ai_synthesis:80 - Synthesizing investment thesis for NKE...
2025-05-26 10:49:42,433 - root - INFO - ai_synthesis:204 - Generated thesis for NKE. Decision: Hold, Strategy: GARP (Growth At a Reasonable Price), Confidence: Medium
2025-05-26 10:49:43,008 - root - INFO - stock_analyzer:279 - Successfully analyzed and saved stock data: NKE (Analysis ID: 1)
2025-05-26 10:49:48,075 - root - INFO - main:108 - --- Main script execution finished. ---
2025-05-26 10:49:48,075 - root - INFO - main:118 - Financial Analysis Script finished at 2025-05-26 07:49:48 UTC
2025-05-26 10:49:48,076 - root - INFO - main:119 - Total execution time: 0:05:42.838082
2025-05-26 10:49:48,076 - root - INFO - main:120 - ===================================================================
2025-05-26 10:49:53,167 - root - INFO - main:113 - ===================================================================
2025-05-26 10:49:53,168 - root - INFO - main:114 - Starting Financial Analysis Script at 2025-05-26 07:49:53 UTC
2025-05-26 10:49:53,168 - root - INFO - main:115 - ===================================================================
2025-05-26 10:49:53,170 - root - INFO - main:54 - --- Generating Today's Email Summary ---
2025-05-26 10:49:54,668 - root - INFO - main:61 - Found 1 stock analyses, 0 IPO analyses, 0 news analyses since 2025-05-26 00:00:00 UTC for email.
2025-05-26 10:49:56,710 - root - INFO - email_service:250 - Email sent successfully to daniprav@gmail.com
2025-05-26 10:49:56,778 - root - INFO - main:108 - --- Main script execution finished. ---
2025-05-26 10:49:56,778 - root - INFO - main:118 - Financial Analysis Script finished at 2025-05-26 07:49:56 UTC
2025-05-26 10:49:56,779 - root - INFO - main:119 - Total execution time: 0:00:03.611321
2025-05-26 10:49:56,779 - root - INFO - main:120 - ===================================================================
2025-05-26 10:51:20,195 - root - INFO - main:113 - ===================================================================
2025-05-26 10:51:20,196 - root - INFO - main:114 - Starting Financial Analysis Script at 2025-05-26 07:51:20 UTC
2025-05-26 10:51:20,196 - root - INFO - main:115 - ===================================================================
2025-05-26 10:51:20,199 - root - INFO - main:89 - Initializing database as per command line argument...
2025-05-26 10:51:20,200 - root - INFO - connection:19 - Initializing database and creating tables...
2025-05-26 10:51:23,772 - root - INFO - connection:23 - Database tables created successfully (if they didn't exist).
2025-05-26 10:51:23,773 - root - INFO - main:90 - Database initialization complete.
2025-05-26 10:51:23,774 - root - INFO - main:108 - --- Main script execution finished. ---
2025-05-26 10:51:23,775 - root - INFO - main:118 - Financial Analysis Script finished at 2025-05-26 07:51:23 UTC
2025-05-26 10:51:23,777 - root - INFO - main:119 - Total execution time: 0:00:03.580024
2025-05-26 10:51:23,777 - root - INFO - main:120 - ===================================================================
2025-05-26 10:51:27,227 - root - INFO - main:113 - ===================================================================
2025-05-26 10:51:27,227 - root - INFO - main:114 - Starting Financial Analysis Script at 2025-05-26 07:51:27 UTC
2025-05-26 10:51:27,228 - root - INFO - main:115 - ===================================================================
2025-05-26 10:51:27,229 - root - INFO - main:16 - --- Starting Individual Stock Analysis for: ['NKE'] ---
2025-05-26 10:51:30,095 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:51:31,596 - root - INFO - stock_analyzer:83 - Fetched profile from FMP for NKE.
2025-05-26 10:51:31,596 - root - INFO - stock_analyzer:124 - Stock NKE not found in DB, creating new entry.
2025-05-26 10:51:32,049 - root - INFO - stock_analyzer:168 - Stock entry for NKE (ID: 1, CIK: 0000320187) ready.
2025-05-26 10:51:32,049 - root - INFO - stock_analyzer:220 - Full analysis pipeline started for NKE...
2025-05-26 10:51:32,050 - root - INFO - data_fetcher:9 - Fetching financial statements for NKE...
2025-05-26 10:51:33,150 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91TVzf...
2025-05-26 10:51:35,828 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/NKE?apikey=62ERGmJoqQgGD0nSGxRZ...
2025-05-26 10:51:38,509 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91T...
2025-05-26 10:51:40,010 - root - INFO - data_fetcher:26 - FMP Annuals for NKE: Income(5), Balance(5), Cashflow(5).
2025-05-26 10:51:40,875 - root - WARNING - base_client:139 - HTTP error on attempt 1/3 for GET https://financialmodelingprep.com/api/v3/income-statement/NKE (Params: {'apikey': '62ER******61uB', 'period': 'quarter', 'limit': 8}, Headers: {}): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-26 10:51:40,878 - root - ERROR - base_client:158 - Client error 403 (Unauthorized/Forbidden) for https://financialmodelingprep.com/api/v3/income-statement/NKE. API key may be invalid or permissions lacking. No retry. Params: {'apikey': '62ER******61uB', 'period': 'quarter', 'limit': 8}
2025-05-26 10:51:42,380 - root - INFO - data_fetcher:31 - FMP Quarterly Income for NKE: 0 records.
2025-05-26 10:51:43,911 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/financials-reported?count=8&freq=quarterly&symbol=NKE&token=d0o7...
2025-05-26 10:51:45,412 - root - INFO - data_fetcher:38 - Fetched 45 quarterly reports from Finnhub for NKE.
2025-05-26 10:51:46,801 - root - INFO - base_client:65 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=INCOME_STATEMENT&symbol=NKE...
2025-05-26 10:52:01,802 - root - INFO - data_fetcher:47 - Fetched 81 quarterly income reports from Alpha Vantage for NKE.
2025-05-26 10:52:03,214 - root - INFO - base_client:65 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=BALANCE_SHEET&symbol=NKE...
2025-05-26 10:52:18,215 - root - INFO - data_fetcher:55 - Fetched 81 quarterly balance reports from Alpha Vantage for NKE.
2025-05-26 10:52:19,592 - root - INFO - base_client:65 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=CASH_FLOW&symbol=NKE...
2025-05-26 10:52:34,593 - root - INFO - data_fetcher:63 - Fetched 81 quarterly cash flow reports from Alpha Vantage for NKE.
2025-05-26 10:52:34,594 - root - INFO - data_fetcher:76 - Fetching key metrics and profile for NKE.
2025-05-26 10:52:35,830 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/NKE?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:52:38,186 - root - WARNING - base_client:139 - HTTP error on attempt 1/3 for GET https://financialmodelingprep.com/api/v3/key-metrics/NKE (Params: {'apikey': '62ER******61uB', 'period': 'quarterly', 'limit': 8}, Headers: {}): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-26 10:52:38,188 - root - ERROR - base_client:158 - Client error 403 (Unauthorized/Forbidden) for https://financialmodelingprep.com/api/v3/key-metrics/NKE. API key may be invalid or permissions lacking. No retry. Params: {'apikey': '62ER******61uB', 'period': 'quarterly', 'limit': 8}
2025-05-26 10:52:40,731 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=NKE&token=d0o7hphr01qqr9alj38gd0o7hphr0...
2025-05-26 10:52:42,232 - root - INFO - data_fetcher:95 - FMP KM Annual for NKE: 5. FMP KM Quarterly for NKE: 0. Finnhub Basic Financials for NKE: OK.
2025-05-26 10:52:42,234 - root - INFO - metrics_calculator:437 - Calculating derived metrics for NKE...
2025-05-26 10:52:42,235 - root - WARNING - metrics_calculator:454 - AlphaVantage overview data not found in cache for NKE. Fetching now.
2025-05-26 10:52:43,118 - root - INFO - base_client:65 - Cached response for: GET:https://www.alphavantage.co/query?apikey=HB6N4X55UTFGN2FP&function=OVERVIEW&symbol=NKE...
2025-05-26 10:52:43,120 - root - INFO - metrics_calculator:279 - Using latest quarterly revenue: 35,212,000,000 (Source: Finnhub) for NKE.
2025-05-26 10:52:43,120 - root - INFO - metrics_calculator:492 - Calculated metrics for NKE: {
  "pe_ratio": 25.306645614035087,
  "pb_ratio": 9.996388080388082,
  "ps_ratio": 1.8526,
  "ev_to_sales": 1.884,
  "ev_to_ebitda": 15.69,
  "dividend_yield": 0.015036616136056905,
  "eps": 3.76,
  "net_profit_margin": 0.0943,
  "gross_profit_margin": 0.43824503066815973,
  "operating_profit_margin": 0.1302519372,
  "interest_coverage_ratio": null,
  "roe": 0.39501039501039503,
  "roa": 0.14956704277092628,
  "roic": 0.3444796354747266,
  "debt_to_equity": 0.8282744282744283,
  "current_ratio": 2.3961106391012934,
  "quick_ratio": 1.5112810346455205,
  "debt_to_ebitda": 1.6704402515723271,
  "revenue_growth_yoy": 0.0028310912392369722,
  "eps_growth_yoy": 0.14984709480122316,
  "revenue_growth_cagr_3yr": 0.04861491256483208,
  "eps_growth_cagr_3yr": -0.009180522194930263,
  "revenue_growth_cagr_5yr": 0.08251487385466705,
  "eps_growth_cagr_5yr": 0.23239537592729897,
  "revenue_growth_qoq": 0.47065948293864596,
  "free_cash_flow_per_share": 5.6166709107885575,
  "free_cash_flow_yield": 0.07456847629026464,
  "free_cash_flow_trend": "Growing",
  "retained_earnings_trend": "Declining"
}
2025-05-26 10:52:43,125 - root - INFO - metrics_calculator:495 - Key metrics snapshot for NKE: {
  "q_revenue_source": "Finnhub",
  "latest_q_revenue": 35212000000.0,
  "avg_historical_q_revenue_for_check": 25153750000.0
}
2025-05-26 10:52:43,126 - root - INFO - dcf_analyzer:55 - Performing simplified DCF analysis for NKE...
2025-05-26 10:52:43,127 - root - INFO - dcf_analyzer:181 - DCF for NKE: Base IV/Share: 72.57, Upside: 20.92%
2025-05-26 10:52:43,127 - root - INFO - qualitative_analyzer:88 - Fetching and attempting to summarize latest 10-K for NKE
2025-05-26 10:52:44,292 - root - INFO - base_client:65 - Cached response for: GET:https://data.sec.gov/submissions/CIK0000320187.json?...
2025-05-26 10:52:44,802 - root - INFO - sec_edgar_client:127 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/320187/000032018724000044/nke-20240531.htm
2025-05-26 10:52:46,040 - root - INFO - base_client:65 - Cached response for: GET:https://www.sec.gov/Archives/edgar/data/320187/000032018724000044/nke-20240531.htm?...
2025-05-26 10:52:46,042 - root - INFO - qualitative_analyzer:114 - Fetched 10-K text (length: 2651098) for NKE. Extracting and summarizing sections.
2025-05-26 10:52:47,477 - root - INFO - qualitative_analyzer:20 - Summarizing 'Business (Item 1)' for NIKE, Inc. (NKE), original length: 10577 chars.
2025-05-26 10:52:47,477 - root - INFO - qualitative_analyzer:23 - Section length 10577 is within single-pass limit (500000). Summarizing directly.
2025-05-26 10:52:58,279 - root - INFO - qualitative_analyzer:141 - Summary for 'Business (Item 1)' (source length 10577): NIKE, Inc.'s core business is the design, development, and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories, and ...
2025-05-26 10:52:58,280 - root - INFO - qualitative_analyzer:20 - Summarizing 'Risk Factors (Item 1A)' for NIKE, Inc. (NKE), original length: 310235 chars.
2025-05-26 10:52:58,281 - root - INFO - qualitative_analyzer:23 - Section length 310235 is within single-pass limit (500000). Summarizing directly.
2025-05-26 10:53:31,946 - root - INFO - qualitative_analyzer:141 - Summary for 'Risk Factors (Item 1A)' (source length 310235): Here are 3-5 of the most significant and company-specific risk factors for NIKE, Inc., focusing on operational and strategic aspects:  1.  **Inability...
2025-05-26 10:53:31,947 - root - INFO - qualitative_analyzer:20 - Summarizing 'Management's Discussion and Analysis (Item 7)' for NIKE, Inc. (NKE), original length: 11209 chars.
2025-05-26 10:53:31,948 - root - INFO - qualitative_analyzer:23 - Section length 11209 is within single-pass limit (500000). Summarizing directly.
2025-05-26 10:53:46,980 - root - INFO - qualitative_analyzer:141 - Summary for 'Management's Discussion and Analysis (Item 7)' (source length 11209): NIKE, Inc.'s fiscal 2024 revenues increased slightly to $51.4 billion from $51.2 billion in fiscal 2023. This growth was driven by a 1% increase in NI...
2025-05-26 10:54:30,501 - root - INFO - qualitative_analyzer:199 - 10-K qualitative summaries and AI interpretations generated for NKE.
2025-05-26 10:54:30,505 - root - INFO - qualitative_analyzer:206 - Fetching and analyzing competitor data for NKE...
2025-05-26 10:54:31,491 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/peers?symbol=NKE&token=d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390...
2025-05-26 10:54:32,992 - root - INFO - qualitative_analyzer:231 - Identified peers for NKE: ['DECK', 'SKX', 'CROX', 'SHOO', 'WWW']. Fetching basic data for comparison.
2025-05-26 10:54:34,215 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/DECK?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:54:36,933 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/DECK?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61u...
2025-05-26 10:54:39,582 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=DECK&token=d0o7hphr01qqr9alj38gd0o7hphr...
2025-05-26 10:54:42,287 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/SKX?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:54:44,961 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/SKX?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:54:47,744 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=SKX&token=d0o7hphr01qqr9alj38gd0o7hphr0...
2025-05-26 10:54:50,415 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/CROX?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:54:53,090 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/CROX?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61u...
2025-05-26 10:54:55,671 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=CROX&token=d0o7hphr01qqr9alj38gd0o7hphr...
2025-05-26 10:54:58,386 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/SHOO?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:55:01,083 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/SHOO?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61u...
2025-05-26 10:55:04,008 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=SHOO&token=d0o7hphr01qqr9alj38gd0o7hphr...
2025-05-26 10:55:06,718 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/WWW?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:55:09,367 - root - INFO - base_client:65 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/WWW?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-26 10:55:11,943 - root - INFO - base_client:65 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=WWW&token=d0o7hphr01qqr9alj38gd0o7hphr0...
2025-05-26 10:55:26,368 - root - INFO - qualitative_analyzer:317 - Competitor analysis summary generated for NKE.
2025-05-26 10:55:26,369 - root - INFO - ai_synthesis:80 - Synthesizing investment thesis for NKE...
2025-05-26 10:55:41,848 - root - INFO - ai_synthesis:204 - Generated thesis for NKE. Decision: Buy, Strategy: GARP (Growth At a Reasonable Price), Confidence: High
2025-05-26 10:55:42,383 - root - INFO - stock_analyzer:279 - Successfully analyzed and saved stock data: NKE (Analysis ID: 1)
2025-05-26 10:55:47,450 - root - INFO - main:108 - --- Main script execution finished. ---
2025-05-26 10:55:47,451 - root - INFO - main:118 - Financial Analysis Script finished at 2025-05-26 07:55:47 UTC
2025-05-26 10:55:47,452 - root - INFO - main:119 - Total execution time: 0:04:20.223658
2025-05-26 10:55:47,452 - root - INFO - main:120 - ===================================================================
2025-05-26 10:55:56,723 - root - INFO - main:113 - ===================================================================
2025-05-26 10:55:56,723 - root - INFO - main:114 - Starting Financial Analysis Script at 2025-05-26 07:55:56 UTC
2025-05-26 10:55:56,724 - root - INFO - main:115 - ===================================================================
2025-05-26 10:55:56,726 - root - INFO - main:54 - --- Generating Today's Email Summary ---
2025-05-26 10:55:58,303 - root - INFO - main:61 - Found 1 stock analyses, 0 IPO analyses, 0 news analyses since 2025-05-26 00:00:00 UTC for email.
2025-05-26 10:56:00,488 - root - INFO - email_service:250 - Email sent successfully to daniprav@gmail.com
2025-05-26 10:56:00,555 - root - INFO - main:108 - --- Main script execution finished. ---
2025-05-26 10:56:00,555 - root - INFO - main:118 - Financial Analysis Script finished at 2025-05-26 07:56:00 UTC
2025-05-26 10:56:00,556 - root - INFO - main:119 - Total execution time: 0:00:03.831823
2025-05-26 10:56:00,556 - root - INFO - main:120 - ===================================================================

---------- END app_analysis.log ----------


---------- main.py ----------
# main.py
import argparse
from datetime import datetime, timezone
from sqlalchemy.orm import joinedload
import time
import sys # For sys.excepthook if enabled

from database.connection import init_db, SessionLocal
from core.logging_setup import logger, handle_global_exception # Import global handler
from services import StockAnalyzer, IPOAnalyzer, NewsAnalyzer, EmailService
from database.models import StockAnalysis, IPOAnalysis, NewsEventAnalysis
from core.config import MAX_NEWS_TO_ANALYZE_PER_RUN


def run_stock_analysis(tickers):
    logger.info(f"--- Starting Individual Stock Analysis for: {tickers} ---")
    results = []
    for ticker in tickers:
        try:
            analyzer = StockAnalyzer(ticker=ticker)
            analysis_result = analyzer.analyze()
            if analysis_result:
                results.append(analysis_result)
            else:
                logger.warning(f"Stock analysis for {ticker} did not return a result object.")
        except RuntimeError as rt_err:
            logger.error(f"Could not run stock analysis for {ticker} due to critical init error: {rt_err}")
        except Exception as e:
            logger.error(f"Error analyzing stock {ticker}: {e}", exc_info=True)
        time.sleep(5)
    return results

def run_ipo_analysis():
    logger.info("--- Starting IPO Analysis Pipeline ---")
    try:
        analyzer = IPOAnalyzer()
        results = analyzer.run_ipo_analysis_pipeline()
        return results
    except Exception as e:
        logger.error(f"Error during IPO analysis pipeline: {e}", exc_info=True)
        return []

def run_news_analysis(category="general", count_to_analyze=MAX_NEWS_TO_ANALYZE_PER_RUN):
    logger.info(f"--- Starting News Analysis Pipeline (Category: {category}, Max to Analyze: {count_to_analyze}) ---")
    try:
        analyzer = NewsAnalyzer()
        results = analyzer.run_news_analysis_pipeline(category=category, count_to_analyze_this_run=count_to_analyze)
        return results
    except Exception as e:
        logger.error(f"Error during news analysis pipeline: {e}", exc_info=True)
        return []

def generate_and_send_todays_email_summary():
    logger.info("--- Generating Today's Email Summary ---")
    db_session = SessionLocal()
    today_start_utc = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
    try:
        recent_stock_analyses = db_session.query(StockAnalysis).filter(StockAnalysis.analysis_date >= today_start_utc).options(joinedload(StockAnalysis.stock)).all()
        recent_ipo_analyses = db_session.query(IPOAnalysis).filter(IPOAnalysis.analysis_date >= today_start_utc).options(joinedload(IPOAnalysis.ipo)).all()
        recent_news_analyses = db_session.query(NewsEventAnalysis).filter(NewsEventAnalysis.analysis_date >= today_start_utc).options(joinedload(NewsEventAnalysis.news_event)).all()
        logger.info(f"Found {len(recent_stock_analyses)} stock analyses, {len(recent_ipo_analyses)} IPO analyses, {len(recent_news_analyses)} news analyses since {today_start_utc.strftime('%Y-%m-%d %H:%M:%S %Z')} for email.")
        if not any([recent_stock_analyses, recent_ipo_analyses, recent_news_analyses]):
            logger.info("No new analyses performed recently to include in the email summary.")
            return
        email_svc = EmailService()
        email_message = email_svc.create_summary_email(stock_analyses=recent_stock_analyses, ipo_analyses=recent_ipo_analyses, news_analyses=recent_news_analyses)
        if email_message:
            email_svc.send_email(email_message)
        else:
            logger.error("Failed to create the email message (returned None).")
    except Exception as e:
        logger.error(f"Error generating or sending email summary: {e}", exc_info=True)
    finally:
        SessionLocal.remove() # Ensure session is closed/removed

def main():
    parser = argparse.ArgumentParser(description="Financial Analysis and Reporting Tool")
    parser.add_argument("--analyze-stocks", nargs="+", metavar="TICKER", help="List of stock tickers to analyze (e.g., AAPL MSFT)")
    parser.add_argument("--analyze-ipos", action="store_true", help="Run IPO analysis pipeline.")
    parser.add_argument("--analyze-news", action="store_true", help="Run news analysis pipeline.")
    parser.add_argument("--news-category", default="general", help="Category for news analysis (e.g., general, forex, crypto, merger).")
    parser.add_argument("--news-count-analyze", type=int, default=MAX_NEWS_TO_ANALYZE_PER_RUN, help=f"Max number of new news items to analyze in this run (default from config: {MAX_NEWS_TO_ANALYZE_PER_RUN}).")
    parser.add_argument("--send-email", action="store_true", help="Generate and send email summary of today's/recent analyses.")
    parser.add_argument("--init-db", action="store_true", help="Initialize the database (create tables).")
    parser.add_argument("--all", action="store_true", help="Run all analyses (stocks from a predefined list, IPOs, News) and send email.")
    args = parser.parse_args()

    if args.init_db:
        logger.info("Initializing database as per command line argument...")
        try: init_db(); logger.info("Database initialization complete.")
        except Exception as e: logger.critical(f"Database initialization failed: {e}", exc_info=True); return

    if args.all:
        default_stocks_for_all = ["AAPL", "MSFT", "GOOGL", "NVDA", "JPM"]
        logger.info(f"Running all analyses for default stocks: {default_stocks_for_all}, IPOs, and News (max {args.news_count_analyze} items).")
        if default_stocks_for_all: run_stock_analysis(default_stocks_for_all)
        time.sleep(5); run_ipo_analysis(); time.sleep(5)
        run_news_analysis(category=args.news_category, count_to_analyze=args.news_count_analyze)
        time.sleep(5); generate_and_send_todays_email_summary()
        logger.info("--- '--all' tasks finished. ---"); return

    if args.analyze_stocks: run_stock_analysis(args.analyze_stocks)
    if args.analyze_ipos: run_ipo_analysis()
    if args.analyze_news: run_news_analysis(category=args.news_category, count_to_analyze=args.news_count_analyze)
    if args.send_email: generate_and_send_todays_email_summary()
    if not (args.analyze_stocks or args.analyze_ipos or args.analyze_news or args.send_email or args.init_db or args.all):
        logger.info("No action specified. Use --help for options."); parser.print_help()
    logger.info("--- Main script execution finished. ---")

if __name__ == "__main__":
    # sys.excepthook = handle_global_exception # Uncomment to enable global exception logging
    script_start_time = datetime.now(timezone.utc)
    logger.info("===================================================================")
    logger.info(f"Starting Financial Analysis Script at {script_start_time.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    logger.info("===================================================================")
    main()
    script_end_time = datetime.now(timezone.utc)
    logger.info(f"Financial Analysis Script finished at {script_end_time.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    logger.info(f"Total execution time: {script_end_time - script_start_time}")
    logger.info("===================================================================")
---------- END main.py ----------


---------- requirements.txt ----------
# requirements.txt
sqlalchemy>=1.4,<2.0
requests>=2.32.0
psycopg2-binary>=2.8.0
pandas>=1.0.0
markdown2>=2.4.0
beautifulsoup4>=4.9.3
lxml>=4.6.3 
# Add other specific versions if needed
# python-dotenv # For managing environment variables if you choose to use .env files
---------- END requirements.txt ----------

--- END OF FILE project_structure_backend.txt ---
