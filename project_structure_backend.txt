--- START OF FILE project_structure_backend.txt ---

.gitignore
api_clients.py
app_analysis.log
config.py
database.py
email_generator.py
error_handler.py
ipo_analyzer.py
main.py
models.py
news_analyzer.py
project_structure.py
requirements.txt
stock_analyzer.py


---------- .gitignore ----------
venv
/__pycache__

---------- END .gitignore ----------


---------- api_clients.py ----------
# api_clients.py
import requests
import time
import json
from datetime import datetime, timedelta, timezone  # Added timezone
from config import (
    GOOGLE_API_KEYS, FINNHUB_API_KEY, FINANCIAL_MODELING_PREP_API_KEY,
    EODHD_API_KEY, RAPIDAPI_UPCOMING_IPO_KEY, API_REQUEST_TIMEOUT,
    API_RETRY_ATTEMPTS, API_RETRY_DELAY, CACHE_EXPIRY_SECONDS
)
from error_handler import logger
from database import SessionLocal
from models import CachedAPIData

current_google_api_key_index = 0


class APIClient:
    def __init__(self, base_url, api_key_name=None, api_key_value=None, headers=None):
        self.base_url = base_url
        self.api_key_name = api_key_name
        self.api_key_value = api_key_value
        self.headers = headers or {}
        if api_key_name and api_key_value:  # For APIs that pass key as query param
            self.params = {api_key_name: api_key_value}
        else:  # For APIs that pass key in headers (like RapidAPI) or have no key for base class
            self.params = {}

    def _get_cached_response(self, request_url_or_params_str):
        session = SessionLocal()
        try:
            current_time_utc = datetime.now(timezone.utc)

            cache_entry = session.query(CachedAPIData).filter(
                CachedAPIData.request_url_or_params == request_url_or_params_str,
                CachedAPIData.expires_at > current_time_utc
            ).first()
            if cache_entry:
                logger.info(f"Cache hit for: {request_url_or_params_str}")
                return cache_entry.response_data
        except Exception as e:
            logger.error(f"Error reading from cache for '{request_url_or_params_str}': {e}", exc_info=True)
        finally:
            session.close()
        return None

    def _cache_response(self, request_url_or_params_str, response_data, api_source):
        session = SessionLocal()
        try:
            expires_at_utc = datetime.now(timezone.utc) + timedelta(seconds=CACHE_EXPIRY_SECONDS)

            session.query(CachedAPIData).filter(
                CachedAPIData.request_url_or_params == request_url_or_params_str).delete()
            session.commit()

            cache_entry = CachedAPIData(
                api_source=api_source,
                request_url_or_params=request_url_or_params_str,
                response_data=response_data,
                timestamp=datetime.now(timezone.utc),
                expires_at=expires_at_utc
            )
            session.add(cache_entry)
            session.commit()
            logger.info(f"Cached response for: {request_url_or_params_str}")
        except Exception as e:
            logger.error(f"Error writing to cache for '{request_url_or_params_str}': {e}", exc_info=True)
            session.rollback()
        finally:
            session.close()

    def request(self, method, endpoint, params=None, data=None, json_data=None, use_cache=True,
                api_source_name="unknown"):
        url = f"{self.base_url}{endpoint}"
        current_call_params = params.copy() if params else {}
        full_query_params = self.params.copy()
        full_query_params.update(current_call_params)

        sorted_params = sorted(full_query_params.items()) if full_query_params else []
        param_string = "&".join([f"{k}={v}" for k, v in sorted_params])
        cache_key_str = f"{method.upper()}:{url}?{param_string}"

        if use_cache:
            cached_data = self._get_cached_response(cache_key_str)
            if cached_data:
                return cached_data

        for attempt in range(API_RETRY_ATTEMPTS):
            try:
                response = requests.request(
                    method, url, params=full_query_params, data=data, json=json_data,
                    headers=self.headers, timeout=API_REQUEST_TIMEOUT
                )
                response.raise_for_status()

                response_json = response.json()
                if use_cache:
                    self._cache_response(cache_key_str, response_json, api_source_name)
                return response_json

            except requests.exceptions.HTTPError as e:
                log_params_for_error = {k: (v[:-6] + '******' if k == self.api_key_name and isinstance(v, str) and len(v) > 6 else v) for k,v in full_query_params.items()} # Obfuscate API key for logging
                if not full_query_params and self.headers.get("X-RapidAPI-Key"): # RapidAPI specific key obfuscation
                    log_params_for_error = {"X-RapidAPI-Key": self.headers["X-RapidAPI-Key"][-6:]}


                logger.warning(
                    f"HTTP error on attempt {attempt + 1}/{API_RETRY_ATTEMPTS} for {url} (Details: {log_params_for_error}): {e.response.status_code} - {e.response.text}")
                if e.response.status_code == 429: # Rate limit
                    logger.info(f"Rate limit hit. Waiting for {API_RETRY_DELAY * (attempt + 1)} seconds.")
                    time.sleep(API_RETRY_DELAY * (attempt + 1))
                elif 500 <= e.response.status_code < 600: # Server error
                    logger.info(f"Server error. Waiting for {API_RETRY_DELAY * (attempt + 1)} seconds.")
                    time.sleep(API_RETRY_DELAY * (attempt + 1))
                else: # Non-retryable client error (like 403 Forbidden)
                    logger.error(f"Non-retryable client error for {url}: {e.response.status_code} {e.response.reason}",
                                 exc_info=False) # Set exc_info to False for client errors unless debugging specific ones
                    return None # Critical: return None for non-retryable client errors.
            except requests.exceptions.RequestException as e:
                logger.warning(f"Request error on attempt {attempt + 1}/{API_RETRY_ATTEMPTS} for {url}: {e}")

            if attempt < API_RETRY_ATTEMPTS - 1:
                time.sleep(API_RETRY_DELAY)
            else:
                logger.error(f"All {API_RETRY_ATTEMPTS} attempts failed for {url}. Params: {full_query_params}") # Log final failure
                return None
        return None


class FinnhubClient(APIClient):
    def __init__(self):
        super().__init__("https://finnhub.io/api/v1", api_key_name="token", api_key_value=FINNHUB_API_KEY)

    def get_market_news(self, category="general", min_id=0):
        params = {"category": category}
        if min_id > 0:
            params["minId"] = min_id
        return self.request("GET", "/news", params=params, api_source_name="finnhub")

    def get_company_profile2(self, ticker):
        return self.request("GET", "/stock/profile2", params={"symbol": ticker}, api_source_name="finnhub")

    def get_financials_reported(self, ticker, freq="quarterly"):
        return self.request("GET", "/stock/financials-reported", params={"symbol": ticker, "freq": freq},
                            api_source_name="finnhub")

    def get_basic_financials(self, ticker, metric_type="all"):
        return self.request("GET", "/stock/metric", params={"symbol": ticker, "metric": metric_type},
                            api_source_name="finnhub")

    def get_ipo_calendar(self, from_date=None, to_date=None):
        # Finnhub's IPO calendar endpoint requires date ranges.
        # Default to a sensible range if not provided.
        if from_date is None:
            from_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        if to_date is None:
            to_date = (datetime.now() + timedelta(days=30)).strftime('%Y-%m-%d')
        params = {"from": from_date, "to": to_date}
        return self.request("GET", "/calendar/ipo", params=params, api_source_name="finnhub_ipo")


class FinancialModelingPrepClient(APIClient):
    def __init__(self):
        super().__init__("https://financialmodelingprep.com/api/v3", api_key_name="apikey",
                         api_key_value=FINANCIAL_MODELING_PREP_API_KEY)

    def get_ipo_calendar(self, from_date=None, to_date=None):
        # This client will likely not be used for IPOs anymore due to subscription issues
        params = {}
        if from_date: params["from"] = from_date
        if to_date: params["to"] = to_date
        # Log a warning if this is still called for IPOs
        logger.warning("FinancialModelingPrepClient.get_ipo_calendar called, but may be restricted by subscription.")
        return self.request("GET", "/ipo_calendar", params=params, api_source_name="fmp_ipo")

    def get_financial_statements(self, ticker, statement_type="income-statement", period="quarter", limit=20):
        return self.request("GET", f"/{statement_type}/{ticker}", params={"period": period, "limit": limit},
                            api_source_name="fmp")

    def get_key_metrics(self, ticker, period="quarter", limit=20):
        return self.request("GET", f"/key-metrics/{ticker}", params={"period": period, "limit": limit},
                            api_source_name="fmp")

    def get_company_profile(self, ticker):
        return self.request("GET", f"/profile/{ticker}", params={}, api_source_name="fmp")


class EODHDClient(APIClient):
    def __init__(self):
        super().__init__("https://eodhistoricaldata.com/api", api_key_name="api_token", api_key_value=EODHD_API_KEY)
        self.params["fmt"] = "json" # Default format for this client

    def get_fundamental_data(self, ticker_with_exchange):
        return self.request("GET", f"/fundamentals/{ticker_with_exchange}",
                            api_source_name="eodhd")

    def get_ipo_calendar(self, from_date=None, to_date=None):
        # This client will likely not be used for IPOs anymore due to subscription issues
        params = {}
        if from_date: params["from"] = from_date
        if to_date: params["to"] = to_date
        # Log a warning if this is still called for IPOs
        logger.warning("EODHDClient.get_ipo_calendar called, but may be restricted by subscription.")
        return self.request("GET", "/calendar/ipos", params=params, api_source_name="eodhd_ipo")


class RapidAPIUpcomingIPOCalendarClient(APIClient):
    def __init__(self):
        headers = {
            "X-RapidAPI-Key": RAPIDAPI_UPCOMING_IPO_KEY,
            "X-RapidAPI-Host": "upcoming-ipo-calendar.p.rapidapi.com"
        }
        super().__init__("https://upcoming-ipo-calendar.p.rapidapi.com", headers=headers)

    def get_ipo_calendar(self):
        # This client will likely not be used for IPOs anymore due to subscription issues
        logger.warning("RapidAPIUpcomingIPOCalendarClient.get_ipo_calendar called, but may be restricted by subscription.")
        return self.request("GET", "/ipo-calendar", params=None, api_source_name="rapidapi_ipo")


class GeminiAPIClient:
    def __init__(self):
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models"
        self.current_key_index = 0 # This will be managed per instance now for key rotation

    def _get_next_api_key_for_attempt(self, overall_attempt_num, max_attempts_per_key, total_keys):
        key_group_index = (overall_attempt_num // max_attempts_per_key) % total_keys
        api_key = GOOGLE_API_KEYS[key_group_index]
        current_retry_for_this_key = (overall_attempt_num % max_attempts_per_key) + 1
        logger.debug(
            f"Gemini: Using key ...{api_key[-4:]} (Index {key_group_index}), Attempt {current_retry_for_this_key}/{max_attempts_per_key}")
        return api_key, current_retry_for_this_key


    def generate_text(self, prompt, model="gemini-2.5-flash-preview-05-20"):
        max_attempts_per_key = API_RETRY_ATTEMPTS
        total_keys = len(GOOGLE_API_KEYS)
        if total_keys == 0:
            logger.error("Gemini: No API keys configured for Google API.")
            return "Error: No Google API keys configured."

        for overall_attempt_num in range(total_keys * max_attempts_per_key):
            api_key, current_retry_for_this_key = self._get_next_api_key_for_attempt(
                overall_attempt_num, max_attempts_per_key, total_keys
            )
            url = f"{self.base_url}/{model}:generateContent?key={api_key}"
            payload = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {
                    "temperature": 0.7,
                    "maxOutputTokens": 65536,  # Adjusted from 65536, check API limits for gemini-pro
                },
                "safetySettings": [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                ]
            }

            try:
                response = requests.post(url, json=payload, timeout=API_REQUEST_TIMEOUT + 60) # Increased timeout for Gemini
                response.raise_for_status()
                response_json = response.json()

                if "promptFeedback" in response_json and response_json["promptFeedback"].get("blockReason"):
                    block_reason = response_json["promptFeedback"]["blockReason"]
                    block_details = response_json["promptFeedback"].get("safetyRatings", "")
                    logger.error(
                        f"Gemini prompt blocked for key ...{api_key[-4:]}. Reason: {block_reason}. Details: {block_details}. Prompt snippet: '{prompt[:100]}...'")
                    # If blocked for safety, it's specific to this prompt and key combo. Loop will try next.
                    time.sleep(API_RETRY_DELAY) # Wait before next attempt (could be next key)
                    continue

                if "candidates" in response_json and response_json["candidates"]:
                    candidate = response_json["candidates"][0]
                    # Valid finish reasons: "STOP", "MAX_TOKENS", "MODEL_LENGTH" (or None if implicit stop)
                    # Other reasons like "SAFETY", "RECITATION", "OTHER" are problematic.
                    finish_reason = candidate.get("finishReason")
                    if finish_reason not in [None, "STOP", "MAX_TOKENS", "MODEL_LENGTH"]:
                        logger.warning(
                            f"Gemini candidate finished with unexpected reason: {finish_reason}. Prompt: '{prompt[:100]}...'")
                        # Depending on the reason, might need to retry or abort. For now, log and continue to extract text if possible.

                    content_part = candidate.get("content", {}).get("parts", [{}])[0]
                    if "text" in content_part:
                        return content_part["text"]
                    else:
                        logger.error(
                            f"Gemini response missing text in content part for key ...{api_key[-4:]}: {response_json}")
                else:
                    logger.error(
                        f"Gemini response malformed or missing candidates for key ...{api_key[-4:]}: {response_json}")

            except requests.exceptions.HTTPError as e:
                logger.warning(
                    f"Gemini API HTTP error for key ...{api_key[-4:]} on attempt {current_retry_for_this_key}/{max_attempts_per_key}: {e.response.status_code} - {e.response.text}. Prompt: '{prompt[:100]}...'")
                if e.response.status_code == 400: # Bad Request (often malformed prompt/payload)
                    logger.error(
                        f"Gemini API Bad Request (400). This is likely a persistent issue with the prompt/payload. Aborting Gemini for this call. Response: {e.response.text}")
                    return f"Error: Gemini API bad request (400). {e.response.text}"
                # Other HTTP errors (429, 5xx) will be retried with the next key/attempt by the loop
            except requests.exceptions.RequestException as e: # Timeout, ConnectionError
                logger.warning(
                    f"Gemini API request error for key ...{api_key[-4:]} on attempt {current_retry_for_this_key}/{max_attempts_per_key}: {e}. Prompt: '{prompt[:100]}...'")

            # Wait before next attempt (could be next retry for this key, or first retry for next key)
            if overall_attempt_num < (total_keys * max_attempts_per_key) - 1 : # If not the very last attempt of all
                 time.sleep(API_RETRY_DELAY * (current_retry_for_this_key)) # Exponential backoff for retries on same key


        logger.error(
            f"All attempts ({total_keys * max_attempts_per_key}) to call Gemini API failed for prompt: {prompt[:100]}...")
        return "Error: Could not get response from Gemini API after multiple attempts across all keys."


    def summarize_text(self, text_to_summarize, context=""):
        prompt = f"Please summarize the following text. {context}\n\nText:\n\"\"\"\n{text_to_summarize}\n\"\"\"\n\nSummary:"
        return self.generate_text(prompt)

    def analyze_sentiment(self, text_to_analyze):
        prompt = f"Analyze the sentiment of the following text. Classify it as 'Positive', 'Negative', or 'Neutral' and provide a brief explanation. Text:\n\"\"\"\n{text_to_analyze}\n\"\"\"\n\nSentiment Analysis:"
        return self.generate_text(prompt)

    def answer_question_from_text(self, text_block, question):
        prompt = f"Based on the following text, please answer the question.\n\nText:\n\"\"\"\n{text_block}\n\"\"\"\n\nQuestion: {question}\n\nAnswer:"
        return self.generate_text(prompt)

    def interpret_financial_data(self, data_description, data_points, context_prompt):
        prompt = f"Interpret the following financial data:\nDescription: {data_description}\nData: {data_points}\nContext/Question: {context_prompt}\n\nInterpretation:"
        return self.generate_text(prompt)


def get_alphavantage_data(params):
    # This function seems unused by the core logic. Placeholder.
    logger.info("AlphaVantage: Not fully implemented. Assumed no API key or using 'demo'. Free tier is limited.")
    return None


def get_tickertick_data(params):
    # This function seems unused by the core logic. Placeholder.
    logger.info("TickerTick-API appears to be a local setup. Not implemented as a cloud API client.")
    return None
---------- END api_clients.py ----------


---------- app_analysis.log ----------
2025-05-24 22:08:17,673 - root - INFO - ===================================================================
2025-05-24 22:08:17,674 - root - INFO - Starting Financial Analysis Script at 2025-05-24 22:08:17.674052
2025-05-24 22:08:17,674 - root - INFO - ===================================================================
2025-05-24 22:08:17,676 - root - INFO - Running all analyses for default stocks: ['AAPL', 'MSFT', 'GOOGL'], IPOs, and News.
2025-05-24 22:08:17,676 - root - INFO - --- Starting Individual Stock Analysis for: ['AAPL', 'MSFT', 'GOOGL'] ---
2025-05-24 22:08:19,279 - root - INFO - Stock AAPL not found in DB, creating new entry.
2025-05-24 22:08:20,722 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/AAPL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB
2025-05-24 22:08:21,152 - root - INFO - Successfully created and committed new stock entry for AAPL (ID: 1).
2025-05-24 22:08:21,241 - root - INFO - Refreshed stock entry for AAPL after creation. Company Name: Apple Inc.
2025-05-24 22:08:21,242 - root - INFO - Starting analysis for AAPL...
2025-05-24 22:08:21,243 - root - INFO - Fetching financial data for AAPL...
2025-05-24 22:08:21,331 - root - INFO - Cache hit for: GET:https://financialmodelingprep.com/api/v3/profile/AAPL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB
2025-05-24 22:08:22,812 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/AAPL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB&limit=5&period=annual
2025-05-24 22:08:24,220 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/AAPL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB&limit=5&period=annual
2025-05-24 22:08:25,633 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/AAPL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB&limit=5&period=annual
2025-05-24 22:08:27,062 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/AAPL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB&limit=5&period=annual
2025-05-24 22:08:28,775 - root - INFO - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=AAPL&token=d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390
2025-05-24 22:08:28,777 - root - INFO - Calculating metrics for AAPL...
2025-05-24 22:08:28,778 - root - INFO - Calculated metrics for AAPL: {'pe_ratio': 37.287278415656736, 'pb_ratio': 61.37243774486391, 'dividend_yield': 0.0043585983369965175, 'debt_to_equity': 1.872326602282704, 'net_profit_margin': None, 'roe': 1.6459350307287095, 'eps': 6.0836, 'revenue_growth': '2.02%', 'current_ratio': 0.8673125765340832, 'retained_earnings_trend': 'Mixed/Stable', 'free_cash_flow_trend': 'Mixed/Stable', 'interest_coverage_ratio': None}
2025-05-24 22:08:28,779 - root - INFO - Analyzing qualitative factors for AAPL using Gemini...
2025-05-24 22:08:39,598 - root - INFO - Qualitative analysis for AAPL complete.
2025-05-24 22:08:39,599 - root - INFO - Determining investment strategy for AAPL...
2025-05-24 22:08:51,175 - root - INFO - Successfully analyzed and saved stock: AAPL (Analysis ID: 1)
2025-05-24 22:08:51,428 - root - INFO - Stock MSFT not found in DB, creating new entry.
2025-05-24 22:08:52,784 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/MSFT?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB
2025-05-24 22:08:53,212 - root - INFO - Successfully created and committed new stock entry for MSFT (ID: 2).
2025-05-24 22:08:53,298 - root - INFO - Refreshed stock entry for MSFT after creation. Company Name: Microsoft Corporation
2025-05-24 22:08:53,299 - root - INFO - Starting analysis for MSFT...
2025-05-24 22:08:53,300 - root - INFO - Fetching financial data for MSFT...
2025-05-24 22:08:53,389 - root - INFO - Cache hit for: GET:https://financialmodelingprep.com/api/v3/profile/MSFT?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB
2025-05-24 22:08:54,905 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/MSFT?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB&limit=5&period=annual
2025-05-24 22:08:56,313 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/MSFT?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB&limit=5&period=annual
2025-05-24 22:08:57,729 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/MSFT?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB&limit=5&period=annual
2025-05-24 22:08:59,125 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/MSFT?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB&limit=5&period=annual
2025-05-24 22:09:00,726 - root - INFO - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=MSFT&token=d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390
2025-05-24 22:09:00,728 - root - INFO - Calculating metrics for MSFT...
2025-05-24 22:09:00,729 - root - INFO - Calculated metrics for MSFT: {'pe_ratio': 38.508221725515114, 'pb_ratio': 12.641532161041727, 'dividend_yield': 0.006414629506176681, 'debt_to_equity': 0.25002886653232864, 'net_profit_margin': None, 'roe': 0.32828137978299815, 'eps': 11.8002, 'revenue_growth': '15.67%', 'interest_coverage_ratio': 45.31822827938671, 'current_ratio': 1.2749549031815206, 'retained_earnings_trend': 'Growing', 'free_cash_flow_trend': 'Mixed/Stable'}
2025-05-24 22:09:00,730 - root - INFO - Analyzing qualitative factors for MSFT using Gemini...
2025-05-24 22:09:16,710 - root - INFO - Qualitative analysis for MSFT complete.
2025-05-24 22:09:16,711 - root - INFO - Determining investment strategy for MSFT...
2025-05-24 22:09:26,499 - root - INFO - Successfully analyzed and saved stock: MSFT (Analysis ID: 2)
2025-05-24 22:09:26,751 - root - INFO - Stock GOOGL not found in DB, creating new entry.
2025-05-24 22:09:28,159 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/GOOGL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB
2025-05-24 22:09:28,591 - root - INFO - Successfully created and committed new stock entry for GOOGL (ID: 3).
2025-05-24 22:09:28,676 - root - INFO - Refreshed stock entry for GOOGL after creation. Company Name: Alphabet Inc.
2025-05-24 22:09:28,677 - root - INFO - Starting analysis for GOOGL...
2025-05-24 22:09:28,678 - root - INFO - Fetching financial data for GOOGL...
2025-05-24 22:09:28,765 - root - INFO - Cache hit for: GET:https://financialmodelingprep.com/api/v3/profile/GOOGL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB
2025-05-24 22:09:30,270 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/GOOGL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB&limit=5&period=annual
2025-05-24 22:09:31,680 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/GOOGL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB&limit=5&period=annual
2025-05-24 22:09:33,119 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/GOOGL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB&limit=5&period=annual
2025-05-24 22:09:34,543 - root - INFO - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/GOOGL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB&limit=5&period=annual
2025-05-24 22:09:35,947 - root - INFO - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=GOOGL&token=d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390
2025-05-24 22:09:35,949 - root - INFO - Calculating metrics for GOOGL...
2025-05-24 22:09:35,950 - root - INFO - Calculated metrics for GOOGL: {'pe_ratio': 23.292381989252682, 'pb_ratio': 7.173489621144074, 'dividend_yield': 0.0031573936506584708, 'debt_to_equity': 0.07832129541903016, 'net_profit_margin': None, 'roe': 0.30797578472025694, 'eps': 7.5203, 'revenue_growth': '13.87%', 'interest_coverage_ratio': 505.2014925373134, 'current_ratio': 1.8369313974102914, 'retained_earnings_trend': 'Growing', 'free_cash_flow_trend': 'Growing'}
2025-05-24 22:09:35,951 - root - INFO - Analyzing qualitative factors for GOOGL using Gemini...
2025-05-24 22:09:50,688 - root - INFO - Qualitative analysis for GOOGL complete.
2025-05-24 22:09:50,690 - root - INFO - Determining investment strategy for GOOGL...
2025-05-24 22:10:00,148 - root - INFO - Successfully analyzed and saved stock: GOOGL (Analysis ID: 3)
2025-05-24 22:10:00,236 - root - INFO - --- Starting IPO Analysis Pipeline ---
2025-05-24 22:10:00,237 - root - INFO - Fetching upcoming IPOs using Finnhub...
2025-05-24 22:10:01,453 - root - INFO - Cached response for: GET:https://finnhub.io/api/v1/calendar/ipo?from=2025-05-17&to=2025-08-22&token=d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390
2025-05-24 22:10:01,455 - root - INFO - Successfully parsed 20 IPOs from Finnhub response.
2025-05-24 22:10:01,456 - root - INFO - Total unique IPOs fetched after deduplication: 19
2025-05-24 22:10:01,457 - root - INFO - Starting analysis for IPO: Vantage Corp (Singapore) from source Finnhub
2025-05-24 22:10:01,629 - root - INFO - Found existing IPO entry for 'Vantage Corp (Singapore)' (ID: 1). Checking for updates.
2025-05-24 22:10:01,816 - root - INFO - Re-analyzing IPO Vantage Corp (Singapore) due to significant calendar change or analysis older than one day.
2025-05-24 22:11:06,998 - root - INFO - Updating existing IPO analysis for Vantage Corp (Singapore) (ID: 1)
2025-05-24 22:11:07,565 - root - INFO - Successfully analyzed and saved IPO: Vantage Corp (Singapore) (Analysis ID: 1)
2025-05-24 22:11:10,567 - root - INFO - Starting analysis for IPO: Fast Track Group from source Finnhub
2025-05-24 22:11:10,651 - root - INFO - Found existing IPO entry for 'Fast Track Group' (ID: 2). Checking for updates.
2025-05-24 22:11:10,819 - root - INFO - Re-analyzing IPO Fast Track Group due to significant calendar change or analysis older than one day.
2025-05-24 22:12:14,864 - root - INFO - Updating existing IPO analysis for Fast Track Group (ID: 2)
2025-05-24 22:12:15,394 - root - INFO - Successfully analyzed and saved IPO: Fast Track Group (Analysis ID: 2)
2025-05-24 22:12:18,397 - root - INFO - Starting analysis for IPO: Pelican Acquisition Corp from source Finnhub
2025-05-24 22:12:18,482 - root - INFO - Found existing IPO entry for 'Pelican Acquisition Corp' (ID: 3). Checking for updates.
2025-05-24 22:12:18,651 - root - INFO - Re-analyzing IPO Pelican Acquisition Corp due to significant calendar change or analysis older than one day.
2025-05-24 22:13:21,459 - root - INFO - Updating existing IPO analysis for Pelican Acquisition Corp (ID: 3)
2025-05-24 22:13:21,977 - root - INFO - Successfully analyzed and saved IPO: Pelican Acquisition Corp (Analysis ID: 3)
2025-05-24 22:13:24,981 - root - INFO - Starting analysis for IPO: Cal Redwood Acquisition Corp. from source Finnhub
2025-05-24 22:13:25,066 - root - INFO - Found existing IPO entry for 'Cal Redwood Acquisition Corp.' (ID: 4). Checking for updates.
2025-05-24 22:13:25,235 - root - INFO - Re-analyzing IPO Cal Redwood Acquisition Corp. due to significant calendar change or analysis older than one day.
2025-05-24 22:14:25,935 - root - INFO - Updating existing IPO analysis for Cal Redwood Acquisition Corp. (ID: 4)
2025-05-24 22:14:26,459 - root - INFO - Successfully analyzed and saved IPO: Cal Redwood Acquisition Corp. (Analysis ID: 4)
2025-05-24 22:14:29,461 - root - INFO - Starting analysis for IPO: Oyster Enterprises II Acquisition Corp from source Finnhub
2025-05-24 22:14:29,547 - root - INFO - Found existing IPO entry for 'Oyster Enterprises II Acquisition Corp' (ID: 5). Checking for updates.
2025-05-24 22:14:29,718 - root - INFO - Re-analyzing IPO Oyster Enterprises II Acquisition Corp due to significant calendar change or analysis older than one day.
2025-05-24 22:15:27,560 - root - INFO - Updating existing IPO analysis for Oyster Enterprises II Acquisition Corp (ID: 5)
2025-05-24 22:15:28,081 - root - INFO - Successfully analyzed and saved IPO: Oyster Enterprises II Acquisition Corp (Analysis ID: 5)
2025-05-24 22:15:31,082 - root - INFO - Starting analysis for IPO: Hinge Health, Inc. from source Finnhub
2025-05-24 22:15:31,167 - root - INFO - Found existing IPO entry for 'Hinge Health, Inc.' (ID: 6). Checking for updates.
2025-05-24 22:15:31,335 - root - INFO - Re-analyzing IPO Hinge Health, Inc. due to significant calendar change or analysis older than one day.
2025-05-24 22:16:40,489 - root - INFO - Updating existing IPO analysis for Hinge Health, Inc. (ID: 6)
2025-05-24 22:16:41,018 - root - INFO - Successfully analyzed and saved IPO: Hinge Health, Inc. (Analysis ID: 6)
2025-05-24 22:16:44,020 - root - INFO - Starting analysis for IPO: MNTN, Inc. from source Finnhub
2025-05-24 22:16:44,106 - root - INFO - Found existing IPO entry for 'MNTN, Inc.' (ID: 7). Checking for updates.
2025-05-24 22:16:44,282 - root - INFO - Re-analyzing IPO MNTN, Inc. due to significant calendar change or analysis older than one day.
2025-05-24 22:17:47,456 - root - INFO - Updating existing IPO analysis for MNTN, Inc. (ID: 7)
2025-05-24 22:17:47,978 - root - INFO - Successfully analyzed and saved IPO: MNTN, Inc. (Analysis ID: 7)
2025-05-24 22:17:50,981 - root - INFO - Starting analysis for IPO: AParadise Acquisition Corp. from source Finnhub
2025-05-24 22:17:51,068 - root - INFO - Found existing IPO entry for 'AParadise Acquisition Corp.' (ID: 8). Checking for updates.
2025-05-24 22:17:51,239 - root - ERROR - Error during IPO analysis pipeline: can't compare offset-naive and offset-aware datetimes
Traceback (most recent call last):
  File "C:\GitHub\stock-alarm\main.py", line 35, in run_ipo_analysis
    results = analyzer.run_ipo_analysis_pipeline()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\GitHub\stock-alarm\ipo_analyzer.py", line 381, in run_ipo_analysis_pipeline
    result = self.analyze_single_ipo(ipo_data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\GitHub\stock-alarm\ipo_analyzer.py", line 236, in analyze_single_ipo
    if existing_analysis and not significant_change and existing_analysis.analysis_date > one_day_ago:
                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: can't compare offset-naive and offset-aware datetimes
2025-05-24 22:17:51,249 - root - INFO - --- Starting News Analysis Pipeline (Category: general, Count: 3) ---
2025-05-24 22:17:51,334 - root - INFO - Fetching latest market news for category: general (max 6)...
2025-05-24 22:17:52,657 - root - INFO - Cached response for: GET:https://finnhub.io/api/v1/news?category=general&token=d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390
2025-05-24 22:17:52,659 - root - INFO - Fetched 100 news items from Finnhub.
2025-05-24 22:17:52,849 - root - INFO - News event 'U.S. travelers say these are the 10 most popular summer destinations around the globe: 'People are craving relaxation'' is new. Storing and analyzing.
2025-05-24 22:17:53,273 - root - INFO - Stored new news event: U.S. travelers say these are the 10 most popular summer destinations around the globe: 'People are craving relaxation'
2025-05-24 22:17:53,275 - root - INFO - Analyzing news: U.S. travelers say these are the 10 most popular summer destinations around the globe: 'People are craving relaxation'
2025-05-24 22:18:59,018 - root - INFO - Successfully analyzed and saved news: U.S. travelers say these are the 10 most popular summer destinations around the globe: 'People are craving relaxation'
2025-05-24 22:19:01,188 - root - INFO - News event 'My wife and I are 73 and retired. Can we afford a $1.2 million home?' is new. Storing and analyzing.
2025-05-24 22:19:01,609 - root - INFO - Stored new news event: My wife and I are 73 and retired. Can we afford a $1.2 million home?
2025-05-24 22:19:01,609 - root - INFO - Analyzing news: My wife and I are 73 and retired. Can we afford a $1.2 million home?
2025-05-24 22:20:15,883 - root - INFO - Successfully analyzed and saved news: My wife and I are 73 and retired. Can we afford a $1.2 million home?
2025-05-24 22:20:18,052 - root - INFO - News event 'I published my first book at 38혰here's exactly how I changed careers to make it happen' is new. Storing and analyzing.
2025-05-24 22:20:18,472 - root - INFO - Stored new news event: I published my first book at 38혰here's exactly how I changed careers to make it happen
2025-05-24 22:20:18,474 - root - INFO - Analyzing news: I published my first book at 38혰here's exactly how I changed careers to make it happen
2025-05-24 22:21:16,792 - root - INFO - Successfully analyzed and saved news: I published my first book at 38혰here's exactly how I changed careers to make it happen
2025-05-24 22:21:18,795 - root - INFO - News analysis pipeline completed. Newly analyzed 3 items.
2025-05-24 22:21:18,796 - root - INFO - --- Generating Today's Email Summary ---
2025-05-24 22:21:19,503 - root - INFO - Found 3 stock analyses, 18 IPO analyses, 3 news analyses for today's email.
2025-05-24 22:21:25,271 - root - INFO - Email sent successfully to daniprav@gmail.com

(Warning: Read file using latin-1 encoding due to UTF-8 decode error)
---------- END app_analysis.log ----------


---------- config.py ----------
# config.py

# API Keys
# It's recommended to load sensitive keys from environment variables or a secure vault in production.
# For this script, we'll list them here as per the prompt.

GOOGLE_API_KEYS = [
    "AIzaSyDLkwkVYBTUjabShS7VfdLkQTe7vZkxcjY",
    "AIzaSyAjECAJZVZz6PzDaUVaAkgfcOeLXCPFA6Y",
    "AIzaSyBRDIgN7ffBvoqAgaizQfuWRQExKc_oVig",
    "AIzaSyC4XLSmSX4U2iuAqW_pvQ87eNyPaJwQpDo",
]

FINNHUB_API_KEY = "d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390"
FINANCIAL_MODELING_PREP_API_KEY = "62ERGmJoqQgGD0nSGxRZS91TVzfz61uB"
EODHD_API_KEY = "683079df749c42.21476005" # Note: EODHD often requires 'demo' or your actual key for API calls.
RAPIDAPI_UPCOMING_IPO_KEY = "0bd9b5144cmsh50c0e6d95c0b662p1cbdefjsn2d1cb0104cde"

# Database Configuration
# Changed "postgres://" to "postgresql://"
DATABASE_URL = "postgresql://avnadmin:AVNS_IeMYS-rv46Au9xqkza2@pg-4d810ff-daxiake-7258.d.aivencloud.com:26922/stock-alarm?sslmode=require"

# Email Configuration (Placeholder - replace with actual SMTP details or email service API)
EMAIL_HOST = "smtp-relay.brevo.com"
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_HOST_USER = "8dca1d001@smtp-brevo.com"
EMAIL_HOST_PASSWORD = "VrNUkDdcR5G9AL8P"
EMAIL_SENDER = "testypesty54@gmail.com"
EMAIL_RECIPIENT = "daniprav@gmail.com"  # The user who receives the summary

# Logging Configuration
LOG_FILE_PATH = "app_analysis.log"
LOG_LEVEL = "INFO" # DEBUG, INFO, WARNING, ERROR, CRITICAL

# API Client Settings
API_REQUEST_TIMEOUT = 30  # seconds
API_RETRY_ATTEMPTS = 3
API_RETRY_DELAY = 5  # seconds

# Analysis Settings
# Example: Define how many news articles to fetch
MAX_NEWS_ARTICLES_PER_QUERY = 10
# Example: Minimum market cap for considering a stock (if applicable)
MIN_MARKET_CAP = 1000000000 # 1 Billion

# Path to store cached API responses if we implement file-based caching as a fallback
CACHE_DIR = "api_cache"
CACHE_EXPIRY_SECONDS = 3600 # 1 hour for general data, financial statements might be longer

# Current Google API Key Index (for rotation)
# This will be managed by the API client, but initialized here or in a state file if persistence between runs is needed without a DB
# For simplicity, let's assume the api_client module handles this in memory for a single run.
# If the script runs frequently and independently, this might need to be stored in the DB or a file.
CURRENT_GOOGLE_API_KEY_INDEX = 0

---------- END config.py ----------


---------- database.py ----------
# database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, scoped_session
from config import DATABASE_URL
from error_handler import logger

try:
    engine = create_engine(DATABASE_URL)
    SessionLocal = scoped_session(sessionmaker(autocommit=False, autoflush=False, bind=engine))
    Base = declarative_base()
    Base.query = SessionLocal.query_property() # For easier querying

    def init_db():
        """Initializes the database and creates tables if they don't exist."""
        try:
            logger.info("Initializing database and creating tables...")
            # Import all modules here that define models so that
            # they are registered with the Base meta-data
            import models # noqa
            Base.metadata.create_all(bind=engine)
            logger.info("Database tables created successfully (if they didn't exist).")
        except Exception as e:
            logger.error(f"Error initializing database: {e}", exc_info=True)
            raise

    def get_db_session():
        """Provides a database session."""
        db = SessionLocal()
        try:
            yield db
        finally:
            db.close()

except Exception as e:
    logger.error(f"Failed to connect to database or setup SQLAlchemy: {e}", exc_info=True)
    # Optionally, re-raise or exit if DB is critical
    raise
---------- END database.py ----------


---------- email_generator.py ----------
# email_generator.py
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from config import EMAIL_HOST, EMAIL_PORT, EMAIL_USE_TLS, EMAIL_HOST_USER, EMAIL_HOST_PASSWORD, EMAIL_SENDER, \
    EMAIL_RECIPIENT
from error_handler import logger
from models import StockAnalysis, IPOAnalysis, NewsEventAnalysis
from datetime import datetime
import json
from markdown2 import Markdown  # Import the markdown library


class EmailGenerator:
    def __init__(self):
        self.markdowner = Markdown()  # Create a Markdown converter instance

    def _md_to_html(self, md_text):
        """Converts a markdown string to HTML. Handles None input."""
        if md_text is None:
            return "<p>N/A</p>"  # Or an empty string, or some other placeholder
        # Basic check to see if it's already HTML (e.g. if AI starts returning HTML directly)
        if md_text.strip().startswith("<") and md_text.strip().endswith(">"):
            return md_text
        return self.markdowner.convert(md_text)

    def _format_stock_analysis_html(self, analysis: StockAnalysis):
        if not analysis: return ""
        stock = analysis.stock
        # Apply markdown conversion to fields that might contain markdown from Gemini
        reasoning_html = self._md_to_html(analysis.reasoning)
        economic_moat_html = self._md_to_html(analysis.economic_moat_summary)
        industry_trends_html = self._md_to_html(analysis.industry_trends_summary)
        management_assessment_html = self._md_to_html(analysis.management_assessment_summary)

        html = f"""
        <div class="analysis-block">
            <h2>Stock Analysis: {stock.company_name} ({stock.ticker})</h2>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M')}</p>
            <p><strong>Decision:</strong> {analysis.investment_decision}</p>
            <p><strong>Strategy Type:</strong> {analysis.strategy_type}</p>

            <details>
                <summary><strong>Reasoning & AI Synthesis (Click to expand)</strong></summary>
                <div class="markdown-content">{reasoning_html}</div>
            </details>

            <details>
                <summary><strong>Key Financial Metrics (Click to expand)</strong></summary>
                <ul>
                    <li>P/E Ratio: {analysis.pe_ratio if analysis.pe_ratio is not None else 'N/A'}</li>
                    <li>P/B Ratio: {analysis.pb_ratio if analysis.pb_ratio is not None else 'N/A'}</li>
                    <li>EPS: {analysis.eps if analysis.eps is not None else 'N/A'}</li>
                    <li>ROE: {f"{analysis.roe * 100:.2f}%" if analysis.roe is not None else 'N/A'}</li>
                    <li>Dividend Yield: {f"{analysis.dividend_yield * 100:.2f}%" if analysis.dividend_yield is not None else 'N/A'}</li>
                    <li>Debt-to-Equity: {analysis.debt_to_equity if analysis.debt_to_equity is not None else 'N/A'}</li>
                    <li>Interest Coverage Ratio: {analysis.interest_coverage_ratio if analysis.interest_coverage_ratio is not None else 'N/A'}</li>
                    <li>Current Ratio: {analysis.current_ratio if analysis.current_ratio is not None else 'N/A'}</li>
                    <li>Net Profit Margin: {f"{analysis.net_profit_margin * 100:.2f}%" if analysis.net_profit_margin is not None else 'N/A'}</li>
                    <li>Revenue Growth (YoY): {analysis.revenue_growth}</li>
                    <li>Retained Earnings Trend: {analysis.retained_earnings_trend}</li>
                    <li>Free Cash Flow Trend: {analysis.free_cash_flow_trend}</li>
                </ul>
                <p><em>Raw data points used for metrics:</em></p>
                <pre>{json.dumps(analysis.key_metrics_snapshot, indent=2) if analysis.key_metrics_snapshot else 'N/A'}</pre>
            </details>

            <details>
                <summary><strong>Qualitative Analysis (Click to expand)</strong></summary>
                <p><strong>Economic Moat:</strong></p>
                <div class="markdown-content">{economic_moat_html}</div>
                <p><strong>Industry Trends:</strong></p>
                <div class="markdown-content">{industry_trends_html}</div>
                <p><strong>Management Assessment (General Factors):</strong></p>
                <div class="markdown-content">{management_assessment_html}</div>
                <p><em>Context for qualitative prompts:</em></p>
                <pre>{json.dumps(analysis.qualitative_sources, indent=2) if analysis.qualitative_sources else 'N/A'}</pre>
            </details>
        </div>
        """
        return html

    def _format_ipo_analysis_html(self, analysis: IPOAnalysis):
        if not analysis: return ""
        ipo = analysis.ipo
        # Apply markdown conversion
        reasoning_html = self._md_to_html(analysis.reasoning)
        business_model_html = self._md_to_html(analysis.business_model_summary)
        competitive_landscape_html = self._md_to_html(analysis.competitive_landscape_summary)
        industry_health_html = self._md_to_html(analysis.industry_health_summary)
        use_of_proceeds_html = self._md_to_html(analysis.use_of_proceeds_summary)
        risk_factors_html = self._md_to_html(analysis.risk_factors_summary)
        pre_ipo_financials_html = self._md_to_html(analysis.pre_ipo_financials_summary)
        # valuation_comparison_summary is likely also markdown
        valuation_comparison_html = self._md_to_html(analysis.valuation_comparison_summary)

        html = f"""
        <div class="analysis-block">
            <h2>IPO Analysis: {ipo.company_name}</h2>
            <p><strong>Expected IPO Date:</strong> {ipo.ipo_date}</p>
            <p><strong>Expected Price Range:</strong> {ipo.expected_price_range}</p>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M')}</p>
            <p><strong>Preliminary Stance:</strong> {analysis.investment_decision}</p>

            <details>
                <summary><strong>AI Synthesized Reasoning & Key Verification Points (Click to expand)</strong></summary>
                <div class="markdown-content">{reasoning_html}</div>
            </details>

            <details>
                <summary><strong>IPO Details & Summaries (Click to expand)</strong></summary>
                <p><strong>Business Model:</strong></p><div class="markdown-content">{business_model_html}</div>
                <p><strong>Competitive Landscape:</strong></p><div class="markdown-content">{competitive_landscape_html}</div>
                <p><strong>Industry Health:</strong></p><div class="markdown-content">{industry_health_html}</div>
                <p><strong>Use of Proceeds (General):</strong></p><div class="markdown-content">{use_of_proceeds_html}</div>
                <p><strong>Risk Factors (General):</strong></p><div class="markdown-content">{risk_factors_html}</div>
                <p><strong>Pre-IPO Financials (Guidance):</strong></p><div class="markdown-content">{pre_ipo_financials_html}</div>
                <p><strong>Valuation Comparison (Guidance):</strong></p><div class="markdown-content">{valuation_comparison_html}</div>
                <p><strong>Underwriter Quality:</strong> {analysis.underwriter_quality}</p>
                <p><strong>Fresh Issue vs OFS:</strong> {analysis.fresh_issue_vs_ofs}</p>
                <p><strong>Lock-up Periods:</strong> {analysis.lock_up_periods_info}</p>
                <p><strong>Investor Demand:</strong> {analysis.investor_demand_summary}</p>
                <p><em>Raw data from IPO calendar API:</em></p>
                <pre>{json.dumps(analysis.key_data_snapshot, indent=2) if analysis.key_data_snapshot else 'N/A'}</pre>
            </details>
        </div>
        """
        return html

    def _format_news_event_analysis_html(self, analysis: NewsEventAnalysis):
        if not analysis: return ""
        news_event = analysis.news_event
        # Apply markdown conversion
        summary_email_html = self._md_to_html(analysis.summary_for_email)
        scope_relevance_html = self._md_to_html(analysis.scope_relevance)
        # affected_stocks_sectors is JSON, handle its 'text_analysis' part if it contains markdown
        affected_stocks_text_analysis = analysis.affected_stocks_sectors.get('text_analysis',
                                                                             'N/A') if analysis.affected_stocks_sectors else 'N/A'
        affected_stocks_html = self._md_to_html(affected_stocks_text_analysis)
        mechanism_of_impact_html = self._md_to_html(analysis.mechanism_of_impact)
        estimated_timing_html = self._md_to_html(analysis.estimated_timing)
        estimated_magnitude_direction_html = self._md_to_html(analysis.estimated_magnitude_direction)
        countervailing_factors_html = self._md_to_html(analysis.countervailing_factors)

        html = f"""
        <div class="analysis-block">
            <h2>News/Event Analysis: {news_event.event_title}</h2>
            <p><strong>Event Date:</strong> {news_event.event_date.strftime('%Y-%m-%d %H:%M') if news_event.event_date else 'N/A'}</p>
            <p><strong>Source:</strong> <a href="{news_event.source_url}">{news_event.source_url}</a></p>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M')}</p>
            <p><strong>Investor Summary:</strong></p>
            <div class="markdown-content">{summary_email_html}</div>

            <details>
                <summary><strong>Detailed Analysis (Click to expand)</strong></summary>
                <p><strong>Scope & Relevance:</strong></p>
                <div class="markdown-content">{scope_relevance_html}</div>
                <p><strong>Affected Stocks/Sectors (AI Analysis & API):</strong></p>
                <div class="markdown-content">
                    <p><em>API Related:</em> {analysis.affected_stocks_sectors.get('api_related', 'N/A') if analysis.affected_stocks_sectors else 'N/A'}</p>
                    <p><em>AI Analysis:</em></p>
                    {affected_stocks_html}
                </div>
                <p><strong>Mechanism of Impact:</strong></p>
                <div class="markdown-content">{mechanism_of_impact_html}</div>
                <p><strong>Estimated Timing & Duration:</strong></p>
                <div class="markdown-content">{estimated_timing_html}</div>
                <p><strong>Estimated Magnitude & Direction:</strong></p>
                <div class="markdown-content">{estimated_magnitude_direction_html}</div>
                <p><strong>Countervailing Factors:</strong></p>
                <div class="markdown-content">{countervailing_factors_html}</div>
                <p><em>Key snippets used for analysis:</em></p>
                <pre>{json.dumps(analysis.key_news_snippets, indent=2) if analysis.key_news_snippets else 'N/A'}</pre>
            </details>
        </div>
        """
        return html

    def create_summary_email(self, stock_analyses=None, ipo_analyses=None, news_analyses=None):
        if not any([stock_analyses, ipo_analyses, news_analyses]):
            logger.info("No analyses provided to create an email.")
            return None

        subject_date = datetime.now().strftime("%Y-%m-%d")
        subject = f"Financial Analysis Summary - {subject_date}"

        # Added .markdown-content style
        html_body = """
        <html>
            <head>
                <style>
                    body { font-family: Arial, sans-serif; margin: 0; padding: 20px; background-color: #f4f4f4; line-height: 1.6; }
                    .container { background-color: #ffffff; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }
                    .analysis-block { border: 1px solid #ddd; padding: 15px; margin-bottom: 20px; border-radius: 5px; background-color: #f9f9f9; }
                    h1 { color: #333; }
                    h2 { color: #555; border-bottom: 1px solid #eee; padding-bottom: 5px;}
                    details > summary { cursor: pointer; font-weight: bold; margin-bottom: 10px; color: #0056b3; }
                    pre { background-color: #eee; padding: 10px; border-radius: 4px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; font-size: 0.9em; }
                    ul { list-style-type: disc; margin-left: 20px; }
                    li { margin-bottom: 5px; }
                    .markdown-content { padding: 5px 0; }
                    .markdown-content p { margin: 0.5em 0; } /* Add some margin to paragraphs generated from markdown */
                    .markdown-content ul, .markdown-content ol { margin-left: 20px; }
                    .markdown-content strong { font-weight: bold; }
                    .markdown-content em { font-style: italic; }
                    .markdown-content h1, .markdown-content h2, .markdown-content h3 { margin-top: 1em; margin-bottom: 0.5em; color: #444; }
                </style>
            </head>
            <body>
                <div class="container">
                    <h1>Financial Analysis Report</h1>
                    <p><em>This email contains automated analysis. Always do your own research before making investment decisions.</em></p>
        """

        if stock_analyses:
            html_body += "<h2>Individual Stock Analyses</h2>"
            for sa in stock_analyses:
                html_body += self._format_stock_analysis_html(sa)

        if ipo_analyses:
            html_body += "<h2>Upcoming IPO Analyses</h2>"
            for ia in ipo_analyses:
                html_body += self._format_ipo_analysis_html(ia)

        if news_analyses:
            html_body += "<h2>Recent News & Event Analyses</h2>"
            for na in news_analyses:
                html_body += self._format_news_event_analysis_html(na)

        html_body += """
                </div>
            </body>
        </html>
        """

        msg = MIMEMultipart('alternative')
        msg['Subject'] = subject
        msg['From'] = EMAIL_SENDER
        msg['To'] = EMAIL_RECIPIENT

        msg.attach(MIMEText(html_body, 'html'))
        return msg

    def send_email(self, message: MIMEMultipart):
        if not message:
            logger.error("No message object provided to send_email.")
            return False
        try:
            with smtplib.SMTP(EMAIL_HOST, EMAIL_PORT) as server:
                if EMAIL_USE_TLS:
                    server.starttls()
                server.login(EMAIL_HOST_USER, EMAIL_HOST_PASSWORD)
                server.sendmail(EMAIL_SENDER, EMAIL_RECIPIENT, message.as_string())
            logger.info(f"Email sent successfully to {EMAIL_RECIPIENT}")
            return True
        except Exception as e:
            logger.error(f"Failed to send email: {e}", exc_info=True)
            return False


# Example Usage (remains the same, but output HTML will be richer):
if __name__ == '__main__':
    logger.info("Starting email generator test...")


    class MockStock:
        def __init__(self, ticker, company_name):
            self.ticker = ticker
            self.company_name = company_name


    class MockStockAnalysis:
        def __init__(self, stock):
            self.stock = stock
            self.analysis_date = datetime.now()
            self.investment_decision = "Hold"
            self.strategy_type = "Value"
            self.reasoning = "The stock appears **fairly valued** with *moderate* growth prospects. AI suggests `monitoring`."
            self.pe_ratio = 15.5;
            self.pb_ratio = 1.2;
            self.eps = 2.5;
            self.roe = 0.16
            self.dividend_yield = 0.025;
            self.debt_to_equity = 0.5;
            self.interest_coverage_ratio = 5
            self.current_ratio = 2.0;
            self.net_profit_margin = 0.10;
            self.revenue_growth = "5.00%"
            self.retained_earnings_trend = "Growing";
            self.free_cash_flow_trend = "Stable"
            self.key_metrics_snapshot = {"price": 100, "marketCap": 1000000000}
            self.economic_moat_summary = "Moderate moat due to **brand recognition** but *increasing competition*."
            self.industry_trends_summary = "Industry is mature with `slow growth`. Shift towards **new tech** is a key trend."
            self.management_assessment_summary = "Management team seems **stable**. Need to check *insider activity*."
            self.qualitative_sources = {"description_snippet": "A leading company in its sector..."}


    mock_sa = MockStockAnalysis(MockStock("MOCK", "MockCorp"))

    email_gen = EmailGenerator()
    email_message = email_gen.create_summary_email(stock_analyses=[mock_sa])

    if email_message:
        logger.info("Email message created. To actually send, uncomment send_email and configure SMTP in config.py")

        with open("test_email_summary_markdown_rendered.html", "w", encoding="utf-8") as f:
            f.write(email_message.get_payload(0).get_payload(decode=True).decode())
        logger.info("Test email HTML saved to test_email_summary_markdown_rendered.html")
    else:
        logger.error("Failed to create email message.")
---------- END email_generator.py ----------


---------- error_handler.py ----------
# error_handler.py
import logging
import sys
from config import LOG_FILE_PATH, LOG_LEVEL

def setup_logging():
    """Configures logging for the application."""
    numeric_level = getattr(logging, LOG_LEVEL.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f"Invalid log level: {LOG_LEVEL}")

    logger = logging.getLogger()
    logger.setLevel(numeric_level)

    # Create handlers
    console_handler = logging.StreamHandler(sys.stdout)
    file_handler = logging.FileHandler(LOG_FILE_PATH)

    # Create formatters and add it to handlers
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    # Add handlers to the logger
    # Avoid adding handlers multiple times if setup_logging is called more than once.
    if not logger.handlers:
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)
    elif not any(isinstance(h, logging.FileHandler) and h.baseFilename == file_handler.baseFilename for h in logger.handlers):
        # Add file handler if not already present (e.g., for subsequent calls in an interactive session)
        logger.addHandler(file_handler)
    if not any(isinstance(h, logging.StreamHandler) for h in logger.handlers):
        logger.addHandler(console_handler)


    return logger

# Initialize logger when this module is imported
logger = setup_logging()

def handle_exception(exc_type, exc_value, exc_traceback):
    """Custom exception handler to log unhandled exceptions."""
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    logger.error("Unhandled exception", exc_info=(exc_type, exc_value, exc_traceback))

# Set the custom exception hook
# sys.excepthook = handle_exception # You might enable this in main.py if desired for top-level unhandled exceptions
---------- END error_handler.py ----------


---------- ipo_analyzer.py ----------
# ipo_analyzer.py
import time
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timedelta  # Ensure timedelta is imported
from api_clients import FinnhubClient, GeminiAPIClient
from database import SessionLocal, get_db_session
from models import IPO, IPOAnalysis
from error_handler import logger
from sqlalchemy.exc import SQLAlchemyError


class IPOAnalyzer:
    def __init__(self):
        self.finnhub = FinnhubClient()  # Primary IPO data source
        self.gemini = GeminiAPIClient()
        self.db_session = next(get_db_session())

    def fetch_upcoming_ipos(self):
        logger.info("Fetching upcoming IPOs using Finnhub...")
        ipos_data_to_process = []  # Renamed to avoid confusion with the final list
        today = datetime.now()
        from_date = (today - timedelta(days=7)).strftime('%Y-%m-%d')
        to_date = (today + timedelta(days=90)).strftime('%Y-%m-%d')

        finnhub_response = self.finnhub.get_ipo_calendar(from_date=from_date, to_date=to_date)
        actual_ipo_list = []

        if finnhub_response and isinstance(finnhub_response, dict) and "ipoCalendar" in finnhub_response:
            actual_ipo_list = finnhub_response["ipoCalendar"]
            if not isinstance(actual_ipo_list, list):
                logger.warning(f"Finnhub response 'ipoCalendar' field is not a list. Found: {type(actual_ipo_list)}")
                actual_ipo_list = []  # Ensure it's an empty list if not a list
            elif not actual_ipo_list:  # It's an empty list
                logger.info("Finnhub 'ipoCalendar' list is empty for the current period.")
        elif finnhub_response is None:
            logger.warning("Failed to fetch IPOs from Finnhub (API call failed or returned None).")
            # actual_ipo_list remains []
        else:
            logger.info(f"No IPOs found or unexpected format from Finnhub. Response type: {type(finnhub_response)}")
            # actual_ipo_list remains []

        if actual_ipo_list:
            for ipo in actual_ipo_list:
                price_range_raw = ipo.get(
                    "price")  # Finnhub gives price as string like "18.00-20.00" or just "10.00" or null
                price_low, price_high = "N/A", "N/A"

                if isinstance(price_range_raw, str):
                    if '-' in price_range_raw:
                        parts = price_range_raw.split('-', 1)
                        price_low = parts[0].strip()
                        price_high = parts[1].strip() if len(parts) > 1 else price_low  # Handle cases like "10.00-"
                    elif price_range_raw:  # Non-empty string, assume fixed price
                        price_low = price_range_raw.strip()
                        price_high = price_range_raw.strip()
                elif isinstance(price_range_raw, (float, int)):  # If price is a number
                    price_low = str(price_range_raw)
                    price_high = str(price_range_raw)

                ipos_data_to_process.append({
                    "name": ipo.get("name"),
                    "symbol": ipo.get("symbol"),
                    "date": ipo.get("date"),
                    "price_range_low": price_low,
                    "price_range_high": price_high,
                    "exchange": ipo.get("exchange"),
                    "status": ipo.get("status"),
                    "number_of_shares": ipo.get("numberOfShares"),
                    "total_shares_value": ipo.get("totalSharesValue"),
                    "source": "Finnhub",
                    "raw_data": ipo
                })
            logger.info(f"Successfully parsed {len(ipos_data_to_process)} IPOs from Finnhub response.")
        # Deduplication
        unique_ipos = []
        seen_names_or_symbols = set()
        for ipo_info in ipos_data_to_process:
            key = ipo_info.get("name") or ipo_info.get("symbol")  # Use name as primary key, symbol as fallback
            if key and key not in seen_names_or_symbols:
                unique_ipos.append(ipo_info)
                seen_names_or_symbols.add(key)
            elif not key:
                logger.warning(f"IPO data missing 'name' or 'symbol', cannot reliably deduplicate: {ipo_info}")
            else:  # Log if duplicate is found (though Finnhub data itself is usually clean)
                logger.debug(
                    f"Duplicate IPO based on name/symbol '{key}' found during deduplication pass, skipping: {ipo_info.get('name')}")

        logger.info(f"Total unique IPOs fetched after deduplication: {len(unique_ipos)}")
        return unique_ipos

    def _get_or_create_ipo_entry(self, ipo_data):
        if not self.db_session.is_active:
            logger.warning(
                f"Session for IPO {ipo_data.get('name')} in _get_or_create_ipo_entry was inactive. Re-establishing.")
            try:
                self.db_session.close()
            except:
                pass
            self.db_session = next(get_db_session())

        ipo_identifier_name = ipo_data.get("name")
        ipo_identifier_symbol = ipo_data.get("symbol")

        if not ipo_identifier_name and not ipo_identifier_symbol:
            logger.error(f"Cannot get or create IPO entry, 'name' and 'symbol' are missing in ipo_data: {ipo_data}")
            return None

        # Prefer matching by symbol if available and name is also present for update scenarios
        # If symbol exists, it's a stronger unique identifier usually.
        ipo_entry = None
        if ipo_identifier_symbol:
            ipo_entry = self.db_session.query(IPO).filter(IPO.symbol == ipo_identifier_symbol).first()

        if not ipo_entry and ipo_identifier_name:  # If not found by symbol, or symbol was None, try by name
            ipo_entry = self.db_session.query(IPO).filter(IPO.company_name == ipo_identifier_name).first()

        if not ipo_entry:
            log_id_for_create = ipo_identifier_name or ipo_identifier_symbol
            logger.info(f"IPO '{log_id_for_create}' not found in DB, creating new entry.")
            price_low = ipo_data.get('price_range_low', "N/A")
            price_high = ipo_data.get('price_range_high', "N/A")
            price_range_str = f"{price_low} - {price_high}" if price_low != "N/A" or price_high != "N/A" else "N/A"
            if price_low == price_high and price_low != "N/A":
                price_range_str = price_low

            ipo_entry = IPO(
                company_name=ipo_identifier_name,
                symbol=ipo_identifier_symbol,
                ipo_date=str(ipo_data.get("date", "N/A")),
                expected_price_range=price_range_str,
                exchange=ipo_data.get("exchange"),
                status=ipo_data.get("status")
            )
            self.db_session.add(ipo_entry)
            try:
                self.db_session.commit()
                self.db_session.refresh(ipo_entry)
                logger.info(f"Created and refreshed IPO entry for '{log_id_for_create}' (ID: {ipo_entry.id})")
            except SQLAlchemyError as e:
                self.db_session.rollback()
                logger.error(f"Error creating IPO entry for '{log_id_for_create}': {e}", exc_info=True)
                return None
        else:
            log_id_for_update = ipo_entry.company_name or ipo_entry.symbol
            logger.info(
                f"Found existing IPO entry for '{log_id_for_update}' (ID: {ipo_entry.id}). Checking for updates.")
            updated = False
            if ipo_identifier_name and ipo_entry.company_name != ipo_identifier_name:  # Update name if provided and different
                ipo_entry.company_name = ipo_identifier_name
                updated = True
            if ipo_identifier_symbol and ipo_entry.symbol != ipo_identifier_symbol:  # Update symbol if provided and different
                ipo_entry.symbol = ipo_identifier_symbol
                updated = True
            if ipo_data.get("date") and ipo_entry.ipo_date != str(ipo_data.get("date")):
                ipo_entry.ipo_date = str(ipo_data.get("date"))
                updated = True

            price_low = ipo_data.get('price_range_low', "N/A")
            price_high = ipo_data.get('price_range_high', "N/A")
            new_price_range_str = f"{price_low} - {price_high}" if price_low != "N/A" or price_high != "N/A" else "N/A"
            if price_low == price_high and price_low != "N/A": new_price_range_str = price_low

            if ipo_entry.expected_price_range != new_price_range_str:
                ipo_entry.expected_price_range = new_price_range_str
                updated = True
            if ipo_data.get("exchange") and ipo_entry.exchange != ipo_data.get("exchange"):
                ipo_entry.exchange = ipo_data.get("exchange")
                updated = True
            if ipo_data.get("status") and ipo_entry.status != ipo_data.get("status"):
                ipo_entry.status = ipo_data.get("status")
                updated = True

            if updated:
                try:
                    self.db_session.commit()
                    self.db_session.refresh(ipo_entry)
                    logger.info(f"Updated existing IPO entry for '{log_id_for_update}' (ID: {ipo_entry.id})")
                except SQLAlchemyError as e:
                    self.db_session.rollback()
                    logger.error(f"Error updating IPO entry for '{log_id_for_update}': {e}", exc_info=True)
        return ipo_entry

    def analyze_single_ipo(self, ipo_data_from_fetch):
        ipo_name = ipo_data_from_fetch.get("name")
        ipo_symbol = ipo_data_from_fetch.get("symbol")
        ipo_identifier = ipo_name or ipo_symbol

        if not ipo_identifier:
            logger.error(f"Cannot analyze IPO, 'name' or 'symbol' is missing: {ipo_data_from_fetch}")
            return None
        logger.info(f"Starting analysis for IPO: {ipo_identifier} from source {ipo_data_from_fetch.get('source')}")

        if not self.db_session.is_active:
            logger.warning(f"Session for IPO {ipo_identifier} in analyze_single_ipo was inactive. Re-establishing.")
            try:
                self.db_session.close()
            except:
                pass
            self.db_session = next(get_db_session())

        ipo_db_entry = self._get_or_create_ipo_entry(ipo_data_from_fetch)
        if not ipo_db_entry:
            logger.error(f"Could not get or create DB entry for IPO {ipo_identifier}. Aborting analysis.")
            return None

        instance_state = sa_inspect(ipo_db_entry)
        if not instance_state.session or instance_state.session is not self.db_session:
            logger.warning(f"IPO DB entry {ipo_identifier} is not bound to the current session. Merging.")
            try:
                ipo_db_entry = self.db_session.merge(ipo_db_entry)
                self.db_session.flush()
            except Exception as e_merge:
                logger.error(f"Failed to merge IPO {ipo_identifier} into session: {e_merge}. Aborting.", exc_info=True)
                return None

        seven_days_ago = datetime.utcnow() - timedelta(days=7)
        existing_analysis = self.db_session.query(IPOAnalysis) \
            .filter(IPOAnalysis.ipo_id == ipo_db_entry.id) \
            .filter(IPOAnalysis.analysis_date >= seven_days_ago) \
            .order_by(IPOAnalysis.analysis_date.desc()) \
            .first()

        significant_change = False
        if existing_analysis and ipo_db_entry:  # Check ipo_db_entry for current calendar data
            # Compare current calendar data with stored snapshot in analysis
            calendar_status = ipo_data_from_fetch.get("status")
            calendar_price = ipo_data_from_fetch.get("price")  # This is the raw price string/float from API

            analyzed_snapshot_status = existing_analysis.key_data_snapshot.get("status")
            analyzed_snapshot_price = existing_analysis.key_data_snapshot.get("price")

            if calendar_status != analyzed_snapshot_status or calendar_price != analyzed_snapshot_price:
                significant_change = True

        one_day_ago = datetime.utcnow() - timedelta(days=1)
        if existing_analysis and not significant_change and existing_analysis.analysis_date > one_day_ago:
            logger.info(
                f"Recent analysis for IPO {ipo_identifier} exists (ID: {existing_analysis.id}, Date: {existing_analysis.analysis_date}) and no significant calendar changes detected. Skipping Gemini re-analysis.")
            return existing_analysis
        elif existing_analysis:
            logger.info(
                f"Re-analyzing IPO {ipo_identifier} due to significant calendar change or analysis older than one day.")

        analysis_payload = {"key_data_snapshot": ipo_data_from_fetch.get("raw_data", {})}
        company_name_for_prompt = ipo_db_entry.company_name or "This company"
        symbol_for_prompt = f" with proposed ticker {ipo_db_entry.symbol}" if ipo_db_entry.symbol else ""

        company_description_for_prompt = (
            f"The company is named '{company_name_for_prompt}'{symbol_for_prompt}."
            f" It is expected to IPO around {ipo_db_entry.ipo_date} on the {ipo_db_entry.exchange} exchange."
            f" Current status: {ipo_db_entry.status}."
            f" Expected price or price range: {ipo_db_entry.expected_price_range}."
        )

        prompt1_text = (
            f"For the upcoming IPO of '{ipo_identifier}', given: {company_description_for_prompt}\n"
            f"Describe its likely business model based on its name and general industry knowledge. "
            f"Also, briefly outline the competitive landscape it likely operates in and the general health of that industry. "
            f"Focus on what can be inferred generally, as prospectus details are not provided here."
        )
        response1 = self.gemini.generate_text(prompt1_text)
        parts = response1.split("Competitive Landscape:") if response1 and not response1.startswith("Error:") else [
            "N/A"]
        analysis_payload["business_model_summary"] = parts[0].replace("Business Model:", "").strip()
        if len(parts) > 1:
            sub_parts = parts[1].split("Industry Health:")
            analysis_payload["competitive_landscape_summary"] = sub_parts[0].strip()
            if len(sub_parts) > 1:
                analysis_payload["industry_health_summary"] = sub_parts[1].strip()
            else:
                analysis_payload["industry_health_summary"] = "N/A (or see competitive landscape)"
        else:
            analysis_payload["competitive_landscape_summary"] = "N/A (or see business model)"
            analysis_payload["industry_health_summary"] = "N/A (or see business model)"

        prompt2_text = (
            f"For a company like '{ipo_identifier}' in its likely industry (as inferred above), "
            f"what are typical uses of IPO proceeds? "
            f"And what are common major risk factors generally disclosed in prospectuses for such companies? Keep it general."
        )
        response2 = self.gemini.generate_text(prompt2_text)
        parts2 = response2.split("Risk Factors:") if response2 and not response2.startswith("Error:") else ["N/A"]
        analysis_payload["use_of_proceeds_summary"] = "General (Not from Prospectus): " + parts2[0].replace(
            "Typical Uses of IPO Proceeds:", "").strip()
        if len(parts2) > 1:
            analysis_payload["risk_factors_summary"] = "General (Not from Prospectus): " + parts2[1].strip()
        else:
            analysis_payload["risk_factors_summary"] = "N/A (or see use of proceeds)"

        prompt3_text = (
            f"What key financial health indicators (e.g., revenue growth trends, path to profitability, cash burn rate) should an investor typically look for in the S-1 prospectus of a company like '{ipo_identifier}'? "
            f"How would one generally approach valuing such an IPO against its public peers (e.g., relevant ratios like P/S if it's a growth company, or P/E if profitable)? Be general."
        )
        analysis_payload["pre_ipo_financials_summary"] = "Guidance (General Approach): " + self.gemini.generate_text(
            prompt3_text)
        analysis_payload[
            "valuation_comparison_summary"] = "Guidance (General Approach): " + "Valuation typically involves comparing to publicly traded peers using metrics like Price/Sales for growth companies, or Price/Earnings if profitable. Discounted Cash Flow (DCF) models may also be used if sufficient financial history and projections are available. Specifics depend on the S-1 filing."

        analysis_payload[
            "underwriter_quality"] = "Underwriters: N/A from Finnhub calendar. Check S-1 filing. (Note: Quality assessment requires research on their track record)."
        analysis_payload[
            "fresh_issue_vs_ofs"] = "N/A from Finnhub calendar. Check S-1 filing for details on primary vs. secondary share offerings."
        analysis_payload[
            "lock_up_periods_info"] = "N/A from Finnhub calendar. Check S-1 filing (typically 90-180 days for insiders)."
        analysis_payload[
            "investor_demand_summary"] = "To be assessed closer to IPO date from news reports on anchor investors, oversubscription levels, and grey market premiums, if available. Not in Finnhub calendar."

        synthesis_prompt = (
            f"Synthesize a brief, cautious investment perspective (max 3-4 sentences) for the IPO of '{ipo_identifier}'. "
            f"Context: {company_description_for_prompt}\n"
            f"General Business Model Idea: {analysis_payload.get('business_model_summary', 'N/A')[:150]}...\n"
            f"General Industry Outlook: {analysis_payload.get('industry_health_summary', 'N/A')[:150]}...\n"
            f"General Risks for such IPOs: {analysis_payload.get('risk_factors_summary', 'N/A')[:150]}...\n"
            f"What are 2-3 *critical* things an investor MUST verify in the actual S-1 prospectus before considering this IPO? "
            f"Suggest a preliminary stance (e.g., 'Potentially interesting, S-1 review critical', 'Approach with significant caution until S-1 verified', 'High risk/reward, depends heavily on S-1 specifics'). "
            f"Do not give financial advice. This is for general informational purposes to guide further research."
        )
        gemini_synthesis = self.gemini.generate_text(synthesis_prompt)

        final_decision = "Research Further / Cautious"
        if gemini_synthesis and not gemini_synthesis.startswith("Error:"):
            if "interesting" in gemini_synthesis.lower() or "potential" in gemini_synthesis.lower() and "critical" not in gemini_synthesis.lower():
                final_decision = "Potentially Interesting / S-1 Review Critical"
            elif "caution" in gemini_synthesis.lower() or "high risk" in gemini_synthesis.lower() or "skeptical" in gemini_synthesis.lower():
                final_decision = "High Caution / Skeptical Pending S-1"

        analysis_payload["investment_decision"] = final_decision
        analysis_payload["reasoning"] = gemini_synthesis

        if existing_analysis and (significant_change or existing_analysis.analysis_date <= one_day_ago):
            logger.info(f"Updating existing IPO analysis for {ipo_identifier} (ID: {existing_analysis.id})")
            ipo_analysis_entry = existing_analysis
            ipo_analysis_entry.analysis_date = datetime.utcnow()
            for key, value in analysis_payload.items():
                setattr(ipo_analysis_entry, key, value)
        else:
            logger.info(f"Creating new IPO analysis entry for {ipo_identifier}")
            ipo_analysis_entry = IPOAnalysis(
                ipo_id=ipo_db_entry.id,
                **analysis_payload
            )
            self.db_session.add(ipo_analysis_entry)

        ipo_db_entry.last_analysis_date = datetime.utcnow()

        try:
            self.db_session.commit()
            logger.info(f"Successfully analyzed and saved IPO: {ipo_identifier} (Analysis ID: {ipo_analysis_entry.id})")
        except SQLAlchemyError as e:
            self.db_session.rollback()
            logger.error(f"Database error saving IPO analysis for {ipo_identifier}: {e}", exc_info=True)
            return None
        time.sleep(1)
        return ipo_analysis_entry

    def run_ipo_analysis_pipeline(self):
        all_upcoming_ipos = self.fetch_upcoming_ipos()
        analyzed_ipos_results = []
        if not all_upcoming_ipos:
            logger.info("No upcoming IPOs found from Finnhub to analyze.")
            if self.db_session.is_active: self.db_session.close()
            return []

        for ipo_data in all_upcoming_ipos:
            if not self.db_session.is_active:
                self.db_session = next(get_db_session())

            status = ipo_data.get("status", "").lower()
            # Broaden accepted statuses to include those that might still be relevant for analysis
            # 'filed' is important, 'expected' is good, 'priced' means it just happened or is about to.
            # 'upcoming' can be a generic term.
            relevant_statuses = ["expected", "filed", "priced", "upcoming", "active"]
            if status not in relevant_statuses:
                logger.info(f"Skipping IPO '{ipo_data.get('name')}' with status '{status}'.")
                continue

            if not ipo_data.get("name") and not ipo_data.get("symbol"):  # Must have at least one identifier
                logger.warning(f"Skipping IPO due to missing name and symbol: {ipo_data}")
                continue

            result = self.analyze_single_ipo(ipo_data)
            if result:
                analyzed_ipos_results.append(result)
            time.sleep(2)

        logger.info(f"IPO analysis pipeline completed. Analyzed/Updated {len(analyzed_ipos_results)} IPOs.")
        if self.db_session.is_active:
            self.db_session.close()
        return analyzed_ipos_results


if __name__ == '__main__':
    from database import init_db

    # from datetime import timedelta # Already imported at top

    try:
        # init_db()
        logger.info("Starting standalone IPO analysis pipeline test...")
        analyzer = IPOAnalyzer()
        results = analyzer.run_ipo_analysis_pipeline()
        if results:
            logger.info(f"Processed {len(results)} IPOs.")
            for res in results:
                ipo_info = res.ipo
                logger.info(
                    f"IPO: {ipo_info.company_name} ({ipo_info.symbol}), Decision: {res.investment_decision}, Date: {ipo_info.ipo_date}, Status: {ipo_info.status}")
        else:
            logger.info("No IPOs were processed or found by Finnhub.")
    except Exception as e:
        logger.error(f"Error during IPO analysis test in __main__: {e}", exc_info=True)
---------- END ipo_analyzer.py ----------


---------- main.py ----------
# main.py
import argparse
from datetime import datetime
from database import init_db, get_db_session
from error_handler import logger  # , setup_logging, handle_exception # uncomment if sys.excepthook is used here
# import sys # uncomment if sys.excepthook is used here

from stock_analyzer import StockAnalyzer
from ipo_analyzer import IPOAnalyzer
from news_analyzer import NewsAnalyzer
from email_generator import EmailGenerator
from models import StockAnalysis, IPOAnalysis, NewsEventAnalysis  # For querying results


# sys.excepthook = handle_exception # Optional: for top-level unhandled exception logging

def run_stock_analysis(tickers):
    logger.info(f"--- Starting Individual Stock Analysis for: {tickers} ---")
    results = []
    for ticker in tickers:
        try:
            analyzer = StockAnalyzer(ticker=ticker)
            analysis_result = analyzer.analyze()
            if analysis_result:
                results.append(analysis_result)
        except Exception as e:
            logger.error(f"Error analyzing stock {ticker}: {e}", exc_info=True)
    return results


def run_ipo_analysis():
    logger.info("--- Starting IPO Analysis Pipeline ---")
    try:
        analyzer = IPOAnalyzer()
        results = analyzer.run_ipo_analysis_pipeline()
        return results
    except Exception as e:
        logger.error(f"Error during IPO analysis pipeline: {e}", exc_info=True)
        return []


def run_news_analysis(category="general", count=5):
    logger.info(f"--- Starting News Analysis Pipeline (Category: {category}, Count: {count}) ---")
    try:
        analyzer = NewsAnalyzer()
        results = analyzer.run_news_analysis_pipeline(category=category, count=count)
        return results
    except Exception as e:
        logger.error(f"Error during news analysis pipeline: {e}", exc_info=True)
        return []


def generate_and_send_todays_email_summary():
    logger.info("--- Generating Today's Email Summary ---")
    db_session = next(get_db_session())
    today_start = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)

    try:
        # Fetch analyses performed today (or recent ones)
        # This assumes analyses are being run and stored before this function is called.
        # If analyses are run *by* this function, then pass results directly.

        # For this example, let's assume we want to send an email for analyses
        # that were just run in the current script execution.
        # If we want to fetch from DB based on date:
        recent_stock_analyses = db_session.query(StockAnalysis).filter(StockAnalysis.analysis_date >= today_start).all()
        recent_ipo_analyses = db_session.query(IPOAnalysis).filter(IPOAnalysis.analysis_date >= today_start).all()
        recent_news_analyses = db_session.query(NewsEventAnalysis).filter(
            NewsEventAnalysis.analysis_date >= today_start).all()

        logger.info(
            f"Found {len(recent_stock_analyses)} stock analyses, {len(recent_ipo_analyses)} IPO analyses, {len(recent_news_analyses)} news analyses for today's email.")

        if not any([recent_stock_analyses, recent_ipo_analyses, recent_news_analyses]):
            logger.info("No new analyses performed today to include in the email summary.")
            return

        email_gen = EmailGenerator()
        email_message = email_gen.create_summary_email(
            stock_analyses=recent_stock_analyses,
            ipo_analyses=recent_ipo_analyses,
            news_analyses=recent_news_analyses
        )

        if email_message:
            # In a real scenario, uncomment to send. Ensure SMTP settings in config.py are correct.
            email_gen.send_email(email_message)
            # logger.info("Email summary created. Sending is commented out in main.py for safety.")
            # For testing, save to file:
            # with open(f"daily_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html", "w", encoding="utf-8") as f:
            #     f.write(email_message.get_payload(0).get_payload(decode=True).decode())  # type: ignore
            # logger.info("Email HTML saved to a file.")
        else:
            logger.error("Failed to create the email message.")

    except Exception as e:
        logger.error(f"Error generating or sending email summary: {e}", exc_info=True)
    finally:
        db_session.close()


def main():
    parser = argparse.ArgumentParser(description="Financial Analysis and Reporting Tool")
    parser.add_argument("--analyze-stocks", nargs="+", metavar="TICKER",
                        help="List of stock tickers to analyze (e.g., AAPL MSFT)")
    parser.add_argument("--analyze-ipos", action="store_true", help="Run IPO analysis pipeline.")
    parser.add_argument("--analyze-news", action="store_true", help="Run news analysis pipeline.")
    parser.add_argument("--news-category", default="general",
                        help="Category for news analysis (e.g., general, forex, crypto, merger).")
    parser.add_argument("--news-count", type=int, default=3, help="Number of new news items to analyze.")
    parser.add_argument("--send-email", action="store_true",
                        help="Generate and send email summary of today's analyses.")
    parser.add_argument("--init-db", action="store_true", help="Initialize the database (create tables).")
    parser.add_argument("--all", action="store_true",
                        help="Run all analyses (stocks from a predefined list, IPOs, News) and send email. Define stock list below.")

    args = parser.parse_args()

    if args.init_db:
        logger.info("Initializing database as per command line argument...")
        try:
            init_db()
            logger.info("Database initialization complete.")
        except Exception as e:
            logger.error(f"Database initialization failed: {e}", exc_info=True)
            return  # Stop if DB init fails

    # --- Execution Logic ---
    stock_analysis_results = []
    ipo_analysis_results = []
    news_analysis_results = []

    if args.all:
        # Define a default list of stocks for '--all' flag
        default_stocks_for_all = ["AAPL", "MSFT", "GOOGL"]  # Example list
        logger.info(f"Running all analyses for default stocks: {default_stocks_for_all}, IPOs, and News.")
        stock_analysis_results = run_stock_analysis(default_stocks_for_all)
        ipo_analysis_results = run_ipo_analysis()
        news_analysis_results = run_news_analysis(category=args.news_category, count=args.news_count)
        # The generate_and_send_todays_email_summary will pick these up if run on the same day
        # Or, you can pass these results directly if you modify generate_and_send_todays_email_summary
        generate_and_send_todays_email_summary()  # This will fetch from DB based on today's date
        return

    if args.analyze_stocks:
        stock_analysis_results = run_stock_analysis(args.analyze_stocks)
        # If only analyzing stocks and not sending a full summary, perhaps print to console or save differently.
        # For now, results are stored in DB.

    if args.analyze_ipos:
        ipo_analysis_results = run_ipo_analysis()

    if args.analyze_news:
        news_analysis_results = run_news_analysis(category=args.news_category, count=args.news_count)

    if args.send_email:
        # This function currently fetches from DB based on today's date.
        # So, it will include any analyses run prior in the same script execution if they were committed.
        generate_and_send_todays_email_summary()

    if not (
            args.analyze_stocks or args.analyze_ipos or args.analyze_news or args.send_email or args.init_db or args.all):
        logger.info("No action specified. Use --help for options.")
        parser.print_help()

    logger.info("--- Script execution finished. ---")


if __name__ == "__main__":
    # Setup logging (error_handler.py already does this when imported, but explicit call is fine too)
    # logger = setup_logging() # Ensures logger is configured if not already

    logger.info("===================================================================")
    logger.info(f"Starting Financial Analysis Script at {datetime.now()}")
    logger.info("===================================================================")
    main()
---------- END main.py ----------


---------- models.py ----------
# models.py
from sqlalchemy import Column, Integer, String, Float, DateTime, Text, JSON, ForeignKey, Boolean, Date # Added Date
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from database import Base


class Stock(Base):
    __tablename__ = "stocks"
    id = Column(Integer, primary_key=True, index=True)
    ticker = Column(String, unique=True, index=True, nullable=False)
    company_name = Column(String)
    last_analysis_date = Column(DateTime(timezone=True), server_default=func.now())

    analyses = relationship("StockAnalysis", back_populates="stock")


class StockAnalysis(Base):
    __tablename__ = "stock_analyses"
    id = Column(Integer, primary_key=True, index=True)
    stock_id = Column(Integer, ForeignKey("stocks.id"), nullable=False)
    analysis_date = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    # Fundamental Analysis
    pe_ratio = Column(Float, nullable=True)
    pb_ratio = Column(Float, nullable=True)
    eps = Column(Float, nullable=True)
    roe = Column(Float, nullable=True)
    dividend_yield = Column(Float, nullable=True)
    debt_to_equity = Column(Float, nullable=True)
    interest_coverage_ratio = Column(Float, nullable=True)
    current_ratio = Column(Float, nullable=True)
    retained_earnings_trend = Column(String, nullable=True)
    revenue_growth = Column(String, nullable=True)
    net_profit_margin = Column(Float, nullable=True)
    free_cash_flow_trend = Column(String, nullable=True)

    # Qualitative Analysis
    economic_moat_summary = Column(Text, nullable=True)
    industry_trends_summary = Column(Text, nullable=True)
    management_assessment_summary = Column(Text, nullable=True)

    # Investment Strategy/Conclusion
    investment_decision = Column(String, nullable=True)
    reasoning = Column(Text, nullable=True)
    strategy_type = Column(String, nullable=True)
    key_metrics_snapshot = Column(JSON, nullable=True)
    qualitative_sources = Column(JSON, nullable=True)

    stock = relationship("Stock", back_populates="analyses")


class IPO(Base):
    __tablename__ = "ipos"
    id = Column(Integer, primary_key=True, index=True)
    company_name = Column(String, index=True, nullable=False) # Not unique, as re-filings might occur or different sources
    symbol = Column(String, index=True, nullable=True) # Proposed or actual ticker
    ipo_date = Column(String, nullable=True) # Store as string for flexibility, or Date for strictness
    expected_price_range = Column(String, nullable=True)
    exchange = Column(String, nullable=True)
    status = Column(String, nullable=True) # e.g., expected, filed, priced, withdrawn
    last_analysis_date = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    analyses = relationship("IPOAnalysis", back_populates="ipo")

    # Consider adding a unique constraint if symbol + date or name + date should be unique
    # from sqlalchemy import UniqueConstraint
    # __table_args__ = (UniqueConstraint('company_name', 'ipo_date', name='uq_company_ipo_date'),)


class IPOAnalysis(Base):
    __tablename__ = "ipo_analyses"
    id = Column(Integer, primary_key=True, index=True)
    ipo_id = Column(Integer, ForeignKey("ipos.id"), nullable=False)
    analysis_date = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    business_model_summary = Column(Text, nullable=True)
    competitive_landscape_summary = Column(Text, nullable=True)
    industry_health_summary = Column(Text, nullable=True)
    use_of_proceeds_summary = Column(Text, nullable=True)
    risk_factors_summary = Column(Text, nullable=True)
    pre_ipo_financials_summary = Column(Text, nullable=True)
    valuation_comparison_summary = Column(Text, nullable=True)
    underwriter_quality = Column(String, nullable=True)
    fresh_issue_vs_ofs = Column(String, nullable=True)
    lock_up_periods_info = Column(String, nullable=True)
    investor_demand_summary = Column(Text, nullable=True)

    investment_decision = Column(String, nullable=True)
    reasoning = Column(Text, nullable=True)
    key_data_snapshot = Column(JSON, nullable=True)

    ipo = relationship("IPO", back_populates="analyses")


class NewsEvent(Base):
    __tablename__ = "news_events"
    id = Column(Integer, primary_key=True, index=True)
    event_title = Column(String, index=True)
    event_date = Column(DateTime(timezone=True), nullable=True)
    source_url = Column(String, unique=True)
    category = Column(String)
    processed_date = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    analyses = relationship("NewsEventAnalysis", back_populates="news_event")


class NewsEventAnalysis(Base):
    __tablename__ = "news_event_analyses"
    id = Column(Integer, primary_key=True, index=True)
    news_event_id = Column(Integer, ForeignKey("news_events.id"), nullable=False)
    analysis_date = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    affected_stocks_sectors = Column(JSON)
    scope_relevance = Column(Text, nullable=True)
    mechanism_of_impact = Column(Text, nullable=True)
    estimated_timing = Column(String, nullable=True)
    estimated_magnitude_direction = Column(String, nullable=True)
    countervailing_factors = Column(Text, nullable=True)

    summary_for_email = Column(Text, nullable=True)
    key_news_snippets = Column(JSON, nullable=True)

    news_event = relationship("NewsEvent", back_populates="analyses")


class CachedAPIData(Base):
    __tablename__ = "cached_api_data"
    id = Column(Integer, primary_key=True, index=True)
    api_source = Column(String, index=True, nullable=False)
    request_url_or_params = Column(String, unique=True, nullable=False)
    response_data = Column(JSON, nullable=False)
    timestamp = Column(DateTime(timezone=True), server_default=func.now()) # Removed onupdate for timestamp
    expires_at = Column(DateTime(timezone=True), nullable=False)
---------- END models.py ----------


---------- news_analyzer.py ----------
# news_analyzer.py
import time
from api_clients import FinnhubClient, GeminiAPIClient
from database import SessionLocal, get_db_session
from models import NewsEvent, NewsEventAnalysis, Stock  # To link news to stocks
from error_handler import logger
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.orm import joinedload
from datetime import datetime
from config import MAX_NEWS_ARTICLES_PER_QUERY


class NewsAnalyzer:
    def __init__(self):
        self.finnhub = FinnhubClient()
        self.gemini = GeminiAPIClient()
        self.db_session = next(get_db_session())

    def fetch_market_news(self, category="general", count=MAX_NEWS_ARTICLES_PER_QUERY):
        logger.info(f"Fetching latest market news for category: {category} (max {count})...")
        # Finnhub's /news endpoint doesn't have a 'count' param, it returns recent news.
        # We might need to paginate or filter if we need more than one batch.
        # For now, one call and take top 'count'.
        news_items = self.finnhub.get_market_news(category=category)

        if news_items and isinstance(news_items, list):
            logger.info(f"Fetched {len(news_items)} news items from Finnhub.")
            return news_items[:count]
        else:
            logger.error(f"Failed to fetch news or received unexpected format: {news_items}")
            return []

    def _get_or_create_news_event(self, news_item_from_api):
        # Assuming news_item_from_api is a dict from Finnhub like:
        # {'category': 'business', 'datetime': 1600000000, 'headline': '...',
        #  'id': 123, 'image': '...', 'related': 'AAPL', 'source': 'Reuters', 'summary': '...', 'url': '...'}

        source_url = news_item_from_api.get("url")
        if not source_url:
            logger.warning(f"News item missing URL, cannot use as unique ID: {news_item_from_api.get('headline')}")
            return None  # Cannot reliably deduplicate without a URL or persistent ID

        event = self.db_session.query(NewsEvent).filter_by(source_url=source_url).first()
        if event:
            logger.info(f"News event already processed: {source_url}")
            return event  # Already processed

        event_datetime = datetime.fromtimestamp(news_item_from_api.get("datetime")) if news_item_from_api.get(
            "datetime") else datetime.utcnow()

        event = NewsEvent(
            event_title=news_item_from_api.get("headline"),
            event_date=event_datetime,
            source_url=source_url,
            category=news_item_from_api.get("category"),
            # related_symbols can be stored if needed, news_item_from_api.get("related") often gives one symbol
        )
        self.db_session.add(event)
        try:
            self.db_session.commit()
            logger.info(f"Stored new news event: {event.event_title}")
            return event
        except SQLAlchemyError as e:
            self.db_session.rollback()
            logger.error(f"Database error storing news event {event.event_title}: {e}", exc_info=True)
            return None

    def analyze_single_news_item(self, news_item_data, existing_event_db_id=None):
        """
        Analyzes a single piece of news.
        news_item_data: dict from Finnhub API.
        existing_event_db_id: If the NewsEvent DB entry already exists.
        """
        if existing_event_db_id:
            news_event_db = self.db_session.query(NewsEvent).get(existing_event_db_id)
        else:  # Should have been created before calling this if it's new
            logger.error("analyze_single_news_item called without a valid news_event_db reference.")
            return None

        if not news_event_db:
            logger.error(f"News event with ID {existing_event_db_id} not found for analysis.")
            return None

        headline = news_event_db.event_title
        summary = news_item_data.get("summary", "")  # Finnhub summary
        news_content_for_analysis = f"Headline: {headline}\nSummary: {summary}"
        # In a more advanced system, you'd fetch full article content from news_event_db.source_url using a browsing tool.
        # For now, we use headline + API summary.

        logger.info(f"Analyzing news: {headline}")
        analysis_payload = {"key_news_snippets": {"headline": headline, "api_summary": summary}}

        # Step 1 & 2: Identify/Categorize & Scope/Relevance (partially done by Finnhub, Gemini can elaborate)
        prompt_scope = (
            f"News: \"{news_content_for_analysis}\"\n\n"
            f"1. Categorize this news more specifically (e.g., earnings, product launch, macroeconomic shift, regulatory change, M&A).\n"
            f"2. What is the potential scope (broad market, specific sectors, few companies) and direct relevance of this news?"
        )
        scope_relevance_text = self.gemini.generate_text(prompt_scope)
        analysis_payload["scope_relevance"] = scope_relevance_text
        # TODO: Parse category from scope_relevance_text if needed to refine NewsEvent.category

        # Step 3: Identify Potentially Affected Stocks/Sectors
        # Finnhub 'related' field gives one symbol. Gemini can find more.
        related_symbols_api = news_item_data.get("related", "")  # e.g., "AAPL" or "MSFT,GOOGL"

        prompt_affected = (
            f"News: \"{news_content_for_analysis}\"\n\n"
            f"Initial related symbol(s) from API: '{related_symbols_api}'.\n"
            f"Besides these, what other specific companies (by ticker symbol if possible) or sectors are likely to be "
            f"significantly affected by this news, and why briefly? "
            f"Think about direct competitors, key suppliers/customers, or companies in related industries."
        )
        affected_text = self.gemini.generate_text(prompt_affected)
        # This text needs parsing to extract tickers and sectors. For now, store as text.
        # A more robust solution would involve named entity recognition for tickers.
        analysis_payload["affected_stocks_sectors"] = {"text_analysis": affected_text,
                                                       "api_related": related_symbols_api}

        # Step 4: Analyze the Mechanism of Impact
        prompt_mechanism = (
            f"News: \"{news_content_for_analysis}\"\n\n"
            f"For the primary company/sector identified ({related_symbols_api} or as determined from context), "
            f"how will this news likely affect its fundamentals (revenue, costs, profitability, growth) or market perception? "
            f"Explain the mechanism."
        )
        analysis_payload["mechanism_of_impact"] = self.gemini.generate_text(prompt_mechanism)

        # Step 5 & 6: Estimate Timing, Duration, Magnitude, Direction
        prompt_timing_magnitude = (
            f"News: \"{news_content_for_analysis}\"\n\n"
            f"Estimate the likely timing (immediate, short-term, medium-term, long-term) and duration of the impact. "
            f"Also, estimate the potential magnitude (small, medium, large) and direction (positive, negative, neutral/mixed) of the impact "
            f"on the primary affected entities."
        )
        timing_magnitude_text = self.gemini.generate_text(prompt_timing_magnitude)
        # Simple split, Gemini might format differently
        parts_tm = timing_magnitude_text.split("Magnitude and Direction:")  # Heuristic split
        analysis_payload["estimated_timing"] = parts_tm[0].replace("Timing and Duration:", "").strip()
        if len(parts_tm) > 1:
            analysis_payload["estimated_magnitude_direction"] = parts_tm[1].strip()
        else:  # If split fails, put all text in timing
            analysis_payload["estimated_magnitude_direction"] = "See timing section or N/A"

        # Step 7: Countervailing Factors (General prompt)
        prompt_counter = (
            f"News: \"{news_content_for_analysis}\"\n\n"
            f"What are potential countervailing factors or broader market sentiments that might moderate or amplify the impact of this news?"
        )
        analysis_payload["countervailing_factors"] = self.gemini.generate_text(prompt_counter)

        # Summary for email
        prompt_summary_email = (
            f"News: \"{news_content_for_analysis}\"\n\n"
            f"Primary affected: {analysis_payload['affected_stocks_sectors']['text_analysis']}\n"
            f"Likely Impact: Mechanism: {analysis_payload['mechanism_of_impact'][:200]}... Direction/Magnitude: {analysis_payload.get('estimated_magnitude_direction', 'N/A')}\n\n"
            f"Provide a concise 2-3 sentence summary of this news and its most critical implication for an investor."
        )
        analysis_payload["summary_for_email"] = self.gemini.generate_text(prompt_summary_email)

        # Store analysis
        news_analysis_entry = NewsEventAnalysis(
            news_event_id=news_event_db.id,
            scope_relevance=analysis_payload.get("scope_relevance"),
            affected_stocks_sectors=analysis_payload.get("affected_stocks_sectors"),
            mechanism_of_impact=analysis_payload.get("mechanism_of_impact"),
            estimated_timing=analysis_payload.get("estimated_timing"),
            estimated_magnitude_direction=analysis_payload.get("estimated_magnitude_direction"),
            countervailing_factors=analysis_payload.get("countervailing_factors"),
            summary_for_email=analysis_payload.get("summary_for_email"),
            key_news_snippets=analysis_payload.get("key_news_snippets")
        )
        self.db_session.add(news_analysis_entry)
        news_event_db.processed_date = datetime.utcnow()

        try:
            self.db_session.commit()
            logger.info(f"Successfully analyzed and saved news: {headline}")
        except SQLAlchemyError as e:
            self.db_session.rollback()
            logger.error(f"Database error saving news analysis for {headline}: {e}", exc_info=True)
            return None

        return news_analysis_entry

    def run_news_analysis_pipeline(self, category="general", count=5):
        """ Fetches news, checks if already processed, analyzes if new, saves."""
        fetched_news_items = self.fetch_market_news(category=category,
                                                    count=count * 2)  # Fetch more to allow for already processed
        if not fetched_news_items:
            logger.info("No news items fetched.")
            return []

        analyzed_news_results = []
        processed_count = 0

        for news_item in fetched_news_items:
            if processed_count >= count:
                break  # Reached desired number of *newly* analyzed items

            # Check if news event exists by URL
            news_event_db_entry = self.db_session.query(NewsEvent).filter_by(source_url=news_item.get("url")).options(
                joinedload(NewsEvent.analyses)).first()

            is_new_analysis_needed = False
            if news_event_db_entry:
                if not news_event_db_entry.analyses:  # Exists but not analyzed
                    logger.info(
                        f"News event '{news_item.get('headline')}' found in DB but not analyzed yet. Analyzing now.")
                    is_new_analysis_needed = True
                else:
                    logger.info(f"News event '{news_item.get('headline')}' already analyzed. Skipping.")
                    # Could add logic to re-analyze if old, but for now, skip if any analysis exists.
            else:  # News event not in DB at all
                logger.info(f"News event '{news_item.get('headline')}' is new. Storing and analyzing.")
                news_event_db_entry = self._get_or_create_news_event(news_item)  # This commits the NewsEvent
                if news_event_db_entry:
                    is_new_analysis_needed = True

            if is_new_analysis_needed and news_event_db_entry:
                # Re-acquire session if it was closed or became inactive
                if not self.db_session.is_active: self.db_session = next(get_db_session())  # type: ignore

                analysis_result = self.analyze_single_news_item(news_item, existing_event_db_id=news_event_db_entry.id)
                if analysis_result:
                    analyzed_news_results.append(analysis_result)
                    processed_count += 1
                time.sleep(2)  # Courtesy delay for Gemini API calls
            elif not news_event_db_entry and not is_new_analysis_needed:  # Failed to create entry
                logger.warning(f"Skipping news item due to failure in DB handling: {news_item.get('headline')}")

        logger.info(f"News analysis pipeline completed. Newly analyzed {len(analyzed_news_results)} items.")
        if self.db_session.is_active:  # type: ignore
            self.db_session.close()
        return analyzed_news_results


# Example usage:
if __name__ == '__main__':
    from database import init_db

    try:
        init_db()
        logger.info("Starting standalone news analysis pipeline test...")
        analyzer = NewsAnalyzer()
        # Analyze 2 new general news items
        results = analyzer.run_news_analysis_pipeline(category="general", count=2)
        if results:
            logger.info(f"Processed {len(results)} new news items.")
            for res in results:
                logger.info(f"News: {res.news_event.event_title}, Summary: {res.summary_for_email}")
        else:
            logger.info("No new news items were processed.")
    except Exception as e:
        logger.error(f"Error during news analysis test: {e}", exc_info=True)

---------- END news_analyzer.py ----------


---------- project_structure.py ----------
# project_to_file_backend.py
import os
import sys
from pathlib import Path

# --- Configuration ---

# Directories to exclude from both structure and content
EXCLUDED_DIRS = [
    '__pycache__',
    '.git',
    '.venv',        # Common virtual environment names
    'venv',
    'env',
    '.env',         # Exclude the .env file itself if it contains secrets
    'build',
    'dist',
    '*.egg-info',   # Python packaging artifacts
    '.pytest_cache',
    '.mypy_cache',
    '.vscode',
    '.idea',
    'node_modules', # If you happen to have node_modules
    'migrations',   # Often contains auto-generated code, review if needed
    'alembic'       # Alembic directory
]

# Specific files to exclude
EXCLUDED_FILES = [
    '.DS_Store',
    '*.pyc',
    '*.pyo',
    '*.pyd',
    '.env',         # Explicitly exclude again just in case
    'project_to_file_backend.py', # Exclude this script itself
    'project_structure_backend.txt', # Exclude the output file
    # Add any other specific files like local config overrides
]

# File extensions to treat as binary/media (content won't be included)
# Add more as needed (e.g., .jpg, .gif, .mp4, .db, .sqlite)
BINARY_EXTENSIONS = [
    '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.webp', '.ico',
    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
    '.zip', '.tar', '.gz', '.rar',
    '.mp3', '.wav', '.ogg',
    '.mp4', '.avi', '.mov', '.wmv',
    '.db', '.sqlite', '.sqlite3',
    '.pkl', '.joblib',
    '.pt', '.pth', '.onnx', # Model files
]

# Output file name
OUTPUT_FILE = 'project_structure_backend.txt'

# --- Script Logic ---

file_structure_tree = ""
file_contents = ""

def should_exclude(path: Path) -> bool:
    """Check if a path should be excluded based on configured lists."""
    # Check against excluded directory names
    if any(part in EXCLUDED_DIRS for part in path.parts):
        return True
    # Check against excluded directory patterns (like *.egg-info)
    if any(path.match(pattern) for pattern in EXCLUDED_DIRS if '*' in pattern):
         return True
    # Check against excluded file names and patterns
    if path.name in EXCLUDED_FILES:
        return True
    if any(path.match(pattern) for pattern in EXCLUDED_FILES if '*' in pattern):
        return True

    return False

def traverse_directory(dir_path: Path, indent: str = ''):
    """Recursively traverses directories and builds the structure/content strings."""
    global file_structure_tree
    global file_contents

    try:
        # Sort entries for consistent ordering: directories first, then files
        entries = sorted(list(dir_path.iterdir()), key=lambda p: (p.is_file(), p.name.lower()))
    except PermissionError:
        print(f"Warning: Permission denied for directory '{dir_path}'. Skipping.")
        return
    except FileNotFoundError:
         print(f"Warning: Directory '{dir_path}' not found during traversal (might have been deleted). Skipping.")
         return


    for entry in entries:
        if should_exclude(entry):
            continue

        if entry.is_dir():
            file_structure_tree += f"{indent}{entry.name}/\n"
            traverse_directory(entry, indent + '  ')
        elif entry.is_file():
            file_structure_tree += f"{indent}{entry.name}\n"
            file_extension = entry.suffix.lower()

            # Add separators for all files
            file_contents += f"\n---------- {entry.name} ----------\n"
            if file_extension in BINARY_EXTENSIONS:
                file_contents += f"(Binary file type {file_extension} - content not included)\n"
            else:
                try:
                    # Try reading with UTF-8 first
                    with entry.open('r', encoding='utf-8') as f:
                        file_contents += f.read() + "\n"
                except UnicodeDecodeError:
                    try:
                        # Fallback to latin-1 if UTF-8 fails
                        with entry.open('r', encoding='latin-1') as f:
                            file_contents += f.read() + "\n"
                        file_contents += "(Warning: Read file using latin-1 encoding due to UTF-8 decode error)\n"
                    except Exception as read_error_fallback:
                         file_contents += f"ERROR READING FILE (Fallback Failed): {read_error_fallback}\n"
                except Exception as read_error:
                    file_contents += f"ERROR READING FILE: {read_error}\n"

            file_contents += f"---------- END {entry.name} ----------\n\n"


def generate_project_structure_and_content(project_root: Path, output_file_path: Path):
    """Generates the combined structure and content file."""
    global file_structure_tree
    global file_contents
    file_structure_tree = "" # Reset global state
    file_contents = ""     # Reset global state

    print(f"Starting traversal from: {project_root}")
    traverse_directory(project_root)

    full_output = f"--- START OF FILE {output_file_path.name} ---\n\n"
    full_output += file_structure_tree
    full_output += "\n" # Separator between tree and content
    full_output += file_contents
    full_output += f"--- END OF FILE {output_file_path.name} ---\n"


    try:
        with output_file_path.open('w', encoding='utf-8') as f:
            f.write(full_output)
        print(f"Project structure and content written to '{output_file_path}'")
    except IOError as e:
        print(f"Error writing to output file '{output_file_path}': {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"An unexpected error occurred during file writing: {e}", file=sys.stderr)
        sys.exit(1)

# --- Main Execution ---
if __name__ == "__main__":
    project_root_path = Path(os.getcwd()) # Get current working directory as Path object
    output_path = project_root_path / OUTPUT_FILE

    print(f"Project Root: {project_root_path}")
    print(f"Output File: {output_path}")

    if not project_root_path.is_dir():
        print(f"Error: Project root directory '{project_root_path}' not found or is not a directory.", file=sys.stderr)
        sys.exit(1)

    try:
        generate_project_structure_and_content(project_root_path, output_path)
    except Exception as e:
        print(f"\nAn unexpected error occurred during execution: {e}", file=sys.stderr)
        sys.exit(1)
---------- END project_structure.py ----------


---------- requirements.txt ----------
sqlalchemy>=1.4,<2.0
requests>=2.32.0
psycopg2-binary>=2.8.0
pandas
markdown2>=2.4.0

---------- END requirements.txt ----------


---------- stock_analyzer.py ----------
# stock_analyzer.py
import pandas as pd
from sqlalchemy import inspect as sa_inspect  # Make sure this import is at the top
from api_clients import FinnhubClient, FinancialModelingPrepClient, EODHDClient, GeminiAPIClient
from database import SessionLocal, get_db_session
from models import Stock, StockAnalysis
from error_handler import logger
from sqlalchemy.exc import SQLAlchemyError
from datetime import datetime


class StockAnalyzer:
    def __init__(self, ticker):
        self.ticker = ticker.upper()
        self.finnhub = FinnhubClient()
        self.fmp = FinancialModelingPrepClient()
        self.eodhd = EODHDClient()
        self.gemini = GeminiAPIClient()
        self.db_session = next(get_db_session())  # Get a session

        self.stock_db_entry = None
        try:
            self._get_or_create_stock_entry()
        except Exception as e:
            logger.error(f"Failed during _get_or_create_stock_entry for {self.ticker}: {e}", exc_info=True)
            # Ensure session is closed if initialization fails badly
            if self.db_session and self.db_session.is_active:
                self.db_session.close()
            raise  # Re-raise to prevent using a potentially broken analyzer object

    def _get_or_create_stock_entry(self):
        # Ensure session is active before query
        if not self.db_session.is_active:
            logger.warning(f"Session for {self.ticker} in _get_or_create_stock_entry was inactive. Re-establishing.")
            try:
                self.db_session.close()  # Close old one if possible
            except Exception as e:
                logger.warning(f"Error closing inactive session in _get_or_create_stock_entry: {e}")
            self.db_session = next(get_db_session())  # Get new one

        self.stock_db_entry = self.db_session.query(Stock).filter_by(ticker=self.ticker).first()
        if not self.stock_db_entry:
            logger.info(f"Stock {self.ticker} not found in DB, creating new entry.")
            profile_fmp = self.fmp.get_company_profile(self.ticker)  # API Call
            company_name_from_api = profile_fmp[0].get('companyName', self.ticker) if profile_fmp and isinstance(
                profile_fmp, list) and profile_fmp[0] else self.ticker

            self.stock_db_entry = Stock(ticker=self.ticker, company_name=company_name_from_api)
            self.db_session.add(self.stock_db_entry)
            try:
                self.db_session.commit()
                logger.info(
                    f"Successfully created and committed new stock entry for {self.ticker} (ID: {self.stock_db_entry.id}).")
                # After commit, the instance's attributes are expired. Refresh to load them.
                # This is crucial before accessing attributes like company_name soon after.
                self.db_session.refresh(self.stock_db_entry)
                logger.info(
                    f"Refreshed stock entry for {self.ticker} after creation. Company Name: {self.stock_db_entry.company_name}")

            except SQLAlchemyError as e:
                self.db_session.rollback()
                logger.error(f"Error creating stock entry for {self.ticker}: {e}", exc_info=True)
                raise
        else:
            # Log existing company name to see if it needs update later
            logger.info(
                f"Found existing stock entry for {self.ticker} (ID: {self.stock_db_entry.id}). Current DB Company Name: {self.stock_db_entry.company_name}")

    def _fetch_financial_data(self):
        logger.info(f"Fetching financial data for {self.ticker}...")
        data = {"profile": None, "financials_fmp": {}, "key_metrics_fmp": None, "financials_finnhub": None,
                "basic_financials_finnhub": None}

        # FMP Profile
        profile_fmp = self.fmp.get_company_profile(self.ticker)  # API Call
        if profile_fmp and isinstance(profile_fmp, list) and profile_fmp[0]:
            data["profile"] = profile_fmp[0]

            # Accessing self.stock_db_entry.company_name should now be safe
            # because it was refreshed after creation if new.
            current_db_company_name = self.stock_db_entry.company_name  # This should be loaded due to refresh
            api_company_name = data["profile"].get('companyName', self.ticker)

            # Update company name in DB if it's missing, was just the ticker, or differs from API
            if self.stock_db_entry and \
                    (
                            not current_db_company_name or current_db_company_name == self.ticker or current_db_company_name != api_company_name):
                if api_company_name != current_db_company_name:  # Only log and update if actually different
                    logger.info(
                        f"Updating company name for {self.ticker} from '{current_db_company_name}' to '{api_company_name}'.")
                    self.stock_db_entry.company_name = api_company_name
                    try:
                        self.db_session.commit()
                        self.db_session.refresh(self.stock_db_entry)  # Refresh after this commit too
                        logger.info(f"Successfully updated company name for {self.ticker} in DB.")
                    except SQLAlchemyError as e:
                        self.db_session.rollback()
                        logger.error(f"Error updating company name for {self.ticker} in DB: {e}")

        # --- FMP Subscription Workaround ---
        # FMP Financial Statements (Annual only due to subscription)
        for statement_type in ["income-statement", "balance-sheet-statement", "cash-flow-statement"]:
            annual = self.fmp.get_financial_statements(self.ticker, statement_type, period="annual",
                                                       limit=5)  # API Call
            data["financials_fmp"][statement_type] = {"annual": annual, "quarterly": None}
            logger.debug(
                f"FMP: Fetched annual {statement_type} for {self.ticker}. Skipping quarterly due to subscription.")

        # FMP Key Metrics (Annual only)
        data["key_metrics_fmp"] = self.fmp.get_key_metrics(self.ticker, period="annual", limit=5)  # API Call
        data["key_metrics_fmp_quarterly"] = None
        logger.debug(f"FMP: Fetched annual key metrics for {self.ticker}. Skipping quarterly due to subscription.")
        # --- End of FMP Subscription Workaround ---

        # Finnhub Basic Financials
        data["basic_financials_finnhub"] = self.finnhub.get_basic_financials(self.ticker)  # API Call

        return data

    def _calculate_metrics(self, raw_data):
        logger.info(f"Calculating metrics for {self.ticker}...")
        metrics = {"key_metrics_snapshot": {}}  # For email raw data

        # From FMP Key Metrics (annual, most recent)
        if raw_data.get("key_metrics_fmp") and isinstance(raw_data["key_metrics_fmp"], list) and raw_data[
            "key_metrics_fmp"]:
            latest_metrics = raw_data["key_metrics_fmp"][0]
            metrics["pe_ratio"] = latest_metrics.get("peRatio")
            metrics["pb_ratio"] = latest_metrics.get("pbRatio")
            metrics["dividend_yield"] = latest_metrics.get(
                "dividendYield")  # FMP provides it as a ratio, not percentage
            metrics["debt_to_equity"] = latest_metrics.get("debtToEquity")
            metrics["net_profit_margin"] = latest_metrics.get("netProfitMargin")
            metrics["roe"] = latest_metrics.get("roe")
            # Add to snapshot for email
            metrics["key_metrics_snapshot"]["FMP_peRatio"] = metrics["pe_ratio"]
            metrics["key_metrics_snapshot"]["FMP_pbRatio"] = metrics["pb_ratio"]
            metrics["key_metrics_snapshot"]["FMP_dividendYield"] = metrics["dividend_yield"]
            metrics["key_metrics_snapshot"]["FMP_debtToEquity"] = metrics["debt_to_equity"]

        # From Finnhub Basic Financials (might be more up-to-date for P/E)
        if raw_data.get("basic_financials_finnhub") and raw_data["basic_financials_finnhub"].get("metric"):
            fin_metrics = raw_data["basic_financials_finnhub"]["metric"]
            # Prioritize FMP if available from annual, but Finnhub can be a good source too
            if metrics.get("pe_ratio") is None and fin_metrics.get("peAnnual"):
                metrics["pe_ratio"] = fin_metrics.get("peAnnual")
            if metrics.get("pb_ratio") is None and fin_metrics.get("pbAnnual"):
                metrics["pb_ratio"] = fin_metrics.get("pbAnnual")
            if metrics.get("eps") is None:  # Finnhub often has direct EPS
                metrics["eps"] = fin_metrics.get("epsAnnual")  # Or epsTTM
            if metrics.get("dividend_yield") is None and fin_metrics.get(
                    "dividendYield"):  # Finnhub dividendYield might be percentage
                # Finnhub dividendYield is usually a direct percentage, convert to ratio if needed or store as is and note in email
                metrics["dividend_yield"] = fin_metrics.get("dividendYield") / 100 if fin_metrics.get(
                    "dividendYield") > 1 else fin_metrics.get("dividendYield")

            metrics["key_metrics_snapshot"]["Finnhub_peAnnual"] = fin_metrics.get("peAnnual")
            metrics["key_metrics_snapshot"]["Finnhub_pbAnnual"] = fin_metrics.get("pbAnnual")
            metrics["key_metrics_snapshot"]["Finnhub_epsAnnual"] = fin_metrics.get("epsAnnual")
            metrics["key_metrics_snapshot"]["Finnhub_dividendYield"] = fin_metrics.get("dividendYield")

        # Income Statement Analysis (from FMP annual)
        income_annual = raw_data.get("financials_fmp", {}).get("income-statement", {}).get("annual")
        if income_annual and isinstance(income_annual, list) and len(income_annual) > 0:
            if len(income_annual) > 1:  # For YoY growth
                metrics["key_metrics_snapshot"]["FMP_Revenue_Recent_Annual"] = income_annual[0].get("revenue")
                rev_curr = income_annual[0].get("revenue")
                rev_prev = income_annual[1].get("revenue")
                if rev_curr is not None and rev_prev is not None and rev_prev != 0:
                    metrics["revenue_growth"] = f"{((rev_curr - rev_prev) / rev_prev * 100):.2f}%"
                metrics["key_metrics_snapshot"]["FMP_Revenue_Growth_YoY"] = metrics.get("revenue_growth")

            # EPS / Net Profit Margin from latest annual income statement if not already found
            if metrics.get("eps") is None: metrics["eps"] = income_annual[0].get("eps")
            if metrics.get("net_profit_margin") is None: metrics["net_profit_margin"] = income_annual[0].get(
                "netProfitMargin")

            # Interest Coverage Ratio (EBIT / Interest Expense)
            ebitda = income_annual[0].get("ebitda")  # FMP often provides ebitda, sometimes ebit.
            # Or use operatingIncome if ebitda is not reliable for this
            # operating_income = income_annual[0].get("operatingIncome")
            interest_expense = income_annual[0].get("interestExpense")
            if ebitda and interest_expense and interest_expense != 0:
                metrics["interest_coverage_ratio"] = ebitda / abs(interest_expense)  # Interest expense can be negative
            metrics["key_metrics_snapshot"]["FMP_interestCoverageRatio (EBITDA based)"] = metrics.get(
                "interest_coverage_ratio")

        # Balance Sheet Analysis (from FMP annual)
        balance_annual = raw_data.get("financials_fmp", {}).get("balance-sheet-statement", {}).get("annual")
        if balance_annual and isinstance(balance_annual, list) and len(balance_annual) > 0:
            latest_balance = balance_annual[0]
            current_assets = latest_balance.get("totalCurrentAssets")
            current_liabilities = latest_balance.get("totalCurrentLiabilities")
            if current_assets is not None and current_liabilities is not None and current_liabilities != 0:
                metrics["current_ratio"] = current_assets / current_liabilities
            metrics["key_metrics_snapshot"]["FMP_currentRatio"] = metrics.get("current_ratio")

            # Debt to Equity (if not from key metrics)
            if metrics.get("debt_to_equity") is None:
                total_debt = latest_balance.get("totalDebt")
                total_equity = latest_balance.get("totalStockholdersEquity")
                if total_debt is not None and total_equity is not None and total_equity != 0:
                    metrics["debt_to_equity"] = total_debt / total_equity

            # Retained Earnings Trend
            if len(balance_annual) > 2:
                re_curr = balance_annual[0].get("retainedEarnings")
                re_prev1 = balance_annual[1].get("retainedEarnings")
                re_prev2 = balance_annual[2].get("retainedEarnings")
                if re_curr is not None and re_prev1 is not None and re_prev2 is not None:
                    if re_curr > re_prev1 > re_prev2:
                        metrics["retained_earnings_trend"] = "Growing"
                    elif re_curr < re_prev1 < re_prev2:
                        metrics["retained_earnings_trend"] = "Declining"
                    else:
                        metrics["retained_earnings_trend"] = "Mixed/Stable"
            metrics["key_metrics_snapshot"]["FMP_retainedEarnings_Recent"] = balance_annual[0].get("retainedEarnings")

        # Cash Flow Statement Analysis (from FMP annual)
        cashflow_annual = raw_data.get("financials_fmp", {}).get("cash-flow-statement", {}).get("annual")
        if cashflow_annual and isinstance(cashflow_annual, list) and len(cashflow_annual) > 0:
            if len(cashflow_annual) > 2:  # Need at least 3 years for a trend
                fcf_curr = cashflow_annual[0].get("freeCashFlow")
                fcf_prev1 = cashflow_annual[1].get("freeCashFlow")
                fcf_prev2 = cashflow_annual[2].get("freeCashFlow")
                if fcf_curr is not None and fcf_prev1 is not None and fcf_prev2 is not None:
                    if fcf_curr > fcf_prev1 > fcf_prev2:
                        metrics["free_cash_flow_trend"] = "Growing"
                    elif fcf_curr < fcf_prev1 < fcf_prev2:
                        metrics["free_cash_flow_trend"] = "Declining"
                    else:
                        metrics["free_cash_flow_trend"] = "Mixed/Stable"
            metrics["key_metrics_snapshot"]["FMP_FCF_Recent"] = cashflow_annual[0].get("freeCashFlow")

        # Placeholder for missing metrics
        for key in ["pe_ratio", "pb_ratio", "eps", "roe", "dividend_yield", "debt_to_equity", "interest_coverage_ratio",
                    "current_ratio", "net_profit_margin"]:
            if key not in metrics or metrics[key] is None:
                metrics[key] = None  # Explicitly set to None if not found/calculated
        for key_trend in ["retained_earnings_trend", "revenue_growth", "free_cash_flow_trend"]:
            if key_trend not in metrics or metrics[key_trend] is None:
                metrics[key_trend] = "Data N/A"

        logger.info(
            f"Calculated metrics for {self.ticker}: { {k: v for k, v in metrics.items() if k != 'key_metrics_snapshot'} }")
        return metrics

    def _analyze_qualitative_factors(self, raw_data):
        logger.info(f"Analyzing qualitative factors for {self.ticker} using Gemini...")
        qual_analysis = {"qualitative_sources": {}}

        company_name_for_prompt = self.stock_db_entry.company_name if self.stock_db_entry and self.stock_db_entry.company_name else self.ticker

        profile = raw_data.get("profile")
        description = profile.get("description", "") if profile else ""
        industry = profile.get("industry", "") if profile else ""
        sector = profile.get("sector", "") if profile else ""

        moat_prompt = (
            f"Based on the company description for {company_name_for_prompt} ({self.ticker}): \"{description}\", "
            f"and its industry '{industry}', what are its likely competitive advantages (economic moat)? "
            f"Consider brand strength, network effects, switching costs, intangible assets (patents, licenses), and cost advantages. "
            f"Provide a concise summary."
        )
        qual_analysis["economic_moat_summary"] = self.gemini.summarize_text(moat_prompt,
                                                                            context="Summarizing economic moat")
        qual_analysis["qualitative_sources"][
            "moat_prompt_context"] = f"Company Description (first 200 chars): {description[:200]}..., Industry: {industry}"

        industry_prompt = (
            f"What are the current key trends, opportunities, and risks for the '{industry}' industry and '{sector}' sector? "
            f"How might {company_name_for_prompt} be positioned regarding these trends? Provide a concise summary."
        )
        qual_analysis["industry_trends_summary"] = self.gemini.summarize_text(industry_prompt,
                                                                              context="Summarizing industry trends")
        qual_analysis["qualitative_sources"]["industry_prompt_context"] = f"Industry: {industry}, Sector: {sector}"

        management_prompt = (
            f"Given the general information about {company_name_for_prompt} ({self.ticker}), "
            f"what are generic positive and negative indicators to look for in its management team? "
            f"This is a general query, not based on specific named executives of this company unless provided. "
            f"Provide a brief summary of factors."
        )
        qual_analysis["management_assessment_summary"] = self.gemini.summarize_text(management_prompt,
                                                                                    context="Generic management assessment factors")
        qual_analysis["qualitative_sources"]["management_prompt_context"] = "General management quality factors query."

        logger.info(f"Qualitative analysis for {self.ticker} complete.")
        return qual_analysis

    def _determine_investment_strategy_and_conclusion(self, fund_metrics, qual_analysis):
        logger.info(f"Determining investment strategy for {self.ticker}...")
        decision_parts = []
        company_name_for_prompt = self.stock_db_entry.company_name if self.stock_db_entry and self.stock_db_entry.company_name else self.ticker

        pe_interpretation = "P/E Ratio: "
        if fund_metrics.get("pe_ratio") is not None:
            pe_val = fund_metrics["pe_ratio"]
            pe_interpretation += f"{pe_val:.2f}. "
            if pe_val < 0:
                pe_interpretation += "Negative P/E (company is loss-making or unusual data). "
            elif pe_val < 15:
                pe_interpretation += "Potentially undervalued or low growth expectations. "
            elif pe_val <= 25:
                pe_interpretation += "Considered fair for many industries. "
            else:
                pe_interpretation += "Potentially overvalued or high growth expectations. "
        else:
            pe_interpretation += "N/A. "
        decision_parts.append(pe_interpretation)

        roe_interpretation = "ROE: "
        if fund_metrics.get("roe") is not None:
            roe_val = fund_metrics["roe"] * 100
            roe_interpretation += f"{roe_val:.2f}%. "
            if roe_val > 20:
                roe_interpretation += "Strong ROE, efficient use of shareholder equity. "
            elif roe_val > 15:
                roe_interpretation += "Good ROE. "
            else:
                roe_interpretation += "Subpar or low ROE. "
        else:
            roe_interpretation += "N/A. "
        decision_parts.append(roe_interpretation)

        de_interpretation = "Debt-to-Equity: "
        if fund_metrics.get("debt_to_equity") is not None:
            de_val = fund_metrics["debt_to_equity"]
            de_interpretation += f"{de_val:.2f}. "
            if de_val > 1.0:
                de_interpretation += "High leverage, be cautious. "
            elif de_val > 0.5:
                de_interpretation += "Moderate leverage. "
            else:
                de_interpretation += "Low leverage. "
        else:
            de_interpretation += "N/A. "
        decision_parts.append(de_interpretation)

        decision_parts.append(f"Revenue Growth (YoY): {fund_metrics.get('revenue_growth', 'N/A')}.")
        decision_parts.append(f"Free Cash Flow Trend: {fund_metrics.get('free_cash_flow_trend', 'N/A')}.")
        decision_parts.append(f"Economic Moat: {qual_analysis.get('economic_moat_summary', 'N/A')[:150]}...")
        decision_parts.append(f"Industry Trends: {qual_analysis.get('industry_trends_summary', 'N/A')[:150]}...")

        # Concise prompt for Gemini synthesis to avoid MAX_TOKENS
        concise_metrics_summary = f"P/E: {fund_metrics.get('pe_ratio', 'N/A'):.1f}, ROE: {fund_metrics.get('roe', 0) * 100:.1f}%, D/E: {fund_metrics.get('debt_to_equity', 'N/A'):.1f}."
        concise_qual_moat = qual_analysis.get('economic_moat_summary', 'N/A')[:200]  # Slightly longer snippet
        concise_qual_industry = qual_analysis.get('industry_trends_summary', 'N/A')[:200]  # Slightly longer snippet

        synthesis_prompt = (
            f"Synthesize an investment thesis for {company_name_for_prompt} ({self.ticker}).\n"
            f"Key Metrics Summary: {concise_metrics_summary}\n"
            f"Revenue Growth: {fund_metrics.get('revenue_growth', 'N/A')}, FCF Trend: {fund_metrics.get('free_cash_flow_trend', 'N/A')}.\n"
            f"Qualitative Highlights: Moat (brief): {concise_qual_moat}..., Industry (brief): {concise_qual_industry}...\n"
            f"Suggest a general investment decision (e.g., 'Consider for Value', 'Monitor for Growth', 'Exercise Caution', 'Potential Buy', 'Leaning Negative') and a brief reasoning (max 3-4 sentences for reasoning). "
            f"Do not give financial advice. This is for informational purposes."
        )
        gemini_synthesis = self.gemini.generate_text(synthesis_prompt)

        final_decision = "Neutral/Monitor"  # Default
        if gemini_synthesis and not gemini_synthesis.startswith("Error:"):
            if "potential buy" in gemini_synthesis.lower() or "consider for" in gemini_synthesis.lower() or "favorable" in gemini_synthesis.lower() or "positive outlook" in gemini_synthesis.lower():
                final_decision = "Potential Buy/Consider"
            elif "caution" in gemini_synthesis.lower() or "negative" in gemini_synthesis.lower() or "avoid" in gemini_synthesis.lower() or "risks outweigh" in gemini_synthesis.lower():
                final_decision = "Caution/Avoid"

        strategy_type = "Undetermined"
        if gemini_synthesis and not gemini_synthesis.startswith("Error:"):
            if "value" in gemini_synthesis.lower() and "growth" in gemini_synthesis.lower():
                strategy_type = "Value/Growth (GARP)"
            elif "value" in gemini_synthesis.lower():
                strategy_type = "Value"
            elif "growth" in gemini_synthesis.lower():
                strategy_type = "Growth"

        return {
            "investment_decision": final_decision,
            "reasoning": f"Rule-based checks summary: {pe_interpretation} {roe_interpretation} {de_interpretation} Revenue Growth: {fund_metrics.get('revenue_growth', 'N/A')}. FCF Trend: {fund_metrics.get('free_cash_flow_trend', 'N/A')}.\n\nAI Synthesis: {gemini_synthesis}",
            "strategy_type": strategy_type
        }

    def analyze(self):
        logger.info(f"Starting analysis for {self.ticker}...")
        final_analysis_entry = None

        try:
            if not self.stock_db_entry:  # Should have been caught by __init__ raising error
                logger.error(f"Stock entry for {self.ticker} was not initialized. Aborting analysis.")
                return None

            # --- Session and Instance State Check/Recovery ---
            if not self.db_session.is_active:
                logger.warning(
                    f"Session for {self.ticker} is INACTIVE at the start of analyze method. Re-establishing.")
                try:
                    self.db_session.close()
                except Exception as e_close:
                    logger.warning(f"Error closing inactive session in analyze(): {e_close}")
                self.db_session = next(get_db_session())

                logger.info(f"Re-querying stock {self.ticker} for new session.")
                re_fetched_stock = self.db_session.query(Stock).filter(Stock.ticker == self.ticker).first()
                if not re_fetched_stock:  # Should not happen if _get_or_create_stock_entry succeeded
                    logger.error(
                        f"Could not re-fetch stock {self.ticker} ({self.stock_db_entry.id if self.stock_db_entry and self.stock_db_entry.id else 'Unknown ID'}) after session re-establishment. Aborting.")
                    return None
                self.stock_db_entry = re_fetched_stock
                logger.info(
                    f"Successfully re-fetched and bound stock {self.ticker} (ID: {self.stock_db_entry.id}) to new session.")

            instance_state = sa_inspect(self.stock_db_entry)
            if not instance_state.session or instance_state.session is not self.db_session:
                log_msg_prefix = "DETACHED" if not instance_state.session else f"bound to a DIFFERENT session (expected {id(self.db_session)}, got {id(instance_state.session)})"
                object_id_for_log = self.stock_db_entry.id if instance_state.has_identity else 'Unknown ID'
                logger.warning(
                    f"Stock entry {self.ticker} (ID: {object_id_for_log}) is {log_msg_prefix}. Attempting to merge into current session.")
                try:
                    if not instance_state.has_identity and self.stock_db_entry.id is None:  # If it's a new, uncommitted obj from a dead session
                        logger.info(
                            f"Re-querying {self.ticker} by ticker as PK is not available on detached instance before merge.")
                        re_fetched_stock = self.db_session.query(Stock).filter(Stock.ticker == self.ticker).first()
                        if not re_fetched_stock:
                            logger.error(
                                f"Could not re-fetch {self.ticker} by ticker to merge (PK was missing). Aborting.")
                            return None
                        self.stock_db_entry = re_fetched_stock
                        logger.info(f"Re-fetched {self.ticker} (ID: {self.stock_db_entry.id}) before potential merge.")

                    merged_stock = self.db_session.merge(self.stock_db_entry)
                    self.stock_db_entry = merged_stock
                    logger.info(
                        f"Successfully merged/re-associated stock {self.ticker} (ID: {self.stock_db_entry.id}) into current session.")
                except Exception as e_merge:
                    logger.error(f"Failed to merge stock {self.ticker} into session: {e_merge}. Aborting analysis.",
                                 exc_info=True)
                    return None
            # --- End of Session and Instance State Check/Recovery ---

            raw_data = self._fetch_financial_data()
            if not raw_data.get("profile") and not raw_data.get("key_metrics_fmp") and not raw_data.get(
                    "basic_financials_finnhub"):
                logger.error(f"Failed to fetch significant data for {self.ticker} from any source. Aborting analysis.")
                return None

            calculated_metrics = self._calculate_metrics(raw_data)
            qualitative_summary = self._analyze_qualitative_factors(raw_data)
            strategy_and_conclusion = self._determine_investment_strategy_and_conclusion(calculated_metrics,
                                                                                         qualitative_summary)

            analysis_entry = StockAnalysis(
                stock_id=self.stock_db_entry.id,  # Accessing .id here should be safe now
                pe_ratio=calculated_metrics.get("pe_ratio"),
                pb_ratio=calculated_metrics.get("pb_ratio"),
                eps=calculated_metrics.get("eps"),
                roe=calculated_metrics.get("roe"),
                dividend_yield=calculated_metrics.get("dividend_yield"),
                debt_to_equity=calculated_metrics.get("debt_to_equity"),
                interest_coverage_ratio=calculated_metrics.get("interest_coverage_ratio"),
                current_ratio=calculated_metrics.get("current_ratio"),
                retained_earnings_trend=calculated_metrics.get("retained_earnings_trend"),
                revenue_growth=calculated_metrics.get("revenue_growth"),
                net_profit_margin=calculated_metrics.get("net_profit_margin"),
                free_cash_flow_trend=calculated_metrics.get("free_cash_flow_trend"),
                economic_moat_summary=qualitative_summary.get("economic_moat_summary"),
                industry_trends_summary=qualitative_summary.get("industry_trends_summary"),
                management_assessment_summary=qualitative_summary.get("management_assessment_summary"),
                investment_decision=strategy_and_conclusion.get("investment_decision"),
                reasoning=strategy_and_conclusion.get("reasoning"),
                strategy_type=strategy_and_conclusion.get("strategy_type"),
                key_metrics_snapshot=calculated_metrics.get("key_metrics_snapshot"),
                qualitative_sources=qualitative_summary.get("qualitative_sources")
            )
            self.db_session.add(analysis_entry)

            # This modification should also be safe now as self.stock_db_entry is session-bound
            self.stock_db_entry.last_analysis_date = datetime.utcnow()

            self.db_session.commit()
            logger.info(f"Successfully analyzed and saved stock: {self.ticker} (Analysis ID: {analysis_entry.id})")
            final_analysis_entry = analysis_entry  # Return the committed analysis

        except Exception as e_outer:
            logger.error(f"Outer exception in analyze() for {self.ticker}: {e_outer}", exc_info=True)
            if self.db_session and self.db_session.is_active:
                try:
                    self.db_session.rollback()
                except Exception as e_rollback:
                    logger.error(f"Error during rollback for {self.ticker}: {e_rollback}")
            # Do not return analysis_entry here as commit might have failed
            return None
        finally:
            if self.db_session and self.db_session.is_active:
                self.db_session.close()
                logger.debug(f"Session closed for {self.ticker} at end of analyze method.")

        return final_analysis_entry


# Example usage:
if __name__ == '__main__':
    from database import init_db

    try:
        # init_db() # Assuming DB is already initialized for subsequent runs
        logger.info("Starting standalone stock analysis test...")

        # Test with GOOG first as per the error log
        analyzer_goog = StockAnalyzer(ticker="GOOG")
        analysis_result_goog = analyzer_goog.analyze()
        if analysis_result_goog:
            logger.info(
                f"Analysis for {analysis_result_goog.stock.ticker} completed. Decision: {analysis_result_goog.investment_decision}")
        else:
            logger.error("Stock analysis failed for GOOG.")

        analyzer_aapl = StockAnalyzer(ticker="AAPL")
        analysis_result_aapl = analyzer_aapl.analyze()
        if analysis_result_aapl:
            logger.info(
                f"Analysis for {analysis_result_aapl.stock.ticker} completed. Decision: {analysis_result_aapl.investment_decision}")
        else:
            logger.error("Stock analysis failed for AAPL.")

    except Exception as e:
        logger.error(f"Error during stock analysis test in __main__: {e}", exc_info=True)
---------- END stock_analyzer.py ----------

--- END OF FILE project_structure_backend.txt ---
