--- START OF FILE project_structure_backend.txt ---

api_clients/
  __init__.py
  alphavantage_client.py
  base_client.py
  eodhd_client.py
  finnhub_client.py
  fmp_client.py
  gemini_client.py
  sec_edgar_client.py
core/
  __init__.py
  config.py
  logging_setup.py
database/
  __init__.py
  connection.py
  models.py
services/
  __init__.py
  email_service.py
  ipo_analyzer.py
  news_analyzer.py
  stock_analyzer.py
.gitignore
main.py
requirements.txt


---------- __init__.py ----------
# api_clients/__init__.py
from .base_client import APIClient, scrape_article_content, extract_S1_text_sections
from .finnhub_client import FinnhubClient
from .fmp_client import FinancialModelingPrepClient
from .alphavantage_client import AlphaVantageClient
from .eodhd_client import EODHDClient
from .sec_edgar_client import SECEDGARClient
from .gemini_client import GeminiAPIClient

__all__ = [
    "APIClient",
    "scrape_article_content",
    "extract_S1_text_sections",
    "FinnhubClient",
    "FinancialModelingPrepClient",
    "AlphaVantageClient",
    "EODHDClient",
    "SECEDGARClient",
    "GeminiAPIClient",
]
---------- END __init__.py ----------


---------- alphavantage_client.py ----------
from .base_client import APIClient
from core.config import ALPHA_VANTAGE_API_KEY


class AlphaVantageClient(APIClient):
    def __init__(self):
        super().__init__("https://www.alphavantage.co", api_key_name="apikey", api_key_value=ALPHA_VANTAGE_API_KEY)

    def get_company_overview(self, ticker):
        params = {"function": "OVERVIEW", "symbol": ticker}
        return self.request("GET", "/query", params=params, api_source_name="alphavantage_overview")

    def get_income_statement_quarterly(self, ticker):
        params = {"function": "INCOME_STATEMENT", "symbol": ticker}
        data = self.request("GET", "/query", params=params, api_source_name="alphavantage_income_quarterly")
        if data and isinstance(data.get("quarterlyReports"), list):
            data["quarterlyReports"].reverse()
        return data

    def get_balance_sheet_quarterly(self, ticker):
        params = {"function": "BALANCE_SHEET", "symbol": ticker}
        data = self.request("GET", "/query", params=params, api_source_name="alphavantage_balance_quarterly")
        if data and isinstance(data.get("quarterlyReports"), list):
            data["quarterlyReports"].reverse()
        return data

    def get_cash_flow_quarterly(self, ticker):
        params = {"function": "CASH_FLOW", "symbol": ticker}
        data = self.request("GET", "/query", params=params, api_source_name="alphavantage_cashflow_quarterly")
        if data and isinstance(data.get("quarterlyReports"), list):
            data["quarterlyReports"].reverse()
        return data
---------- END alphavantage_client.py ----------


---------- base_client.py ----------
# api_clients/base_client.py
import requests
import time
import json
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
import re

from core.config import (
    API_REQUEST_TIMEOUT, API_RETRY_ATTEMPTS, API_RETRY_DELAY,
    CACHE_EXPIRY_SECONDS
)
from core.logging_setup import logger
from database.connection import SessionLocal
from database.models import CachedAPIData


class APIClient:
    def __init__(self, base_url, api_key_name=None, api_key_value=None, headers=None):
        self.base_url = base_url
        self.api_key_name = api_key_name
        self.api_key_value = api_key_value
        self.headers = headers or {}
        if api_key_name and api_key_value:
            self.params = {api_key_name: api_key_value}
        else:
            self.params = {}

    def _get_cached_response(self, request_url_or_params_str):
        session = SessionLocal()
        try:
            current_time_utc = datetime.now(timezone.utc)
            cache_entry = session.query(CachedAPIData).filter(
                CachedAPIData.request_url_or_params == request_url_or_params_str,
                CachedAPIData.expires_at > current_time_utc
            ).first()
            if cache_entry:
                logger.info(f"Cache hit for: {request_url_or_params_str[:100]}...")
                return cache_entry.response_data
        except Exception as e:
            logger.error(f"Error reading from cache for '{request_url_or_params_str[:100]}...': {e}", exc_info=True)
        finally:
            session.close()
        return None

    def _cache_response(self, request_url_or_params_str, response_data, api_source):
        session = SessionLocal()
        try:
            now_utc = datetime.now(timezone.utc)
            expires_at_utc = now_utc + timedelta(seconds=CACHE_EXPIRY_SECONDS)

            session.query(CachedAPIData).filter(
                CachedAPIData.request_url_or_params == request_url_or_params_str).delete(synchronize_session=False)

            new_cache_entry = CachedAPIData(
                api_source=api_source,
                request_url_or_params=request_url_or_params_str,
                response_data=response_data,
                timestamp=now_utc,
                expires_at=expires_at_utc
            )
            session.add(new_cache_entry)
            session.commit()
            logger.info(f"Cached response for: {request_url_or_params_str[:100]}...")
        except Exception as e:
            logger.error(f"Error writing to cache for '{request_url_or_params_str[:100]}...': {e}", exc_info=True)
            session.rollback()
        finally:
            session.close()

    def request(self, method, endpoint, params=None, data=None, json_data=None, use_cache=True,
                api_source_name="unknown", is_json_response=True):
        url = f"{self.base_url}{endpoint}"
        current_call_params = params.copy() if params else {}
        full_query_params = self.params.copy()
        full_query_params.update(current_call_params)

        sorted_params = sorted(full_query_params.items()) if full_query_params else []
        param_string = "&".join([f"{k}={v}" for k, v in sorted_params])
        cache_key_str = f"{method.upper()}:{url}?{param_string}"
        if json_data:
            try:
                sorted_json_data_str = json.dumps(json_data, sort_keys=True, separators=(',', ':'))
                cache_key_str += f"|BODY:{sorted_json_data_str}"
            except TypeError as e:
                logger.warning(f"Could not serialize json_data for cache key for {url}: {e}. Cache key may be less effective.")
                cache_key_str += f"|BODY_UNSERIALIZED:{str(json_data)}"

        if use_cache:
            cached_data = self._get_cached_response(cache_key_str)
            if cached_data is not None:
                return cached_data

        for attempt in range(API_RETRY_ATTEMPTS):
            try:
                response = requests.request(
                    method, url, params=full_query_params, data=data, json=json_data,
                    headers=self.headers, timeout=API_REQUEST_TIMEOUT
                )
                response.raise_for_status()

                if not is_json_response:
                    response_content = response.text
                    if use_cache:
                        self._cache_response(cache_key_str, response_content, api_source_name)
                    return response_content

                response_json = response.json()
                if use_cache:
                    self._cache_response(cache_key_str, response_json, api_source_name)
                return response_json

            except requests.exceptions.HTTPError as e:
                log_params_for_error = {k: (str(v)[:4] + '******' + str(v)[-4:] if k == self.api_key_name and isinstance(v, str) and len(str(v)) > 8 else v) for k,v in full_query_params.items()}
                log_headers_for_error = self.headers.copy()
                sensitive_header_keys = ["X-RapidAPI-Key", "Authorization", "Token", self.api_key_name]
                for h_key in sensitive_header_keys:
                    if h_key in log_headers_for_error and isinstance(log_headers_for_error[h_key], str) and len(log_headers_for_error[h_key]) > 8:
                        log_headers_for_error[h_key] = log_headers_for_error[h_key][:4] + "******" + log_headers_for_error[h_key][-4:]
                status_code = e.response.status_code if e.response is not None else "Unknown"
                response_text_preview = e.response.text[:200] if e.response is not None else "No response body"
                logger.warning(
                    f"HTTP error on attempt {attempt + 1}/{API_RETRY_ATTEMPTS} for {method} {url} "
                    f"(Params: {log_params_for_error}, Headers: {log_headers_for_error}): "
                    f"{status_code} - {response_text_preview}..."
                )
                if api_source_name.startswith("alphavantage") and e.response is not None and "Our standard API call frequency is 25 requests per day." in e.response.text:
                    logger.error(f"Alpha Vantage API daily limit likely reached. Params: {log_params_for_error}")
                    return None
                if e.response is not None:
                    if status_code == 429:
                        delay = API_RETRY_DELAY * (2 ** attempt)
                        logger.info(f"Rate limit hit (429). Waiting for {delay} seconds.")
                        time.sleep(delay)
                    elif 500 <= status_code < 600:
                        delay = API_RETRY_DELAY * (2 ** attempt)
                        logger.info(f"Server error ({status_code}). Waiting for {delay} seconds before retry.")
                        time.sleep(delay)
                    elif status_code == 401 or status_code == 403:
                        logger.error(f"Client error {status_code} (Unauthorized/Forbidden) for {url}. API key may be invalid or permissions lacking. No retry. Params: {log_params_for_error}")
                        return None
                    else:
                        logger.error(f"Non-retryable client error {status_code} for {url}: {e.response.reason if e.response else 'Unknown reason'}", exc_info=False)
                        return None
                else:
                    logger.error(f"HTTPError without response object for {url}. Cannot retry effectively.")
                    return None
            except requests.exceptions.RequestException as e:
                logger.warning(f"Request error on attempt {attempt + 1}/{API_RETRY_ATTEMPTS} for {url}: {e}")
                if attempt < API_RETRY_ATTEMPTS - 1:
                    delay = API_RETRY_DELAY * (2 ** attempt)
                    time.sleep(delay)
            except json.JSONDecodeError as e_json:
                logger.error(f"JSON decode error for {url} on attempt {attempt + 1}. Response text: {response.text[:500] if 'response' in locals() else 'Response object not available'}... Error: {e_json}")
                if attempt < API_RETRY_ATTEMPTS - 1:
                    delay = API_RETRY_DELAY * (2 ** attempt)
                    time.sleep(delay)
                else:
                    return None
        logger.error(f"All {API_RETRY_ATTEMPTS} attempts failed for {url}. Last query params: {full_query_params}")
        return None


# --- Helper functions for scraping and parsing (moved from old api_clients.py) ---
def scrape_article_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9', 'Connection': 'keep-alive'
        }
        response = requests.get(url, headers=headers, timeout=API_REQUEST_TIMEOUT - 10, allow_redirects=True)
        response.raise_for_status()
        content_type = response.headers.get('content-type', '').lower()
        if 'html' not in content_type:
            logger.warning(f"Content type for {url} is not HTML ('{content_type}'). Skipping scrape."); return None

        soup = BeautifulSoup(response.content, 'lxml')

        for tag_name in ['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'iframe', 'noscript', 'link', 'meta', 'button', 'input', 'select', 'textarea', 'figure', 'figcaption']:
            for tag in soup.find_all(tag_name):
                tag.decompose()

        main_content_html = None
        selectors = [
            'article', 'main', 'div[role="main"]',
            'div[class*="article-body"]', 'div[class*="article-content"]', 'div[id*="article-body"]', 'div[id*="article-content"]',
            'div[class*="post-content"]', 'div[class*="entry-content"]',
            'div[class*="story-body"]', 'div[class*="main-content"]', 'section[class*="content"]'
        ]
        for selector in selectors:
            tag = soup.select_one(selector)
            if tag:
                for unwanted_pattern in ['ad', 'social', 'related', 'share', 'comment', 'promo', 'sidebar', 'popup', 'banner', 'meta-info', 'byline', 'author', 'timestamp', 'tags', 'breadcrumb', 'pagination', 'tools', 'print-button', 'advertisement', 'figcaption', 'read-more', 'newsletter', 'modal']:
                    for sub_tag in tag.find_all(lambda t: any(unwanted_pattern in c.lower() for c in t.get('class', [])) or \
                                                              any(unwanted_pattern in i.lower() for i in t.get('id', [])) or \
                                                              unwanted_pattern in t.get('role', '').lower() or \
                                                              unwanted_pattern in t.get('aria-label', '').lower()):
                        sub_tag.decompose()
                main_content_html = tag
                break

        article_text = ""
        if main_content_html:
            text_parts = []
            for element in main_content_html.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div', 'span', 'td', 'th']):
                text = element.get_text(separator=' ', strip=True)
                if text:
                    if element.name == 'div' and element.find(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
                        continue
                    text_parts.append(text)
            article_text = '\n'.join(filter(None, text_parts))
        elif soup.body:
            logger.info(f"Main content selectors failed for {url}, trying body text. This might be noisy.")
            article_text = soup.body.get_text(separator='\n', strip=True)
        else:
            logger.warning(f"Could not extract main content or body text from {url}."); return None

        article_text = re.sub(r'[ \t]+', ' ', article_text)
        article_text = re.sub(r'\n\s*\n', '\n\n', article_text)
        article_text = re.sub(r'\n{3,}', '\n\n', article_text).strip()

        if len(article_text) < 200:
            logger.info(f"Extracted text from {url} is very short ({len(article_text)} chars). Might be a stub, paywall, or primarily non-text content.")
        logger.info(f"Successfully scraped ~{len(article_text)} chars from {url}")
        return article_text

    except requests.exceptions.Timeout:
        logger.error(f"Timeout error scraping {url}."); return None
    except requests.exceptions.RequestException as e:
        logger.error(f"Request error scraping {url}: {e}"); return None
    except Exception as e:
        logger.error(f"General error scraping {url}: {e}", exc_info=True); return None


def extract_S1_text_sections(filing_text, sections_map):
    if not filing_text or not sections_map: return {}
    extracted_sections = {}
    try:
        soup = BeautifulSoup(filing_text, 'lxml')
    except Exception:
        try:
            logger.warning("lxml parsing failed for SEC filing, trying html.parser.")
            soup = BeautifulSoup(filing_text, 'html.parser')
        except Exception as e_bs_parse:
            logger.error(f"BeautifulSoup failed to parse filing text with lxml and html.parser: {e_bs_parse}. Using raw text and regex matching might be less accurate.")
            normalized_text = re.sub(r'\s*\n\s*', '\n', filing_text.strip())
            normalized_text = ''.join(filter(lambda x: x.isprintable() or x.isspace(), normalized_text))
            soup = None

    if soup:
        for invisible_element_name in ['style', 'script', 'head', 'title', 'meta', 'link', 'noscript']:
            for element in soup.find_all(invisible_element_name):
                element.decompose()
        page_text = []
        for element in soup.find_all(['p', 'div', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'td', 'tr', 'table', 'body']):
            text = element.get_text(separator='\n', strip=True)
            if text:
                page_text.append(text)
        normalized_text = '\n\n'.join(page_text)
        normalized_text = re.sub(r'\s*\n\s*', '\n', normalized_text)
        normalized_text = re.sub(r'\n{3,}', '\n\n', normalized_text)
        normalized_text = ''.join(filter(lambda x: x.isprintable() or x.isspace(), normalized_text))
    else: # soup is None, normalized_text was already prepared
        pass

    section_patterns = []
    for key, patterns_list in sections_map.items():
        item_num_pattern_str = patterns_list[0].replace('.', r'\.?')
        base_item_regex = r"(?:ITEM|Item)\s*" + item_num_pattern_str.split()[-1] + r"\.?\s*:?\s*"
        if len(patterns_list) > 1:
            descriptive_name_regex = re.escape(patterns_list[1])
            start_regex_str_item_desc = base_item_regex + descriptive_name_regex
            section_patterns.append({"key": key, "start_regex": re.compile(start_regex_str_item_desc, re.IGNORECASE)})
            start_regex_str_desc_only = r"^\s*" + descriptive_name_regex + r"\s*$"
            section_patterns.append({"key": key, "start_regex": re.compile(start_regex_str_desc_only, re.IGNORECASE | re.MULTILINE)})
        else:
            section_patterns.append({"key": key, "start_regex": re.compile(base_item_regex, re.IGNORECASE)})

    found_sections_matches = []
    for pattern_info in section_patterns:
        for match in pattern_info["start_regex"].finditer(normalized_text):
            found_sections_matches.append({
                "key": pattern_info["key"],
                "start": match.start(),
                "end_of_header": match.end(),
                "header_text": match.group(0).strip()
            })

    if not found_sections_matches:
        logger.warning("No sections extracted from SEC filing based on ITEM X or descriptive name patterns."); return {}

    found_sections_matches.sort(key=lambda x: x["start"])

    for i, current_sec_info in enumerate(found_sections_matches):
        start_index = current_sec_info["end_of_header"]
        end_index = len(normalized_text)
        for j in range(i + 1, len(found_sections_matches)):
            next_sec_info = found_sections_matches[j]
            if next_sec_info["key"] != current_sec_info["key"]:
                end_index = next_sec_info["start"]
                break
        section_text = normalized_text[start_index:end_index].strip()
        section_text = re.sub(r'(?i)\btable\s+of\s+contents\b.*?\n', '', section_text, flags=re.MULTILINE)
        section_text = re.sub(r'^\s*(?:Page\s+\d+|\d+|PART\s+[IVXLCDM]+)\s*$', '', section_text, flags=re.MULTILINE)
        section_text = re.sub(r'\n{3,}', '\n\n', section_text).strip()

        if section_text:
            if current_sec_info["key"] not in extracted_sections or len(section_text) > len(extracted_sections.get(current_sec_info["key"], "")):
                extracted_sections[current_sec_info["key"]] = section_text
                logger.debug(f"Extracted section '{current_sec_info['key']}' (header: '{current_sec_info['header_text']}') len {len(section_text)}")

    if not extracted_sections:
        logger.warning("No text content could be extracted for any identified section headers after processing.")
    return extracted_sections
---------- END base_client.py ----------


---------- eodhd_client.py ----------
from .base_client import APIClient
from core.config import EODHD_API_KEY
from core.logging_setup import logger


class EODHDClient(APIClient):
    def __init__(self):
        super().__init__("https://eodhistoricaldata.com/api", api_key_name="api_token", api_key_value=EODHD_API_KEY)
        self.params["fmt"] = "json" # Default format

    def get_fundamental_data(self, ticker_with_exchange): # e.g., AAPL.US
        return self.request("GET", f"/fundamentals/{ticker_with_exchange}", api_source_name="eodhd_fundamentals")

    def get_ipo_calendar(self, from_date=None, to_date=None):
        params = {}
        if from_date: params["from"] = from_date
        if to_date: params["to"] = to_date
        logger.info("EODHDClient.get_ipo_calendar called. Data quality/availability may vary by subscription.")
        return self.request("GET", "/calendar/ipos", params=params, api_source_name="eodhd_ipo_calendar")
---------- END eodhd_client.py ----------


---------- finnhub_client.py ----------
from datetime import datetime, timedelta, timezone
from .base_client import APIClient
from core.config import FINNHUB_API_KEY


class FinnhubClient(APIClient):
    def __init__(self):
        super().__init__("https://finnhub.io/api/v1", api_key_name="token", api_key_value=FINNHUB_API_KEY)

    def get_market_news(self, category="general", min_id=0):
        params = {"category": category}
        if min_id > 0: params["minId"] = min_id
        return self.request("GET", "/news", params=params, api_source_name="finnhub_news")

    def get_company_profile2(self, ticker):
        return self.request("GET", "/stock/profile2", params={"symbol": ticker}, api_source_name="finnhub_profile")

    def get_financials_reported(self, ticker, freq="quarterly", count=20): # count specifies number of periods
        params = {"symbol": ticker, "freq": freq, "count": count}
        return self.request("GET", "/stock/financials-reported", params=params,
                            api_source_name="finnhub_financials_reported")

    def get_basic_financials(self, ticker, metric_type="all"):
        return self.request("GET", "/stock/metric", params={"symbol": ticker, "metric": metric_type},
                            api_source_name="finnhub_metrics")

    def get_ipo_calendar(self, from_date=None, to_date=None):
        if from_date is None: from_date = (datetime.now(timezone.utc) - timedelta(days=30)).strftime('%Y-%m-%d')
        if to_date is None: to_date = (datetime.now(timezone.utc) + timedelta(days=90)).strftime('%Y-%m-%d')
        params = {"from": from_date, "to": to_date}
        return self.request("GET", "/calendar/ipo", params=params, api_source_name="finnhub_ipo_calendar")

    def get_sec_filings(self, ticker, from_date=None, to_date=None):
        if from_date is None: from_date = (datetime.now(timezone.utc) - timedelta(days=365 * 2)).strftime('%Y-%m-%d')
        if to_date is None: to_date = datetime.now(timezone.utc).strftime('%Y-%m-%d')
        params = {"symbol": ticker, "from": from_date, "to": to_date}
        return self.request("GET", "/stock/filings", params=params, api_source_name="finnhub_filings")

    def get_company_peers(self, ticker):
        """Gets a list of company peers."""
        return self.request("GET", "/stock/peers", params={"symbol": ticker}, api_source_name="finnhub_peers")

---------- END finnhub_client.py ----------


---------- fmp_client.py ----------
# api_clients/fmp_client.py
from .base_client import APIClient
from core.config import FINANCIAL_MODELING_PREP_API_KEY
from core.logging_setup import logger


class FinancialModelingPrepClient(APIClient):
    def __init__(self):
        super().__init__("https://financialmodelingprep.com/api/v3", api_key_name="apikey",
                         api_key_value=FINANCIAL_MODELING_PREP_API_KEY)

    def get_ipo_calendar(self, from_date=None, to_date=None):
        # Note: FMP's free tier might not support this well or at all.
        params = {}
        if from_date: params["from"] = from_date
        if to_date: params["to"] = to_date
        logger.info("FinancialModelingPrepClient.get_ipo_calendar called. Availability depends on FMP subscription.")
        return self.request("GET", "/ipo_calendar", params=params, api_source_name="fmp_ipo_calendar")

    def get_financial_statements(self, ticker, statement_type="income-statement", period="quarter", limit=40):
        actual_limit = limit
        if period == "annual": actual_limit = min(limit, 15)
        elif period == "quarter": actual_limit = min(limit, 60)

        return self.request("GET", f"/{statement_type}/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_{statement_type.replace('-', '_')}_{period}")

    def get_income_statement_growth(self, ticker, period="quarter", limit=40):
        actual_limit = limit
        if period == "annual": actual_limit = min(limit, 15)
        elif period == "quarter": actual_limit = min(limit, 60)
        return self.request("GET", f"/income-statement-growth/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_income_statement_growth_{period}")

    def get_key_metrics(self, ticker, period="quarter", limit=40):
        actual_limit = limit
        if period == "annual": actual_limit = min(limit, 15)
        elif period == "quarter": actual_limit = min(limit, 60)
        return self.request("GET", f"/key-metrics/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_key_metrics_{period}")

    def get_ratios(self, ticker, period="quarter", limit=40):
        actual_limit = limit
        if period == "annual": actual_limit = min(limit, 15)
        elif period == "quarter": actual_limit = min(limit, 60)
        return self.request("GET", f"/ratios/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_ratios_{period}")

    def get_company_profile(self, ticker):
        return self.request("GET", f"/profile/{ticker}", params={}, api_source_name="fmp_profile")

    def get_analyst_estimates(self, ticker, period="annual"):
        logger.info(f"FMP get_analyst_estimates for {ticker} called. Availability depends on FMP subscription.")
        return self.request("GET", f"/analyst-estimates/{ticker}", params={"period": period},
                            api_source_name="fmp_analyst_estimates")
---------- END fmp_client.py ----------


---------- gemini_client.py ----------
import requests
import time
import json

from core.config import (
    GOOGLE_API_KEYS, API_REQUEST_TIMEOUT, API_RETRY_ATTEMPTS,
    API_RETRY_DELAY, GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE
)
from core.logging_setup import logger


class GeminiAPIClient:
    def __init__(self):
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models"
        self.model_name = "gemini-1.5-flash-latest"

    def _get_next_api_key_for_attempt(self, overall_attempt_num, max_attempts_per_key, total_keys):
        if total_keys == 0: return None, 0
        key_group_index = (overall_attempt_num // max_attempts_per_key) % total_keys
        api_key = GOOGLE_API_KEYS[key_group_index]
        current_retry_for_this_key = (overall_attempt_num % max_attempts_per_key) + 1
        logger.debug(f"Gemini: Using key ...{api_key[-4:]} (Index {key_group_index}), Attempt {current_retry_for_this_key}/{max_attempts_per_key}")
        return api_key, current_retry_for_this_key

    def generate_text(self, prompt, model=None):
        if model is None: model = self.model_name

        max_attempts_per_key = API_RETRY_ATTEMPTS
        total_keys = len(GOOGLE_API_KEYS)
        if total_keys == 0:
            logger.error("Gemini: No API keys configured in GOOGLE_API_KEYS."); return "Error: No Google API keys."

        if len(prompt) > GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE:
            original_len = len(prompt)
            prompt = prompt[:GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE]
            logger.warning(
                f"Gemini prompt (original length {original_len}) exceeded hard limit {GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE}. "
                f"Truncated to {len(prompt)} chars."
            )
            trunc_note = "\n...[PROMPT TRUNCATED DUE TO EXCESSIVE LENGTH]..."
            if len(prompt) + len(trunc_note) <= GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE:
                prompt += trunc_note
            else:
                prompt = prompt[:GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE - len(trunc_note)] + trunc_note

        for overall_attempt_num in range(total_keys * max_attempts_per_key):
            api_key, current_retry_for_this_key = self._get_next_api_key_for_attempt(
                overall_attempt_num, max_attempts_per_key, total_keys
            )
            if api_key is None: break

            url = f"{self.base_url}/{model}:generateContent?key={api_key}"
            payload = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {
                    "temperature": 0.6, "maxOutputTokens": 8192,
                    "topP": 0.9, "topK": 40
                },
                "safetySettings": [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                ]
            }
            try:
                response = requests.post(url, json=payload, timeout=API_REQUEST_TIMEOUT + 120)
                response.raise_for_status()
                response_json = response.json()

                if response_json.get("promptFeedback", {}).get("blockReason"):
                    reason = response_json["promptFeedback"]["blockReason"]
                    logger.error(f"Gemini prompt blocked for key ...{api_key[-4:]}. Reason: {reason}. Prompt: '{prompt[:150]}...'")
                    time.sleep(API_RETRY_DELAY); continue

                if "candidates" in response_json and response_json["candidates"]:
                    candidate = response_json["candidates"][0]
                    finish_reason = candidate.get("finishReason")
                    if finish_reason not in [None, "STOP", "MAX_TOKENS", "MODEL_LENGTH", "OK", "OTHER"]:
                        logger.warning(f"Gemini unusual finish reason: {finish_reason} for key ...{api_key[-4:]}. Prompt: '{prompt[:150]}...'")
                        if finish_reason == "SAFETY":
                            logger.error(f"Gemini candidate content blocked by safety settings for key ...{api_key[-4:]}.")
                            time.sleep(API_RETRY_DELAY); continue

                    content_part = candidate.get("content", {}).get("parts", [{}])[0]
                    if "text" in content_part:
                        return content_part["text"]
                    else:
                        logger.error(f"Gemini response missing 'text' in content part for key ...{api_key[-4:]}: {response_json}")
                else:
                    logger.error(f"Gemini response malformed or no candidates for key ...{api_key[-4:]}: {response_json}")

            except requests.exceptions.HTTPError as e:
                response_text = e.response.text[:200] if e.response is not None else "N/A"
                status_code = e.response.status_code if e.response is not None else "N/A"
                logger.warning(
                    f"Gemini API HTTP error key ...{api_key[-4:]} attempt {current_retry_for_this_key}: {status_code} - {response_text}. Prompt: '{prompt[:150]}...'")
                if e.response is not None and e.response.status_code == 400:
                    if "API key not valid" in e.response.text or "API_KEY_INVALID" in e.response.text:
                        logger.error(f"Gemini API key ...{api_key[-4:]} reported as invalid. Skipping further retries with this key for this call.")
                        overall_attempt_num = ( (overall_attempt_num // max_attempts_per_key) + 1) * max_attempts_per_key -1
                        continue
                    else:
                        logger.error(f"Gemini API Bad Request (400). Aborting for this prompt. Response: {e.response.text[:500]}")
                        return f"Error: Gemini API bad request (400). {e.response.text[:200]}"
            except requests.exceptions.RequestException as e:
                logger.warning(f"Gemini API request error key ...{api_key[-4:]} attempt {current_retry_for_this_key}: {e}. Prompt: '{prompt[:150]}...'")
            except json.JSONDecodeError as e_json_gemini:
                resp_text_for_log = response.text[:500] if 'response' in locals() and hasattr(response, 'text') else "N/A"
                logger.error(f"Gemini API JSON decode error key ...{api_key[-4:]} attempt {current_retry_for_this_key}. Resp: {resp_text_for_log}. Err: {e_json_gemini}")

            if overall_attempt_num < (total_keys * max_attempts_per_key) - 1:
                time.sleep(API_RETRY_DELAY * current_retry_for_this_key)

        logger.error(f"All attempts ({total_keys * max_attempts_per_key}) for Gemini API failed for prompt: {prompt[:150]}...")
        return "Error: Could not get response from Gemini API after multiple attempts."

    def summarize_text_with_context(self, text_to_summarize, context_summary, desired_output_instruction):
        prompt = (
            f"Context: {context_summary}\n\n"
            f"Text to Analyze:\n\"\"\"\n{text_to_summarize}\n\"\"\"\n\n"
            f"Instructions: {desired_output_instruction}\n\n"
            f"Provide a concise and factual summary based on the text and guided by the context and instructions."
        )
        return self.generate_text(prompt)

    def analyze_sentiment_with_reasoning(self, text_to_analyze, context=""):
        prompt = (
            f"Analyze the sentiment of the following text. "
            f"Context for analysis (if any): '{context}'.\n\n"
            f"Text to Analyze:\n\"\"\"\n{text_to_analyze}\n\"\"\"\n\n"
            f"Instructions: Respond with the sentiment classification and reasoning, structured as follows:\n"
            f"Sentiment: [Choose one: Positive, Negative, Neutral]\n"
            f"Reasoning: [Provide a brief 1-2 sentence explanation, citing specific phrases from the text if possible to justify the sentiment.]"
        )
        return self.generate_text(prompt)
---------- END gemini_client.py ----------


---------- sec_edgar_client.py ----------
import requests
import json
from datetime import datetime

from .base_client import APIClient
from core.config import EDGAR_USER_AGENT, API_REQUEST_TIMEOUT
from core.logging_setup import logger


class SECEDGARClient(APIClient):
    def __init__(self):
        self.company_tickers_url = "https://www.sec.gov/files/company_tickers.json"
        super().__init__("https://data.sec.gov/submissions/")
        self.headers = {"User-Agent": EDGAR_USER_AGENT, "Accept-Encoding": "gzip, deflate"}
        self._cik_map = None
        self._archives_base = "https://www.sec.gov/Archives/edgar/data/"

    def _load_cik_map(self):
        if self._cik_map is None:
            logger.info("Fetching CIK map from SEC...")
            cache_key_str = f"GET:{self.company_tickers_url}"
            cached_map = self._get_cached_response(cache_key_str)
            if cached_map:
                self._cik_map = cached_map
                logger.info(f"CIK map loaded from cache with {len(self._cik_map)} entries.")
                return self._cik_map

            try:
                response = requests.get(self.company_tickers_url, headers=self.headers, timeout=API_REQUEST_TIMEOUT)
                response.raise_for_status()
                data = response.json()
                self._cik_map = {item['ticker']: str(item['cik_str']).zfill(10)
                                 for item in data.values() if 'ticker' in item and 'cik_str' in item}
                self._cache_response(cache_key_str, self._cik_map, "sec_cik_map")
                logger.info(f"CIK map fetched and cached with {len(self._cik_map)} entries.")
            except requests.exceptions.RequestException as e:
                logger.error(f"Error fetching CIK map from SEC: {e}", exc_info=True)
                self._cik_map = {}
            except json.JSONDecodeError as e_json:
                logger.error(f"Error decoding CIK map JSON from SEC: {e_json}", exc_info=True)
                self._cik_map = {}
        return self._cik_map

    def get_cik_by_ticker(self, ticker):
        ticker = ticker.upper()
        try:
            cik_map = self._load_cik_map()
            return cik_map.get(ticker)
        except Exception as e:
            logger.error(f"Unexpected error in get_cik_by_ticker for {ticker}: {e}", exc_info=True)
            return None

    def get_company_filings_summary(self, cik):
        if not cik: return None
        formatted_cik_for_api = str(cik).zfill(10)
        return self.request("GET", f"CIK{formatted_cik_for_api}.json", api_source_name="edgar_filings_summary")

    def get_filing_document_url(self, cik, form_type="10-K", priordate_str=None, count=1):
        if not cik: return None if count == 1 else []
        company_summary = self.get_company_filings_summary(cik)

        if not company_summary or "filings" not in company_summary or "recent" not in company_summary["filings"]:
            logger.warning(f"No recent filings data for CIK {cik} in company summary.")
            return None if count == 1 else []

        recent_filings = company_summary["filings"]["recent"]
        target_filings_info = []

        required_keys = ["form", "accessionNumber", "primaryDocument", "filingDate"]
        min_len = float('inf')
        for key in required_keys:
            if key not in recent_filings or not isinstance(recent_filings[key], list):
                logger.warning(f"Missing or invalid '{key}' in recent filings for CIK {cik}.")
                return None if count == 1 else []
            min_len = min(min_len, len(recent_filings[key]))

        if min_len == float('inf') or min_len == 0:
             logger.warning(f"No usable filing entries for CIK {cik}.")
             return None if count == 1 else []

        forms = recent_filings["form"][:min_len]
        accession_numbers = recent_filings["accessionNumber"][:min_len]
        primary_documents = recent_filings["primaryDocument"][:min_len]
        filing_dates = recent_filings["filingDate"][:min_len]

        priordate_dt = None
        if priordate_str:
            try:
                priordate_dt = datetime.strptime(priordate_str, '%Y-%m-%d').date()
            except ValueError:
                logger.warning(f"Invalid priordate_str format: {priordate_str}. Should be YYYY-MM-DD. Ignoring.")

        for i, form_val in enumerate(forms):
            if form_val.upper() == form_type.upper():
                try:
                    current_filing_date = datetime.strptime(filing_dates[i], '%Y-%m-%d').date()
                except ValueError:
                    logger.warning(f"Invalid filingDate format '{filing_dates[i]}' for CIK {cik}, entry {i}. Skipping.")
                    continue

                if priordate_dt and current_filing_date > priordate_dt:
                    continue

                acc_num_no_hyphens = accession_numbers[i].replace('-', '')
                try:
                    cik_int_for_url = int(cik)
                except ValueError:
                    logger.error(f"CIK '{cik}' for URL construction is not a valid integer. Skipping filing.")
                    continue

                doc_url = f"{self._archives_base}{cik_int_for_url}/{acc_num_no_hyphens}/{primary_documents[i]}"
                target_filings_info.append({"url": doc_url, "date": current_filing_date, "form": form_val})

        if not target_filings_info:
            logger.info(f"No '{form_type}' filings found for CIK {cik} matching criteria.")
            return None if count == 1 else []

        target_filings_info.sort(key=lambda x: x["date"], reverse=True)

        if count == 1:
            return target_filings_info[0]["url"]
        else:
            return [f_info["url"] for f_info in target_filings_info[:count]]

    def get_filing_text(self, filing_url):
        if not filing_url: return None
        logger.info(f"Fetching filing text from: {filing_url}")
        try:
            text_content = self.request("GET", filing_url, use_cache=True,
                                        api_source_name="edgar_filing_text_content",
                                        is_json_response=False)
            if text_content:
                if isinstance(text_content, bytes):
                    try:
                        text_content = text_content.decode('utf-8')
                    except UnicodeDecodeError:
                        logger.warning(f"UTF-8 decode failed for {filing_url}, trying latin-1.")
                        text_content = text_content.decode('latin-1', errors='replace')
            return text_content
        except requests.exceptions.RequestException as e:
            logger.error(f"Error fetching SEC filing text from {filing_url}: {e}")
            return None
---------- END sec_edgar_client.py ----------


---------- __init__.py ----------
from .config import *
from .logging_setup import logger, setup_logging, handle_global_exception

__all__ = [
    # From config (example, list all you need)
    "GOOGLE_API_KEYS", "FINNHUB_API_KEY", "DATABASE_URL", "LOG_FILE_PATH",
    # From logging_setup
    "logger", "setup_logging", "handle_global_exception"
]
---------- END __init__.py ----------


---------- config.py ----------
# core/config.py

GOOGLE_API_KEYS = [
    "AIzaSyDLkwkVYBTUjabShS7VfdLkQTe7vZkxcjY", # Replace with your actual key
    "AIzaSyAjECAJZVZz6PzDaUVaAkgfcOeLXCPFA6Y", # Replace with your actual key
    "AIzaSyBRDIgN7ffBvoqAgaizQfuWRQExKc_oVig", # Replace with your actual key
    "AIzaSyC4XLSmSX4U2iuAqW_pvQ87eNyPaJwQpDo", # Replace with your actual key
]

FINNHUB_API_KEY = "d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390"  # Replace with your actual key
FINANCIAL_MODELING_PREP_API_KEY = "62ERGmJoqQgGD0nSGxRZS91TVzfz61uB"  # Replace with your actual key
EODHD_API_KEY = "683079df749c42.21476005"  # Replace with your actual key or "demo"
RAPIDAPI_UPCOMING_IPO_KEY = "0bd9b5144cmsh50c0e6d95c0b662p1cbdefjsn2d1cb0104cde"  # Replace with your actual key
ALPHA_VANTAGE_API_KEY = "HB6N4X55UTFGN2FP" # Replace with your actual Alpha Vantage Key

# SEC EDGAR Configuration
EDGAR_USER_AGENT = "FinancialAnalysisBot/1.0 YourCompanyName YourContactEmail@example.com"  # Be specific and polite

# Database Configuration
DATABASE_URL = "postgresql://avnadmin:AVNS_IeMYS-rv46Au9xqkza2@pg-4d810ff-daxiake-7258.d.aivencloud.com:26922/stock-alarm?sslmode=require"

# Email Configuration
EMAIL_HOST = "smtp-relay.brevo.com"
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_HOST_USER = "8dca1d001@smtp-brevo.com"
EMAIL_HOST_PASSWORD = "VrNUkDdcR5G9AL8P"
EMAIL_SENDER = "testypesty54@gmail.com"
EMAIL_RECIPIENT = "daniprav@gmail.com"

# Logging Configuration
LOG_FILE_PATH = "app_analysis.log"
LOG_LEVEL = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# API Client Settings
API_REQUEST_TIMEOUT = 45  # seconds
API_RETRY_ATTEMPTS = 3
API_RETRY_DELAY = 10  # seconds

# Gemini API Configuration
GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE = 400000

# Chunking for Summarization
SUMMARIZATION_CHUNK_SIZE_CHARS = 80000
SUMMARIZATION_CHUNK_OVERLAP_CHARS = 5000
SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS = 100000

# Analysis Settings
MAX_NEWS_ARTICLES_PER_QUERY = 10
MAX_NEWS_TO_ANALYZE_PER_RUN = 5
MIN_MARKET_CAP = 1000000000
STOCK_FINANCIAL_YEARS = 7
IPO_ANALYSIS_REANALYZE_DAYS = 7

# Cache Settings
CACHE_EXPIRY_SECONDS = 3600 * 6

# DCF Analysis Defaults
DEFAULT_DISCOUNT_RATE = 0.09
DEFAULT_PERPETUAL_GROWTH_RATE = 0.025
DEFAULT_FCF_PROJECTION_YEARS = 5

# News Analysis
NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION = GEMINI_PROMPT_MAX_CHARS_HARD_TRUNCATE - 10000

# IPO/10-K Sections
S1_KEY_SECTIONS = {
    "business": ["Item 1.", "Business"],
    "risk_factors": ["Item 1A.", "Risk Factors"],
    "mda": ["Item 7.", "Management's Discussion and Analysis of Financial Condition and Results of Operations"],
    "financial_statements": ["Item 8.", "Financial Statements and Supplementary Data"]
}
TEN_K_KEY_SECTIONS = S1_KEY_SECTIONS

# Stock Analyzer specific settings
MAX_COMPETITORS_TO_ANALYZE = 5
Q_REVENUE_SANITY_CHECK_DEVIATION_THRESHOLD = 0.75
PRIORITY_REVENUE_SOURCES = ["fmp_quarterly", "finnhub_quarterly", "alphavantage_quarterly"]
---------- END config.py ----------


---------- logging_setup.py ----------
import logging
import sys
from .config import LOG_FILE_PATH, LOG_LEVEL

_logging_configured = False

def setup_logging():
    """Configures logging for the application."""
    global _logging_configured
    if _logging_configured:
        return logging.getLogger()

    numeric_level = getattr(logging, LOG_LEVEL.upper(), None)
    if not isinstance(numeric_level, int):
        logging.warning(f"Invalid log level: {LOG_LEVEL} in config. Defaulting to INFO.")
        numeric_level = logging.INFO

    logger_obj = logging.getLogger()
    logger_obj.setLevel(numeric_level)

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')

    if not any(isinstance(h, logging.StreamHandler) for h in logger_obj.handlers):
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        logger_obj.addHandler(console_handler)

    if not any(isinstance(h, logging.FileHandler) and getattr(h, 'baseFilename', None) == LOG_FILE_PATH for h in logger_obj.handlers):
        try:
            file_handler = logging.FileHandler(LOG_FILE_PATH, mode='a')
            file_handler.setFormatter(formatter)
            logger_obj.addHandler(file_handler)
        except Exception as e:
            logging.error(f"Failed to set up file handler for {LOG_FILE_PATH}: {e}", exc_info=True)

    _logging_configured = True
    return logger_obj

logger = setup_logging()

def handle_global_exception(exc_type, exc_value, exc_traceback):
    """Custom global exception handler to log unhandled exceptions."""
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    logger.critical("Unhandled global exception:", exc_info=(exc_type, exc_value, exc_traceback))

# To use the global exception handler, uncomment the following line in your main script (e.g., main.py)
# sys.excepthook = handle_global_exception
---------- END logging_setup.py ----------


---------- __init__.py ----------
from .connection import Base, engine, SessionLocal, init_db, get_db_session
from .models import Stock, StockAnalysis, IPO, IPOAnalysis, NewsEvent, NewsEventAnalysis, CachedAPIData

__all__ = [
    "Base", "engine", "SessionLocal", "init_db", "get_db_session",
    "Stock", "StockAnalysis", "IPO", "IPOAnalysis",
    "NewsEvent", "NewsEventAnalysis", "CachedAPIData"
]
---------- END __init__.py ----------


---------- connection.py ----------
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, scoped_session
from core.config import DATABASE_URL
from core.logging_setup import logger

Base = declarative_base() # Moved from models.py

try:
    engine = create_engine(DATABASE_URL, pool_pre_ping=True)
    SessionFactory = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    SessionLocal = scoped_session(SessionFactory)

    # Base.query = SessionLocal.query_property() # This is optional

    def init_db():
        """Initializes the database and creates tables if they don't exist."""
        try:
            logger.info("Initializing database and creating tables...")
            # Import models here to ensure they are registered with Base metadata
            from . import models # noqa F401
            Base.metadata.create_all(bind=engine)
            logger.info("Database tables created successfully (if they didn't exist).")
        except Exception as e:
            logger.critical(f"CRITICAL Error initializing database: {e}", exc_info=True)
            raise

    def get_db_session():
        """Provides a database session. Caller is responsible for closing."""
        db = SessionLocal()
        try:
            yield db
        finally:
            # SessionLocal.remove() is called automatically by analyzers or when scope ends
            # For direct usage in main.py, ensure SessionLocal.remove() or db.close() is called.
             if db.is_active: # Check if session is still active before closing
                db.close()


except Exception as e:
    logger.critical(f"CRITICAL Failed to connect to database or setup SQLAlchemy: {e}", exc_info=True)
    raise
---------- END connection.py ----------


---------- models.py ----------
from sqlalchemy import Column, Integer, String, Float, DateTime, Text, JSON, ForeignKey, Boolean, Date, UniqueConstraint
from sqlalchemy.orm import relationship
from datetime import datetime, timezone
from .connection import Base  # Import Base from connection.py


class Stock(Base):
    __tablename__ = "stocks"
    id = Column(Integer, primary_key=True, index=True)
    ticker = Column(String, unique=True, index=True, nullable=False)
    company_name = Column(String)
    industry = Column(String, nullable=True)
    sector = Column(String, nullable=True)
    last_analysis_date = Column(DateTime(timezone=True),
                                default=lambda: datetime.now(timezone.utc),
                                onupdate=lambda: datetime.now(timezone.utc))
    cik = Column(String, nullable=True, index=True)

    analyses = relationship("StockAnalysis", back_populates="stock", cascade="all, delete-orphan")


class StockAnalysis(Base):
    __tablename__ = "stock_analyses"
    id = Column(Integer, primary_key=True, index=True)
    stock_id = Column(Integer, ForeignKey("stocks.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    pe_ratio = Column(Float, nullable=True)
    pb_ratio = Column(Float, nullable=True)
    ps_ratio = Column(Float, nullable=True)
    ev_to_sales = Column(Float, nullable=True)
    ev_to_ebitda = Column(Float, nullable=True)
    eps = Column(Float, nullable=True)
    roe = Column(Float, nullable=True)
    roa = Column(Float, nullable=True)
    roic = Column(Float, nullable=True)
    dividend_yield = Column(Float, nullable=True)
    debt_to_equity = Column(Float, nullable=True)
    debt_to_ebitda = Column(Float, nullable=True)
    interest_coverage_ratio = Column(Float, nullable=True)
    current_ratio = Column(Float, nullable=True)
    quick_ratio = Column(Float, nullable=True)
    revenue_growth_yoy = Column(Float, nullable=True)
    revenue_growth_qoq = Column(Float, nullable=True)
    revenue_growth_cagr_3yr = Column(Float, nullable=True)
    revenue_growth_cagr_5yr = Column(Float, nullable=True)
    eps_growth_yoy = Column(Float, nullable=True)
    eps_growth_cagr_3yr = Column(Float, nullable=True)
    eps_growth_cagr_5yr = Column(Float, nullable=True)
    net_profit_margin = Column(Float, nullable=True)
    gross_profit_margin = Column(Float, nullable=True)
    operating_profit_margin = Column(Float, nullable=True)
    free_cash_flow_per_share = Column(Float, nullable=True)
    free_cash_flow_yield = Column(Float, nullable=True)
    free_cash_flow_trend = Column(String, nullable=True)
    retained_earnings_trend = Column(String, nullable=True)
    dcf_intrinsic_value = Column(Float, nullable=True)
    dcf_upside_percentage = Column(Float, nullable=True)
    dcf_assumptions = Column(JSON, nullable=True)
    business_summary = Column(Text, nullable=True)
    economic_moat_summary = Column(Text, nullable=True)
    industry_trends_summary = Column(Text, nullable=True)
    competitive_landscape_summary = Column(Text, nullable=True)
    management_assessment_summary = Column(Text, nullable=True)
    risk_factors_summary = Column(Text, nullable=True)
    investment_thesis_full = Column(Text, nullable=True)
    investment_decision = Column(String, nullable=True)
    reasoning = Column(Text, nullable=True)
    strategy_type = Column(String, nullable=True)
    confidence_level = Column(String, nullable=True)
    key_metrics_snapshot = Column(JSON, nullable=True)
    qualitative_sources_summary = Column(JSON, nullable=True)

    stock = relationship("Stock", back_populates="analyses")


class IPO(Base):
    __tablename__ = "ipos"
    id = Column(Integer, primary_key=True, index=True)
    company_name = Column(String, index=True, nullable=False)
    symbol = Column(String, index=True, nullable=True)
    ipo_date_str = Column(String, nullable=True)
    ipo_date = Column(Date, nullable=True)
    expected_price_range_low = Column(Float, nullable=True)
    expected_price_range_high = Column(Float, nullable=True)
    expected_price_currency = Column(String, nullable=True, default="USD")
    offered_shares = Column(Float, nullable=True)
    total_shares_value = Column(Float, nullable=True)
    exchange = Column(String, nullable=True)
    status = Column(String, nullable=True)
    cik = Column(String, nullable=True, index=True)
    last_analysis_date = Column(DateTime(timezone=True),
                                default=lambda: datetime.now(timezone.utc),
                                onupdate=lambda: datetime.now(timezone.utc))
    s1_filing_url = Column(String, nullable=True)

    analyses = relationship("IPOAnalysis", back_populates="ipo", cascade="all, delete-orphan")
    __table_args__ = (UniqueConstraint('company_name', 'ipo_date_str', 'symbol', name='uq_ipo_name_date_symbol'),)


class IPOAnalysis(Base):
    __tablename__ = "ipo_analyses"
    id = Column(Integer, primary_key=True, index=True)
    ipo_id = Column(Integer, ForeignKey("ipos.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    s1_business_summary = Column(Text, nullable=True)
    s1_risk_factors_summary = Column(Text, nullable=True)
    s1_mda_summary = Column(Text, nullable=True)
    s1_financial_health_summary = Column(Text, nullable=True)
    competitive_landscape_summary = Column(Text, nullable=True)
    industry_outlook_summary = Column(Text, nullable=True)
    management_team_assessment = Column(Text, nullable=True)
    use_of_proceeds_summary = Column(Text, nullable=True)
    underwriter_quality_assessment = Column(String, nullable=True)
    business_model_summary = Column(Text, nullable=True)
    risk_factors_summary = Column(Text, nullable=True)
    pre_ipo_financials_summary = Column(Text, nullable=True)
    valuation_comparison_summary = Column(Text, nullable=True)
    investment_decision = Column(String, nullable=True)
    reasoning = Column(Text, nullable=True)
    key_data_snapshot = Column(JSON, nullable=True)
    s1_sections_used = Column(JSON, nullable=True)

    ipo = relationship("IPO", back_populates="analyses")


class NewsEvent(Base):
    __tablename__ = "news_events"
    id = Column(Integer, primary_key=True, index=True)
    event_title = Column(String, index=True)
    event_date = Column(DateTime(timezone=True), nullable=True)
    source_url = Column(String, unique=True, nullable=False, index=True)
    source_name = Column(String, nullable=True)
    category = Column(String, nullable=True)
    processed_date = Column(DateTime(timezone=True),
                            default=lambda: datetime.now(timezone.utc))
    last_analyzed_date = Column(DateTime(timezone=True), nullable=True,
                                onupdate=lambda: datetime.now(timezone.utc))
    full_article_text = Column(Text, nullable=True)

    analyses = relationship("NewsEventAnalysis", back_populates="news_event", cascade="all, delete-orphan")
    __table_args__ = (UniqueConstraint('source_url', name='uq_news_source_url'),)


class NewsEventAnalysis(Base):
    __tablename__ = "news_event_analyses"
    id = Column(Integer, primary_key=True, index=True)
    news_event_id = Column(Integer, ForeignKey("news_events.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    sentiment = Column(String, nullable=True)
    sentiment_reasoning = Column(Text, nullable=True)
    affected_stocks_explicit = Column(JSON, nullable=True)
    affected_sectors_explicit = Column(JSON, nullable=True)
    news_summary_detailed = Column(Text, nullable=True)
    potential_impact_on_market = Column(Text, nullable=True)
    potential_impact_on_companies = Column(Text, nullable=True)
    potential_impact_on_sectors = Column(Text, nullable=True)
    mechanism_of_impact = Column(Text, nullable=True)
    estimated_timing_duration = Column(String, nullable=True)
    estimated_magnitude_direction = Column(String, nullable=True)
    confidence_of_assessment = Column(String, nullable=True)
    summary_for_email = Column(Text, nullable=True)
    key_news_snippets = Column(JSON, nullable=True)

    news_event = relationship("NewsEvent", back_populates="analyses")


class CachedAPIData(Base):
    __tablename__ = "cached_api_data"
    id = Column(Integer, primary_key=True, index=True)
    api_source = Column(String, index=True, nullable=False)
    request_url_or_params = Column(String, unique=True, nullable=False, index=True)
    response_data = Column(JSON, nullable=False)
    timestamp = Column(DateTime(timezone=True),
                       default=lambda: datetime.now(timezone.utc))
    expires_at = Column(DateTime(timezone=True), nullable=False, index=True)
---------- END models.py ----------


---------- __init__.py ----------
# services/__init__.py
from .stock_analyzer import StockAnalyzer
from .ipo_analyzer import IPOAnalyzer
from .news_analyzer import NewsAnalyzer
from .email_service import EmailService

__all__ = [
    "StockAnalyzer",
    "IPOAnalyzer",
    "NewsAnalyzer",
    "EmailService",
]
---------- END __init__.py ----------


---------- email_service.py ----------
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from datetime import datetime, timezone
import json
import math
from markdown2 import Markdown

from core.config import (
    EMAIL_HOST, EMAIL_PORT, EMAIL_USE_TLS, EMAIL_HOST_USER,
    EMAIL_HOST_PASSWORD, EMAIL_SENDER, EMAIL_RECIPIENT
)
from core.logging_setup import logger
from database.models import StockAnalysis, IPOAnalysis, NewsEventAnalysis


class EmailService:
    def __init__(self):
        self.markdowner = Markdown(extras=["tables", "fenced-code-blocks", "break-on-newline"])

    def _md_to_html(self, md_text):
        if md_text is None: return "<p>N/A</p>"
        if isinstance(md_text, (dict, list)):
            return f"<pre>{json.dumps(md_text, indent=2)}</pre>"
        if not isinstance(md_text, str): md_text = str(md_text)
        if "<" in md_text and ">" in md_text and ("<p>" in md_text.lower() or "<div>" in md_text.lower()):
            return md_text
        return self.markdowner.convert(md_text)

    def _format_stock_analysis_html(self, analysis: StockAnalysis):
        if not analysis: return ""
        stock = analysis.stock

        def fmt_num(val, type="decimal", na_val="N/A"):
            if val is None or (isinstance(val, float) and (math.isnan(val) or math.isinf(val))): return na_val
            if type == "percent": return f"{val * 100:.2f}%"
            if type == "decimal": return f"{val:.2f}"
            return str(val)

        business_summary_html = self._md_to_html(analysis.business_summary)
        economic_moat_html = self._md_to_html(analysis.economic_moat_summary)
        industry_trends_html = self._md_to_html(analysis.industry_trends_summary)
        competitive_landscape_html = self._md_to_html(analysis.competitive_landscape_summary)
        management_assessment_html = self._md_to_html(analysis.management_assessment_summary)
        risk_factors_html = self._md_to_html(analysis.risk_factors_summary)
        investment_thesis_html = self._md_to_html(analysis.investment_thesis_full)
        reasoning_points_html = self._md_to_html(analysis.reasoning)

        dcf_assumptions_html = "<ul>"
        if analysis.dcf_assumptions and isinstance(analysis.dcf_assumptions, dict):
            assumptions_data = analysis.dcf_assumptions
            dcf_assumptions_html += f"<li>Discount Rate: {fmt_num(assumptions_data.get('discount_rate'), 'percent')}</li>"
            dcf_assumptions_html += f"<li>Perpetual Growth Rate: {fmt_num(assumptions_data.get('perpetual_growth_rate'), 'percent')}</li>"
            dcf_assumptions_html += f"<li>FCF Projection Years: {assumptions_data.get('projection_years', 'N/A')}</li>"
            dcf_assumptions_html += f"<li>Starting FCF: {fmt_num(assumptions_data.get('start_fcf'))}</li>"
            fcf_growth_proj = assumptions_data.get('fcf_growth_rates_projection')
            if fcf_growth_proj and isinstance(fcf_growth_proj, list):
                 dcf_assumptions_html += f"<li>Projected FCF Growth Rates: {', '.join([fmt_num(r, 'percent') for r in fcf_growth_proj])}</li>"
        else:
            dcf_assumptions_html += "<li>N/A</li>"
        dcf_assumptions_html += "</ul>"

        html = f"""
        <div class="analysis-block stock-analysis">
            <h2>Stock Analysis: {stock.company_name} ({stock.ticker})</h2>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Industry:</strong> {stock.industry or 'N/A'}, <strong>Sector:</strong> {stock.sector or 'N/A'}</p>
            <p><strong>Investment Decision:</strong> {analysis.investment_decision or 'N/A'}</p>
            <p><strong>Strategy Type:</strong> {analysis.strategy_type or 'N/A'}</p>
            <p><strong>Confidence Level:</strong> {analysis.confidence_level or 'N/A'}</p>
            <details>
                <summary><strong>Investment Thesis & Reasoning (Click to expand)</strong></summary>
                <h4>Full Thesis:</h4><div class="markdown-content">{investment_thesis_html}</div>
                <h4>Key Reasoning Points:</h4><div class="markdown-content">{reasoning_points_html}</div>
            </details>
            <details>
                <summary><strong>Key Financial Metrics (Click to expand)</strong></summary>
                <ul>
                    <li>P/E Ratio: {fmt_num(analysis.pe_ratio)}</li><li>P/B Ratio: {fmt_num(analysis.pb_ratio)}</li>
                    <li>P/S Ratio: {fmt_num(analysis.ps_ratio)}</li><li>EV/Sales: {fmt_num(analysis.ev_to_sales)}</li>
                    <li>EV/EBITDA: {fmt_num(analysis.ev_to_ebitda)}</li><li>EPS: {fmt_num(analysis.eps)}</li>
                    <li>ROE: {fmt_num(analysis.roe, 'percent')}</li><li>ROA: {fmt_num(analysis.roa, 'percent')}</li>
                    <li>ROIC: {fmt_num(analysis.roic, 'percent')}</li><li>Dividend Yield: {fmt_num(analysis.dividend_yield, 'percent')}</li>
                    <li>Debt-to-Equity: {fmt_num(analysis.debt_to_equity)}</li><li>Debt-to-EBITDA: {fmt_num(analysis.debt_to_ebitda)}</li>
                    <li>Interest Coverage: {fmt_num(analysis.interest_coverage_ratio)}x</li><li>Current Ratio: {fmt_num(analysis.current_ratio)}</li>
                    <li>Quick Ratio: {fmt_num(analysis.quick_ratio)}</li>
                    <li>Gross Profit Margin: {fmt_num(analysis.gross_profit_margin, 'percent')}</li>
                    <li>Operating Profit Margin: {fmt_num(analysis.operating_profit_margin, 'percent')}</li>
                    <li>Net Profit Margin: {fmt_num(analysis.net_profit_margin, 'percent')}</li>
                    <li>Revenue Growth YoY: {fmt_num(analysis.revenue_growth_yoy, 'percent')} (QoQ: {fmt_num(analysis.revenue_growth_qoq, 'percent')})</li>
                    <li>Revenue Growth CAGR (3yr/5yr): {fmt_num(analysis.revenue_growth_cagr_3yr, 'percent')} / {fmt_num(analysis.revenue_growth_cagr_5yr, 'percent')}</li>
                    <li>EPS Growth YoY: {fmt_num(analysis.eps_growth_yoy, 'percent')}</li>
                    <li>EPS Growth CAGR (3yr/5yr): {fmt_num(analysis.eps_growth_cagr_3yr, 'percent')} / {fmt_num(analysis.eps_growth_cagr_5yr, 'percent')}</li>
                    <li>FCF per Share: {fmt_num(analysis.free_cash_flow_per_share)}</li><li>FCF Yield: {fmt_num(analysis.free_cash_flow_yield, 'percent')}</li>
                    <li>FCF Trend: {analysis.free_cash_flow_trend or 'N/A'}</li><li>Retained Earnings Trend: {analysis.retained_earnings_trend or 'N/A'}</li>
                </ul>
            </details>
            <details>
                <summary><strong>DCF Analysis (Simplified) (Click to expand)</strong></summary>
                <ul>
                    <li>Intrinsic Value per Share: {fmt_num(analysis.dcf_intrinsic_value)}</li>
                    <li>Upside/Downside: {fmt_num(analysis.dcf_upside_percentage, 'percent')}</li>
                </ul>
                <p><em>Key Assumptions Used:</em></p>
                {dcf_assumptions_html}
            </details>
            <details>
                <summary><strong>Qualitative Analysis (from 10-K/Profile & AI) (Click to expand)</strong></summary>
                <p><strong>Business Summary:</strong></p><div class="markdown-content">{business_summary_html}</div>
                <p><strong>Economic Moat:</strong></p><div class="markdown-content">{economic_moat_html}</div>
                <p><strong>Industry Trends & Position:</strong></p><div class="markdown-content">{industry_trends_html}</div>
                <p><strong>Competitive Landscape:</strong></p><div class="markdown-content">{competitive_landscape_html}</div>
                <p><strong>Management Discussion Highlights (MD&A/Assessment):</strong></p><div class="markdown-content">{management_assessment_html}</div>
                <p><strong>Key Risk Factors:</strong></p><div class="markdown-content">{risk_factors_html}</div>
            </details>
            <details>
                <summary><strong>Supporting Data Snapshots (Click to expand)</strong></summary>
                <p><em>Key Metrics Data Points Used:</em></p><div class="markdown-content">{self._md_to_html(analysis.key_metrics_snapshot)}</div>
                <p><em>Qualitative Analysis Sources Summary:</em></p><div class="markdown-content">{self._md_to_html(analysis.qualitative_sources_summary)}</div>
            </details>
        </div>
        """
        return html

    def _format_ipo_analysis_html(self, analysis: IPOAnalysis):
        if not analysis: return ""
        ipo = analysis.ipo
        def fmt_price(val_low, val_high, currency="USD"):
            if val_low is None and val_high is None: return "N/A"
            if val_low is not None and val_high is not None:
                if val_low == val_high: return f"{val_low:.2f} {currency}"
                return f"{val_low:.2f} - {val_high:.2f} {currency}"
            if val_low is not None: return f"{val_low:.2f} {currency}"
            if val_high is not None: return f"{val_high:.2f} {currency}"
            return "N/A"
        reasoning_html = self._md_to_html(analysis.reasoning)
        s1_business_summary_html = self._md_to_html(analysis.s1_business_summary or analysis.business_model_summary)
        s1_risk_factors_summary_html = self._md_to_html(analysis.s1_risk_factors_summary or analysis.risk_factors_summary)
        s1_mda_summary_html = self._md_to_html(analysis.s1_mda_summary)
        s1_financial_health_summary_html = self._md_to_html(analysis.s1_financial_health_summary or analysis.pre_ipo_financials_summary)
        competitive_landscape_html = self._md_to_html(analysis.competitive_landscape_summary)
        industry_outlook_html = self._md_to_html(analysis.industry_outlook_summary)
        use_of_proceeds_html = self._md_to_html(analysis.use_of_proceeds_summary)
        management_team_html = self._md_to_html(analysis.management_team_assessment)
        underwriter_html = self._md_to_html(analysis.underwriter_quality_assessment)
        valuation_html = self._md_to_html(analysis.valuation_comparison_summary)
        html = f"""
        <div class="analysis-block ipo-analysis">
            <h2>IPO Analysis: {ipo.company_name} ({ipo.symbol or 'N/A'})</h2>
            <p><strong>Expected IPO Date:</strong> {ipo.ipo_date.strftime('%Y-%m-%d') if ipo.ipo_date else ipo.ipo_date_str or 'N/A'}</p>
            <p><strong>Expected Price Range:</strong> {fmt_price(ipo.expected_price_range_low, ipo.expected_price_range_high, ipo.expected_price_currency)}</p>
            <p><strong>Exchange:</strong> {ipo.exchange or 'N/A'}, <strong>Status:</strong> {ipo.status or 'N/A'}</p>
            <p><strong>S-1 Filing URL:</strong> {f'<a href="{ipo.s1_filing_url}">{ipo.s1_filing_url}</a>' if ipo.s1_filing_url else 'Not Found'}</p>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Preliminary Stance:</strong> {analysis.investment_decision or 'N/A'}</p>
            <details><summary><strong>AI Synthesized Reasoning & Critical Verification Points (Click to expand)</strong></summary><div class="markdown-content">{reasoning_html}</div></details>
            <details>
                <summary><strong>S-1 Based Summaries (if available) & AI Analysis (Click to expand)</strong></summary>
                <p><strong>Business Summary (S-1/inferred):</strong></p><div class="markdown-content">{s1_business_summary_html}</div>
                <p><strong>Competitive Landscape:</strong></p><div class="markdown-content">{competitive_landscape_html}</div>
                <p><strong>Industry Outlook:</strong></p><div class="markdown-content">{industry_outlook_html}</div>
                <p><strong>Risk Factors Summary (S-1/inferred):</strong></p><div class="markdown-content">{s1_risk_factors_summary_html}</div>
                <p><strong>Use of Proceeds (S-1/inferred):</strong></p><div class="markdown-content">{use_of_proceeds_html}</div>
                <p><strong>MD&A / Financial Health Summary (S-1/inferred):</strong></p><div class="markdown-content">{s1_mda_summary_html if s1_mda_summary_html and not s1_mda_summary_html.startswith("Section not found") else s1_financial_health_summary_html}</div>
                <p><strong>Management Team Assessment:</strong></p><div class="markdown-content">{management_team_html}</div>
                <p><strong>Underwriter Quality Assessment:</strong></p><div class="markdown-content">{underwriter_html}</div>
                <p><strong>Valuation Comparison Guidance:</strong></p><div class="markdown-content">{valuation_html}</div>
            </details>
            <details><summary><strong>Supporting Data (Click to expand)</strong></summary>
                <p><em>Raw IPO calendar API data:</em></p><div class="markdown-content">{self._md_to_html(analysis.key_data_snapshot)}</div>
                <p><em>S-1 Sections Used (True if found & used):</em></p><div class="markdown-content">{self._md_to_html(analysis.s1_sections_used)}</div>
            </details>
        </div>"""
        return html

    def _format_news_event_analysis_html(self, analysis: NewsEventAnalysis):
        if not analysis: return ""
        news_event = analysis.news_event
        sentiment_html = self._md_to_html(f"**Sentiment:** {analysis.sentiment or 'N/A'}\n**Reasoning:** {analysis.sentiment_reasoning or 'N/A'}")
        news_summary_detailed_html = self._md_to_html(analysis.news_summary_detailed)
        impact_companies_html = self._md_to_html(analysis.potential_impact_on_companies)
        impact_sectors_html = self._md_to_html(analysis.potential_impact_on_sectors)
        mechanism_html = self._md_to_html(analysis.mechanism_of_impact)
        timing_duration_html = self._md_to_html(analysis.estimated_timing_duration)
        magnitude_direction_html = self._md_to_html(analysis.estimated_magnitude_direction)
        confidence_html = self._md_to_html(analysis.confidence_of_assessment)
        investor_summary_html = self._md_to_html(analysis.summary_for_email)
        html = f"""
        <div class="analysis-block news-analysis">
            <h2>News/Event Analysis: {news_event.event_title}</h2>
            <p><strong>Event Date:</strong> {news_event.event_date.strftime('%Y-%m-%d %H:%M %Z') if news_event.event_date else 'N/A'}</p>
            <p><strong>Source:</strong> <a href="{news_event.source_url}">{news_event.source_name or news_event.source_url}</a></p>
            <p><strong>Full Article Scraped:</strong> {'Yes' if news_event.full_article_text else 'No (Analysis based on headline/summary if available)'}</p>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Investor Summary:</strong></p><div class="markdown-content">{investor_summary_html}</div>
            <details><summary><strong>Detailed AI Analysis (Click to expand)</strong></summary>
                <p><strong>Sentiment Analysis:</strong></p><div class="markdown-content">{sentiment_html}</div>
                <p><strong>Detailed News Summary:</strong></p><div class="markdown-content">{news_summary_detailed_html}</div>
                <p><strong>Potentially Affected Companies/Stocks:</strong></p><div class="markdown-content">{impact_companies_html}</div>
                <p><strong>Potentially Affected Sectors:</strong></p><div class="markdown-content">{impact_sectors_html}</div>
                <p><strong>Mechanism of Impact:</strong></p><div class="markdown-content">{mechanism_html}</div>
                <p><strong>Estimated Timing & Duration:</strong></p><div class="markdown-content">{timing_duration_html}</div>
                <p><strong>Estimated Magnitude & Direction:</strong></p><div class="markdown-content">{magnitude_direction_html}</div>
                <p><strong>Confidence of Assessment:</strong></p><div class="markdown-content">{confidence_html}</div>
            </details>
            <details><summary><strong>Key Snippets Used for Analysis (Click to expand)</strong></summary><div class="markdown-content">{self._md_to_html(analysis.key_news_snippets)}</div></details>
        </div>"""
        return html

    def create_summary_email(self, stock_analyses=None, ipo_analyses=None, news_analyses=None):
        if not any([stock_analyses, ipo_analyses, news_analyses]):
            logger.info("No analyses provided to create an email."); return None
        subject_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        subject = f"Financial Analysis Summary - {subject_date}"
        html_body = f"""
        <html><head><style>
            body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; background-color: #f4f4f4; line-height: 1.6; color: #333; }}
            .container {{ background-color: #ffffff; padding: 20px; border-radius: 8px; box-shadow: 0 0 15px rgba(0,0,0,0.1); max-width: 900px; margin: auto; }}
            .analysis-block {{ border: 1px solid #ddd; padding: 15px; margin-bottom: 25px; border-radius: 5px; background-color: #fdfdfd; box-shadow: 0 2px 4px rgba(0,0,0,0.05);}}
            .stock-analysis {{ border-left: 5px solid #4CAF50; }} .ipo-analysis {{ border-left: 5px solid #2196F3; }} .news-analysis {{ border-left: 5px solid #FFC107; }}
            h1 {{ color: #2c3e50; text-align: center; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
            h2 {{ color: #34495e; border-bottom: 1px solid #eee; padding-bottom: 5px; margin-top: 0; }}
            h4 {{ color: #555; margin-top: 15px; margin-bottom: 5px; }}
            details > summary {{ cursor: pointer; font-weight: bold; margin-bottom: 10px; color: #2980b9; padding: 5px; background-color: #ecf0f1; border-radius:3px; }}
            details[open] > summary {{ background-color: #dde5e8; }}
            pre {{ background-color: #eee; padding: 10px; border-radius: 4px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; font-size: 0.85em; border: 1px solid #ccc; }}
            ul {{ list-style-type: disc; margin-left: 20px; padding-left: 5px; }} li {{ margin-bottom: 8px; }}
            .markdown-content p {{ margin: 0.5em 0; }} .markdown-content ul, .markdown-content ol {{ margin-left: 20px; }}
            .markdown-content table {{ border-collapse: collapse; width: 100%; margin-bottom: 1em;}}
            .markdown-content th, .markdown-content td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }} .markdown-content th {{ background-color: #f2f2f2; }}
            .report-footer {{ text-align: center; font-size: 0.9em; color: #777; margin-top: 30px; }}
        </style></head><body><div class="container">
            <h1>Financial Analysis Report</h1>
            <p style="text-align:center; font-style:italic; color:#555;">Generated: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z')}</p>
            <p style="text-align:center; font-style:italic; color:#7f8c8d;"><em>This email contains automated analysis. Always do your own research before making investment decisions.</em></p>"""
        if stock_analyses: html_body += "<h2>Individual Stock Analyses</h2>"; [html_body := html_body + self._format_stock_analysis_html(sa) for sa in stock_analyses]
        if ipo_analyses: html_body += "<h2>Upcoming IPO Analyses</h2>"; [html_body := html_body + self._format_ipo_analysis_html(ia) for ia in ipo_analyses]
        if news_analyses: html_body += "<h2>Recent News & Event Analyses</h2>"; [html_body := html_body + self._format_news_event_analysis_html(na) for na in news_analyses]
        html_body += """<div class="report-footer"><p>© Automated Financial Analysis System</p></div></div></body></html>"""
        msg = MIMEMultipart('alternative'); msg['Subject'], msg['From'], msg['To'] = subject, EMAIL_SENDER, EMAIL_RECIPIENT
        msg.attach(MIMEText(html_body, 'html', 'utf-8')); return msg

    def send_email(self, message: MIMEMultipart):
        if not message: logger.error("No message object provided to send_email."); return False
        try:
            smtp_server = smtplib.SMTP(EMAIL_HOST, EMAIL_PORT, timeout=20)
            if EMAIL_USE_TLS: smtp_server.starttls()
            smtp_server.login(EMAIL_HOST_USER, EMAIL_HOST_PASSWORD)
            smtp_server.sendmail(EMAIL_SENDER, EMAIL_RECIPIENT, message.as_string())
            smtp_server.quit(); logger.info(f"Email sent successfully to {EMAIL_RECIPIENT}"); return True
        except smtplib.SMTPException as e_smtp: logger.error(f"SMTP error sending email: {e_smtp}", exc_info=True); return False
        except Exception as e: logger.error(f"General error sending email: {e}", exc_info=True); return False

if __name__ == '__main__':
    logger.info("Starting email service test...")
    class MockStock: __init__ = lambda self, ticker, company_name, industry="Tech", sector="Software": setattr(self, 'ticker', ticker) or setattr(self, 'company_name', company_name) or setattr(self, 'industry', industry) or setattr(self, 'sector', sector)
    class MockIPO: __init__ = lambda self, company_name, symbol, ipo_date_str="2025-07-15": setattr(self, 'company_name', company_name) or setattr(self, 'symbol', symbol) or setattr(self, 'ipo_date_str', ipo_date_str) or setattr(self, 'ipo_date', datetime.strptime(ipo_date_str, "%Y-%m-%d").date() if ipo_date_str else None) or setattr(self, 'expected_price_range_low', 18.00) or setattr(self, 'expected_price_range_high', 22.00) or setattr(self, 'expected_price_currency', "USD") or setattr(self, 'exchange', "NASDAQ") or setattr(self, 'status', "Filed") or setattr(self, 's1_filing_url', "http://example.com/s1")
    class MockNewsEvent: __init__ = lambda self, title, url, event_date_str="2025-05-25 10:00:00": setattr(self, 'event_title', title) or setattr(self, 'source_url', url) or setattr(self, 'source_name', "Mock News") or setattr(self, 'event_date', datetime.strptime(event_date_str, "%Y-%m-%d %H:%M:%S").replace(tzinfo=timezone.utc)) or setattr(self, 'full_article_text', "Full article text...")
    class MockStockAnalysis:
        def __init__(self, stock): self.stock, self.analysis_date, self.investment_decision, self.strategy_type, self.confidence_level, self.investment_thesis_full, self.reasoning = stock, datetime.now(timezone.utc), "Buy", "GARP", "Medium", "Thesis...", "Reasons..."
        self.dcf_assumptions = {"discount_rate": 0.095, "perpetual_growth_rate": 0.025, "projection_years": 5, "start_fcf": 1.2e9, "fcf_growth_rates_projection": [0.08, 0.07, 0.06, 0.05, 0.04]}
        self.pe_ratio, self.pb_ratio, self.ps_ratio, self.ev_to_sales, self.ev_to_ebitda, self.eps, self.roe, self.roa, self.roic, self.dividend_yield, self.debt_to_equity, self.debt_to_ebitda, self.interest_coverage_ratio, self.current_ratio, self.quick_ratio, self.gross_profit_margin, self.operating_profit_margin, self.net_profit_margin, self.revenue_growth_yoy, self.revenue_growth_qoq, self.revenue_growth_cagr_3yr, self.revenue_growth_cagr_5yr, self.eps_growth_yoy, self.eps_growth_cagr_3yr, self.eps_growth_cagr_5yr, self.free_cash_flow_per_share, self.free_cash_flow_yield, self.free_cash_flow_trend, self.retained_earnings_trend, self.dcf_intrinsic_value, self.dcf_upside_percentage, self.business_summary, self.economic_moat_summary, self.industry_trends_summary, self.competitive_landscape_summary, self.management_assessment_summary, self.risk_factors_summary, self.key_metrics_snapshot, self.qualitative_sources_summary = (18.5, 3.2, 2.5, 2.8, 12.0, 2.50, 0.22, 0.10, 0.15, 0.015, 0.5, 2.1, 8.0, 1.8, 1.2, 0.60, 0.20, 0.12, 0.15, 0.04, 0.12, 0.10, 0.20, 0.18, 0.15, 1.80, 0.05, "Growing", "Growing", 120.50, 0.205, "Biz Sum.", "Moat Sum.", "Ind Sum.", "Comp Sum.", "Mgmt Sum.", "Risk Sum.", {"price": 100}, {"10k_url": "url"})
    class MockIPOAnalysis: __init__ = lambda self, ipo: setattr(self, 'ipo', ipo) or setattr(self, 'analysis_date', datetime.now(timezone.utc))
    class MockNewsEventAnalysis: __init__ = lambda self, news_event: setattr(self, 'news_event', news_event) or setattr(self, 'analysis_date', datetime.now(timezone.utc))
    mock_sa = MockStockAnalysis(MockStock("MOCK", "MockCorp Inc."))
    mock_ipo_a = MockIPOAnalysis(MockIPO("NewIPO Inc.", "NIPO"))
    mock_news_a = MockNewsEventAnalysis(MockNewsEvent("Major Tech Breakthrough", "http://example.com/news1"))
    email_svc = EmailService()
    email_message = email_svc.create_summary_email(stock_analyses=[mock_sa], ipo_analyses=[mock_ipo_a], news_analyses=[mock_news_a])
    if email_message:
        logger.info("Email message created successfully.")
        output_filename = f"test_email_summary_refactored_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(output_filename, "w", encoding="utf-8") as f:
            payload_html = "";
            if email_message.is_multipart():
                for part in email_message.get_payload():
                    if part.get_content_type() == "text/html": payload_html = part.get_payload(decode=True).decode(part.get_content_charset() or 'utf-8'); break
            else: payload_html = email_message.get_payload(decode=True).decode(email_message.get_content_charset() or 'utf-8')
            if payload_html: f.write(payload_html); logger.info(f"Test email HTML saved to {output_filename}")
            else: logger.error("Could not extract HTML payload.")
        # if email_svc.send_email(email_message): logger.info("Test email sent.") else: logger.error("Failed to send test email.")
    else: logger.error("Failed to create email message.")
---------- END email_service.py ----------


---------- ipo_analyzer.py ----------
import time
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timedelta, timezone
from dateutil import parser as date_parser
import concurrent.futures

from api_clients import FinnhubClient, GeminiAPIClient, SECEDGARClient, extract_S1_text_sections
from database import SessionLocal, get_db_session, IPO, IPOAnalysis
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError
from core.config import (
    S1_KEY_SECTIONS, IPO_ANALYSIS_REANALYZE_DAYS,
    SUMMARIZATION_CHUNK_SIZE_CHARS
)

MAX_IPO_ANALYSIS_WORKERS = 1 # Module-level constant


class IPOAnalyzer:
    def __init__(self):
        self.finnhub = FinnhubClient()
        self.gemini = GeminiAPIClient()
        self.sec_edgar = SECEDGARClient()

    def _parse_ipo_date(self, date_str):
        if not date_str:
            return None
        try:
            return date_parser.parse(date_str).date()
        except (ValueError, TypeError) as e:
            logger.warning(f"Could not parse IPO date string '{date_str}': {e}")
            return None

    def fetch_upcoming_ipos(self):
        logger.info("Fetching upcoming IPOs using Finnhub...")
        ipos_data_to_process = []
        today = datetime.now(timezone.utc)
        from_date = (today - timedelta(days=60)).strftime('%Y-%m-%d')
        to_date = (today + timedelta(days=180)).strftime('%Y-%m-%d')

        finnhub_response = self.finnhub.get_ipo_calendar(from_date=from_date, to_date=to_date)
        actual_ipo_list = []

        if finnhub_response and isinstance(finnhub_response, dict) and "ipoCalendar" in finnhub_response:
            actual_ipo_list = finnhub_response["ipoCalendar"]
            if not isinstance(actual_ipo_list, list):
                logger.warning(f"Finnhub response 'ipoCalendar' field is not a list. Found: {type(actual_ipo_list)}")
                actual_ipo_list = []
            elif not actual_ipo_list:
                logger.info("Finnhub 'ipoCalendar' list is empty for the current period.")
        elif finnhub_response is None:
            logger.error("Failed to fetch IPOs from Finnhub (API call failed or returned None).")
        else:
            logger.info(f"No IPOs found or unexpected format from Finnhub. Response: {str(finnhub_response)[:200]}")

        if actual_ipo_list:
            for ipo_api_data in actual_ipo_list:
                if not isinstance(ipo_api_data, dict):
                    logger.warning(f"Skipping non-dictionary item in Finnhub IPO calendar: {ipo_api_data}")
                    continue
                price_range_raw = ipo_api_data.get("price")
                price_low, price_high = None, None
                if isinstance(price_range_raw, str) and price_range_raw.strip():
                    parts = price_range_raw.split('-', 1)
                    try: price_low = float(parts[0].strip())
                    except: pass # pylint: disable=bare-except
                    try: price_high = float(parts[1].strip()) if len(parts) > 1 and parts[1].strip() else price_low
                    except: price_high = price_low if price_low is not None else None # pylint: disable=bare-except
                elif isinstance(price_range_raw, (float, int)):
                    price_low = float(price_range_raw)
                    price_high = float(price_range_raw)

                parsed_date = self._parse_ipo_date(ipo_api_data.get("date"))
                ipos_data_to_process.append({
                    "company_name": ipo_api_data.get("name"), "symbol": ipo_api_data.get("symbol"),
                    "ipo_date_str": ipo_api_data.get("date"), "ipo_date": parsed_date,
                    "expected_price_range_low": price_low, "expected_price_range_high": price_high,
                    "exchange": ipo_api_data.get("exchange"), "status": ipo_api_data.get("status"),
                    "offered_shares": ipo_api_data.get("numberOfShares"),
                    "total_shares_value": ipo_api_data.get("totalSharesValue"),
                    "source_api": "Finnhub", "raw_data": ipo_api_data
                })
            logger.info(f"Successfully parsed {len(ipos_data_to_process)} IPOs from Finnhub API response.")

        unique_ipos = []
        seen_keys = set()
        for ipo_info in ipos_data_to_process:
            key_name = ipo_info.get("company_name", "").strip().lower() if ipo_info.get("company_name") else "unknown_company"
            key_symbol = ipo_info.get("symbol", "").strip().upper() if ipo_info.get("symbol") else "NO_SYMBOL"
            key_date = ipo_info.get("ipo_date_str", "")
            unique_tuple = (key_name, key_symbol, key_date)
            if unique_tuple not in seen_keys:
                unique_ipos.append(ipo_info)
                seen_keys.add(unique_tuple)
        logger.info(f"Total unique IPOs fetched after deduplication: {len(unique_ipos)}")
        return unique_ipos

    def _get_or_create_ipo_db_entry(self, db_session, ipo_data_from_fetch):
        ipo_db_entry = None
        if ipo_data_from_fetch.get("symbol"):
            ipo_db_entry = db_session.query(IPO).filter(IPO.symbol == ipo_data_from_fetch["symbol"]).first()

        if not ipo_db_entry and ipo_data_from_fetch.get("company_name") and ipo_data_from_fetch.get("ipo_date_str"):
            ipo_db_entry = db_session.query(IPO).filter(
                IPO.company_name == ipo_data_from_fetch["company_name"],
                IPO.ipo_date_str == ipo_data_from_fetch["ipo_date_str"]
            ).first()

        cik_to_store = ipo_data_from_fetch.get("cik")
        if not cik_to_store and ipo_data_from_fetch.get("symbol"):
            cik_to_store = self.sec_edgar.get_cik_by_ticker(ipo_data_from_fetch["symbol"])
            time.sleep(0.5)
        elif not cik_to_store and ipo_db_entry and ipo_db_entry.symbol and not ipo_db_entry.cik:
            cik_to_store = self.sec_edgar.get_cik_by_ticker(ipo_db_entry.symbol)
            time.sleep(0.5)

        if not ipo_db_entry:
            logger.info(f"IPO '{ipo_data_from_fetch.get('company_name')}' not found in DB, creating new entry.")
            ipo_db_entry = IPO(
                company_name=ipo_data_from_fetch.get("company_name"), symbol=ipo_data_from_fetch.get("symbol"),
                ipo_date_str=ipo_data_from_fetch.get("ipo_date_str"), ipo_date=ipo_data_from_fetch.get("ipo_date"),
                expected_price_range_low=ipo_data_from_fetch.get("expected_price_range_low"),
                expected_price_range_high=ipo_data_from_fetch.get("expected_price_range_high"),
                offered_shares=ipo_data_from_fetch.get("offered_shares"),
                total_shares_value=ipo_data_from_fetch.get("total_shares_value"),
                exchange=ipo_data_from_fetch.get("exchange"), status=ipo_data_from_fetch.get("status"),
                cik=cik_to_store
            )
            db_session.add(ipo_db_entry)
            try:
                db_session.commit(); db_session.refresh(ipo_db_entry)
                logger.info(f"Created IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}, CIK: {ipo_db_entry.cik})")
            except SQLAlchemyError as e:
                db_session.rollback(); logger.error(f"Error creating IPO: {e}", exc_info=True); return None
        else:
            updated = False
            fields_to_update = ["company_name", "symbol", "ipo_date_str", "ipo_date", "expected_price_range_low",
                                "expected_price_range_high", "offered_shares", "total_shares_value", "exchange", "status"]
            for field in fields_to_update:
                new_val = ipo_data_from_fetch.get(field)
                if new_val is not None and getattr(ipo_db_entry, field) != new_val:
                    setattr(ipo_db_entry, field, new_val); updated = True
            if cik_to_store and ipo_db_entry.cik != cik_to_store:
                ipo_db_entry.cik = cik_to_store; updated = True
            if updated:
                try:
                    db_session.commit(); db_session.refresh(ipo_db_entry)
                    logger.info(f"Updated IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}).")
                except SQLAlchemyError as e:
                    db_session.rollback(); logger.error(f"Error updating IPO: {e}", exc_info=True)
        return ipo_db_entry

    def _fetch_s1_data(self, db_session, ipo_db_entry):
        if not ipo_db_entry: return None, None
        target_cik = ipo_db_entry.cik
        if not target_cik:
            if ipo_db_entry.symbol:
                target_cik = self.sec_edgar.get_cik_by_ticker(ipo_db_entry.symbol); time.sleep(0.5)
                if target_cik:
                    ipo_db_entry.cik = target_cik
                    try: db_session.commit()
                    except SQLAlchemyError as e: db_session.rollback(); logger.error(f"Failed to update CIK for {ipo_db_entry.company_name}: {e}")
                else: logger.warning(f"No CIK via symbol {ipo_db_entry.symbol} for '{ipo_db_entry.company_name}'."); return None, None
            else: logger.warning(f"No CIK/symbol for '{ipo_db_entry.company_name}'. Cannot fetch S-1."); return None, None

        logger.info(f"Attempting to fetch S-1/F-1 for {ipo_db_entry.company_name} (CIK: {target_cik})")
        s1_url = None
        for form_type in ["S-1", "S-1/A", "F-1", "F-1/A"]:
            s1_url = self.sec_edgar.get_filing_document_url(cik=target_cik, form_type=form_type); time.sleep(0.5)
            if s1_url: logger.info(f"Found {form_type} URL for {ipo_db_entry.company_name}: {s1_url}"); break
        if s1_url:
            if ipo_db_entry.s1_filing_url != s1_url:
                ipo_db_entry.s1_filing_url = s1_url
                try: db_session.commit()
                except SQLAlchemyError as e: db_session.rollback(); logger.warning(f"Failed to update S1 filing URL for {ipo_db_entry.company_name} due to: {e}")
            filing_text = self.sec_edgar.get_filing_text(s1_url)
            if filing_text: logger.info(f"Fetched S-1/F-1 text (len: {len(filing_text)}) for {ipo_db_entry.company_name}"); return filing_text, s1_url
            else: logger.warning(f"Failed to fetch S-1/F-1 text from {s1_url}")
        else: logger.warning(f"No S-1 or F-1 URL found for {ipo_db_entry.company_name} (CIK: {target_cik}).")
        return None, None

    def _analyze_single_ipo_task(self, db_session, ipo_data_from_fetch):
        ipo_identifier = ipo_data_from_fetch.get("company_name") or ipo_data_from_fetch.get("symbol")
        logger.info(f"Task: Starting analysis for IPO: {ipo_identifier} from source {ipo_data_from_fetch.get('source_api')}")
        ipo_db_entry = self._get_or_create_ipo_db_entry(db_session, ipo_data_from_fetch)
        if not ipo_db_entry: logger.error(f"Task: Could not get/create DB entry for IPO {ipo_identifier}. Aborting."); return None

        reanalyze_threshold = datetime.now(timezone.utc) - timedelta(days=IPO_ANALYSIS_REANALYZE_DAYS)
        existing_analysis = db_session.query(IPOAnalysis).filter(IPOAnalysis.ipo_id == ipo_db_entry.id).order_by(IPOAnalysis.analysis_date.desc()).first()
        significant_change = False
        if existing_analysis:
            snap = existing_analysis.key_data_snapshot or {}
            parsed_snap_date = self._parse_ipo_date(snap.get("date"))
            if (ipo_db_entry.ipo_date != parsed_snap_date or ipo_db_entry.status != snap.get("status") or
                ipo_db_entry.expected_price_range_low != snap.get("price_range_low") or # type: ignore
                ipo_db_entry.expected_price_range_high != snap.get("price_range_high")): # type: ignore
                significant_change = True
            if not significant_change and existing_analysis.analysis_date >= reanalyze_threshold:
                logger.info(f"Task: Recent analysis for {ipo_identifier} exists. Skipping."); return existing_analysis

        s1_text, _ = self._fetch_s1_data(db_session, ipo_db_entry)
        s1_sections = extract_S1_text_sections(s1_text, S1_KEY_SECTIONS) if s1_text else {}
        analysis_payload = {"key_data_snapshot": ipo_data_from_fetch.get("raw_data", {}), "s1_sections_used": {k: bool(v) for k,v in s1_sections.items()}}
        company_prompt_id = f"{ipo_db_entry.company_name} ({ipo_db_entry.symbol or 'N/A'})"
        max_section_len_for_prompt = SUMMARIZATION_CHUNK_SIZE_CHARS
        biz_text = (s1_sections.get("business", "") or "")[:max_section_len_for_prompt]
        risk_text = (s1_sections.get("risk_factors", "") or "")[:max_section_len_for_prompt]
        mda_text = (s1_sections.get("mda", "") or "")[:max_section_len_for_prompt]
        prompt_parts = [f"IPO: {company_prompt_id}"]
        if biz_text: prompt_parts.append(f"Business Summary from S-1: {biz_text}")
        if risk_text: prompt_parts.append(f"Risk Factors Summary from S-1: {risk_text}")
        if mda_text: prompt_parts.append(f"MD&A Summary from S-1: {mda_text}")
        prompt_ctx = ". ".join(prompt_parts)

        prompt1 = prompt_ctx + " Based on the S-1 information (if provided), summarize: Business Model, Competitive Landscape, Industry Outlook."
        resp1 = self.gemini.generate_text(prompt1); time.sleep(3)
        analysis_payload["s1_business_summary"] = self._parse_ai_section(resp1, "Business Model")
        analysis_payload["competitive_landscape_summary"] = self._parse_ai_section(resp1, "Competitive Landscape")
        analysis_payload["industry_outlook_summary"] = self._parse_ai_section(resp1, "Industry Outlook")

        prompt2 = prompt_ctx + " Based on the S-1 information (if provided), summarize: Key Risk Factors, Use of IPO Proceeds, Financial Health from MD&A."
        resp2 = self.gemini.generate_text(prompt2); time.sleep(3)
        analysis_payload["s1_risk_factors_summary"] = self._parse_ai_section(resp2, ["Key Risk Factors", "Risk Factors Summary"])
        analysis_payload["use_of_proceeds_summary"] = self._parse_ai_section(resp2, "Use of IPO Proceeds")
        analysis_payload["s1_financial_health_summary"] = self._parse_ai_section(resp2, "Financial Health from MD&A")
        analysis_payload["s1_mda_summary"] = analysis_payload["s1_financial_health_summary"] # Alias for consistency

        if not analysis_payload.get("s1_business_summary") or analysis_payload.get("s1_business_summary","").startswith("Section not found"):
            analysis_payload["business_model_summary"] = analysis_payload.get("s1_business_summary")
        if not analysis_payload.get("s1_risk_factors_summary") or analysis_payload.get("s1_risk_factors_summary","").startswith("Section not found"):
            analysis_payload["risk_factors_summary"] = analysis_payload.get("s1_risk_factors_summary")

        synth_prompt_parts = [f"Synthesize an IPO investment perspective for {company_prompt_id}."]
        if analysis_payload.get('s1_business_summary') and "Section not found" not in analysis_payload['s1_business_summary']: synth_prompt_parts.append(f"Business={analysis_payload['s1_business_summary'][:150]}")
        if analysis_payload.get('s1_risk_factors_summary') and "Section not found" not in analysis_payload['s1_risk_factors_summary']: synth_prompt_parts.append(f"Risks={analysis_payload['s1_risk_factors_summary'][:150]}")
        if analysis_payload.get('s1_financial_health_summary') and "Section not found" not in analysis_payload['s1_financial_health_summary']: synth_prompt_parts.append(f"Financials={analysis_payload['s1_financial_health_summary'][:150]}")
        synth_prompt_parts.append("Provide: Investment Stance (e.g., Monitor, Attractive Risk/Reward, Avoid), Reasoning, Critical Verification Points from S-1.")
        synth_prompt = " ".join(synth_prompt_parts)
        gemini_synth = self.gemini.generate_text(synth_prompt); time.sleep(3)
        parsed_synth = self._parse_ai_synthesis(gemini_synth)
        analysis_payload.update(parsed_synth)

        current_time = datetime.now(timezone.utc)
        if existing_analysis:
            for key, value in analysis_payload.items(): setattr(existing_analysis, key, value)
            existing_analysis.analysis_date = current_time; entry_to_save = existing_analysis
        else:
            entry_to_save = IPOAnalysis(ipo_id=ipo_db_entry.id, analysis_date=current_time, **analysis_payload)
            db_session.add(entry_to_save)
        ipo_db_entry.last_analysis_date = current_time
        try:
            db_session.commit(); logger.info(f"Task: Saved IPO analysis for {ipo_identifier} (ID: {entry_to_save.id})")
        except SQLAlchemyError as e:
            db_session.rollback(); logger.error(f"Task: DB error saving IPO analysis for {ipo_identifier}: {e}", exc_info=True); return None
        return entry_to_save

    def _parse_ai_section(self, ai_text, section_header_keywords):
        if not ai_text or ai_text.startswith("Error:"): return "AI Error or No Text"
        keywords = [k.lower() for k in ([section_header_keywords] if isinstance(section_header_keywords, str) else section_header_keywords)]
        lines, capture, content = ai_text.split('\n'), False, []
        all_headers = ["business model:", "competitive landscape:", "industry outlook:", "significant risk factors:", "key risk factors:", "risk factors summary:", "use of ipo proceeds:", "financial health from md&a:", "financial health summary:", "investment stance:", "reasoning:", "critical verification points:"]
        for line in lines:
            norm_line = line.strip().lower()
            matched_kw = next((kw for kw in keywords if norm_line.startswith(kw + ":") or norm_line == kw), None)
            if matched_kw:
                capture = True; line_content = line.strip()[len(matched_kw):].lstrip(':').strip()
                if line_content: content.append(line_content)
                continue
            if capture:
                if any(norm_line.startswith(h) for h in all_headers if h not in keywords): break
                content.append(line)
        return "\n".join(content).strip() or "Section not found or empty."

    def _parse_ai_synthesis(self, ai_response):
        parsed = {}
        if ai_response.startswith("Error:") or not ai_response:
            parsed["investment_decision"] = "AI Error"; parsed["reasoning"] = ai_response if ai_response else "AI Error: Empty response."; return parsed
        parsed["investment_decision"] = self._parse_ai_section(ai_response, "Investment Stance")
        parsed["reasoning"] = self._parse_ai_section(ai_response, ["Reasoning", "Critical Verification Points"])
        if parsed["investment_decision"].startswith("Section not found"): parsed["investment_decision"] = "Review AI Output"
        if parsed["reasoning"].startswith("Section not found"): parsed["reasoning"] = ai_response
        return parsed

    def run_ipo_analysis_pipeline(self):
        all_upcoming_ipos = self.fetch_upcoming_ipos()
        analyzed_results = []
        if not all_upcoming_ipos: logger.info("No upcoming IPOs found to analyze."); return []
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_IPO_ANALYSIS_WORKERS) as executor:
            future_to_ipo_data = {}
            for ipo_data in all_upcoming_ipos:
                status = ipo_data.get("status", "").lower()
                relevant_statuses = ["expected", "filed", "priced", "upcoming", "active"]
                if status not in relevant_statuses or not ipo_data.get("company_name"):
                    logger.debug(f"Skipping IPO '{ipo_data.get('company_name')}' due to status '{status}' or missing name."); continue
                future = executor.submit(self._thread_worker_analyze_ipo, ipo_data)
                future_to_ipo_data[future] = ipo_data.get("company_name")
            for future in concurrent.futures.as_completed(future_to_ipo_data):
                ipo_name = future_to_ipo_data[future]
                try:
                    result = future.result()
                    if result: analyzed_results.append(result)
                except Exception as exc: logger.error(f"IPO analysis for '{ipo_name}' generated an exception: {exc}", exc_info=True)
        logger.info(f"IPO analysis pipeline completed. Processed {len(analyzed_results)} IPOs.")
        return analyzed_results

    def _thread_worker_analyze_ipo(self, ipo_data_from_fetch):
        db_session = SessionLocal() # New session for each thread
        try:
            return self._analyze_single_ipo_task(db_session, ipo_data_from_fetch)
        finally:
            SessionLocal.remove() # Remove session associated with this thread

if __name__ == '__main__':
    from database import init_db
    # init_db()
    logger.info("Starting standalone IPO analysis pipeline test...")
    analyzer = IPOAnalyzer()
    results = analyzer.run_ipo_analysis_pipeline()
    if results:
        logger.info(f"Processed {len(results)} IPOs.")
        for res in results:
            if hasattr(res, 'ipo') and res.ipo:
                logger.info(f"IPO: {res.ipo.company_name} ({res.ipo.symbol}), Decision: {res.investment_decision}, Date: {res.ipo.ipo_date}, Status: {res.ipo.status}")
            else: logger.warning(f"Result item missing 'ipo' or ipo is None: {res}")
    else: logger.info("No IPOs were processed or found by the pipeline.")
---------- END ipo_analyzer.py ----------


---------- news_analyzer.py ----------
import time
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timezone, timedelta

from api_clients import FinnhubClient, GeminiAPIClient, scrape_article_content
from database import SessionLocal, get_db_session, NewsEvent, NewsEventAnalysis
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError
from core.config import (
    MAX_NEWS_ARTICLES_PER_QUERY, MAX_NEWS_TO_ANALYZE_PER_RUN,
    NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION
)


class NewsAnalyzer:
    def __init__(self):
        self.finnhub = FinnhubClient()
        self.gemini = GeminiAPIClient()
        # Each NewsAnalyzer instance gets its own session, managed carefully
        self.db_session = next(get_db_session())

    def _close_session_if_active(self):
        if self.db_session and self.db_session.is_active:
            try:
                self.db_session.close()
                logger.debug("DB session closed in NewsAnalyzer.")
            except Exception as e_close:
                logger.warning(f"Error closing session in NewsAnalyzer: {e_close}")

    def fetch_market_news(self, category="general", count_to_fetch_from_api=MAX_NEWS_ARTICLES_PER_QUERY):
        logger.info(f"Fetching latest market news for category: {category} (max {count_to_fetch_from_api} from API)...")
        news_items = self.finnhub.get_market_news(category=category)
        if news_items and isinstance(news_items, list):
            logger.info(f"Fetched {len(news_items)} news items from Finnhub.")
            return news_items[:count_to_fetch_from_api]
        else:
            logger.warning(f"Failed to fetch news or received unexpected format from Finnhub: {news_items}")
            return []

    def _ensure_news_event_session_active(self, news_identifier_for_log="Unknown News"):
        if not self.db_session or not self.db_session.is_active:
            logger.warning(f"Session for News '{news_identifier_for_log}' was inactive/closed. Re-establishing.")
            self._close_session_if_active()  # Close any old one
            self.db_session = next(get_db_session())  # Get a fresh session

    def _ensure_news_event_is_bound(self, news_event_db_obj):
        if not news_event_db_obj: return None
        self._ensure_news_event_session_active(
            news_event_db_obj.event_title[:50] if news_event_db_obj.event_title else 'Unknown News')

        instance_state = sa_inspect(news_event_db_obj)
        if not instance_state.session or instance_state.session is not self.db_session:
            obj_id_log = news_event_db_obj.id if instance_state.has_identity else 'Transient'
            logger.warning(
                f"NewsEvent DB entry '{news_event_db_obj.event_title[:50]}...' (ID: {obj_id_log}) not bound to current session. Merging.")
            try:
                if not instance_state.has_identity and news_event_db_obj.id is None:  # Truly transient object
                    existing_in_session = self.db_session.query(NewsEvent).filter_by(
                        source_url=news_event_db_obj.source_url).first()
                    if existing_in_session:
                        news_event_db_obj = existing_in_session
                        logger.info(
                            f"Replaced transient NewsEvent for '{news_event_db_obj.source_url}' with instance from session.")
                        return news_event_db_obj
                # If it has an ID or was not found by URL, try merging
                merged_event = self.db_session.merge(news_event_db_obj)
                logger.info(
                    f"Successfully merged NewsEvent '{merged_event.event_title[:50]}...' (ID: {merged_event.id}) into session.")
                return merged_event
            except Exception as e_merge:
                logger.error(
                    f"Failed to merge NewsEvent '{news_event_db_obj.event_title[:50]}...' into session: {e_merge}. Re-fetching.",
                    exc_info=True)
                fallback_event = None
                if instance_state.has_identity and news_event_db_obj.id:  # If it had an ID
                    fallback_event = self.db_session.query(NewsEvent).get(news_event_db_obj.id)
                elif news_event_db_obj.source_url:  # If it had a URL
                    fallback_event = self.db_session.query(NewsEvent).filter_by(
                        source_url=news_event_db_obj.source_url).first()

                if not fallback_event:
                    logger.critical(
                        f"CRITICAL: Failed to re-associate NewsEvent '{news_event_db_obj.event_title[:50]}...' with session after merge failure.");
                    return None
                logger.info(
                    f"Successfully re-fetched NewsEvent '{fallback_event.event_title[:50]}...' after merge failure.")
                return fallback_event
        return news_event_db_obj

    def _get_or_create_news_event(self, news_item_from_api):
        self._ensure_news_event_session_active(news_item_from_api.get('headline', 'Unknown News'))
        source_url = news_item_from_api.get("url")
        if not source_url: logger.warning(
            f"News item missing URL, cannot process: {news_item_from_api.get('headline')}"); return None

        event = self.db_session.query(NewsEvent).filter_by(source_url=source_url).first()
        full_article_text_scraped_now = None
        if not event or (event and not event.full_article_text):
            logger.info(f"Attempting to scrape full article for: {source_url}")
            full_article_text_scraped_now = scrape_article_content(source_url);
            time.sleep(1)
            if full_article_text_scraped_now:
                logger.info(f"Scraped ~{len(full_article_text_scraped_now)} chars for {source_url}")
            else:
                logger.warning(
                    f"Failed to scrape full article for {source_url}. Analysis will use summary if available.")

        current_time_utc = datetime.now(timezone.utc)
        if event:
            logger.debug(f"News event '{event.event_title[:70]}...' (URL: {source_url}) already in DB.")
            if full_article_text_scraped_now and not event.full_article_text:
                logger.info(f"Updating existing event {event.id} with newly scraped full article text.")
                event.full_article_text = full_article_text_scraped_now
                event.processed_date = current_time_utc
                try:
                    self.db_session.commit()
                except SQLAlchemyError as e:
                    self.db_session.rollback(); logger.error(
                        f"Error updating full_article_text for existing event {source_url}: {e}")
            return event

        event_timestamp = news_item_from_api.get("datetime")
        event_datetime_utc = datetime.fromtimestamp(event_timestamp,
                                                    timezone.utc) if event_timestamp else current_time_utc
        new_event = NewsEvent(
            event_title=news_item_from_api.get("headline"), event_date=event_datetime_utc,
            source_url=source_url, source_name=news_item_from_api.get("source"),
            category=news_item_from_api.get("category"), full_article_text=full_article_text_scraped_now,
            processed_date=current_time_utc
        )
        self.db_session.add(new_event)
        try:
            self.db_session.commit();
            self.db_session.refresh(new_event)
            logger.info(f"Stored new news event: {new_event.event_title[:70]}... (ID: {new_event.id})")
            return new_event
        except SQLAlchemyError as e:
            self.db_session.rollback()
            logger.error(f"Database error storing new news event '{news_item_from_api.get('headline')}': {e}",
                         exc_info=True)
            return None

    def analyze_single_news_item(self, news_event_db):
        if not news_event_db: logger.error("analyze_single_news_item called with no NewsEvent DB object."); return None
        news_event_db = self._ensure_news_event_is_bound(news_event_db)  # Ensure bound to current session
        if not news_event_db: return None  # If binding failed critically

        headline = news_event_db.event_title
        content_for_analysis, analysis_source_type = news_event_db.full_article_text, "full article"
        if not content_for_analysis:
            content_for_analysis, analysis_source_type = headline, "headline only"
            logger.warning(f"No full article text for '{headline}'. Analyzing based on headline only.")
        if len(content_for_analysis) > NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION:
            content_for_analysis = content_for_analysis[
                                   :NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION] + "\n... [CONTENT TRUNCATED FOR AI ANALYSIS] ..."
            logger.info(
                f"Truncated news content for '{headline}' to {NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI_SUMMARIZATION} chars for Gemini.")
            analysis_source_type += " (truncated)"

        logger.info(f"Analyzing news: '{headline[:70]}...' (using {analysis_source_type})")
        analysis_payload = {"key_news_snippets": {"headline": headline, "source_type_used": analysis_source_type}}
        sentiment_response = self.gemini.analyze_sentiment_with_reasoning(content_for_analysis,
                                                                          context=f"News headline for context: {headline}");
        time.sleep(2)
        if not sentiment_response.startswith("Error:"):
            try:
                parts = sentiment_response.split("Reasoning:", 1)
                analysis_payload["sentiment"] = \
                parts[0].split(":", 1)[1].strip().split(' ')[0].split('.')[0].split(',')[0].strip() if ":" in parts[
                    0] else parts[0].strip().split(' ')[0]
                analysis_payload["sentiment_reasoning"] = parts[1].strip() if len(parts) > 1 else sentiment_response
            except Exception as e_parse_sent:
                logger.warning(
                    f"Could not parse sentiment response for '{headline}': {sentiment_response}. Error: {e_parse_sent}. Storing raw.")
                analysis_payload["sentiment"], analysis_payload[
                    "sentiment_reasoning"] = "Error Parsing", sentiment_response
        else:
            analysis_payload["sentiment"], analysis_payload["sentiment_reasoning"] = "AI Error", sentiment_response

        prompt_detailed_analysis = (
            f"News Headline: \"{headline}\"\nNews Content (may be truncated): \"{content_for_analysis}\"\n\n"
            f"Instructions for Analysis:\n"
            f"1. News Summary: Provide a comprehensive yet concise summary of this news article (3-5 key sentences).\n"
            f"2. Affected Entities: Identify specific companies (with ticker symbols if known and highly relevant) and/or specific industry sectors directly or significantly indirectly affected by this news. Explain why briefly for each.\n"
            f"3. Mechanism of Impact: For the primary affected entities, describe how this news will likely affect their fundamentals (e.g., revenue, costs, market share, customer sentiment) or market perception.\n"
            f"4. Estimated Timing & Duration: Estimate the likely timing (e.g., Immediate, Short-term <3mo, Medium-term 3-12mo, Long-term >1yr) and duration of the impact.\n"
            f"5. Estimated Magnitude & Direction: Estimate the potential magnitude (e.g., Low, Medium, High) and direction (e.g., Positive, Negative, Neutral/Mixed) of the impact on the primary affected entities.\n"
            f"6. Confidence Level: State your confidence (High, Medium, Low) in this overall impact assessment, briefly justifying it (e.g., based on clarity of news, directness of impact).\n"
            f"7. Investor Summary: Provide a final 2-sentence summary specifically for an investor, highlighting the most critical implication or takeaway.\n\n"
            f"Structure your response clearly with headings for each point (e.g., 'News Summary:', 'Affected Entities:', etc.).")
        impact_analysis_response = self.gemini.generate_text(prompt_detailed_analysis);
        time.sleep(2)
        if not impact_analysis_response.startswith("Error:"):
            analysis_payload["news_summary_detailed"] = self._parse_ai_section(impact_analysis_response,
                                                                               "News Summary:")
            analysis_payload["potential_impact_on_companies"] = self._parse_ai_section(impact_analysis_response,
                                                                                       ["Affected Entities:",
                                                                                        "Affected Companies:",
                                                                                        "Affected Stocks/Sectors:"])
            sectors_text = self._parse_ai_section(impact_analysis_response, "Affected Sectors:")
            if sectors_text and not sectors_text.startswith("Section not found"):
                analysis_payload["potential_impact_on_sectors"] = sectors_text
            elif analysis_payload["potential_impact_on_companies"] and not analysis_payload.get(
                "potential_impact_on_sectors"):
                analysis_payload["potential_impact_on_sectors"] = analysis_payload["potential_impact_on_companies"]
            analysis_payload["mechanism_of_impact"] = self._parse_ai_section(impact_analysis_response,
                                                                             "Mechanism of Impact:")
            analysis_payload["estimated_timing_duration"] = self._parse_ai_section(impact_analysis_response,
                                                                                   ["Estimated Timing & Duration:",
                                                                                    "Estimated Timing:"])
            analysis_payload["estimated_magnitude_direction"] = self._parse_ai_section(impact_analysis_response, [
                "Estimated Magnitude & Direction:", "Estimated Magnitude/Direction:"])
            analysis_payload["confidence_of_assessment"] = self._parse_ai_section(impact_analysis_response,
                                                                                  "Confidence Level:")
            analysis_payload["summary_for_email"] = self._parse_ai_section(impact_analysis_response,
                                                                           ["Investor Summary:",
                                                                            "Final Summary for Investor:"])
        else:
            logger.error(
                f"Gemini failed to provide detailed impact analysis for '{headline}': {impact_analysis_response}");
            analysis_payload["news_summary_detailed"] = impact_analysis_response

        current_analysis_time = datetime.now(timezone.utc)
        news_analysis_entry = NewsEventAnalysis(news_event_id=news_event_db.id, analysis_date=current_analysis_time,
                                                **analysis_payload)
        self.db_session.add(news_analysis_entry)
        news_event_db.last_analyzed_date = current_analysis_time
        try:
            self.db_session.commit()
            logger.info(
                f"Successfully analyzed and saved news: '{headline[:70]}...' (Analysis ID: {news_analysis_entry.id})")
        except SQLAlchemyError as e:
            self.db_session.rollback();
            logger.error(f"Database error saving news analysis for '{headline[:70]}...': {e}", exc_info=True);
            return None
        return news_analysis_entry

    def _parse_ai_section(self, ai_text, section_header_keywords):
        if not ai_text or ai_text.startswith("Error:"): return "AI Error or No Text"
        keywords_to_check = [k.lower().strip() for k in (
            [section_header_keywords] if isinstance(section_header_keywords, str) else section_header_keywords)]
        lines, capture, section_content = ai_text.split('\n'), False, []
        all_known_headers_lower_prefixes = ["news summary:", "affected entities:", "affected companies:",
                                            "affected stocks/sectors:", "mechanism of impact:",
                                            "estimated timing & duration:", "estimated timing:",
                                            "estimated magnitude & direction:", "estimated magnitude/direction:",
                                            "confidence level:", "investor summary:", "final summary for investor:"]
        for line_original in lines:
            line_stripped_lower = line_original.strip().lower()
            matched_current_keyword = next((kw_lower for kw_lower in keywords_to_check if
                                            line_stripped_lower.startswith(
                                                kw_lower + ":") or line_stripped_lower == kw_lower), None)
            if matched_current_keyword:
                capture = True;
                content_on_header_line = line_original.strip()[len(matched_current_keyword):].strip()
                if content_on_header_line.startswith(":"): content_on_header_line = content_on_header_line[1:].strip()
                if content_on_header_line: section_content.append(content_on_header_line)
                continue
            if capture:
                is_another_known_header = any(
                    line_stripped_lower.startswith(known_header_prefix) for known_header_prefix in
                    all_known_headers_lower_prefixes if known_header_prefix not in keywords_to_check)
                if is_another_known_header: break
                section_content.append(line_original)
        return "\n".join(section_content).strip() if section_content else "Section not found or empty."

    def run_news_analysis_pipeline(self, category="general", count_to_fetch_from_api=MAX_NEWS_ARTICLES_PER_QUERY,
                                   count_to_analyze_this_run=MAX_NEWS_TO_ANALYZE_PER_RUN):
        try:
            fetched_news_items_api = self.fetch_market_news(category=category,
                                                            count_to_fetch_from_api=count_to_fetch_from_api)
            if not fetched_news_items_api: logger.info("No news items fetched from API for analysis."); return []
            analyzed_news_results, newly_analyzed_count_this_run = [], 0
            reanalyze_older_than_days, reanalyze_threshold_date = 2, datetime.now(timezone.utc) - timedelta(days=2)

            for news_item_api_data in fetched_news_items_api:
                if newly_analyzed_count_this_run >= count_to_analyze_this_run:
                    logger.info(
                        f"Reached analysis limit of {count_to_analyze_this_run} new/re-analyzed items for this run.");
                    break
                try:
                    news_event_db = self._get_or_create_news_event(news_item_api_data)
                    if not news_event_db: logger.warning(
                        f"Skipping news item (could not get/create in DB): {news_item_api_data.get('headline')}"); continue
                    news_event_db = self._ensure_news_event_is_bound(news_event_db)
                    if not news_event_db: continue

                    analysis_needed, latest_analysis = False, None
                    if news_event_db.analyses:  # Relationship is loaded by _ensure_news_event_is_bound if merged
                        sorted_analyses = sorted(news_event_db.analyses, key=lambda x: x.analysis_date, reverse=True)
                        if sorted_analyses: latest_analysis = sorted_analyses[0]

                    if not latest_analysis:
                        analysis_needed = True; logger.info(
                            f"News '{news_event_db.event_title[:50]}...' requires new analysis.")
                    elif latest_analysis.analysis_date < reanalyze_threshold_date:
                        analysis_needed = True; logger.info(
                            f"News '{news_event_db.event_title[:50]}...' requires re-analysis (older than {reanalyze_older_than_days} days).")
                    elif news_event_db.full_article_text and latest_analysis and (
                            not latest_analysis.key_news_snippets or "full article" not in latest_analysis.key_news_snippets.get(
                            "source_type_used", "")):
                        analysis_needed = True;
                        logger.info(
                            f"News '{news_event_db.event_title[:50]}...' re-analyzing with newly available/confirmed full text.")
                    else:
                        logger.info(
                            f"News '{news_event_db.event_title[:50]}...' already recently analyzed with available text. Skipping.")

                    if analysis_needed:
                        analysis_result = self.analyze_single_news_item(news_event_db)
                        if analysis_result: analyzed_news_results.append(
                            analysis_result); newly_analyzed_count_this_run += 1
                        time.sleep(3)
                except Exception as e:
                    logger.error(f"Failed to process or analyze news item '{news_item_api_data.get('headline')}': {e}",
                                 exc_info=True)
                    self._ensure_news_event_session_active(
                        news_item_api_data.get('headline'))  # Ensure session is active for next item
                    if self.db_session: self.db_session.rollback()  # Rollback any partial transaction for this item
            logger.info(
                f"News analysis pipeline completed. Newly analyzed/re-analyzed {newly_analyzed_count_this_run} items.")
            return analyzed_news_results
        finally:
            self._close_session_if_active()  # Close session at the end of the pipeline run


if __name__ == '__main__':
    from database import init_db

    # init_db()
    logger.info("Starting standalone news analysis pipeline test...")
    analyzer = NewsAnalyzer()
    results = analyzer.run_news_analysis_pipeline(category="general", count_to_fetch_from_api=10,
                                                  count_to_analyze_this_run=3)
    if results:
        logger.info(f"Pipeline processed {len(results)} news items this run.")
        for res_idx, res in enumerate(results):
            if hasattr(res, 'news_event') and res.news_event:
                logger.info(f"--- Result {res_idx + 1} ---")
                logger.info(f"News: {res.news_event.event_title[:100]}...")
                logger.info(f"Source: {res.news_event.source_url}")
                logger.info(f"Sentiment: {res.sentiment} - Reasoning: {res.sentiment_reasoning[:100]}...")
                logger.info(f"Investor Summary: {res.summary_for_email}")
                logger.info(f"Full Article Scraped: {'Yes' if res.news_event.full_article_text else 'No'}")
                if res.news_event.full_article_text: logger.debug(
                    f"Full Article Snippet: {res.news_event.full_article_text[:200]}...")
            else:
                logger.warning(f"Processed news result item missing 'news_event' or news_event is None. Result: {res}")
    else:
        logger.info("No new news items were processed in this run.")
---------- END news_analyzer.py ----------


---------- stock_analyzer.py ----------
import pandas as pd
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timezone, timedelta
import math
import time
import warnings
from bs4 import XMLParsedAsHTMLWarning
import re
import json

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

from api_clients import (
    FinnhubClient, FinancialModelingPrepClient, AlphaVantageClient,
    EODHDClient, GeminiAPIClient, SECEDGARClient, extract_S1_text_sections
)
from database import SessionLocal, get_db_session, Stock, StockAnalysis
from core.logging_setup import logger
from sqlalchemy.exc import SQLAlchemyError
from core.config import (
    STOCK_FINANCIAL_YEARS, DEFAULT_DISCOUNT_RATE,
    DEFAULT_PERPETUAL_GROWTH_RATE, DEFAULT_FCF_PROJECTION_YEARS,
    TEN_K_KEY_SECTIONS,
    SUMMARIZATION_CHUNK_SIZE_CHARS, SUMMARIZATION_CHUNK_OVERLAP_CHARS,
    SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS,
    MAX_COMPETITORS_TO_ANALYZE, Q_REVENUE_SANITY_CHECK_DEVIATION_THRESHOLD,
    PRIORITY_REVENUE_SOURCES
)


def safe_get_float(data_dict, key, default=None):
    if data_dict is None or not isinstance(data_dict, dict): return default
    val = data_dict.get(key)
    if val is None or val == "None" or val == "" or str(val).lower() == "n/a" or str(val).lower() == "-": return default
    try: return float(val)
    except (ValueError, TypeError): return default

def calculate_cagr(end_value, start_value, years):
    if start_value is None or end_value is None or not isinstance(years, (int, float)) or years <= 0: return None
    if start_value == 0: return None
    if (start_value < 0 and end_value > 0) or (start_value > 0 and end_value < 0): return None
    if start_value < 0 and end_value < 0: return None
    if end_value == 0 and start_value > 0: return -1.0
    return ((float(end_value) / float(start_value)) ** (1 / float(years))) - 1

def calculate_growth(current_value, previous_value):
    if previous_value is None or current_value is None: return None
    if float(previous_value) == 0:
        return None if float(current_value) == 0 else (float('inf') if float(current_value) > 0 else float('-inf'))
    try: return (float(current_value) - float(previous_value)) / abs(float(previous_value))
    except (ValueError, TypeError): return None

def get_value_from_statement_list(data_list, field, year_offset=0, report_date_for_log=None):
    if data_list and isinstance(data_list, list) and len(data_list) > year_offset:
        report = data_list[year_offset]
        if report and isinstance(report, dict):
            val = safe_get_float(report, field)
            # if val is None:
            #     date_info = report_date_for_log or report.get('date', 'Unknown Date')
            #     logger.debug(f"Field '{field}' not found or invalid in report for {date_info} (offset {year_offset}).")
            return val
    return None

def get_finnhub_concept_value(finnhub_quarterly_reports_data, report_section_key, concept_names_list, quarter_offset=0):
    if not finnhub_quarterly_reports_data or len(finnhub_quarterly_reports_data) <= quarter_offset: return None
    report_data = finnhub_quarterly_reports_data[quarter_offset]
    if 'report' not in report_data or report_section_key not in report_data['report']: return None
    section_items = report_data['report'][report_section_key]
    if not section_items: return None
    for item in section_items:
        if item.get('concept') in concept_names_list or item.get('label') in concept_names_list:
            return safe_get_float(item, 'value')
    return None

def get_alphavantage_value(av_quarterly_reports, field_name, quarter_offset_from_latest=0):
    if not av_quarterly_reports or len(av_quarterly_reports) <= quarter_offset_from_latest: return None
    report = av_quarterly_reports[quarter_offset_from_latest]
    return safe_get_float(report, field_name)

def get_fmp_value(fmp_quarterly_reports, field_name, quarter_offset_from_latest=0):
    if not fmp_quarterly_reports or len(fmp_quarterly_reports) <= quarter_offset_from_latest: return None
    report = fmp_quarterly_reports[quarter_offset_from_latest]
    return safe_get_float(report, field_name)


class StockAnalyzer:
    def __init__(self, ticker):
        self.ticker = ticker.upper()
        self.finnhub = FinnhubClient()
        self.fmp = FinancialModelingPrepClient()
        self.alphavantage = AlphaVantageClient()
        self.eodhd = EODHDClient()
        self.gemini = GeminiAPIClient()
        self.sec_edgar = SECEDGARClient()
        self.db_session = next(get_db_session())
        self.stock_db_entry = None
        self._financial_data_cache = {}
        self.data_quality_warnings = []
        try:
            self._get_or_create_stock_entry()
        except Exception as e:
            logger.error(f"CRITICAL: Failed during _get_or_create_stock_entry for {self.ticker}: {e}", exc_info=True)
            self._close_session_if_active()
            raise RuntimeError(f"StockAnalyzer for {self.ticker} could not be initialized due to DB/API issues during stock entry setup.") from e

    def _close_session_if_active(self):
        if self.db_session and self.db_session.is_active:
            try: self.db_session.close(); logger.debug(f"DB session closed for {self.ticker}.")
            except Exception as e_close: logger.warning(f"Error closing session for {self.ticker}: {e_close}")

    def _get_or_create_stock_entry(self):
        if not self.db_session.is_active:
            logger.warning(f"Session for {self.ticker} inactive in _get_or_create. Re-establishing.")
            self._close_session_if_active(); self.db_session = next(get_db_session())
        self.stock_db_entry = self.db_session.query(Stock).filter_by(ticker=self.ticker).first()
        company_name, industry, sector, cik = None, None, None, None
        profile_fmp_list = self.fmp.get_company_profile(self.ticker); time.sleep(1.5)
        if profile_fmp_list and isinstance(profile_fmp_list, list) and len(profile_fmp_list) > 0 and profile_fmp_list[0]:
            data = profile_fmp_list[0]; self._financial_data_cache['profile_fmp'] = data
            company_name, industry, sector, cik = data.get('companyName'), data.get('industry'), data.get('sector'), data.get('cik')
            if cik: cik = str(cik).zfill(10)
            logger.info(f"Fetched profile from FMP for {self.ticker}.")
        else:
            logger.warning(f"FMP profile failed or empty for {self.ticker}. Trying Finnhub.")
            profile_fh = self.finnhub.get_company_profile2(self.ticker); time.sleep(1.5)
            if profile_fh:
                self._financial_data_cache['profile_finnhub'] = profile_fh
                company_name, industry = profile_fh.get('name'), profile_fh.get('finnhubIndustry')
                logger.info(f"Fetched profile from Finnhub for {self.ticker}.")
            else:
                logger.warning(f"Finnhub profile failed for {self.ticker}. Trying Alpha Vantage Overview.")
                overview_av = self.alphavantage.get_company_overview(self.ticker); time.sleep(2)
                if overview_av and overview_av.get("Symbol") == self.ticker:
                    self._financial_data_cache['overview_alphavantage'] = overview_av
                    company_name, industry, sector, cik = overview_av.get('Name'), overview_av.get('Industry'), overview_av.get('Sector'), overview_av.get('CIK')
                    if cik: cik = str(cik).zfill(10)
                    logger.info(f"Fetched overview from Alpha Vantage for {self.ticker}.")
                else: logger.warning(f"All primary profile fetches (FMP, Finnhub, AV) failed or incomplete for {self.ticker}.")
        if not company_name: company_name = self.ticker
        if not cik and self.ticker:
            logger.info(f"CIK not found from profiles for {self.ticker}. Querying SEC EDGAR CIK map.")
            cik_from_edgar = self.sec_edgar.get_cik_by_ticker(self.ticker); time.sleep(0.5)
            if cik_from_edgar: cik = str(cik_from_edgar).zfill(10); logger.info(f"Fetched CIK {cik} from SEC EDGAR CIK map for {self.ticker}.")
            else: logger.warning(f"Could not fetch CIK from SEC EDGAR CIK map for {self.ticker}.")

        if not self.stock_db_entry:
            logger.info(f"Stock {self.ticker} not found in DB, creating new entry.")
            self.stock_db_entry = Stock(ticker=self.ticker, company_name=company_name, industry=industry, sector=sector, cik=cik)
            self.db_session.add(self.stock_db_entry)
            try: self.db_session.commit(); self.db_session.refresh(self.stock_db_entry)
            except SQLAlchemyError as e: self.db_session.rollback(); logger.error(f"Error creating stock entry for {self.ticker}: {e}", exc_info=True); raise
        else:
            updated = False
            if company_name and self.stock_db_entry.company_name != company_name: self.stock_db_entry.company_name = company_name; updated = True
            if industry and self.stock_db_entry.industry != industry: self.stock_db_entry.industry = industry; updated = True
            if sector and self.stock_db_entry.sector != sector: self.stock_db_entry.sector = sector; updated = True
            if cik and self.stock_db_entry.cik != cik: self.stock_db_entry.cik = cik; updated = True
            elif not self.stock_db_entry.cik and cik: self.stock_db_entry.cik = cik; updated = True
            if updated:
                try: self.db_session.commit(); self.db_session.refresh(self.stock_db_entry); logger.info(f"Updated stock entry for {self.ticker} with new profile data.")
                except SQLAlchemyError as e: self.db_session.rollback(); logger.error(f"Error updating stock entry for {self.ticker}: {e}")
        logger.info(f"Stock entry for {self.ticker} (ID: {self.stock_db_entry.id if self.stock_db_entry else 'N/A'}, CIK: {self.stock_db_entry.cik if self.stock_db_entry and self.stock_db_entry.cik else 'N/A'}) ready.")

    def _fetch_financial_statements(self):
        logger.info(f"Fetching financial statements for {self.ticker}...")
        statements_cache = {"fmp_income_annual": [], "fmp_balance_annual": [], "fmp_cashflow_annual": [], "fmp_income_quarterly": [], "finnhub_financials_quarterly_reported": {"data": []}, "alphavantage_income_quarterly": {"quarterlyReports": []}, "alphavantage_balance_quarterly": {"quarterlyReports": []}, "alphavantage_cashflow_quarterly": {"quarterlyReports": []}}
        try:
            statements_cache["fmp_income_annual"] = self.fmp.get_financial_statements(self.ticker, "income-statement", "annual", STOCK_FINANCIAL_YEARS) or []; time.sleep(1.5)
            statements_cache["fmp_balance_annual"] = self.fmp.get_financial_statements(self.ticker, "balance-sheet-statement", "annual", STOCK_FINANCIAL_YEARS) or []; time.sleep(1.5)
            statements_cache["fmp_cashflow_annual"] = self.fmp.get_financial_statements(self.ticker, "cash-flow-statement", "annual", STOCK_FINANCIAL_YEARS) or []; time.sleep(1.5)
            logger.info(f"FMP Annuals for {self.ticker}: Income({len(statements_cache['fmp_income_annual'])}), Balance({len(statements_cache['fmp_balance_annual'])}), Cashflow({len(statements_cache['fmp_cashflow_annual'])}).")
            statements_cache["fmp_income_quarterly"] = self.fmp.get_financial_statements(self.ticker, "income-statement", "quarter", 8) or []; time.sleep(1.5)
            logger.info(f"FMP Quarterly Income for {self.ticker}: {len(statements_cache['fmp_income_quarterly'])} records.")
            fh_q_data = self.finnhub.get_financials_reported(self.ticker, freq="quarterly", count=8); time.sleep(1.5)
            if fh_q_data and isinstance(fh_q_data, dict) and fh_q_data.get("data"): statements_cache["finnhub_financials_quarterly_reported"] = fh_q_data; logger.info(f"Fetched {len(fh_q_data['data'])} quarterly reports from Finnhub for {self.ticker}.")
            else: logger.warning(f"Finnhub quarterly financials reported data missing or malformed for {self.ticker}.")
            av_income_q = self.alphavantage.get_income_statement_quarterly(self.ticker); time.sleep(15)
            if av_income_q and isinstance(av_income_q, dict) and av_income_q.get("quarterlyReports"): statements_cache["alphavantage_income_quarterly"] = av_income_q; logger.info(f"Fetched {len(av_income_q['quarterlyReports'])} quarterly income reports from Alpha Vantage for {self.ticker}.")
            else: logger.warning(f"Alpha Vantage quarterly income reports missing or malformed for {self.ticker}.")
            av_balance_q = self.alphavantage.get_balance_sheet_quarterly(self.ticker); time.sleep(15)
            if av_balance_q and isinstance(av_balance_q, dict) and av_balance_q.get("quarterlyReports"): statements_cache["alphavantage_balance_quarterly"] = av_balance_q; logger.info(f"Fetched {len(av_balance_q['quarterlyReports'])} quarterly balance reports from Alpha Vantage for {self.ticker}.")
            else: logger.warning(f"Alpha Vantage quarterly balance reports missing or malformed for {self.ticker}.")
            av_cashflow_q = self.alphavantage.get_cash_flow_quarterly(self.ticker); time.sleep(15)
            if av_cashflow_q and isinstance(av_cashflow_q, dict) and av_cashflow_q.get("quarterlyReports"): statements_cache["alphavantage_cashflow_quarterly"] = av_cashflow_q; logger.info(f"Fetched {len(av_cashflow_q['quarterlyReports'])} quarterly cash flow reports from Alpha Vantage for {self.ticker}.")
            else: logger.warning(f"Alpha Vantage quarterly cash flow reports missing or malformed for {self.ticker}.")
        except Exception as e: logger.warning(f"Error during financial statements fetch for {self.ticker}: {e}.", exc_info=True)
        self._financial_data_cache['financial_statements'] = statements_cache
        return statements_cache

    def _fetch_key_metrics_and_profile_data(self):
        logger.info(f"Fetching key metrics and profile for {self.ticker}.")
        self._financial_data_cache['key_metrics_annual_fmp'] = self.fmp.get_key_metrics(self.ticker, "annual", STOCK_FINANCIAL_YEARS + 2) or []; time.sleep(1.5)
        key_metrics_quarterly_fmp = self.fmp.get_key_metrics(self.ticker, "quarterly", 8); time.sleep(1.5)
        self._financial_data_cache['key_metrics_quarterly_fmp'] = key_metrics_quarterly_fmp if key_metrics_quarterly_fmp is not None else []
        self._financial_data_cache['basic_financials_finnhub'] = self.finnhub.get_basic_financials(self.ticker) or {}; time.sleep(1.5)
        if 'profile_fmp' not in self._financial_data_cache or not self._financial_data_cache.get('profile_fmp'):
            profile_fmp_list = self.fmp.get_company_profile(self.ticker); time.sleep(1.5)
            self._financial_data_cache['profile_fmp'] = profile_fmp_list[0] if profile_fmp_list and isinstance(profile_fmp_list, list) and profile_fmp_list[0] else {}
        logger.info(f"FMP KM Annual for {self.ticker}: {len(self._financial_data_cache['key_metrics_annual_fmp'])}. FMP KM Quarterly for {self.ticker}: {len(self._financial_data_cache['key_metrics_quarterly_fmp'])}. Finnhub Basic Financials for {self.ticker}: {'OK' if self._financial_data_cache.get('basic_financials_finnhub', {}).get('metric') else 'Data missing'}.")

    def _calculate_valuation_ratios(self, latest_km_q_fmp, latest_km_a_fmp, basic_fin_fh_metric):
        ratios = {}
        ratios["pe_ratio"] = safe_get_float(latest_km_q_fmp, "peRatioTTM") or safe_get_float(latest_km_a_fmp, "peRatio") or safe_get_float(basic_fin_fh_metric, "peTTM")
        ratios["pb_ratio"] = safe_get_float(latest_km_q_fmp, "priceToBookRatioTTM") or safe_get_float(latest_km_a_fmp, "pbRatio") or safe_get_float(basic_fin_fh_metric, "pbAnnual")
        ratios["ps_ratio"] = safe_get_float(latest_km_q_fmp, "priceToSalesRatioTTM") or safe_get_float(latest_km_a_fmp, "priceSalesRatio") or safe_get_float(basic_fin_fh_metric, "psTTM")
        ratios["ev_to_sales"] = safe_get_float(latest_km_q_fmp, "enterpriseValueOverRevenueTTM") or safe_get_float(latest_km_a_fmp, "enterpriseValueOverRevenue")
        ratios["ev_to_ebitda"] = safe_get_float(latest_km_q_fmp, "evToEbitdaTTM") or safe_get_float(latest_km_a_fmp, "evToEbitda")
        div_yield_fmp_q = safe_get_float(latest_km_q_fmp, "dividendYieldTTM"); div_yield_fmp_a = safe_get_float(latest_km_a_fmp, "dividendYield")
        div_yield_fh_raw = safe_get_float(basic_fin_fh_metric, "dividendYieldAnnual"); div_yield_fh = div_yield_fh_raw / 100.0 if div_yield_fh_raw is not None else None
        ratios["dividend_yield"] = div_yield_fmp_q if div_yield_fmp_q is not None else (div_yield_fmp_a if div_yield_fmp_a is not None else div_yield_fh)
        return ratios

    def _calculate_profitability_metrics(self, income_annual_fmp, balance_annual_fmp, latest_km_a_fmp):
        metrics = {}
        if income_annual_fmp:
            latest_ia = income_annual_fmp[0]
            metrics["eps"] = safe_get_float(latest_ia, "eps") or safe_get_float(latest_km_a_fmp, "eps")
            metrics["net_profit_margin"] = safe_get_float(latest_ia, "netProfitMargin")
            metrics["gross_profit_margin"] = safe_get_float(latest_ia, "grossProfitMargin")
            metrics["operating_profit_margin"] = safe_get_float(latest_ia, "operatingIncomeRatio")
            ebit = safe_get_float(latest_ia, "operatingIncome"); interest_expense = safe_get_float(latest_ia, "interestExpense")
            if ebit is not None and interest_expense is not None and abs(interest_expense) > 1e-6: metrics["interest_coverage_ratio"] = ebit / abs(interest_expense)
        if balance_annual_fmp and income_annual_fmp:
            total_equity = get_value_from_statement_list(balance_annual_fmp, "totalStockholdersEquity", 0)
            total_assets = get_value_from_statement_list(balance_annual_fmp, "totalAssets", 0)
            latest_net_income = get_value_from_statement_list(income_annual_fmp, "netIncome", 0)
            if total_equity and total_equity != 0 and latest_net_income is not None: metrics["roe"] = latest_net_income / total_equity
            if total_assets and total_assets != 0 and latest_net_income is not None: metrics["roa"] = latest_net_income / total_assets
            ebit_roic = get_value_from_statement_list(income_annual_fmp, "operatingIncome", 0)
            income_tax_expense_roic = get_value_from_statement_list(income_annual_fmp, "incomeTaxExpense", 0)
            income_before_tax_roic = get_value_from_statement_list(income_annual_fmp, "incomeBeforeTax", 0)
            effective_tax_rate = 0.21
            if income_tax_expense_roic is not None and income_before_tax_roic is not None and income_before_tax_roic != 0:
                calculated_tax_rate = income_tax_expense_roic / income_before_tax_roic
                if 0 <= calculated_tax_rate <= 0.50: effective_tax_rate = calculated_tax_rate
                else: logger.debug(f"Calculated tax rate {calculated_tax_rate:.2%} for {self.ticker} is unusual. Using default {effective_tax_rate:.2%}.")
            nopat = ebit_roic * (1 - effective_tax_rate) if ebit_roic is not None else None
            total_debt_roic = get_value_from_statement_list(balance_annual_fmp, "totalDebt", 0)
            cash_equivalents_roic = get_value_from_statement_list(balance_annual_fmp, "cashAndCashEquivalents", 0) or 0
            if total_debt_roic is not None and total_equity is not None:
                invested_capital = total_debt_roic + total_equity - cash_equivalents_roic
                if nopat is not None and invested_capital is not None and invested_capital != 0: metrics["roic"] = nopat / invested_capital
        return metrics

    def _calculate_financial_health_metrics(self, balance_annual_fmp, income_annual_fmp, latest_km_a_fmp):
        metrics = {}
        if balance_annual_fmp:
            latest_ba = balance_annual_fmp[0]; total_equity = safe_get_float(latest_ba, "totalStockholdersEquity")
            metrics["debt_to_equity"] = safe_get_float(latest_km_a_fmp, "debtToEquity")
            if metrics["debt_to_equity"] is None:
                total_debt_ba = safe_get_float(latest_ba, "totalDebt")
                if total_debt_ba is not None and total_equity and total_equity != 0: metrics["debt_to_equity"] = total_debt_ba / total_equity
            current_assets = safe_get_float(latest_ba, "totalCurrentAssets"); current_liabilities = safe_get_float(latest_ba, "totalCurrentLiabilities")
            if current_assets is not None and current_liabilities is not None and current_liabilities != 0: metrics["current_ratio"] = current_assets / current_liabilities
            cash_equivalents = safe_get_float(latest_ba, "cashAndCashEquivalents", 0); short_term_investments = safe_get_float(latest_ba, "shortTermInvestments", 0); net_receivables = safe_get_float(latest_ba, "netReceivables", 0)
            if current_liabilities is not None and current_liabilities != 0: metrics["quick_ratio"] = (cash_equivalents + short_term_investments + net_receivables) / current_liabilities
        latest_annual_ebitda_km = safe_get_float(latest_km_a_fmp, "ebitda"); latest_annual_ebitda_is = get_value_from_statement_list(income_annual_fmp, "ebitda", 0)
        latest_annual_ebitda = latest_annual_ebitda_km if latest_annual_ebitda_km is not None else latest_annual_ebitda_is
        if latest_annual_ebitda and latest_annual_ebitda != 0 and balance_annual_fmp:
            total_debt_val = get_value_from_statement_list(balance_annual_fmp, "totalDebt", 0)
            if total_debt_val is not None: metrics["debt_to_ebitda"] = total_debt_val / latest_annual_ebitda
        return metrics

    def _get_cross_validated_quarterly_revenue(self, statements_cache):
        latest_q_revenue, previous_q_revenue, source_name, historical_revenues = None, None, None, []
        revenue_fields = {"fmp_quarterly": "revenue", "alphavantage_quarterly": "totalRevenue", "finnhub_quarterly": ["Revenues", "RevenueFromContractWithCustomerExcludingAssessedTax", "TotalRevenues", "NetSales"]}
        for src_key in PRIORITY_REVENUE_SOURCES:
            try:
                if src_key == "fmp_quarterly" and statements_cache.get('fmp_income_quarterly'):
                    reports = statements_cache['fmp_income_quarterly']
                    if not reports: continue
                    latest_val = get_fmp_value(reports, revenue_fields[src_key], 0); prev_val = get_fmp_value(reports, revenue_fields[src_key], 1) if len(reports) > 1 else None
                    if latest_val is not None: latest_q_revenue, previous_q_revenue, source_name = latest_val, prev_val, "FMP"; [historical_revenues.append(h) for i in range(min(len(reports), 5)) if (h := get_fmp_value(reports, revenue_fields[src_key], i)) is not None]; break
                elif src_key == "alphavantage_quarterly" and statements_cache.get('alphavantage_income_quarterly', {}).get('quarterlyReports'):
                    reports = statements_cache['alphavantage_income_quarterly']['quarterlyReports']
                    if not reports: continue
                    latest_val = get_alphavantage_value(reports, revenue_fields[src_key], 0); prev_val = get_alphavantage_value(reports, revenue_fields[src_key], 1) if len(reports) > 1 else None
                    if latest_val is not None: latest_q_revenue, previous_q_revenue, source_name = latest_val, prev_val, "AlphaVantage"; [historical_revenues.append(h) for i in range(min(len(reports), 5)) if (h := get_alphavantage_value(reports, revenue_fields[src_key], i)) is not None]; break
                elif src_key == "finnhub_quarterly" and statements_cache.get('finnhub_financials_quarterly_reported', {}).get('data'):
                    reports = statements_cache['finnhub_financials_quarterly_reported']['data']
                    if not reports: continue
                    latest_val = get_finnhub_concept_value(reports, 'ic', revenue_fields[src_key], 0); prev_val = get_finnhub_concept_value(reports, 'ic', revenue_fields[src_key], 1) if len(reports) > 1 else None
                    if latest_val is not None: latest_q_revenue, previous_q_revenue, source_name = latest_val, prev_val, "Finnhub"; [historical_revenues.append(h) for i in range(min(len(reports), 5)) if (h := get_finnhub_concept_value(reports, 'ic', revenue_fields[src_key], i)) is not None]; break
            except Exception as e: logger.warning(f"Error processing quarterly revenue from {src_key} for {self.ticker}: {e}"); continue
        avg_historical_q_revenue = None
        if historical_revenues:
            points_for_avg = [r for r in historical_revenues if r is not None and r > 0]
            avg_base_points = points_for_avg[1:] if points_for_avg and points_for_avg[0] == latest_q_revenue and len(points_for_avg) > 1 else points_for_avg
            if len(avg_base_points) > 1:
                avg_historical_q_revenue = sum(avg_base_points) / len(avg_base_points)
                if latest_q_revenue is not None and avg_historical_q_revenue > 0:
                    deviation = abs(latest_q_revenue - avg_historical_q_revenue) / avg_historical_q_revenue
                    if deviation > Q_REVENUE_SANITY_CHECK_DEVIATION_THRESHOLD:
                        warning_msg = f"DATA QUALITY WARNING: Latest quarterly revenue ({latest_q_revenue:,.0f} from {source_name}) deviates by {deviation:.2%} from avg of recent historical quarters ({avg_historical_q_revenue:,.0f}). Review data accuracy."
                        logger.warning(warning_msg); self.data_quality_warnings.append(warning_msg)
        else: logger.info(f"Not enough historical quarterly revenue data to perform sanity check for {self.ticker}.")
        if latest_q_revenue is None: logger.error(f"Could not determine latest quarterly revenue for {self.ticker} from any source."); self.data_quality_warnings.append("CRITICAL: Latest quarterly revenue could not be determined.")
        else: logger.info(f"Using latest quarterly revenue: {latest_q_revenue:,.0f} (Source: {source_name}) for {self.ticker}.")
        return latest_q_revenue, previous_q_revenue, source_name, avg_historical_q_revenue

    def _calculate_growth_metrics(self, income_annual_fmp, statements_cache):
        metrics = {"key_metrics_snapshot": {}}
        metrics["revenue_growth_yoy"] = calculate_growth(get_value_from_statement_list(income_annual_fmp, "revenue", 0), get_value_from_statement_list(income_annual_fmp, "revenue", 1))
        metrics["eps_growth_yoy"] = calculate_growth(get_value_from_statement_list(income_annual_fmp, "eps", 0), get_value_from_statement_list(income_annual_fmp, "eps", 1))
        if len(income_annual_fmp) >= 3:
            metrics["revenue_growth_cagr_3yr"] = calculate_cagr(get_value_from_statement_list(income_annual_fmp, "revenue", 0), get_value_from_statement_list(income_annual_fmp, "revenue", 2), 2)
            metrics["eps_growth_cagr_3yr"] = calculate_cagr(get_value_from_statement_list(income_annual_fmp, "eps", 0), get_value_from_statement_list(income_annual_fmp, "eps", 2), 2)
        if len(income_annual_fmp) >= 5:
            metrics["revenue_growth_cagr_5yr"] = calculate_cagr(get_value_from_statement_list(income_annual_fmp, "revenue", 0), get_value_from_statement_list(income_annual_fmp, "revenue", 4), 4)
            metrics["eps_growth_cagr_5yr"] = calculate_cagr(get_value_from_statement_list(income_annual_fmp, "eps", 0), get_value_from_statement_list(income_annual_fmp, "eps", 4), 4)
        latest_q_rev, prev_q_rev, rev_src_name, _ = self._get_cross_validated_quarterly_revenue(statements_cache)
        if latest_q_rev is not None:
            metrics["key_metrics_snapshot"]["q_revenue_source"] = rev_src_name; metrics["key_metrics_snapshot"]["latest_q_revenue"] = latest_q_rev
            if prev_q_rev is not None: metrics["revenue_growth_qoq"] = calculate_growth(latest_q_rev, prev_q_rev)
            else: logger.info(f"Previous quarter revenue not available from source {rev_src_name} for {self.ticker}. Cannot calculate QoQ revenue growth."); metrics["revenue_growth_qoq"] = None
        else: metrics["revenue_growth_qoq"] = None; metrics["key_metrics_snapshot"]["q_revenue_source"] = "N/A"; metrics["key_metrics_snapshot"]["latest_q_revenue"] = None
        return metrics

    def _calculate_cash_flow_and_trend_metrics(self, cashflow_annual_fmp, balance_annual_fmp, profile_fmp):
        metrics = {}
        if cashflow_annual_fmp:
            fcf_latest_annual = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 0)
            shares_outstanding_profile = safe_get_float(profile_fmp, "sharesOutstanding"); mkt_cap_profile = safe_get_float(profile_fmp, "mktCap"); price_profile = safe_get_float(profile_fmp, "price")
            shares_outstanding_calc = (mkt_cap_profile / price_profile) if mkt_cap_profile and price_profile and price_profile != 0 else None
            shares_outstanding = shares_outstanding_profile if shares_outstanding_profile is not None and shares_outstanding_profile > 0 else shares_outstanding_calc
            if fcf_latest_annual is not None and shares_outstanding and shares_outstanding != 0:
                metrics["free_cash_flow_per_share"] = fcf_latest_annual / shares_outstanding
                if mkt_cap_profile and mkt_cap_profile != 0: metrics["free_cash_flow_yield"] = fcf_latest_annual / mkt_cap_profile
            if len(cashflow_annual_fmp) >= 3:
                fcf0, fcf1, fcf2 = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 0), get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 1), get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 2)
                if all(isinstance(x, (int, float)) for x in [fcf0, fcf1, fcf2] if x is not None) and all(x is not None for x in [fcf0, fcf1, fcf2]):
                    if fcf0 > fcf1 > fcf2: metrics["free_cash_flow_trend"] = "Growing"
                    elif fcf0 < fcf1 < fcf2: metrics["free_cash_flow_trend"] = "Declining"
                    elif fcf0 > fcf1 and fcf1 < fcf2: metrics["free_cash_flow_trend"] = "Volatile (Dip then Rise)"
                    elif fcf0 < fcf1 and fcf1 > fcf2: metrics["free_cash_flow_trend"] = "Volatile (Rise then Dip)"
                    else: metrics["free_cash_flow_trend"] = "Mixed/Stable"
                else: metrics["free_cash_flow_trend"] = "Data Incomplete/Non-Numeric"
            else: metrics["free_cash_flow_trend"] = "Data N/A (<3 yrs)"
        if len(balance_annual_fmp) >= 3:
            re0, re1, re2 = get_value_from_statement_list(balance_annual_fmp, "retainedEarnings", 0), get_value_from_statement_list(balance_annual_fmp, "retainedEarnings", 1), get_value_from_statement_list(balance_annual_fmp, "retainedEarnings", 2)
            if all(isinstance(x, (int, float)) for x in [re0, re1, re2] if x is not None) and all(x is not None for x in [re0, re1, re2]):
                if re0 > re1 > re2: metrics["retained_earnings_trend"] = "Growing"
                elif re0 < re1 < re2: metrics["retained_earnings_trend"] = "Declining"
                else: metrics["retained_earnings_trend"] = "Mixed/Stable"
            else: metrics["retained_earnings_trend"] = "Data Incomplete/Non-Numeric"
        else: metrics["retained_earnings_trend"] = "Data N/A (<3 yrs)"
        return metrics

    def _calculate_derived_metrics(self):
        logger.info(f"Calculating derived metrics for {self.ticker}...")
        all_metrics = {}
        statements = self._financial_data_cache.get('financial_statements', {})
        income_annual_fmp, balance_annual_fmp, cashflow_annual_fmp = statements.get('fmp_income_annual', []), statements.get('fmp_balance_annual', []), statements.get('fmp_cashflow_annual', [])
        key_metrics_annual_fmp, key_metrics_quarterly_fmp = self._financial_data_cache.get('key_metrics_annual_fmp', []), self._financial_data_cache.get('key_metrics_quarterly_fmp', [])
        basic_fin_fh_metric = self._financial_data_cache.get('basic_financials_finnhub', {}).get('metric', {}); profile_fmp = self._financial_data_cache.get('profile_fmp', {})
        latest_km_q_fmp, latest_km_a_fmp = key_metrics_quarterly_fmp[0] if key_metrics_quarterly_fmp else {}, key_metrics_annual_fmp[0] if key_metrics_annual_fmp else {}
        all_metrics.update(self._calculate_valuation_ratios(latest_km_q_fmp, latest_km_a_fmp, basic_fin_fh_metric))
        all_metrics.update(self._calculate_profitability_metrics(income_annual_fmp, balance_annual_fmp, latest_km_a_fmp))
        all_metrics.update(self._calculate_financial_health_metrics(balance_annual_fmp, income_annual_fmp, latest_km_a_fmp))
        growth_metrics_result = self._calculate_growth_metrics(income_annual_fmp, statements)
        all_metrics.update(growth_metrics_result)
        all_metrics.update(self._calculate_cash_flow_and_trend_metrics(cashflow_annual_fmp, balance_annual_fmp, profile_fmp))
        final_metrics = {}
        for k, v in all_metrics.items():
            if k == "key_metrics_snapshot": final_metrics[k] = {sk: sv for sk, sv in v.items() if sv is not None and not (isinstance(sv, float) and (math.isnan(sv) or math.isinf(sv)))}
            elif isinstance(v, float): final_metrics[k] = v if not (math.isnan(v) or math.isinf(v)) else None
            elif v is not None: final_metrics[k] = v
            else: final_metrics[k] = None
        log_metrics = {k: v for k, v in final_metrics.items() if k != "key_metrics_snapshot"}
        logger.info(f"Calculated metrics for {self.ticker}: {json.dumps(log_metrics, indent=2)}")
        self._financial_data_cache['calculated_metrics'] = final_metrics
        return final_metrics

    def _perform_dcf_analysis(self):
        logger.info(f"Performing simplified DCF analysis for {self.ticker}...")
        dcf_results = {"dcf_intrinsic_value": None, "dcf_upside_percentage": None, "dcf_assumptions": {"discount_rate": DEFAULT_DISCOUNT_RATE, "perpetual_growth_rate": DEFAULT_PERPETUAL_GROWTH_RATE, "projection_years": DEFAULT_FCF_PROJECTION_YEARS, "start_fcf": None, "start_fcf_basis": "N/A", "fcf_growth_rates_projection": [], "initial_fcf_growth_rate_used": None, "initial_fcf_growth_rate_basis": "N/A", "sensitivity_analysis": []}}
        assumptions = dcf_results["dcf_assumptions"]
        cashflow_annual_fmp = self._financial_data_cache.get('financial_statements', {}).get('fmp_cashflow_annual', []); profile_fmp = self._financial_data_cache.get('profile_fmp', {}); calculated_metrics = self._financial_data_cache.get('calculated_metrics', {})
        current_price = safe_get_float(profile_fmp, "price"); shares_outstanding_profile = safe_get_float(profile_fmp, "sharesOutstanding"); mkt_cap_profile = safe_get_float(profile_fmp, "mktCap")
        shares_outstanding_calc = (mkt_cap_profile / current_price) if mkt_cap_profile and current_price and current_price != 0 else None
        shares_outstanding = shares_outstanding_profile if shares_outstanding_profile is not None and shares_outstanding_profile > 0 else shares_outstanding_calc
        if not cashflow_annual_fmp or not profile_fmp or current_price is None or shares_outstanding is None or shares_outstanding == 0: logger.warning(f"Insufficient data for DCF for {self.ticker} (FCF statements, profile, price, or shares missing/zero)."); return dcf_results
        current_fcf_annual = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 0)
        if current_fcf_annual is None or current_fcf_annual <= 10000: logger.warning(f"Current annual FCF for {self.ticker} is {current_fcf_annual}. DCF requires substantial positive FCF. Skipping DCF."); return dcf_results
        assumptions["start_fcf"] = current_fcf_annual; assumptions["start_fcf_basis"] = f"Latest Annual FCF ({cashflow_annual_fmp[0].get('date') if cashflow_annual_fmp else 'N/A'})"
        fcf_growth_rate_3yr_cagr = None
        if len(cashflow_annual_fmp) >= 4:
            fcf_start_for_cagr = get_value_from_statement_list(cashflow_annual_fmp, "freeCashFlow", 3)
            if fcf_start_for_cagr and fcf_start_for_cagr > 0: fcf_growth_rate_3yr_cagr = calculate_cagr(current_fcf_annual, fcf_start_for_cagr, 3)
        initial_fcf_growth_rate = DEFAULT_PERPETUAL_GROWTH_RATE; assumptions["initial_fcf_growth_rate_basis"] = "Default (Perpetual Growth Rate)"
        if fcf_growth_rate_3yr_cagr is not None: initial_fcf_growth_rate = fcf_growth_rate_3yr_cagr; assumptions["initial_fcf_growth_rate_basis"] = "Historical 3yr FCF CAGR"
        elif calculated_metrics.get("revenue_growth_cagr_3yr") is not None: initial_fcf_growth_rate = calculated_metrics["revenue_growth_cagr_3yr"]; assumptions["initial_fcf_growth_rate_basis"] = "Proxy: Revenue Growth CAGR (3yr)"
        elif calculated_metrics.get("revenue_growth_yoy") is not None: initial_fcf_growth_rate = calculated_metrics["revenue_growth_yoy"]; assumptions["initial_fcf_growth_rate_basis"] = "Proxy: Revenue Growth YoY"
        if not isinstance(initial_fcf_growth_rate, (int, float)): initial_fcf_growth_rate = DEFAULT_PERPETUAL_GROWTH_RATE
        initial_fcf_growth_rate = min(max(initial_fcf_growth_rate, -0.05), 0.15); assumptions["initial_fcf_growth_rate_used"] = initial_fcf_growth_rate

        def calculate_dcf_value(start_fcf, initial_growth, discount_rate, perpetual_growth, proj_years, shares):
            projected_fcfs, last_projected_fcf, current_year_growth_rates = [], start_fcf, []
            growth_rate_decline_per_year = (initial_growth - perpetual_growth) / float(proj_years) if proj_years > 0 else 0
            for i in range(proj_years):
                current_year_growth_rate = max(initial_growth - (growth_rate_decline_per_year * i), perpetual_growth)
                projected_fcf = last_projected_fcf * (1 + current_year_growth_rate)
                projected_fcfs.append(projected_fcf); last_projected_fcf = projected_fcf; current_year_growth_rates.append(round(current_year_growth_rate, 4))
            if not projected_fcfs: return None, []
            terminal_year_fcf_for_tv = projected_fcfs[-1] * (1 + perpetual_growth); terminal_value_denominator = discount_rate - perpetual_growth
            terminal_value = 0 if terminal_value_denominator <= 1e-6 else terminal_year_fcf_for_tv / terminal_value_denominator
            if terminal_value_denominator <= 1e-6 : logger.warning(f"DCF for {self.ticker}: Discount rate ({discount_rate}) near/below perpetual growth ({perpetual_growth}). Terminal Value unreliable.")
            sum_discounted_fcf = sum(fcf / ((1 + discount_rate) ** (i + 1)) for i, fcf in enumerate(projected_fcfs))
            discounted_terminal_value = terminal_value / ((1 + discount_rate) ** proj_years)
            intrinsic_equity_value = sum_discounted_fcf + discounted_terminal_value
            return intrinsic_equity_value / shares if shares != 0 else None, current_year_growth_rates

        base_iv_per_share, base_fcf_growth_rates = calculate_dcf_value(assumptions["start_fcf"], assumptions["initial_fcf_growth_rate_used"], assumptions["discount_rate"], assumptions["perpetual_growth_rate"], assumptions["projection_years"], shares_outstanding)
        if base_iv_per_share is not None:
            dcf_results["dcf_intrinsic_value"] = base_iv_per_share; assumptions["fcf_growth_rates_projection"] = base_fcf_growth_rates
            if current_price and current_price != 0: dcf_results["dcf_upside_percentage"] = (base_iv_per_share - current_price) / current_price
        else: logger.error(f"DCF base case calculation failed for {self.ticker}."); return dcf_results
        sensitivity_scenarios = [{"dr_adj": -0.005, "pgr_adj": 0.0, "label": "Discount Rate -0.5%"}, {"dr_adj": +0.005, "pgr_adj": 0.0, "label": "Discount Rate +0.5%"}, {"dr_adj": 0.0, "pgr_adj": -0.0025, "label": "Perp. Growth -0.25%"}, {"dr_adj": 0.0, "pgr_adj": +0.0025, "label": "Perp. Growth +0.25%"}]
        for scenario in sensitivity_scenarios:
            sens_dr, sens_pgr = assumptions["discount_rate"] + scenario["dr_adj"], assumptions["perpetual_growth_rate"] + scenario["pgr_adj"]
            if sens_pgr >= sens_dr - 0.001: logger.debug(f"Skipping DCF sensitivity scenario '{scenario['label']}' for {self.ticker} as PGR ({sens_pgr:.3f}) >= DR ({sens_dr:.3f})."); continue
            iv_sens, _ = calculate_dcf_value(assumptions["start_fcf"], assumptions["initial_fcf_growth_rate_used"], sens_dr, sens_pgr, assumptions["projection_years"], shares_outstanding)
            if iv_sens is not None:
                upside_sens = (iv_sens - current_price) / current_price if current_price and current_price != 0 else None
                assumptions["sensitivity_analysis"].append({"scenario": scenario["label"], "discount_rate": sens_dr, "perpetual_growth_rate": sens_pgr, "intrinsic_value": iv_sens, "upside": upside_sens})
        logger.info(f"DCF for {self.ticker}: Base IV/Share: {dcf_results.get('dcf_intrinsic_value', 'N/A'):.2f}, Upside: {dcf_results.get('dcf_upside_percentage', 'N/A') * 100 if dcf_results.get('dcf_upside_percentage') is not None else 'N/A':.2f}%")
        self._financial_data_cache['dcf_results'] = dcf_results
        return dcf_results

    def _summarize_text_chunked(self, text_to_summarize, base_context, section_specific_instruction, company_name_ticker_prompt):
        if not text_to_summarize or not isinstance(text_to_summarize, str) or not text_to_summarize.strip(): return "No text provided for summarization.", 0
        text_len = len(text_to_summarize)
        logger.info(f"Summarizing '{base_context}' for {company_name_ticker_prompt}, original length: {text_len} chars.")
        if text_len < SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS:
            logger.info(f"Section length {text_len} is within single-pass limit ({SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS}). Summarizing directly.")
            summary = self.gemini.summarize_text_with_context(text_to_summarize, f"{base_context} for {company_name_ticker_prompt}.", section_specific_instruction); time.sleep(2)
            return (summary if summary and not summary.startswith("Error:") else f"AI summary error or no content for '{base_context}'."), text_len
        logger.info(f"Section length {text_len} exceeds single-pass limit. Applying chunked summarization (Chunk size: {SUMMARIZATION_CHUNK_SIZE_CHARS}, Overlap: {SUMMARIZATION_CHUNK_OVERLAP_CHARS}).")
        chunks, start = [], 0
        while start < text_len: end = start + SUMMARIZATION_CHUNK_SIZE_CHARS; chunks.append(text_to_summarize[start:end]); start = end - SUMMARIZATION_CHUNK_OVERLAP_CHARS if end < text_len and SUMMARIZATION_CHUNK_OVERLAP_CHARS < SUMMARIZATION_CHUNK_SIZE_CHARS else end
        chunk_summaries = []
        for i, chunk in enumerate(chunks):
            logger.info(f"Summarizing chunk {i + 1}/{len(chunks)} for '{base_context}' of {company_name_ticker_prompt} (length: {len(chunk)} chars).")
            summary = self.gemini.summarize_text_with_context(chunk, f"This is chunk {i + 1} of {len(chunks)} from the '{base_context}' section for {company_name_ticker_prompt}.", f"Summarize this chunk. Focus on key facts and figures relevant to: {section_specific_instruction}"); time.sleep(2)
            chunk_summaries.append(summary if summary and not summary.startswith("Error:") else f"[AI error or no content for chunk {i + 1} of '{base_context}']")
        if not chunk_summaries: return f"No summaries generated from chunks for '{base_context}'.", text_len
        concatenated_summaries = "\n\n---\n\n".join(chunk_summaries)
        logger.info(f"Concatenated chunk summaries length for '{base_context}': {len(concatenated_summaries)} chars.")
        if not concatenated_summaries.strip() or all("[AI error" in s for s in chunk_summaries): return f"Failed to generate summaries for any chunk of '{base_context}'.", text_len
        if len(concatenated_summaries) > SUMMARIZATION_MAX_CONCAT_SUMMARIES_CHARS:
            logger.info(f"Concatenated summaries for '{base_context}' too long. Performing a final 'summary of summaries' pass.")
            final_summary = self.gemini.summarize_text_with_context(concatenated_summaries, f"The following are collated summaries from different parts of the '{base_context}' section for {company_name_ticker_prompt}.", f"Synthesize these individual chunk summaries into a single, cohesive overview of the '{base_context}', maintaining factual accuracy and addressing the original goal: {section_specific_instruction}."); time.sleep(2)
            return (final_summary if final_summary and not final_summary.startswith("Error:") else f"AI error in final summary pass for '{base_context}'."), text_len
        else: return concatenated_summaries, text_len

    def _fetch_and_summarize_10k(self):
        logger.info(f"Fetching and attempting to summarize latest 10-K for {self.ticker}")
        summary_results = {"qualitative_sources_summary": {}}
        if not self.stock_db_entry or not self.stock_db_entry.cik: logger.warning(f"No CIK for {self.ticker}. Cannot fetch 10-K."); return summary_results
        filing_url = self.sec_edgar.get_filing_document_url(self.stock_db_entry.cik, "10-K"); time.sleep(0.5)
        if not filing_url: logger.info(f"No recent 10-K found for {self.ticker}, trying 10-K/A."); filing_url = self.sec_edgar.get_filing_document_url(self.stock_db_entry.cik, "10-K/A"); time.sleep(0.5)
        if not filing_url: logger.warning(f"No 10-K or 10-K/A URL found for {self.ticker} (CIK: {self.stock_db_entry.cik})"); return summary_results
        summary_results["qualitative_sources_summary"]["10k_filing_url_used"] = filing_url
        text_content = self.sec_edgar.get_filing_text(filing_url)
        if not text_content: logger.warning(f"Failed to fetch/load 10-K text from {filing_url}"); return summary_results
        logger.info(f"Fetched 10-K text (length: {len(text_content)}) for {self.ticker}. Extracting and summarizing sections.")
        sections = extract_S1_text_sections(text_content, TEN_K_KEY_SECTIONS)
        company_name_for_prompt = self.stock_db_entry.company_name or self.ticker
        section_details = {"business": ("Business (Item 1)", "Summarize the company's core business operations, primary products/services, revenue generation model, key customer segments, and primary markets. Highlight any recent strategic shifts mentioned."), "risk_factors": ("Risk Factors (Item 1A)", "Identify and summarize the 3-5 most significant and company-specific risk factors disclosed. Focus on operational and strategic risks rather than generic market risks. Briefly explain the potential impact of each."), "mda": ("Management's Discussion and Analysis (Item 7)", "Summarize key insights into financial performance drivers (revenue, costs, profitability), financial condition (liquidity, capital resources), and management's outlook or significant focus areas. Note any discussion on margin pressures or segment performance changes.")}
        for section_key, (prompt_section_name, specific_instruction) in section_details.items():
            section_text = sections.get(section_key)
            if not section_text:
                logger.warning(f"Section '{prompt_section_name}' not found in 10-K for {self.ticker}.")
                summary_results[f"{section_key}_summary"] = "Section not found in 10-K document."; summary_results["qualitative_sources_summary"][f"{section_key}_10k_source_length"] = 0; continue
            summary, source_len = self._summarize_text_chunked(section_text, prompt_section_name, specific_instruction, f"{company_name_for_prompt} ({self.ticker})")
            summary_results[f"{section_key}_summary"] = summary; summary_results["qualitative_sources_summary"][f"{section_key}_10k_source_length"] = source_len
            logger.info(f"Summary for '{prompt_section_name}' (source length {source_len}): {summary[:150].replace(chr(10), ' ')}...")
        biz_summary_str = summary_results.get("business_summary", ""); mda_summary_str = summary_results.get("mda_summary", ""); risk_summary_str = summary_results.get("risk_factors_summary", "")
        if biz_summary_str.startswith(("Section not found", "AI summary error")): biz_summary_str = ""
        if mda_summary_str.startswith(("Section not found", "AI summary error")): mda_summary_str = ""
        if risk_summary_str.startswith(("Section not found", "AI summary error")): risk_summary_str = ""
        moat_input_text = (f"Business Summary:\n{biz_summary_str}\n\nRisk Factors Summary:\n{risk_summary_str}").strip()
        if moat_input_text and (biz_summary_str or risk_summary_str):
            moat_prompt = (f"Analyze the primary economic moats (e.g., brand strength, network effects, switching costs, intangible assets like patents/IP, cost advantages from scale/process) for {company_name_for_prompt} ({self.ticker}), based on the following summaries from its 10-K:\n\n{moat_input_text}\n\nProvide a concise analysis of its key economic moats. For each identified moat, briefly explain the evidence from the text and assess its perceived strength (e.g., Very Strong, Strong, Moderate, Weak). If certain moats are not strongly evident, state that.")
            moat_summary = self.gemini.generate_text(moat_prompt); time.sleep(3)
            summary_results["economic_moat_summary"] = moat_summary if moat_summary and not moat_summary.startswith("Error:") else "AI analysis for economic moat failed or no input."
        else: summary_results["economic_moat_summary"] = "Insufficient input from 10-K summaries for economic moat analysis."
        industry_context_text = (f"Company: {company_name_for_prompt} ({self.ticker})\nIndustry: {self.stock_db_entry.industry or 'Not Specified'}\nSector: {self.stock_db_entry.sector or 'Not Specified'}\n\nBusiness Summary (from 10-K):\n{biz_summary_str}\n\nMD&A Highlights (from 10-K):\n{mda_summary_str}").strip()
        if biz_summary_str:
            industry_prompt = (f"Based on the provided information for {company_name_for_prompt} ({self.ticker}):\n\n{industry_context_text}\n\nAnalyze key industry trends relevant to this company. Discuss significant opportunities and challenges within this industry context. How does the company appear to be positioned to capitalize on opportunities and mitigate challenges, based on its business summary and MD&A highlights? Be specific and use information from the text.")
            industry_summary = self.gemini.generate_text(industry_prompt); time.sleep(3)
            summary_results["industry_trends_summary"] = industry_summary if industry_summary and not industry_summary.startswith("Error:") else "AI analysis for industry trends failed or no input."
        else: summary_results["industry_trends_summary"] = "Insufficient input from 10-K (Business Summary missing) for industry analysis."
        if "mda_summary" in summary_results:
            summary_results["management_assessment_summary"] = summary_results.pop("mda_summary")
            if "mda_10k_source_length" in summary_results["qualitative_sources_summary"]: summary_results["qualitative_sources_summary"]["management_assessment_10k_source_length"] = summary_results["qualitative_sources_summary"].pop("mda_10k_source_length")
        logger.info(f"10-K qualitative summaries and AI interpretations generated for {self.ticker}.")
        self._financial_data_cache['10k_summaries'] = summary_results
        return summary_results

    def _fetch_and_analyze_competitors(self):
        logger.info(f"Fetching and analyzing competitor data for {self.ticker}...")
        competitor_analysis_summary = "Competitor analysis not performed or failed."
        peers_data_finnhub = self.finnhub.get_company_peers(self.ticker); time.sleep(1.5)
        if not peers_data_finnhub or not isinstance(peers_data_finnhub, list) or not peers_data_finnhub[0]:
            logger.warning(f"No direct peer data found from Finnhub for {self.ticker}.")
            self._financial_data_cache['competitor_analysis'] = {"summary": "No peer data found from primary source (Finnhub).", "peers_data": []}; return "No peer data found from primary source (Finnhub)."
        if isinstance(peers_data_finnhub[0], list): peers_data_finnhub = peers_data_finnhub[0]
        peer_tickers = [p for p in peers_data_finnhub if p and p.upper() != self.ticker.upper()][:MAX_COMPETITORS_TO_ANALYZE]
        if not peer_tickers:
            logger.info(f"No distinct competitor tickers found after filtering for {self.ticker}.")
            self._financial_data_cache['competitor_analysis'] = {"summary": "No distinct competitor tickers identified.", "peers_data": []}; return "No distinct competitor tickers identified."
        logger.info(f"Identified peers for {self.ticker}: {peer_tickers}. Fetching basic data for comparison.")
        peer_details_list = []
        for peer_ticker in peer_tickers:
            try:
                logger.debug(f"Fetching basic data for peer: {peer_ticker}")
                peer_profile_fmp_list = self.fmp.get_company_profile(peer_ticker); time.sleep(1.5)
                peer_profile_fmp = peer_profile_fmp_list[0] if peer_profile_fmp_list and isinstance(peer_profile_fmp_list, list) and peer_profile_fmp_list[0] else {}
                peer_metrics_fmp_list = self.fmp.get_key_metrics(peer_ticker, period="annual", limit=1); time.sleep(1.5)
                peer_metrics_fmp = peer_metrics_fmp_list[0] if peer_metrics_fmp_list and isinstance(peer_metrics_fmp_list, list) and peer_metrics_fmp_list[0] else {}
                peer_fh_basics = {}
                if not peer_metrics_fmp.get("peRatio") or not peer_metrics_fmp.get("priceSalesRatio"):
                    peer_fh_basics_data = self.finnhub.get_basic_financials(peer_ticker); time.sleep(1.5)
                    peer_fh_basics = peer_fh_basics_data.get("metric", {}) if peer_fh_basics_data else {}
                peer_name = peer_profile_fmp.get("companyName", peer_ticker); market_cap = safe_get_float(peer_profile_fmp, "mktCap")
                pe_ratio = safe_get_float(peer_metrics_fmp, "peRatio") or safe_get_float(peer_fh_basics, "peTTM")
                ps_ratio = safe_get_float(peer_metrics_fmp, "priceSalesRatio") or safe_get_float(peer_fh_basics, "psTTM")
                peer_info = {"ticker": peer_ticker, "name": peer_name, "market_cap": market_cap, "pe_ratio": pe_ratio, "ps_ratio": ps_ratio}
                if peer_name != peer_ticker or market_cap or pe_ratio or ps_ratio: peer_details_list.append(peer_info)
            except Exception as e: logger.warning(f"Error fetching data for peer {peer_ticker}: {e}", exc_info=True)
            if len(peer_details_list) >= MAX_COMPETITORS_TO_ANALYZE: break
        if not peer_details_list:
            competitor_analysis_summary = "Could not fetch sufficient data for identified competitors."
            self._financial_data_cache['competitor_analysis'] = {"summary": competitor_analysis_summary, "peers_data": []}; return competitor_analysis_summary
        company_name_for_prompt = self.stock_db_entry.company_name or self.ticker
        k_summaries = self._financial_data_cache.get('10k_summaries', {}); biz_summary_10k = k_summaries.get('business_summary', 'N/A')
        if biz_summary_10k.startswith("Section not found") or biz_summary_10k.startswith("AI summary error"): biz_summary_10k = "Business summary from 10-K not available or failed."
        prompt_context = (f"Company being analyzed: {company_name_for_prompt} ({self.ticker}).\nIts 10-K Business Summary: {biz_summary_10k}\n\nIdentified Competitors and their basic data:\n")
        for peer in peer_details_list:
            mc_str = f"{peer['market_cap']:,.0f}" if peer['market_cap'] else "N/A"; pe_str = f"{peer['pe_ratio']:.2f}" if peer['pe_ratio'] is not None else "N/A"; ps_str = f"{peer['ps_ratio']:.2f}" if peer['ps_ratio'] is not None else "N/A"
            prompt_context += f"- {peer['name']} ({peer['ticker']}): Market Cap: {mc_str}, P/E: {pe_str}, P/S: {ps_str}\n"
        comp_prompt = (f"{prompt_context}\n\nInstruction: Based on the business summary of {company_name_for_prompt} and the list of its competitors with their financial metrics, provide a concise analysis of the competitive landscape. Discuss {company_name_for_prompt}'s market positioning relative to these competitors. Highlight any key differences in scale (market cap) or valuation (P/E, P/S) that stand out. Address the intensity of competition. Do not invent information not present. If competitor data is sparse, acknowledge that. This summary should complement, not merely repeat, the 10-K business description.")
        comp_summary_ai = self.gemini.generate_text(comp_prompt); time.sleep(3)
        if comp_summary_ai and not comp_summary_ai.startswith("Error:"): competitor_analysis_summary = comp_summary_ai
        else: competitor_analysis_summary = "AI synthesis of competitor data failed. Basic peer data might be available in snapshot."; self.data_quality_warnings.append("Competitor analysis AI synthesis failed.")
        self._financial_data_cache['competitor_analysis'] = {"summary": competitor_analysis_summary, "peers_data": peer_details_list}
        logger.info(f"Competitor analysis summary generated for {self.ticker}.")
        return competitor_analysis_summary

    def _parse_ai_investment_thesis_response(self, ai_response_text):
        parsed_data = {"investment_thesis_full": "AI response not fully processed or 'Investment Thesis:' section missing.", "investment_decision": "Review AI Output", "strategy_type": "Not Specified by AI", "confidence_level": "Not Specified by AI", "reasoning": "AI response not fully processed or 'Key Reasoning Points:' section missing."}
        if not ai_response_text or ai_response_text.startswith("Error:"):
            error_message = ai_response_text if ai_response_text else "Error: Empty response from AI for thesis."
            parsed_data["investment_thesis_full"] = error_message; parsed_data["reasoning"] = error_message; parsed_data["investment_decision"] = "AI Error"; parsed_data["strategy_type"] = "AI Error"; parsed_data["confidence_level"] = "AI Error"; return parsed_data
        text_content = ai_response_text.replace('\r\n', '\n').strip()
        patterns = {"investment_thesis_full": re.compile(r"^\s*Investment Thesis:\s*\n?(.*?)(?=\n\s*(?:Investment Decision:|Strategy Type:|Confidence Level:|Key Reasoning Points:)|^\s*$|\Z)", re.I | re.M | re.S), "investment_decision": re.compile(r"^\s*Investment Decision:\s*\n?(.*?)(?=\n\s*(?:Investment Thesis:|Strategy Type:|Confidence Level:|Key Reasoning Points:)|^\s*$|\Z)", re.I | re.M | re.S), "strategy_type": re.compile(r"^\s*Strategy Type:\s*\n?(.*?)(?=\n\s*(?:Investment Thesis:|Investment Decision:|Confidence Level:|Key Reasoning Points:)|^\s*$|\Z)", re.I | re.M | re.S), "confidence_level": re.compile(r"^\s*Confidence Level:\s*\n?(.*?)(?=\n\s*(?:Investment Thesis:|Investment Decision:|Strategy Type:|Key Reasoning Points:)|^\s*$|\Z)", re.I | re.M | re.S), "reasoning": re.compile(r"^\s*Key Reasoning Points:\s*\n?(.*?)(?=\n\s*(?:Investment Thesis:|Investment Decision:|Strategy Type:|Confidence Level:)|^\s*$|\Z)", re.I | re.M | re.S)}
        found_any_section = False
        for key, pattern in patterns.items():
            match = pattern.search(text_content)
            if match:
                content = match.group(1).strip()
                if content: parsed_data[key] = content.split('\n')[0].strip() if key in ["investment_decision", "strategy_type", "confidence_level"] else content; found_any_section = True
                else: parsed_data[key] = f"'{key.replace('_', ' ').title()}:' section found but content empty."
        if not found_any_section and not ai_response_text.startswith("Error:"):
            logger.warning(f"Could not parse distinct sections from AI thesis response for {self.ticker}. Full response will be in 'investment_thesis_full'."); parsed_data["investment_thesis_full"] = text_content
        return parsed_data

    def _determine_investment_thesis(self, warn=None):
        logger.info(f"Synthesizing investment thesis for {self.ticker}...")
        metrics, qual_summaries, dcf_results, profile, competitor_analysis = self._financial_data_cache.get('calculated_metrics', {}), self._financial_data_cache.get('10k_summaries', {}), self._financial_data_cache.get('dcf_results', {}), self._financial_data_cache.get('profile_fmp', {}), self._financial_data_cache.get('competitor_analysis', {}).get("summary", "N/A")
        company_name, industry, sector = self.stock_db_entry.company_name or self.ticker, self.stock_db_entry.industry or "N/A", self.stock_db_entry.sector or "N/A"
        prompt = f"Company: {company_name} ({self.ticker})\nIndustry: {industry}, Sector: {sector}\n\nKey Financial Metrics & Data:\n"
        metrics_for_prompt = {"P/E Ratio": metrics.get("pe_ratio"), "P/B Ratio": metrics.get("pb_ratio"), "P/S Ratio": metrics.get("ps_ratio"), "Dividend Yield": metrics.get("dividend_yield"), "ROE": metrics.get("roe"), "ROIC": metrics.get("roic"), "Debt-to-Equity": metrics.get("debt_to_equity"), "Debt-to-EBITDA": metrics.get("debt_to_ebitda"), "Revenue Growth YoY": metrics.get("revenue_growth_yoy"), "Revenue Growth QoQ": metrics.get("revenue_growth_qoq"), f"Latest Quarterly Revenue (Source: {metrics.get('key_metrics_snapshot', {}).get('q_revenue_source', 'N/A')})": metrics.get('key_metrics_snapshot', {}).get('latest_q_revenue'), "EPS Growth YoY": metrics.get("eps_growth_yoy"), "Net Profit Margin": metrics.get("net_profit_margin"), "Operating Profit Margin": metrics.get("operating_profit_margin"), "Free Cash Flow Yield": metrics.get("free_cash_flow_yield"), "FCF Trend (3yr)": metrics.get("free_cash_flow_trend"), "Retained Earnings Trend (3yr)": metrics.get("retained_earnings_trend")}
        for name, val in metrics_for_prompt.items():
            if val is not None: prompt += f"- {name}: {f'{val:.2%}' if isinstance(val, float) and any(kw in name.lower() for kw in ['yield', 'growth', 'margin', 'roe', 'roic']) else (f'{val:,.0f}' if 'revenue' in name.lower() and 'growth' not in name.lower() else (f'{val:.2f}' if isinstance(val, float) else str(val)))}\n"
        current_stock_price = safe_get_float(profile, "price"); dcf_iv = dcf_results.get("dcf_intrinsic_value"); dcf_upside = dcf_results.get("dcf_upside_percentage")
        if current_stock_price is not None: prompt += f"- Current Stock Price: {current_stock_price:.2f}\n"
        if dcf_iv is not None: prompt += f"- DCF Intrinsic Value/Share (Base Case): {dcf_iv:.2f}\n"
        if dcf_upside is not None: prompt += f"- DCF Upside/Downside (Base Case): {dcf_upside:.2%}\n"
        if dcf_results.get("dcf_assumptions", {}).get("sensitivity_analysis"):
            prompt += "- DCF Sensitivity Highlights:\n"; [prompt := prompt + f"  - {s['scenario']}: IV {s['intrinsic_value']:.2f} (Upside: {s['upside']:.2% if s['upside'] is not None else 'N/A'})\n" for s in dcf_results["dcf_assumptions"]["sensitivity_analysis"][:2]]
        prompt += "\nQualitative Summaries (from 10-K & AI analysis):\n"
        qual_for_prompt = {"Business Model": qual_summaries.get("business_summary"), "Economic Moat": qual_summaries.get("economic_moat_summary"), "Industry Trends & Positioning": qual_summaries.get("industry_trends_summary"), "Competitive Landscape": competitor_analysis, "Management Discussion Highlights (MD&A)": qual_summaries.get("management_assessment_summary"), "Key Risk Factors (from 10-K)": qual_summaries.get("risk_factors_summary")}
        for name, text_val in qual_for_prompt.items():
            if text_val and isinstance(text_val, str) and not text_val.startswith(("AI analysis", "Section not found", "Insufficient input")): prompt += f"- {name}:\n{text_val[:500].replace('...', '').strip()}...\n\n"
            elif text_val: prompt += f"- {name}: {text_val}\n\n"
        if self.data_quality_warnings: prompt += "IMPORTANT DATA QUALITY CONSIDERATIONS:\n"; [prompt := prompt + f"- WARNING {i+1}: {w}\n" for i,w in enumerate(self.data_quality_warnings)]; prompt += "Acknowledge these warnings in your risk assessment or confidence level.\n\n"
        prompt += ("Instructions for AI: Based on ALL the above information (quantitative, qualitative, DCF, competitor data, and data quality warnings), provide a detailed financial analysis and investment thesis. Structure your response *EXACTLY* as follows, using these specific headings on separate lines:\n\nInvestment Thesis:\n[Comprehensive thesis (2-4 paragraphs) synthesizing all data. Discuss positives, negatives, outlook. If revenue growth is stagnant/negative but EPS growth is positive, explain the drivers (e.g., buybacks, margin expansion) and sustainability. Address any points on margin pressures (e.g., in DTC if mentioned in MD&A) or changes in segment profitability.]\n\nInvestment Decision:\n[Choose ONE: Strong Buy, Buy, Hold, Monitor, Reduce, Sell, Avoid. Base this on the overall analysis.]\n\nStrategy Type:\n[Choose ONE that best fits: Value, GARP (Growth At a Reasonable Price), Growth, Income, Speculative, Special Situation, Turnaround.]\n\nConfidence Level:\n[Choose ONE: High, Medium, Low. This reflects confidence in YOUR analysis and decision, considering data quality and completeness.]\n\nKey Reasoning Points:\n[3-7 bullet points. Each point should be a concise summary of a key factor supporting your decision. Cover: Valuation (DCF, comparables if any), Financial Health & Profitability, Growth Prospects (Revenue & EPS), Economic Moat, Competitive Position, Key Risks (including data quality issues if significant), Management & Strategy (if inferable).]\n")
        ai_response_text = self.gemini.generate_text(prompt); parsed_thesis_data = self._parse_ai_investment_thesis_response(ai_response_text)
        if any("CRITICAL:" in warn for warn in self.data_quality_warnings) or any("DATA QUALITY WARNING:" in warn for warn in self.data_quality_warnings and "revenue" in warn):
            if parsed_thesis_data.get("confidence_level", "").lower() == "high": logger.warning(f"Downgrading AI confidence from High to Medium for {self.ticker} due to critical data quality warnings."); parsed_thesis_data["confidence_level"] = "Medium"
            elif parsed_thesis_data.get("confidence_level", "").lower() == "medium": logger.warning(f"Downgrading AI confidence from Medium to Low for {self.ticker} due to critical data quality warnings."); parsed_thesis_data["confidence_level"] = "Low"
        logger.info(f"Generated thesis for {self.ticker}. Decision: {parsed_thesis_data.get('investment_decision')}, Strategy: {parsed_thesis_data.get('strategy_type')}, Confidence: {parsed_thesis_data.get('confidence_level')}")
        return parsed_thesis_data

    def analyze(self):
        logger.info(f"Full analysis pipeline started for {self.ticker}...")
        final_data_for_db = {}
        try:
            if not self.stock_db_entry: logger.error(f"Stock DB entry for {self.ticker} not initialized properly. Aborting analysis."); return None
            self._ensure_stock_db_entry_is_bound()
            self._fetch_financial_statements(); self._fetch_key_metrics_and_profile_data()
            final_data_for_db.update(self._calculate_derived_metrics()); final_data_for_db.update(self._perform_dcf_analysis())
            qual_summaries = self._fetch_and_summarize_10k(); final_data_for_db.update(qual_summaries)
            final_data_for_db["competitive_landscape_summary"] = self._fetch_and_analyze_competitors()
            final_data_for_db.update(self._determine_investment_thesis())
            analysis_entry = StockAnalysis(stock_id=self.stock_db_entry.id, analysis_date=datetime.now(timezone.utc))
            model_fields = [c.key for c in StockAnalysis.__table__.columns if c.key not in ['id', 'stock_id', 'analysis_date']]
            for field_name in model_fields:
                if field_name in final_data_for_db:
                    value_to_set = final_data_for_db[field_name]; target_column_type = getattr(StockAnalysis, field_name).type.python_type
                    if target_column_type == float:
                        if isinstance(value_to_set, str): 
                            try: 
                                value_to_set = float(value_to_set)
                            except ValueError: value_to_set = None
                        if isinstance(value_to_set, float) and (math.isnan(value_to_set) or math.isinf(value_to_set)): value_to_set = None
                    elif target_column_type == dict and not isinstance(value_to_set, dict): value_to_set = None
                    elif target_column_type == str and not isinstance(value_to_set, str): value_to_set = str(value_to_set) if value_to_set is not None else None
                    setattr(analysis_entry, field_name, value_to_set)
            self.db_session.add(analysis_entry); self.stock_db_entry.last_analysis_date = analysis_entry.analysis_date
            self.db_session.commit(); logger.info(f"Successfully analyzed and saved stock data: {self.ticker} (Analysis ID: {analysis_entry.id})")
            return analysis_entry
        except RuntimeError as rt_err: logger.critical(f"Runtime error during full analysis for {self.ticker}: {rt_err}", exc_info=True); return None
        except Exception as e:
            logger.error(f"CRITICAL error in full analysis pipeline for {self.ticker}: {e}", exc_info=True)
            if self.db_session and self.db_session.is_active:
                try: self.db_session.rollback(); logger.info(f"Rolled back DB transaction for {self.ticker} due to error.")
                except Exception as e_rb: logger.error(f"Rollback error for {self.ticker}: {e_rb}")
            return None
        finally: self._close_session_if_active()

    def _ensure_stock_db_entry_is_bound(self):
        if not self.stock_db_entry: raise RuntimeError(f"Stock entry for {self.ticker} is None during binding check. Prior initialization failure.")
        if not self.db_session.is_active:
            logger.warning(f"DB Session for {self.ticker} was INACTIVE before operation. Re-establishing.")
            self._close_session_if_active(); self.db_session = next(get_db_session())
            re_fetched_stock = self.db_session.query(Stock).filter(Stock.ticker == self.ticker).first()
            if not re_fetched_stock: raise RuntimeError(f"Failed to re-fetch stock {self.ticker} for new session after inactivity. Critical state.")
            self.stock_db_entry = re_fetched_stock; logger.info(f"Re-fetched and bound stock {self.ticker} (ID: {self.stock_db_entry.id}) to new active session.")
            return
        instance_state = sa_inspect(self.stock_db_entry)
        if not instance_state.session or instance_state.session is not self.db_session:
            obj_id_log = self.stock_db_entry.id if instance_state.has_identity else 'Transient/No ID'
            logger.warning(f"Stock {self.ticker} (ID: {obj_id_log}) DETACHED or bound to DIFFERENT session. Attempting to merge.")
            try:
                self.stock_db_entry = self.db_session.merge(self.stock_db_entry); self.db_session.flush()
                logger.info(f"Successfully merged stock {self.ticker} (ID: {self.stock_db_entry.id}) into current session.")
            except Exception as e_merge:
                logger.error(f"Failed to merge stock {self.ticker} into session: {e_merge}. Re-fetching as a fallback.", exc_info=True)
                re_fetched_from_db_after_merge_fail = self.db_session.query(Stock).filter(Stock.ticker == self.ticker).first()
                if re_fetched_from_db_after_merge_fail: self.stock_db_entry = re_fetched_from_db_after_merge_fail; logger.info(f"Successfully re-fetched stock {self.ticker} (ID: {self.stock_db_entry.id}) after merge failure.")
                else: raise RuntimeError(f"Failed to bind stock {self.ticker} to session after merge failure and re-fetch attempt. Analysis cannot proceed.")

if __name__ == '__main__':
    from database import init_db
    # init_db()
    logger.info("Starting standalone stock analysis test...")
    tickers_to_test = ["AAPL", "MSFT", "NKE"]
    for ticker_symbol in tickers_to_test:
        analysis_result_obj = None
        try:
            logger.info(f"--- Analyzing {ticker_symbol} ---")
            analyzer_instance = StockAnalyzer(ticker=ticker_symbol)
            analysis_result_obj = analyzer_instance.analyze()
            if analysis_result_obj and hasattr(analysis_result_obj, 'stock'):
                logger.info(f"Analysis for {analysis_result_obj.stock.ticker} completed. Decision: {analysis_result_obj.investment_decision}, Strategy: {analysis_result_obj.strategy_type}, Confidence: {analysis_result_obj.confidence_level}")
                if analysis_result_obj.dcf_intrinsic_value is not None: logger.info(f"  DCF Value: {analysis_result_obj.dcf_intrinsic_value:.2f}, Upside: {analysis_result_obj.dcf_upside_percentage:.2% if analysis_result_obj.dcf_upside_percentage is not None else 'N/A'}")
                logger.info(f"  QoQ Revenue Growth: {analysis_result_obj.revenue_growth_qoq if analysis_result_obj.revenue_growth_qoq is not None else 'N/A'} (Source: {analysis_result_obj.key_metrics_snapshot.get('q_revenue_source', 'N/A') if analysis_result_obj.key_metrics_snapshot else 'N/A'}, Value: {analysis_result_obj.key_metrics_snapshot.get('latest_q_revenue', 'N/A') if analysis_result_obj.key_metrics_snapshot else 'N/A'})")
                logger.info(f"  P/E: {analysis_result_obj.pe_ratio}, P/B: {analysis_result_obj.pb_ratio}, ROE: {analysis_result_obj.roe}")
                if analysis_result_obj.qualitative_sources_summary: logger.info(f"  10K URL used: {analysis_result_obj.qualitative_sources_summary.get('10k_filing_url_used', 'N/A')}")
                if analysis_result_obj.competitive_landscape_summary: logger.info(f"  Competitive Landscape Summary: {analysis_result_obj.competitive_landscape_summary[:200]}...")
            else: logger.error(f"Stock analysis pipeline FAILED or returned invalid result for {ticker_symbol}.")
        except RuntimeError as rt_err: logger.error(f"Could not run StockAnalyzer for {ticker_symbol} due to critical init error: {rt_err}")
        except Exception as e_main_loop: logger.error(f"Unhandled error analyzing {ticker_symbol} in __main__ loop: {e_main_loop}", exc_info=True)
        finally: logger.info(f"--- Finished processing {ticker_symbol} ---"); time.sleep(20)
---------- END stock_analyzer.py ----------


---------- .gitignore ----------
venv
/__pycache__

---------- END .gitignore ----------


---------- main.py ----------
# main.py
import argparse
from datetime import datetime, timezone
from sqlalchemy.orm import joinedload
import time
import sys # For sys.excepthook if enabled

from database.connection import init_db, SessionLocal
from core.logging_setup import logger, handle_global_exception # Import global handler
from services import StockAnalyzer, IPOAnalyzer, NewsAnalyzer, EmailService
from database.models import StockAnalysis, IPOAnalysis, NewsEventAnalysis
from core.config import MAX_NEWS_TO_ANALYZE_PER_RUN


def run_stock_analysis(tickers):
    logger.info(f"--- Starting Individual Stock Analysis for: {tickers} ---")
    results = []
    for ticker in tickers:
        try:
            analyzer = StockAnalyzer(ticker=ticker)
            analysis_result = analyzer.analyze()
            if analysis_result:
                results.append(analysis_result)
            else:
                logger.warning(f"Stock analysis for {ticker} did not return a result object.")
        except RuntimeError as rt_err:
            logger.error(f"Could not run stock analysis for {ticker} due to critical init error: {rt_err}")
        except Exception as e:
            logger.error(f"Error analyzing stock {ticker}: {e}", exc_info=True)
        time.sleep(5)
    return results

def run_ipo_analysis():
    logger.info("--- Starting IPO Analysis Pipeline ---")
    try:
        analyzer = IPOAnalyzer()
        results = analyzer.run_ipo_analysis_pipeline()
        return results
    except Exception as e:
        logger.error(f"Error during IPO analysis pipeline: {e}", exc_info=True)
        return []

def run_news_analysis(category="general", count_to_analyze=MAX_NEWS_TO_ANALYZE_PER_RUN):
    logger.info(f"--- Starting News Analysis Pipeline (Category: {category}, Max to Analyze: {count_to_analyze}) ---")
    try:
        analyzer = NewsAnalyzer()
        results = analyzer.run_news_analysis_pipeline(category=category, count_to_analyze_this_run=count_to_analyze)
        return results
    except Exception as e:
        logger.error(f"Error during news analysis pipeline: {e}", exc_info=True)
        return []

def generate_and_send_todays_email_summary():
    logger.info("--- Generating Today's Email Summary ---")
    db_session = SessionLocal()
    today_start_utc = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
    try:
        recent_stock_analyses = db_session.query(StockAnalysis).filter(StockAnalysis.analysis_date >= today_start_utc).options(joinedload(StockAnalysis.stock)).all()
        recent_ipo_analyses = db_session.query(IPOAnalysis).filter(IPOAnalysis.analysis_date >= today_start_utc).options(joinedload(IPOAnalysis.ipo)).all()
        recent_news_analyses = db_session.query(NewsEventAnalysis).filter(NewsEventAnalysis.analysis_date >= today_start_utc).options(joinedload(NewsEventAnalysis.news_event)).all()
        logger.info(f"Found {len(recent_stock_analyses)} stock analyses, {len(recent_ipo_analyses)} IPO analyses, {len(recent_news_analyses)} news analyses since {today_start_utc.strftime('%Y-%m-%d %H:%M:%S %Z')} for email.")
        if not any([recent_stock_analyses, recent_ipo_analyses, recent_news_analyses]):
            logger.info("No new analyses performed recently to include in the email summary.")
            return
        email_svc = EmailService()
        email_message = email_svc.create_summary_email(stock_analyses=recent_stock_analyses, ipo_analyses=recent_ipo_analyses, news_analyses=recent_news_analyses)
        if email_message:
            email_svc.send_email(email_message)
        else:
            logger.error("Failed to create the email message (returned None).")
    except Exception as e:
        logger.error(f"Error generating or sending email summary: {e}", exc_info=True)
    finally:
        SessionLocal.remove() # Ensure session is closed/removed

def main():
    parser = argparse.ArgumentParser(description="Financial Analysis and Reporting Tool")
    parser.add_argument("--analyze-stocks", nargs="+", metavar="TICKER", help="List of stock tickers to analyze (e.g., AAPL MSFT)")
    parser.add_argument("--analyze-ipos", action="store_true", help="Run IPO analysis pipeline.")
    parser.add_argument("--analyze-news", action="store_true", help="Run news analysis pipeline.")
    parser.add_argument("--news-category", default="general", help="Category for news analysis (e.g., general, forex, crypto, merger).")
    parser.add_argument("--news-count-analyze", type=int, default=MAX_NEWS_TO_ANALYZE_PER_RUN, help=f"Max number of new news items to analyze in this run (default from config: {MAX_NEWS_TO_ANALYZE_PER_RUN}).")
    parser.add_argument("--send-email", action="store_true", help="Generate and send email summary of today's/recent analyses.")
    parser.add_argument("--init-db", action="store_true", help="Initialize the database (create tables).")
    parser.add_argument("--all", action="store_true", help="Run all analyses (stocks from a predefined list, IPOs, News) and send email.")
    args = parser.parse_args()

    if args.init_db:
        logger.info("Initializing database as per command line argument...")
        try: init_db(); logger.info("Database initialization complete.")
        except Exception as e: logger.critical(f"Database initialization failed: {e}", exc_info=True); return

    if args.all:
        default_stocks_for_all = ["AAPL", "MSFT", "GOOGL", "NVDA", "JPM"]
        logger.info(f"Running all analyses for default stocks: {default_stocks_for_all}, IPOs, and News (max {args.news_count_analyze} items).")
        if default_stocks_for_all: run_stock_analysis(default_stocks_for_all)
        time.sleep(5); run_ipo_analysis(); time.sleep(5)
        run_news_analysis(category=args.news_category, count_to_analyze=args.news_count_analyze)
        time.sleep(5); generate_and_send_todays_email_summary()
        logger.info("--- '--all' tasks finished. ---"); return

    if args.analyze_stocks: run_stock_analysis(args.analyze_stocks)
    if args.analyze_ipos: run_ipo_analysis()
    if args.analyze_news: run_news_analysis(category=args.news_category, count_to_analyze=args.news_count_analyze)
    if args.send_email: generate_and_send_todays_email_summary()
    if not (args.analyze_stocks or args.analyze_ipos or args.analyze_news or args.send_email or args.init_db or args.all):
        logger.info("No action specified. Use --help for options."); parser.print_help()
    logger.info("--- Main script execution finished. ---")

if __name__ == "__main__":
    # sys.excepthook = handle_global_exception # Uncomment to enable global exception logging
    script_start_time = datetime.now(timezone.utc)
    logger.info("===================================================================")
    logger.info(f"Starting Financial Analysis Script at {script_start_time.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    logger.info("===================================================================")
    main()
    script_end_time = datetime.now(timezone.utc)
    logger.info(f"Financial Analysis Script finished at {script_end_time.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    logger.info(f"Total execution time: {script_end_time - script_start_time}")
    logger.info("===================================================================")
---------- END main.py ----------


---------- requirements.txt ----------
# requirements.txt
sqlalchemy>=1.4,<2.0
requests>=2.32.0
psycopg2-binary>=2.8.0
pandas>=1.0.0
markdown2>=2.4.0
beautifulsoup4>=4.9.3
lxml>=4.6.3 
# Add other specific versions if needed
# python-dotenv # For managing environment variables if you choose to use .env files
---------- END requirements.txt ----------

--- END OF FILE project_structure_backend.txt ---
