--- START OF FILE project_structure_backend.txt ---

.gitignore
api_clients.py
app_analysis.log
config.py
database.py
email_generator.py
error_handler.py
ipo_analyzer.py
main.py
models.py
news_analyzer.py
project_structure.py
requirements.txt
stock_analyzer.py


---------- .gitignore ----------
venv
/__pycache__

---------- END .gitignore ----------


---------- api_clients.py ----------
# api_clients.py
import requests
import time
import json
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup  # For news scraping
import re  # For S-1/10-K parsing

from config import (
    GOOGLE_API_KEYS, FINNHUB_API_KEY, FINANCIAL_MODELING_PREP_API_KEY,
    EODHD_API_KEY, RAPIDAPI_UPCOMING_IPO_KEY, API_REQUEST_TIMEOUT,
    API_RETRY_ATTEMPTS, API_RETRY_DELAY, CACHE_EXPIRY_SECONDS, EDGAR_USER_AGENT
)
from error_handler import logger
from database import SessionLocal  # Direct import for SessionLocal
from models import CachedAPIData

current_google_api_key_index = 0  # This is not used if rotation is per-instance or per-call


class APIClient:
    def __init__(self, base_url, api_key_name=None, api_key_value=None, headers=None):
        self.base_url = base_url
        self.api_key_name = api_key_name
        self.api_key_value = api_key_value
        self.headers = headers or {}
        if api_key_name and api_key_value:
            self.params = {api_key_name: api_key_value}
        else:
            self.params = {}

    def _get_cached_response(self, request_url_or_params_str):
        session = SessionLocal()
        try:
            current_time_utc = datetime.now(timezone.utc)
            cache_entry = session.query(CachedAPIData).filter(
                CachedAPIData.request_url_or_params == request_url_or_params_str,
                CachedAPIData.expires_at > current_time_utc
            ).first()
            if cache_entry:
                logger.info(f"Cache hit for: {request_url_or_params_str[:100]}...")  # Truncate long keys
                return cache_entry.response_data
        except Exception as e:
            logger.error(f"Error reading from cache for '{request_url_or_params_str[:100]}...': {e}", exc_info=True)
        finally:
            session.close()
        return None

    def _cache_response(self, request_url_or_params_str, response_data, api_source):
        session = SessionLocal()
        try:
            now_utc = datetime.now(timezone.utc)
            expires_at_utc = now_utc + timedelta(seconds=CACHE_EXPIRY_SECONDS)

            # Delete existing entry for this key first to avoid conflicts and ensure fresh expiry
            session.query(CachedAPIData).filter(
                CachedAPIData.request_url_or_params == request_url_or_params_str).delete(synchronize_session=False)
            # No commit needed here yet if part of a larger transaction, but for caching, standalone is fine.
            # However, to be safe with session scope, commit delete before add. Or handle potential IntegrityError.

            new_cache_entry = CachedAPIData(
                api_source=api_source,
                request_url_or_params=request_url_or_params_str,
                response_data=response_data,
                timestamp=now_utc,
                expires_at=expires_at_utc
            )
            session.add(new_cache_entry)
            session.commit()
            logger.info(f"Cached response for: {request_url_or_params_str[:100]}...")
        except Exception as e:
            logger.error(f"Error writing to cache for '{request_url_or_params_str[:100]}...': {e}", exc_info=True)
            session.rollback()
        finally:
            session.close()

    def request(self, method, endpoint, params=None, data=None, json_data=None, use_cache=True,
                api_source_name="unknown", is_json_response=True):
        url = f"{self.base_url}{endpoint}"
        current_call_params = params.copy() if params else {}
        full_query_params = self.params.copy()  # Base params (like API key in query)
        full_query_params.update(current_call_params)  # Add call-specific params

        # Create cache key string
        # Sort query params for consistent cache key
        sorted_params = sorted(full_query_params.items()) if full_query_params else []
        param_string = "&".join([f"{k}={v}" for k, v in sorted_params])
        cache_key_str = f"{method.upper()}:{url}?{param_string}"
        if json_data:  # If there's a JSON body, include its sorted representation in the cache key
            sorted_json_data_str = json.dumps(json_data, sort_keys=True)
            cache_key_str += f"|BODY:{sorted_json_data_str}"

        if use_cache:
            cached_data = self._get_cached_response(cache_key_str)
            if cached_data is not None:  # Explicitly check for None, as empty string or dict could be valid cache
                return cached_data

        for attempt in range(API_RETRY_ATTEMPTS):
            try:
                response = requests.request(
                    method, url, params=full_query_params, data=data, json=json_data,
                    headers=self.headers, timeout=API_REQUEST_TIMEOUT
                )
                response.raise_for_status()  # Raises HTTPError for bad responses (4XX or 5XX)

                if not is_json_response:
                    response_content = response.text  # Assuming text for non-JSON
                    if use_cache:
                        self._cache_response(cache_key_str, response_content, api_source_name)
                    return response_content

                response_json = response.json()
                if use_cache:
                    self._cache_response(cache_key_str, response_json, api_source_name)
                return response_json

            except requests.exceptions.HTTPError as e:
                # Obfuscate API key for logging if it's in query params
                log_params_for_error = {k: (
                    str(v)[:-6] + '******' if k == self.api_key_name and isinstance(v, str) and len(str(v)) > 6 else v)
                                        for k, v in full_query_params.items()}
                # Obfuscate API key if in headers (like RapidAPI)
                log_headers_for_error = self.headers.copy()
                if "X-RapidAPI-Key" in log_headers_for_error:
                    log_headers_for_error["X-RapidAPI-Key"] = log_headers_for_error["X-RapidAPI-Key"][-6:] + "******"
                # Add other sensitive headers if needed

                status_code = e.response.status_code
                logger.warning(
                    f"HTTP error on attempt {attempt + 1}/{API_RETRY_ATTEMPTS} for {url} "
                    f"(Params: {log_params_for_error}, Headers: {log_headers_for_error if 'X-RapidAPI-Key' in log_headers_for_error else 'Default'}): "
                    f"{status_code} - {e.response.text[:200]}..."  # Log only snippet of error response
                )
                if status_code == 429:  # Rate limit
                    delay = API_RETRY_DELAY * (2 ** attempt)  # Exponential backoff
                    logger.info(f"Rate limit hit. Waiting for {delay} seconds.")
                    time.sleep(delay)
                elif 500 <= status_code < 600:  # Server error
                    delay = API_RETRY_DELAY * (2 ** attempt)
                    logger.info(f"Server error. Waiting for {delay} seconds.")
                    time.sleep(delay)
                else:  # Non-retryable client error (e.g., 400, 401, 403, 404)
                    logger.error(f"Non-retryable client error for {url}: {status_code} {e.response.reason}",
                                 exc_info=False)
                    return None  # Critical: return None for these.
            except requests.exceptions.RequestException as e:  # Timeout, ConnectionError, etc.
                logger.warning(f"Request error on attempt {attempt + 1}/{API_RETRY_ATTEMPTS} for {url}: {e}")
                if attempt < API_RETRY_ATTEMPTS - 1:
                    delay = API_RETRY_DELAY * (2 ** attempt)
                    time.sleep(delay)  # Wait before retrying network-related issues
                # If it's the last attempt, loop will exit and return None
            except json.JSONDecodeError as e_json:
                logger.error(
                    f"JSON decode error for {url} on attempt {attempt + 1}. Response text: {response.text[:500]}... Error: {e_json}")
                if attempt < API_RETRY_ATTEMPTS - 1:
                    delay = API_RETRY_DELAY * (2 ** attempt)
                    time.sleep(delay)
                else:  # Failed after all retries
                    return None

        logger.error(f"All {API_RETRY_ATTEMPTS} attempts failed for {url}. Last query params: {full_query_params}")
        return None


class FinnhubClient(APIClient):
    def __init__(self):
        super().__init__("https://finnhub.io/api/v1", api_key_name="token", api_key_value=FINNHUB_API_KEY)

    def get_market_news(self, category="general", min_id=0):
        params = {"category": category}
        if min_id > 0: params["minId"] = min_id
        return self.request("GET", "/news", params=params, api_source_name="finnhub_news")

    def get_company_profile2(self, ticker):
        return self.request("GET", "/stock/profile2", params={"symbol": ticker}, api_source_name="finnhub_profile")

    def get_financials_reported(self, ticker, freq="quarterly"):  # Finnhub default is quarterly
        params = {"symbol": ticker, "freq": freq}
        return self.request("GET", "/stock/financials-reported", params=params, api_source_name="finnhub_financials")

    def get_basic_financials(self, ticker, metric_type="all"):
        return self.request("GET", "/stock/metric", params={"symbol": ticker, "metric": metric_type},
                            api_source_name="finnhub_metrics")

    def get_ipo_calendar(self, from_date=None, to_date=None):
        if from_date is None: from_date = (datetime.now(timezone.utc) - timedelta(days=30)).strftime('%Y-%m-%d')
        if to_date is None: to_date = (datetime.now(timezone.utc) + timedelta(days=90)).strftime('%Y-%m-%d')
        params = {"from": from_date, "to": to_date}
        return self.request("GET", "/calendar/ipo", params=params, api_source_name="finnhub_ipo_calendar")

    def get_sec_filings(self, ticker, from_date=None, to_date=None):  # For finding S-1/10-K URLs if EDGAR direct fails
        if from_date is None: from_date = (datetime.now(timezone.utc) - timedelta(days=365 * 2)).strftime('%Y-%m-%d')
        if to_date is None: to_date = datetime.now(timezone.utc).strftime('%Y-%m-%d')
        params = {"symbol": ticker, "from": from_date, "to": to_date}
        return self.request("GET", "/stock/filings", params=params, api_source_name="finnhub_filings")


class FinancialModelingPrepClient(APIClient):
    def __init__(self):
        super().__init__("https://financialmodelingprep.com/api/v3", api_key_name="apikey",
                         api_key_value=FINANCIAL_MODELING_PREP_API_KEY)

    def get_ipo_calendar(self, from_date=None, to_date=None):
        params = {}
        if from_date: params["from"] = from_date
        if to_date: params["to"] = to_date
        logger.warning("FinancialModelingPrepClient.get_ipo_calendar called, but may be restricted by subscription.")
        return self.request("GET", "/ipo_calendar", params=params, api_source_name="fmp_ipo_calendar")

    def get_financial_statements(self, ticker, statement_type="income-statement", period="quarter", limit=40):
        # FMP limit is often 5 for free tier, or 120 for annuals on paid. Max 40 quarters (10 years).
        actual_limit = limit
        if period == "annual" and limit > 15:
            actual_limit = 15  # Adjusted max reasonable for annuals
        elif period == "quarter" and limit > 60:
            actual_limit = 60  # Adjusted max reasonable for quarters (15 yrs)

        return self.request("GET", f"/{statement_type}/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name=f"fmp_{statement_type.replace('-', '_')}")

    def get_key_metrics(self, ticker, period="quarter", limit=40):  # TTM and historical
        actual_limit = limit
        if period == "annual" and limit > 15:
            actual_limit = 15
        elif period == "quarter" and limit > 60:
            actual_limit = 60
        return self.request("GET", f"/key-metrics/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name="fmp_key_metrics")

    def get_ratios(self, ticker, period="quarter", limit=40):  # FMP provides a separate ratios endpoint
        actual_limit = limit
        if period == "annual" and limit > 15:
            actual_limit = 15
        elif period == "quarter" and limit > 60:
            actual_limit = 60
        return self.request("GET", f"/ratios/{ticker}", params={"period": period, "limit": actual_limit},
                            api_source_name="fmp_ratios")

    def get_company_profile(self, ticker):
        return self.request("GET", f"/profile/{ticker}", params={}, api_source_name="fmp_profile")

    def get_analyst_estimates(self, ticker, period="annual"):
        return self.request("GET", f"/analyst-estimates/{ticker}", params={"period": period},
                            api_source_name="fmp_analyst_estimates")


class EODHDClient(APIClient):  # Less used now, but kept for potential specific data points
    def __init__(self):
        super().__init__("https://eodhistoricaldata.com/api", api_key_name="api_token", api_key_value=EODHD_API_KEY)
        self.params["fmt"] = "json"

    def get_fundamental_data(self, ticker_with_exchange):  # e.g., AAPL.US
        return self.request("GET", f"/fundamentals/{ticker_with_exchange}", api_source_name="eodhd_fundamentals")

    def get_ipo_calendar(self, from_date=None, to_date=None):
        params = {}
        if from_date: params["from"] = from_date
        if to_date: params["to"] = to_date
        logger.warning("EODHDClient.get_ipo_calendar called, but may be restricted or have limited data.")
        return self.request("GET", "/calendar/ipos", params=params, api_source_name="eodhd_ipo_calendar")


class SECEDGARClient(APIClient):
    def __init__(self):
        self.company_tickers_url = "https://www.sec.gov/files/company_tickers.json"
        # Base URL for submissions. Actual document URLs are absolute.
        super().__init__("https://data.sec.gov/submissions/")
        self.headers = {"User-Agent": EDGAR_USER_AGENT, "Accept-Encoding": "gzip, deflate"}
        self._cik_map = None
        self._company_submissions_base = "https://data.sec.gov/submissions/"
        self._archives_base = "https://www.sec.gov/Archives/edgar/data/"

    def _load_cik_map(self):  # Renamed for clarity
        if self._cik_map is None:
            logger.info("Fetching CIK map from SEC...")
            # This specific request doesn't use the base self.request as it's a one-off to a different SEC URL
            try:
                response = requests.get(self.company_tickers_url, headers=self.headers, timeout=API_REQUEST_TIMEOUT)
                response.raise_for_status()
                data = response.json()
                # Data is like: {"0": {"cik_str": 320193, "ticker": "AAPL", "title": "Apple Inc."}, ...}
                # We need a map from ticker to CIK.
                self._cik_map = {item['ticker']: str(item['cik_str']).zfill(10)
                                 for item in data.values() if 'ticker' in item and 'cik_str' in item}
                logger.info(f"CIK map loaded with {len(self._cik_map)} entries.")
            except requests.exceptions.RequestException as e:
                logger.error(f"Error fetching CIK map from SEC: {e}", exc_info=True)
                self._cik_map = {}  # Avoid refetching on subsequent errors in same run
            except json.JSONDecodeError as e_json:
                logger.error(f"Error decoding CIK map JSON from SEC: {e_json}", exc_info=True)
                self._cik_map = {}
        return self._cik_map

    def get_cik_by_ticker(self, ticker):
        ticker = ticker.upper()
        try:
            cik_map = self._load_cik_map()
            return cik_map.get(ticker)
        except Exception as e:  # Catch any unexpected error during CIK map loading/access
            logger.error(f"Unexpected error in get_cik_by_ticker for {ticker}: {e}", exc_info=True)
            return None

    def get_company_filings_summary(self, cik):  # Renamed for clarity
        if not cik: return None
        formatted_cik_for_api = str(cik).zfill(10)  # Ensure CIK is 10 digits, zero-padded for submissions API
        # Uses the base class request method, so caching will apply here.
        return self.request("GET", f"CIK{formatted_cik_for_api}.json", api_source_name="edgar_filings_summary")

    def get_filing_document_url(self, cik, form_type="10-K", priordate_str=None, count=1):
        if not cik: return None if count == 1 else []

        company_summary = self.get_company_filings_summary(cik)  # This uses self.request with caching
        if not company_summary or "filings" not in company_summary or "recent" not in company_summary["filings"]:
            logger.warning(f"No recent filings data found for CIK {cik} in company summary.")
            return None if count == 1 else []

        recent_filings = company_summary["filings"]["recent"]
        target_filings_info = []

        forms = recent_filings.get("form", [])
        accession_numbers = recent_filings.get("accessionNumber", [])
        primary_documents = recent_filings.get("primaryDocument", [])
        filing_dates = recent_filings.get("filingDate", [])  # These are 'YYYY-MM-DD'

        priordate_dt = None
        if priordate_str:
            try:
                priordate_dt = datetime.strptime(priordate_str, '%Y-%m-%d').date()
            except ValueError:
                logger.warning(f"Invalid priordate_str format: {priordate_str}. Ignoring.")

        for i, form in enumerate(forms):
            if form.upper() == form_type.upper():
                current_filing_date = datetime.strptime(filing_dates[i], '%Y-%m-%d').date()
                if priordate_dt and current_filing_date > priordate_dt:
                    continue  # Skip filings after the priordate if we're looking for older ones

                # Construct full document URL
                acc_num_no_hyphens = accession_numbers[i].replace('-', '')
                # CIK for URL needs to be the original numeric CIK, not zero-padded for data.sec.gov paths
                # but the directory structure under Archives/edgar/data uses the numeric CIK.
                # Assuming `cik` passed is the numeric CIK.
                doc_url = f"{self._archives_base}{int(cik)}/{acc_num_no_hyphens}/{primary_documents[i]}"
                target_filings_info.append({"url": doc_url, "date": current_filing_date, "form": form})

        if not target_filings_info:
            logger.info(f"No '{form_type}' filings found for CIK {cik} matching criteria.")
            return None if count == 1 else []

        # Sort by date descending to get the most recent first
        target_filings_info.sort(key=lambda x: x["date"], reverse=True)

        if count == 1:
            return target_filings_info[0]["url"] if target_filings_info else None
        else:
            return [f_info["url"] for f_info in target_filings_info[:count]]

    def get_filing_text(self, filing_url):
        if not filing_url: return None
        logger.info(f"Fetching filing text from: {filing_url}")
        # This is a direct GET, not using the submissions API base_url, so self.request needs full URL.
        # To make it work with self.request, we'd need a different base_url or handle it specially.
        # For simplicity, let's use requests.get directly for fetching actual document content.
        # Caching for these large text docs via DB might be inefficient if not careful.
        # Using a file-based cache or a more nuanced DB cache for large text might be better.
        # For now, let's try to use the existing cache but be mindful of size.

        # Construct a cache key that's just based on the URL for SEC documents
        cache_key_str = f"GET_SEC_DOC:{filing_url}"
        cached_text = self._get_cached_response(cache_key_str)
        if cached_text is not None:
            return cached_text

        try:
            response = requests.get(filing_url, headers=self.headers,
                                    timeout=API_REQUEST_TIMEOUT + 30)  # Longer timeout for large files
            response.raise_for_status()
            # Try to decode with utf-8, fallback to latin-1 for SEC filings that sometimes have mixed encodings
            try:
                text_content = response.content.decode('utf-8')
            except UnicodeDecodeError:
                logger.warning(f"UTF-8 decode failed for {filing_url}, trying latin-1.")
                text_content = response.content.decode('latin-1', errors='replace')

            self._cache_response(cache_key_str, text_content, "edgar_filing_text_content")
            return text_content
        except requests.exceptions.RequestException as e:
            logger.error(f"Error fetching SEC filing text from {filing_url}: {e}")
            return None


class GeminiAPIClient:  # (Assumed mostly complete from previous, ensure consistency)
    def __init__(self):
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models"

    def _get_next_api_key_for_attempt(self, overall_attempt_num, max_attempts_per_key, total_keys):
        key_group_index = (overall_attempt_num // max_attempts_per_key) % total_keys
        api_key = GOOGLE_API_KEYS[key_group_index]
        current_retry_for_this_key = (overall_attempt_num % max_attempts_per_key) + 1
        logger.debug(
            f"Gemini: Using key ...{api_key[-4:]} (Index {key_group_index}), Attempt {current_retry_for_this_key}/{max_attempts_per_key}")
        return api_key, current_retry_for_this_key

    def generate_text(self, prompt, model="gemini-1.5-flash-latest"):
        max_attempts_per_key = API_RETRY_ATTEMPTS
        total_keys = len(GOOGLE_API_KEYS)
        if total_keys == 0:
            logger.error("Gemini: No API keys configured for Google API.")
            return "Error: No Google API keys configured."

        if len(prompt) > 30000:  # Gemini Pro has limit around 32k tokens (~120k chars), flash is less
            logger.warning(f"Gemini prompt length {len(prompt)} is very long. Truncating to 30000 characters.")
            prompt = prompt[:30000] + "\n...[PROMPT TRUNCATED DUE TO LENGTH]..."

        for overall_attempt_num in range(total_keys * max_attempts_per_key):
            api_key, current_retry_for_this_key = self._get_next_api_key_for_attempt(
                overall_attempt_num, max_attempts_per_key, total_keys
            )
            url = f"{self.base_url}/{model}:generateContent?key={api_key}"
            payload = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {
                    "temperature": 0.5,  # Slightly lower for more factual synthesis, was 0.6, 0.7 before
                    "maxOutputTokens": 8192,
                    "topP": 0.9,  # Adjusted from 0.95
                    "topK": 35  # Adjusted from 40
                },
                "safetySettings": [  # Keep reasonably strict
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                ]
            }

            try:
                response = requests.post(url, json=payload, timeout=API_REQUEST_TIMEOUT + 120)  # Increased timeout
                response.raise_for_status()
                response_json = response.json()

                if "promptFeedback" in response_json and response_json["promptFeedback"].get("blockReason"):
                    block_reason = response_json["promptFeedback"]["blockReason"]
                    block_details = response_json["promptFeedback"].get("safetyRatings", "")
                    logger.error(
                        f"Gemini prompt blocked for key ...{api_key[-4:]}. Reason: {block_reason}. Details: {block_details}. Prompt snippet: '{prompt[:100]}...'")
                    time.sleep(API_RETRY_DELAY)  # Wait before next attempt (could be next key)
                    continue

                if "candidates" in response_json and response_json["candidates"]:
                    candidate = response_json["candidates"][0]
                    finish_reason = candidate.get("finishReason")
                    # Valid finish reasons: "STOP", "MAX_TOKENS", "MODEL_LENGTH" (or None if implicit stop)
                    if finish_reason not in [None, "STOP", "MAX_TOKENS", "MODEL_LENGTH", "OK"]:
                        logger.warning(
                            f"Gemini candidate finished with unexpected reason: {finish_reason} for key ...{api_key[-4:]}. Prompt: '{prompt[:100]}...'. Response: {json.dumps(response_json, indent=2)}")
                        if finish_reason == "SAFETY":
                            logger.error(
                                f"Gemini candidate blocked by safety settings for key ...{api_key[-4:]}. This prompt/content is problematic.")
                            # This specific prompt/key is problematic. Loop will try next key/attempt.
                            time.sleep(API_RETRY_DELAY)
                            continue
                        # Other reasons might be retried by the loop.

                    content_part = candidate.get("content", {}).get("parts", [{}])[0]
                    if "text" in content_part:
                        return content_part["text"]
                    else:
                        logger.error(
                            f"Gemini response missing text in content part for key ...{api_key[-4:]}: {response_json}")
                else:  # No candidates or malformed
                    logger.error(
                        f"Gemini response malformed or missing candidates for key ...{api_key[-4:]}: {response_json}")

            except requests.exceptions.HTTPError as e:
                logger.warning(
                    f"Gemini API HTTP error for key ...{api_key[-4:]} on attempt {current_retry_for_this_key}/{max_attempts_per_key}: {e.response.status_code} - {e.response.text[:200]}. Prompt: '{prompt[:100]}...'")
                if e.response.status_code == 400:
                    logger.error(
                        f"Gemini API Bad Request (400). Likely persistent issue with prompt/payload. Aborting Gemini for this call. Response: {e.response.text[:500]}")
                    return f"Error: Gemini API bad request (400). {e.response.text[:200]}"
                # Other HTTP errors (429, 5xx) will be retried with the next key/attempt by the loop
            except requests.exceptions.RequestException as e:  # Timeout, ConnectionError
                logger.warning(
                    f"Gemini API request error for key ...{api_key[-4:]} on attempt {current_retry_for_this_key}/{max_attempts_per_key}: {e}. Prompt: '{prompt[:100]}...'")
            except json.JSONDecodeError as e_json_gemini:
                logger.error(
                    f"Gemini API JSON decode error for key ...{api_key[-4:]} on attempt {current_retry_for_this_key}/{max_attempts_per_key}. Response: {response.text[:500]}. Error: {e_json_gemini}")

            # Wait before next attempt (could be next retry for this key, or first retry for next key)
            if overall_attempt_num < (total_keys * max_attempts_per_key) - 1:
                time.sleep(
                    API_RETRY_DELAY * ((overall_attempt_num % max_attempts_per_key) + 1))  # Linear backoff per key

        logger.error(
            f"All attempts ({total_keys * max_attempts_per_key}) to call Gemini API failed for prompt: {prompt[:100]}...")
        return "Error: Could not get response from Gemini API after multiple attempts across all keys."

    def summarize_text_with_context(self, text_to_summarize, context_summary, max_length=None):
        # This method now exists in stock_analyzer and ipo_analyzer, can be used if generic summarization is needed here.
        # For specific tasks, dedicated methods are better.
        # This is a simple wrapper.
        if max_length and len(text_to_summarize) > max_length:
            text_to_summarize = text_to_summarize[:max_length] + "\n... [TRUNCATED FOR BREVITY] ..."
            logger.info(f"Truncated text for Gemini summary due to length: {max_length}")

        prompt = f"Context: {context_summary}\n\nPlease provide a concise and factual summary of the following text, focusing on key information relevant to the context:\n\nText:\n\"\"\"\n{text_to_summarize}\n\"\"\"\n\nSummary:"
        return self.generate_text(prompt)

    def analyze_sentiment_with_reasoning(self, text_to_analyze, context=""):
        prompt = (f"Analyze the sentiment of the following text. Classify it as 'Positive', 'Negative', or 'Neutral'. "
                  f"Provide a brief explanation for your classification (1-2 sentences), citing specific phrases or elements from the text. "
                  f"{f'Consider this context: {context}. ' if context else ''}"
                  f"Text:\n\"\"\"\n{text_to_analyze}\n\"\"\"\n\nSentiment Analysis (Classification and Reasoning):")
        return self.generate_text(prompt)

    # Other specific Gemini use cases (interpret_financial_data, answer_question) can be added as needed


# --- Helper functions for scraping and parsing SEC filings ---
def scrape_article_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Connection': 'keep-alive'
        }
        response = requests.get(url, headers=headers, timeout=API_REQUEST_TIMEOUT - 10, allow_redirects=True)
        response.raise_for_status()

        content_type = response.headers.get('content-type', '').lower()
        if 'html' not in content_type:
            logger.warning(f"Content type for {url} is not HTML ({content_type}). Skipping scrape.")
            return None

        soup = BeautifulSoup(response.content, 'lxml')

        # Remove common unwanted tags
        for unwanted_tag_name in ['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'iframe', 'noscript',
                                  'link', 'meta']:
            for tag_to_remove in soup.find_all(unwanted_tag_name):
                tag_to_remove.decompose()

        # Attempt to find main content using common semantic tags or typical class/id patterns
        main_content_html = None
        selectors = ['article', 'main', 'div[role="main"]',
                     'div[class*="article-content"]', 'div[class*="post-content"]', 'div[class*="entry-content"]',
                     'div[id*="article-body"]', 'div[class*="article-body"]', 'div[id="content"]',
                     'div[class*="content"]'
                     ]  # Ordered by likely relevance
        for selector in selectors:
            tag = soup.select_one(selector)
            if tag:
                # Further clean within the selected tag (remove ads, social shares etc.)
                for unwanted_class_pattern in ['ad', 'social', 'related', 'share', 'comment', 'promo', 'sidebar',
                                               'newsletter', 'cookie', 'banner', 'modal', 'popup']:
                    for sub_tag_to_remove in tag.find_all(
                            lambda t: any(unwanted_class_pattern in c for c in t.get('class', [])) or \
                                      any(unwanted_class_pattern in i for i in t.get('id', []))):
                        sub_tag_to_remove.decompose()
                main_content_html = tag
                break

        if main_content_html:
            text_parts = [p.get_text(separator=' ', strip=True) for p in
                          main_content_html.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'span', 'div'],
                                                     recursive=True) if p.get_text(strip=True)]
            article_text = '\n'.join(filter(None, text_parts))  # Join paragraphs/elements with newlines
            # Further clean: remove excessive whitespace, multiple newlines
            article_text = re.sub(r'\s+\n\s*', '\n', article_text)  # Consolidate lines with only whitespace
            article_text = re.sub(r'\n{3,}', '\n\n', article_text)  # Max 2 newlines
        elif soup.body:  # Fallback: get all text from body if specific selectors fail
            logger.info(f"Main content selectors failed for {url}, trying body text.")
            article_text = soup.body.get_text(separator='\n', strip=True)
            article_text = re.sub(r'\n{3,}', '\n\n', article_text)
        else:  # Absolute fallback
            logger.warning(f"Could not extract significant main content from {url} using any method.")
            return None

        if len(article_text) < 150:  # Arbitrary threshold for meaningful content
            logger.info(
                f"Extracted text from {url} is very short ({len(article_text)} chars). May not be main article.")
            # Potentially return None if too short, or let caller decide.

        logger.info(f"Successfully scraped ~{len(article_text)} characters from {url}")
        return article_text.strip()

    except requests.exceptions.RequestException as e:
        logger.error(f"Request error fetching URL {url} for scraping: {e}")
        return None
    except Exception as e:
        logger.error(f"General error scraping article content from {url}: {e}", exc_info=True)
        return None


def extract_S1_text_sections(filing_text, sections_map):
    if not filing_text or not sections_map:
        return {}

    extracted_sections = {}
    # Normalize text: remove excessive newlines and leading/trailing whitespace
    # Handle potential HTML entities that might remain if it's not pure text
    soup_text = BeautifulSoup(filing_text, 'lxml').get_text(separator='\n')  # Basic HTML cleaning
    normalized_text = re.sub(r'\n\s*\n', '\n\n', soup_text.strip())
    normalized_text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\xff]', '', normalized_text)  # Remove non-printables

    # This regex attempts to find "ITEM X." or "Item X." more robustly.
    # It looks for the word ITEM (case-insensitive) followed by spaces, then digits and optional letter/dot.
    # It's still basic and won't handle all SEC filing structures perfectly.
    # A more advanced solution would use NLP or dedicated SEC parsing libraries.

    # Create a list of (key, regex_pattern_for_start, optional_regex_pattern_for_end)
    section_patterns = []
    for key, patterns_list in sections_map.items():
        # Main pattern for item start e.g. "Item 1." or "Item 1A."
        item_num_pattern_str = patterns_list[0].replace('.', r'\.?')  # Make dot optional
        # More flexible regex for item start
        # Looks for ITEM (case insensitive), optional space, the number part, optional dot, space or newline
        start_regex_str = r"(?:ITEM|Item)\s*" + item_num_pattern_str.split()[-1] + r"\.?\s+"
        if len(patterns_list) > 1:  # Add descriptive name if available
            start_regex_str += r"\s*" + re.escape(patterns_list[1])

        section_patterns.append({"key": key, "start_regex": re.compile(start_regex_str, re.IGNORECASE)})

    # Find start indices of all sections
    found_sections = []
    for pattern_info in section_patterns:
        for match in pattern_info["start_regex"].finditer(normalized_text):
            found_sections.append(
                {"key": pattern_info["key"], "start": match.start(), "header_text": match.group(0).strip()})

    if not found_sections:
        logger.warning(f"No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.")
        # Fallback: if no ITEM headers, try to find by descriptive names only (less reliable)
        for key, patterns_list in sections_map.items():
            if len(patterns_list) > 1:  # Has descriptive name
                desc_name_pattern = re.compile(r"^\s*" + re.escape(patterns_list[1]) + r"\s*$",
                                               re.IGNORECASE | re.MULTILINE)
                for match in desc_name_pattern.finditer(normalized_text):
                    found_sections.append({"key": key, "start": match.start(), "header_text": match.group(0).strip()})

    if not found_sections:
        logger.warning("No sections extracted from SEC filing text based on provided patterns.")
        return {}

    # Sort found sections by their start index
    found_sections.sort(key=lambda x: x["start"])

    for i, current_sec_info in enumerate(found_sections):
        start_index = current_sec_info["start"] + len(current_sec_info["header_text"])  # Start after the header
        end_index = None
        if i + 1 < len(found_sections):  # If there's a next section, end before it starts
            end_index = found_sections[i + 1]["start"]

        section_text = normalized_text[start_index:end_index].strip()

        if section_text:  # Only add if content exists
            # If a section key is already found (e.g. from descriptive name match after ITEM match),
            # prefer the one found earlier or based on some priority (e.g. ITEM match > desc name match)
            # For simplicity, this will overwrite if multiple patterns match the same conceptual section.
            # A more robust system would handle overlaps or prioritize.
            if current_sec_info["key"] not in extracted_sections or len(section_text) > len(
                    extracted_sections.get(current_sec_info["key"], "")):
                extracted_sections[current_sec_info["key"]] = section_text
                logger.debug(
                    f"Extracted section '{current_sec_info['key']}' (header: '{current_sec_info['header_text']}') with length {len(section_text)}")

    return extracted_sections

---------- END api_clients.py ----------


---------- app_analysis.log ----------
2025-05-24 23:25:37,242 - root - INFO - main:163 - ===================================================================
2025-05-24 23:25:37,242 - root - INFO - main:164 - Starting Financial Analysis Script at 2025-05-24 20:25:37 UTC
2025-05-24 23:25:37,242 - root - INFO - main:165 - ===================================================================
2025-05-24 23:25:37,243 - root - INFO - main:119 - Initializing database as per command line argument...
2025-05-24 23:25:37,244 - root - INFO - database:21 - Initializing database and creating tables...
2025-05-24 23:25:41,637 - root - INFO - database:26 - Database tables created successfully (if they didn't exist).
2025-05-24 23:25:41,638 - root - INFO - main:122 - Database initialization complete.
2025-05-24 23:25:41,639 - root - INFO - main:158 - --- Main script execution finished. ---
2025-05-24 23:25:41,640 - root - INFO - main:170 - Financial Analysis Script finished at 2025-05-24 20:25:41 UTC
2025-05-24 23:25:41,641 - root - INFO - main:171 - Total execution time: 0:00:04.396938
2025-05-24 23:25:41,641 - root - INFO - main:172 - ===================================================================
2025-05-24 23:25:46,230 - root - INFO - main:163 - ===================================================================
2025-05-24 23:25:46,230 - root - INFO - main:164 - Starting Financial Analysis Script at 2025-05-24 20:25:46 UTC
2025-05-24 23:25:46,231 - root - INFO - main:165 - ===================================================================
2025-05-24 23:25:46,232 - root - INFO - main:18 - --- Starting Individual Stock Analysis for: ['GOOG'] ---
2025-05-24 23:25:49,075 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/GOOG?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-24 23:25:49,077 - root - INFO - stock_analyzer:104 - Fetched profile from FMP for GOOG.
2025-05-24 23:25:49,078 - root - INFO - stock_analyzer:129 - Stock GOOG not found in DB, creating new entry.
2025-05-24 23:25:49,666 - root - INFO - stock_analyzer:141 - Created and refreshed stock entry for GOOG (ID: 1). Name: Alphabet Inc., CIK: 0001652044
2025-05-24 23:25:49,667 - root - INFO - stock_analyzer:782 - Full analysis pipeline started for GOOG...
2025-05-24 23:25:49,668 - root - INFO - stock_analyzer:177 - Fetching financial statements for GOOG for the last 7 years.
2025-05-24 23:25:50,893 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/GOOG?apikey=62ERGmJoqQgGD0nSGxRZS91TVz...
2025-05-24 23:25:52,283 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/GOOG?apikey=62ERGmJoqQgGD0nSGxR...
2025-05-24 23:25:53,725 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/GOOG?apikey=62ERGmJoqQgGD0nSGxRZS91...
2025-05-24 23:25:54,728 - root - WARNING - api_clients:129 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/income-statement/GOOG (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-24 23:25:54,730 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/income-statement/GOOG: 403 Forbidden
2025-05-24 23:25:54,732 - root - WARNING - stock_analyzer:195 - Failed to fetch FMP quarterly income statements for GOOG (likely subscription issue). QoQ analysis will be limited.
2025-05-24 23:25:54,733 - root - INFO - stock_analyzer:201 - Fetched from FMP: 5 income, 5 balance, 5 cashflow annual statements. 0 quarterly income statements.
2025-05-24 23:25:54,734 - root - INFO - stock_analyzer:213 - Fetching key metrics and profile for GOOG.
2025-05-24 23:25:56,132 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/GOOG?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61u...
2025-05-24 23:25:57,132 - root - WARNING - api_clients:129 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/key-metrics/GOOG (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-24 23:25:57,134 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/key-metrics/GOOG: 403 Forbidden
2025-05-24 23:25:57,135 - root - WARNING - stock_analyzer:220 - Failed to fetch FMP quarterly key metrics for GOOG (likely subscription issue). Latest metrics might rely on annual or Finnhub.
2025-05-24 23:25:58,906 - root - INFO - api_clients:70 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=GOOG&token=d0o7hphr01qqr9alj38gd0o7hphr...
2025-05-24 23:25:58,908 - root - INFO - stock_analyzer:235 - Fetched Key Metrics from FMP (Annual: 5, Quarterly: 0). Fetched Finnhub Basic Financials.
2025-05-24 23:25:58,909 - root - INFO - stock_analyzer:239 - Calculating derived metrics for GOOG...
2025-05-24 23:25:58,910 - root - ERROR - stock_analyzer:852 - CRITICAL error during full analysis pipeline for GOOG: name 'calculate_growth' is not defined
Traceback (most recent call last):
  File "C:\GitHub\stock-alarm\stock_analyzer.py", line 795, in analyze
    calculated_metrics = self._calculate_derived_metrics()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\GitHub\stock-alarm\stock_analyzer.py", line 340, in _calculate_derived_metrics
    metrics["revenue_growth_yoy"] = calculate_growth(get_value_from_statement_list(income_annual, "revenue", 0),
                                    ^^^^^^^^^^^^^^^^
NameError: name 'calculate_growth' is not defined
2025-05-24 23:25:58,949 - root - INFO - stock_analyzer:856 - Rolled back transaction for GOOG due to error.
2025-05-24 23:25:58,950 - root - WARNING - main:27 - Stock analysis for GOOG did not return a result object.
2025-05-24 23:26:00,952 - root - INFO - main:158 - --- Main script execution finished. ---
2025-05-24 23:26:00,953 - root - INFO - main:170 - Financial Analysis Script finished at 2025-05-24 20:26:00 UTC
2025-05-24 23:26:00,954 - root - INFO - main:171 - Total execution time: 0:00:14.722313
2025-05-24 23:26:00,955 - root - INFO - main:172 - ===================================================================
2025-05-24 23:26:25,947 - root - INFO - main:163 - ===================================================================
2025-05-24 23:26:25,947 - root - INFO - main:164 - Starting Financial Analysis Script at 2025-05-24 20:26:25 UTC
2025-05-24 23:26:25,948 - root - INFO - main:165 - ===================================================================
2025-05-24 23:26:25,949 - root - INFO - main:59 - --- Generating Today's Email Summary ---
2025-05-24 23:26:27,759 - root - INFO - main:72 - Found 0 stock analyses, 0 IPO analyses, 0 news analyses since 2025-05-24 00:00:00 UTC for email.
2025-05-24 23:26:27,760 - root - INFO - main:78 - No new analyses performed recently to include in the email summary.
2025-05-24 23:26:27,844 - root - INFO - main:158 - --- Main script execution finished. ---
2025-05-24 23:26:27,845 - root - INFO - main:170 - Financial Analysis Script finished at 2025-05-24 20:26:27 UTC
2025-05-24 23:26:27,846 - root - INFO - main:171 - Total execution time: 0:00:01.897858
2025-05-24 23:26:27,847 - root - INFO - main:172 - ===================================================================
2025-05-24 23:27:21,358 - root - INFO - main:163 - ===================================================================
2025-05-24 23:27:21,359 - root - INFO - main:164 - Starting Financial Analysis Script at 2025-05-24 20:27:21 UTC
2025-05-24 23:27:21,359 - root - INFO - main:165 - ===================================================================
2025-05-24 23:27:21,361 - root - INFO - main:129 - Running all analyses for default stocks: ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'JPM'], IPOs, and News (max 5 items).
2025-05-24 23:27:21,361 - root - INFO - main:18 - --- Starting Individual Stock Analysis for: ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'JPM'] ---
2025-05-24 23:27:24,287 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/AAPL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-24 23:27:24,289 - root - INFO - stock_analyzer:104 - Fetched profile from FMP for AAPL.
2025-05-24 23:27:24,290 - root - INFO - stock_analyzer:129 - Stock AAPL not found in DB, creating new entry.
2025-05-24 23:27:24,889 - root - INFO - stock_analyzer:141 - Created and refreshed stock entry for AAPL (ID: 2). Name: Apple Inc., CIK: 0000320193
2025-05-24 23:27:24,890 - root - INFO - stock_analyzer:782 - Full analysis pipeline started for AAPL...
2025-05-24 23:27:24,891 - root - INFO - stock_analyzer:177 - Fetching financial statements for AAPL for the last 7 years.
2025-05-24 23:27:26,178 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/AAPL?apikey=62ERGmJoqQgGD0nSGxRZS91TVz...
2025-05-24 23:27:27,643 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/AAPL?apikey=62ERGmJoqQgGD0nSGxR...
2025-05-24 23:27:29,118 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/AAPL?apikey=62ERGmJoqQgGD0nSGxRZS91...
2025-05-24 23:27:30,156 - root - WARNING - api_clients:129 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/income-statement/AAPL (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-24 23:27:30,158 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/income-statement/AAPL: 403 Forbidden
2025-05-24 23:27:30,159 - root - WARNING - stock_analyzer:195 - Failed to fetch FMP quarterly income statements for AAPL (likely subscription issue). QoQ analysis will be limited.
2025-05-24 23:27:30,160 - root - INFO - stock_analyzer:201 - Fetched from FMP: 5 income, 5 balance, 5 cashflow annual statements. 0 quarterly income statements.
2025-05-24 23:27:30,161 - root - INFO - stock_analyzer:213 - Fetching key metrics and profile for AAPL.
2025-05-24 23:27:31,623 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/AAPL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61u...
2025-05-24 23:27:32,638 - root - WARNING - api_clients:129 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/key-metrics/AAPL (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-24 23:27:32,639 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/key-metrics/AAPL: 403 Forbidden
2025-05-24 23:27:32,641 - root - WARNING - stock_analyzer:220 - Failed to fetch FMP quarterly key metrics for AAPL (likely subscription issue). Latest metrics might rely on annual or Finnhub.
2025-05-24 23:27:34,483 - root - INFO - api_clients:70 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=AAPL&token=d0o7hphr01qqr9alj38gd0o7hphr...
2025-05-24 23:27:34,485 - root - INFO - stock_analyzer:235 - Fetched Key Metrics from FMP (Annual: 5, Quarterly: 0). Fetched Finnhub Basic Financials.
2025-05-24 23:27:34,486 - root - INFO - stock_analyzer:239 - Calculating derived metrics for AAPL...
2025-05-24 23:27:34,487 - root - ERROR - stock_analyzer:852 - CRITICAL error during full analysis pipeline for AAPL: name 'calculate_growth' is not defined
Traceback (most recent call last):
  File "C:\GitHub\stock-alarm\stock_analyzer.py", line 795, in analyze
    calculated_metrics = self._calculate_derived_metrics()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\GitHub\stock-alarm\stock_analyzer.py", line 340, in _calculate_derived_metrics
    metrics["revenue_growth_yoy"] = calculate_growth(get_value_from_statement_list(income_annual, "revenue", 0),
                                    ^^^^^^^^^^^^^^^^
NameError: name 'calculate_growth' is not defined
2025-05-24 23:27:34,518 - root - INFO - stock_analyzer:856 - Rolled back transaction for AAPL due to error.
2025-05-24 23:27:34,519 - root - WARNING - main:27 - Stock analysis for AAPL did not return a result object.
2025-05-24 23:27:38,050 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/MSFT?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-24 23:27:38,052 - root - INFO - stock_analyzer:104 - Fetched profile from FMP for MSFT.
2025-05-24 23:27:38,053 - root - INFO - stock_analyzer:129 - Stock MSFT not found in DB, creating new entry.
2025-05-24 23:27:38,640 - root - INFO - stock_analyzer:141 - Created and refreshed stock entry for MSFT (ID: 3). Name: Microsoft Corporation, CIK: 0000789019
2025-05-24 23:27:38,643 - root - INFO - stock_analyzer:782 - Full analysis pipeline started for MSFT...
2025-05-24 23:27:38,643 - root - INFO - stock_analyzer:177 - Fetching financial statements for MSFT for the last 7 years.
2025-05-24 23:27:39,929 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/MSFT?apikey=62ERGmJoqQgGD0nSGxRZS91TVz...
2025-05-24 23:27:41,394 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/MSFT?apikey=62ERGmJoqQgGD0nSGxR...
2025-05-24 23:27:42,872 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/MSFT?apikey=62ERGmJoqQgGD0nSGxRZS91...
2025-05-24 23:27:43,901 - root - WARNING - api_clients:129 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/income-statement/MSFT (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-24 23:27:43,902 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/income-statement/MSFT: 403 Forbidden
2025-05-24 23:27:43,904 - root - WARNING - stock_analyzer:195 - Failed to fetch FMP quarterly income statements for MSFT (likely subscription issue). QoQ analysis will be limited.
2025-05-24 23:27:43,905 - root - INFO - stock_analyzer:201 - Fetched from FMP: 5 income, 5 balance, 5 cashflow annual statements. 0 quarterly income statements.
2025-05-24 23:27:43,906 - root - INFO - stock_analyzer:213 - Fetching key metrics and profile for MSFT.
2025-05-24 23:27:45,345 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/MSFT?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61u...
2025-05-24 23:27:46,355 - root - WARNING - api_clients:129 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/key-metrics/MSFT (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-24 23:27:46,356 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/key-metrics/MSFT: 403 Forbidden
2025-05-24 23:27:46,359 - root - WARNING - stock_analyzer:220 - Failed to fetch FMP quarterly key metrics for MSFT (likely subscription issue). Latest metrics might rely on annual or Finnhub.
2025-05-24 23:27:47,728 - root - INFO - api_clients:70 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=MSFT&token=d0o7hphr01qqr9alj38gd0o7hphr...
2025-05-24 23:27:47,729 - root - INFO - stock_analyzer:235 - Fetched Key Metrics from FMP (Annual: 5, Quarterly: 0). Fetched Finnhub Basic Financials.
2025-05-24 23:27:47,729 - root - INFO - stock_analyzer:239 - Calculating derived metrics for MSFT...
2025-05-24 23:27:47,730 - root - ERROR - stock_analyzer:852 - CRITICAL error during full analysis pipeline for MSFT: name 'calculate_growth' is not defined
Traceback (most recent call last):
  File "C:\GitHub\stock-alarm\stock_analyzer.py", line 795, in analyze
    calculated_metrics = self._calculate_derived_metrics()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\GitHub\stock-alarm\stock_analyzer.py", line 340, in _calculate_derived_metrics
    metrics["revenue_growth_yoy"] = calculate_growth(get_value_from_statement_list(income_annual, "revenue", 0),
                                    ^^^^^^^^^^^^^^^^
NameError: name 'calculate_growth' is not defined
2025-05-24 23:27:47,738 - root - INFO - stock_analyzer:856 - Rolled back transaction for MSFT due to error.
2025-05-24 23:27:47,738 - root - WARNING - main:27 - Stock analysis for MSFT did not return a result object.
2025-05-24 23:27:51,249 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/GOOGL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-24 23:27:51,250 - root - INFO - stock_analyzer:104 - Fetched profile from FMP for GOOGL.
2025-05-24 23:27:51,250 - root - INFO - stock_analyzer:129 - Stock GOOGL not found in DB, creating new entry.
2025-05-24 23:27:51,831 - root - INFO - stock_analyzer:141 - Created and refreshed stock entry for GOOGL (ID: 4). Name: Alphabet Inc., CIK: 0001652044
2025-05-24 23:27:51,832 - root - INFO - stock_analyzer:782 - Full analysis pipeline started for GOOGL...
2025-05-24 23:27:51,832 - root - INFO - stock_analyzer:177 - Fetching financial statements for GOOGL for the last 7 years.
2025-05-24 23:27:53,096 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/GOOGL?apikey=62ERGmJoqQgGD0nSGxRZS91TV...
2025-05-24 23:27:54,532 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/GOOGL?apikey=62ERGmJoqQgGD0nSGx...
2025-05-24 23:27:55,980 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/GOOGL?apikey=62ERGmJoqQgGD0nSGxRZS9...
2025-05-24 23:27:57,002 - root - WARNING - api_clients:129 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/income-statement/GOOGL (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-24 23:27:57,004 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/income-statement/GOOGL: 403 Forbidden
2025-05-24 23:27:57,006 - root - WARNING - stock_analyzer:195 - Failed to fetch FMP quarterly income statements for GOOGL (likely subscription issue). QoQ analysis will be limited.
2025-05-24 23:27:57,007 - root - INFO - stock_analyzer:201 - Fetched from FMP: 5 income, 5 balance, 5 cashflow annual statements. 0 quarterly income statements.
2025-05-24 23:27:57,007 - root - INFO - stock_analyzer:213 - Fetching key metrics and profile for GOOGL.
2025-05-24 23:27:58,421 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/GOOGL?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61...
2025-05-24 23:27:59,440 - root - WARNING - api_clients:129 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/key-metrics/GOOGL (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-24 23:27:59,442 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/key-metrics/GOOGL: 403 Forbidden
2025-05-24 23:27:59,444 - root - WARNING - stock_analyzer:220 - Failed to fetch FMP quarterly key metrics for GOOGL (likely subscription issue). Latest metrics might rely on annual or Finnhub.
2025-05-24 23:28:01,041 - root - INFO - api_clients:70 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=GOOGL&token=d0o7hphr01qqr9alj38gd0o7hph...
2025-05-24 23:28:01,041 - root - INFO - stock_analyzer:235 - Fetched Key Metrics from FMP (Annual: 5, Quarterly: 0). Fetched Finnhub Basic Financials.
2025-05-24 23:28:01,042 - root - INFO - stock_analyzer:239 - Calculating derived metrics for GOOGL...
2025-05-24 23:28:01,042 - root - ERROR - stock_analyzer:852 - CRITICAL error during full analysis pipeline for GOOGL: name 'calculate_growth' is not defined
Traceback (most recent call last):
  File "C:\GitHub\stock-alarm\stock_analyzer.py", line 795, in analyze
    calculated_metrics = self._calculate_derived_metrics()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\GitHub\stock-alarm\stock_analyzer.py", line 340, in _calculate_derived_metrics
    metrics["revenue_growth_yoy"] = calculate_growth(get_value_from_statement_list(income_annual, "revenue", 0),
                                    ^^^^^^^^^^^^^^^^
NameError: name 'calculate_growth' is not defined
2025-05-24 23:28:01,050 - root - INFO - stock_analyzer:856 - Rolled back transaction for GOOGL due to error.
2025-05-24 23:28:01,050 - root - WARNING - main:27 - Stock analysis for GOOGL did not return a result object.
2025-05-24 23:28:04,643 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/NVDA?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-24 23:28:04,645 - root - INFO - stock_analyzer:104 - Fetched profile from FMP for NVDA.
2025-05-24 23:28:04,646 - root - INFO - stock_analyzer:129 - Stock NVDA not found in DB, creating new entry.
2025-05-24 23:28:05,228 - root - INFO - stock_analyzer:141 - Created and refreshed stock entry for NVDA (ID: 5). Name: NVIDIA Corporation, CIK: 0001045810
2025-05-24 23:28:05,230 - root - INFO - stock_analyzer:782 - Full analysis pipeline started for NVDA...
2025-05-24 23:28:05,231 - root - INFO - stock_analyzer:177 - Fetching financial statements for NVDA for the last 7 years.
2025-05-24 23:28:06,513 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/NVDA?apikey=62ERGmJoqQgGD0nSGxRZS91TVz...
2025-05-24 23:28:07,962 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/NVDA?apikey=62ERGmJoqQgGD0nSGxR...
2025-05-24 23:28:09,428 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/NVDA?apikey=62ERGmJoqQgGD0nSGxRZS91...
2025-05-24 23:28:10,456 - root - WARNING - api_clients:129 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/income-statement/NVDA (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-24 23:28:10,459 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/income-statement/NVDA: 403 Forbidden
2025-05-24 23:28:10,460 - root - WARNING - stock_analyzer:195 - Failed to fetch FMP quarterly income statements for NVDA (likely subscription issue). QoQ analysis will be limited.
2025-05-24 23:28:10,461 - root - INFO - stock_analyzer:201 - Fetched from FMP: 5 income, 5 balance, 5 cashflow annual statements. 0 quarterly income statements.
2025-05-24 23:28:10,462 - root - INFO - stock_analyzer:213 - Fetching key metrics and profile for NVDA.
2025-05-24 23:28:11,925 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/NVDA?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61u...
2025-05-24 23:28:12,929 - root - WARNING - api_clients:129 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/key-metrics/NVDA (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-24 23:28:12,931 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/key-metrics/NVDA: 403 Forbidden
2025-05-24 23:28:12,933 - root - WARNING - stock_analyzer:220 - Failed to fetch FMP quarterly key metrics for NVDA (likely subscription issue). Latest metrics might rely on annual or Finnhub.
2025-05-24 23:28:14,599 - root - INFO - api_clients:70 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=NVDA&token=d0o7hphr01qqr9alj38gd0o7hphr...
2025-05-24 23:28:14,601 - root - INFO - stock_analyzer:235 - Fetched Key Metrics from FMP (Annual: 5, Quarterly: 0). Fetched Finnhub Basic Financials.
2025-05-24 23:28:14,602 - root - INFO - stock_analyzer:239 - Calculating derived metrics for NVDA...
2025-05-24 23:28:14,603 - root - ERROR - stock_analyzer:852 - CRITICAL error during full analysis pipeline for NVDA: name 'calculate_growth' is not defined
Traceback (most recent call last):
  File "C:\GitHub\stock-alarm\stock_analyzer.py", line 795, in analyze
    calculated_metrics = self._calculate_derived_metrics()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\GitHub\stock-alarm\stock_analyzer.py", line 340, in _calculate_derived_metrics
    metrics["revenue_growth_yoy"] = calculate_growth(get_value_from_statement_list(income_annual, "revenue", 0),
                                    ^^^^^^^^^^^^^^^^
NameError: name 'calculate_growth' is not defined
2025-05-24 23:28:14,631 - root - INFO - stock_analyzer:856 - Rolled back transaction for NVDA due to error.
2025-05-24 23:28:14,631 - root - WARNING - main:27 - Stock analysis for NVDA did not return a result object.
2025-05-24 23:28:18,178 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/profile/JPM?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-24 23:28:18,180 - root - INFO - stock_analyzer:104 - Fetched profile from FMP for JPM.
2025-05-24 23:28:18,181 - root - INFO - stock_analyzer:129 - Stock JPM not found in DB, creating new entry.
2025-05-24 23:28:18,767 - root - INFO - stock_analyzer:141 - Created and refreshed stock entry for JPM (ID: 6). Name: JPMorgan Chase & Co., CIK: 0000019617
2025-05-24 23:28:18,769 - root - INFO - stock_analyzer:782 - Full analysis pipeline started for JPM...
2025-05-24 23:28:18,770 - root - INFO - stock_analyzer:177 - Fetching financial statements for JPM for the last 7 years.
2025-05-24 23:28:20,052 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/income-statement/JPM?apikey=62ERGmJoqQgGD0nSGxRZS91TVzf...
2025-05-24 23:28:21,512 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/balance-sheet-statement/JPM?apikey=62ERGmJoqQgGD0nSGxRZ...
2025-05-24 23:28:22,958 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/cash-flow-statement/JPM?apikey=62ERGmJoqQgGD0nSGxRZS91T...
2025-05-24 23:28:24,043 - root - WARNING - api_clients:129 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/income-statement/JPM (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-24 23:28:24,044 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/income-statement/JPM: 403 Forbidden
2025-05-24 23:28:24,046 - root - WARNING - stock_analyzer:195 - Failed to fetch FMP quarterly income statements for JPM (likely subscription issue). QoQ analysis will be limited.
2025-05-24 23:28:24,047 - root - INFO - stock_analyzer:201 - Fetched from FMP: 5 income, 5 balance, 5 cashflow annual statements. 0 quarterly income statements.
2025-05-24 23:28:24,048 - root - INFO - stock_analyzer:213 - Fetching key metrics and profile for JPM.
2025-05-24 23:28:25,573 - root - INFO - api_clients:70 - Cached response for: GET:https://financialmodelingprep.com/api/v3/key-metrics/JPM?apikey=62ERGmJoqQgGD0nSGxRZS91TVzfz61uB...
2025-05-24 23:28:26,609 - root - WARNING - api_clients:129 - HTTP error on attempt 1/3 for https://financialmodelingprep.com/api/v3/key-metrics/JPM (Params: {'apikey': '62ERGmJoqQgGD0nSGxRZS91TVz******', 'period': 'quarterly', 'limit': 8}, Headers: Default): 403 - {
  "Error Message": "Exclusive Endpoint : This endpoint is not available under your current subscription agreement, please visit our subscription page to upgrade your plan or contact us at https://si...
2025-05-24 23:28:26,611 - root - ERROR - api_clients:143 - Non-retryable client error for https://financialmodelingprep.com/api/v3/key-metrics/JPM: 403 Forbidden
2025-05-24 23:28:26,612 - root - WARNING - stock_analyzer:220 - Failed to fetch FMP quarterly key metrics for JPM (likely subscription issue). Latest metrics might rely on annual or Finnhub.
2025-05-24 23:28:27,991 - root - INFO - api_clients:70 - Cached response for: GET:https://finnhub.io/api/v1/stock/metric?metric=all&symbol=JPM&token=d0o7hphr01qqr9alj38gd0o7hphr0...
2025-05-24 23:28:27,993 - root - INFO - stock_analyzer:235 - Fetched Key Metrics from FMP (Annual: 5, Quarterly: 0). Fetched Finnhub Basic Financials.
2025-05-24 23:28:27,994 - root - INFO - stock_analyzer:239 - Calculating derived metrics for JPM...
2025-05-24 23:28:27,995 - root - ERROR - stock_analyzer:852 - CRITICAL error during full analysis pipeline for JPM: name 'calculate_growth' is not defined
Traceback (most recent call last):
  File "C:\GitHub\stock-alarm\stock_analyzer.py", line 795, in analyze
    calculated_metrics = self._calculate_derived_metrics()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\GitHub\stock-alarm\stock_analyzer.py", line 340, in _calculate_derived_metrics
    metrics["revenue_growth_yoy"] = calculate_growth(get_value_from_statement_list(income_annual, "revenue", 0),
                                    ^^^^^^^^^^^^^^^^
NameError: name 'calculate_growth' is not defined
2025-05-24 23:28:28,027 - root - INFO - stock_analyzer:856 - Rolled back transaction for JPM due to error.
2025-05-24 23:28:28,029 - root - WARNING - main:27 - Stock analysis for JPM did not return a result object.
2025-05-24 23:28:35,031 - root - INFO - main:37 - --- Starting IPO Analysis Pipeline ---
2025-05-24 23:28:35,032 - root - INFO - ipo_analyzer:675 - Fetching upcoming IPOs using Finnhub...
2025-05-24 23:28:36,441 - root - INFO - api_clients:70 - Cached response for: GET:https://finnhub.io/api/v1/calendar/ipo?from=2025-03-25&to=2025-11-20&token=d0o7hphr01qqr9alj38gd...
2025-05-24 23:28:36,465 - root - INFO - ipo_analyzer:737 - Successfully parsed 153 IPOs from Finnhub API response.
2025-05-24 23:28:36,465 - root - INFO - ipo_analyzer:754 - Total unique IPOs fetched after deduplication: 153
2025-05-24 23:28:36,466 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Vantage Corp (Singapore) from source Finnhub
2025-05-24 23:28:36,812 - root - INFO - api_clients:275 - Fetching CIK map from SEC...
2025-05-24 23:28:37,195 - root - INFO - api_clients:285 - CIK map loaded with 10046 entries.
2025-05-24 23:28:37,197 - root - INFO - ipo_analyzer:779 - IPO 'Vantage Corp (Singapore)' not found in DB, creating new entry.
2025-05-24 23:28:37,620 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Vantage Corp (Singapore)' (ID: 1, CIK: 0002027160)
2025-05-24 23:28:37,715 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Vantage Corp (Singapore) (CIK: 0002027160)
2025-05-24 23:28:39,087 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002027160.json?...
2025-05-24 23:28:39,088 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002027160 matching criteria.
2025-05-24 23:28:39,351 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002027160.json?...
2025-05-24 23:28:39,435 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002027160 matching criteria.
2025-05-24 23:28:39,691 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002027160.json?...
2025-05-24 23:28:39,777 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for Vantage Corp (Singapore): https://www.sec.gov/Archives/edgar/data/2027160/000149315224040628/formf-1.htm
2025-05-24 23:28:39,778 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2027160/000149315224040628/formf-1.htm
2025-05-24 23:28:44,484 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2027160/000149315224040628/formf-1.htm...
2025-05-24 23:28:44,486 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2016965) for Vantage Corp (Singapore)
2025-05-24 23:28:45,072 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:29:00,868 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Vantage Corp (Singapore)
2025-05-24 23:29:01,465 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Vantage Corp (Singapore) (Analysis ID: 1)
2025-05-24 23:29:09,467 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Fast Track Group from source Finnhub
2025-05-24 23:29:09,641 - root - INFO - ipo_analyzer:779 - IPO 'Fast Track Group' not found in DB, creating new entry.
2025-05-24 23:29:10,059 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Fast Track Group' (ID: 2, CIK: 0002027262)
2025-05-24 23:29:10,145 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Fast Track Group (CIK: 0002027262)
2025-05-24 23:29:11,106 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002027262.json?...
2025-05-24 23:29:11,106 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002027262 matching criteria.
2025-05-24 23:29:11,360 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002027262.json?...
2025-05-24 23:29:11,446 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002027262 matching criteria.
2025-05-24 23:29:11,699 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002027262.json?...
2025-05-24 23:29:11,784 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for Fast Track Group: https://www.sec.gov/Archives/edgar/data/2027262/000164117225004569/formf-1.htm
2025-05-24 23:29:11,785 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2027262/000164117225004569/formf-1.htm
2025-05-24 23:29:15,111 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2027262/000164117225004569/formf-1.htm...
2025-05-24 23:29:15,113 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 3354458) for Fast Track Group
2025-05-24 23:29:16,046 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:29:31,340 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Fast Track Group
2025-05-24 23:29:31,934 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Fast Track Group (Analysis ID: 2)
2025-05-24 23:29:39,936 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Pelican Acquisition Corp from source Finnhub
2025-05-24 23:29:40,105 - root - INFO - ipo_analyzer:779 - IPO 'Pelican Acquisition Corp' not found in DB, creating new entry.
2025-05-24 23:29:40,523 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Pelican Acquisition Corp' (ID: 3, CIK: None)
2025-05-24 23:29:40,611 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol PELIU for IPO 'Pelican Acquisition Corp'.
2025-05-24 23:29:56,051 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Pelican Acquisition Corp
2025-05-24 23:29:56,561 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Pelican Acquisition Corp (Analysis ID: 3)
2025-05-24 23:30:04,563 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Cal Redwood Acquisition Corp. from source Finnhub
2025-05-24 23:30:04,735 - root - INFO - ipo_analyzer:779 - IPO 'Cal Redwood Acquisition Corp.' not found in DB, creating new entry.
2025-05-24 23:30:05,152 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Cal Redwood Acquisition Corp.' (ID: 4, CIK: None)
2025-05-24 23:30:05,237 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol CRAQU for IPO 'Cal Redwood Acquisition Corp.'.
2025-05-24 23:30:20,803 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Cal Redwood Acquisition Corp.
2025-05-24 23:30:21,313 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Cal Redwood Acquisition Corp. (Analysis ID: 4)
2025-05-24 23:30:29,314 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Oyster Enterprises II Acquisition Corp from source Finnhub
2025-05-24 23:30:29,486 - root - INFO - ipo_analyzer:779 - IPO 'Oyster Enterprises II Acquisition Corp' not found in DB, creating new entry.
2025-05-24 23:30:29,907 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Oyster Enterprises II Acquisition Corp' (ID: 5, CIK: None)
2025-05-24 23:30:29,994 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol OYSEU for IPO 'Oyster Enterprises II Acquisition Corp'.
2025-05-24 23:30:46,003 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Oyster Enterprises II Acquisition Corp
2025-05-24 23:30:46,513 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Oyster Enterprises II Acquisition Corp (Analysis ID: 5)
2025-05-24 23:30:54,515 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Hinge Health, Inc. from source Finnhub
2025-05-24 23:30:54,684 - root - INFO - ipo_analyzer:779 - IPO 'Hinge Health, Inc.' not found in DB, creating new entry.
2025-05-24 23:30:55,107 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Hinge Health, Inc.' (ID: 6, CIK: 0001673743)
2025-05-24 23:30:55,194 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Hinge Health, Inc. (CIK: 0001673743)
2025-05-24 23:30:56,148 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001673743.json?...
2025-05-24 23:30:56,150 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Hinge Health, Inc.: https://www.sec.gov/Archives/edgar/data/1673743/000119312525051004/d829170ds1.htm
2025-05-24 23:30:56,151 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1673743/000119312525051004/d829170ds1.htm
2025-05-24 23:30:58,926 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1673743/000119312525051004/d829170ds1.htm...
2025-05-24 23:30:58,927 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2654569) for Hinge Health, Inc.
2025-05-24 23:30:59,979 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:31:16,051 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Hinge Health, Inc.
2025-05-24 23:31:16,662 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Hinge Health, Inc. (Analysis ID: 6)
2025-05-24 23:31:24,664 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: MNTN, Inc. from source Finnhub
2025-05-24 23:31:24,833 - root - INFO - ipo_analyzer:779 - IPO 'MNTN, Inc.' not found in DB, creating new entry.
2025-05-24 23:31:25,254 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'MNTN, Inc.' (ID: 7, CIK: 0001891027)
2025-05-24 23:31:25,342 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for MNTN, Inc. (CIK: 0001891027)
2025-05-24 23:31:26,317 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001891027.json?...
2025-05-24 23:31:26,319 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for MNTN, Inc.: https://www.sec.gov/Archives/edgar/data/1891027/000110465925019247/tm2413466-11_s1.htm
2025-05-24 23:31:26,320 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1891027/000110465925019247/tm2413466-11_s1.htm
2025-05-24 23:31:29,618 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1891027/000110465925019247/tm2413466-11_s1.htm...
2025-05-24 23:31:29,620 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 3281190) for MNTN, Inc.
2025-05-24 23:31:30,638 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:31:47,546 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for MNTN, Inc.
2025-05-24 23:31:48,130 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: MNTN, Inc. (Analysis ID: 7)
2025-05-24 23:31:56,132 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: AParadise Acquisition Corp. from source Finnhub
2025-05-24 23:31:56,302 - root - INFO - ipo_analyzer:779 - IPO 'AParadise Acquisition Corp.' not found in DB, creating new entry.
2025-05-24 23:31:56,724 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'AParadise Acquisition Corp.' (ID: 8, CIK: None)
2025-05-24 23:31:56,811 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol APADU for IPO 'AParadise Acquisition Corp.'.
2025-05-24 23:32:12,152 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for AParadise Acquisition Corp.
2025-05-24 23:32:12,660 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: AParadise Acquisition Corp. (Analysis ID: 8)
2025-05-24 23:32:20,662 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: ProCap Acquisition Corp from source Finnhub
2025-05-24 23:32:20,831 - root - INFO - ipo_analyzer:779 - IPO 'ProCap Acquisition Corp' not found in DB, creating new entry.
2025-05-24 23:32:21,250 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'ProCap Acquisition Corp' (ID: 9, CIK: 0002056634)
2025-05-24 23:32:21,342 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for ProCap Acquisition Corp (CIK: 0002056634)
2025-05-24 23:32:22,285 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002056634.json?...
2025-05-24 23:32:22,287 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for ProCap Acquisition Corp: https://www.sec.gov/Archives/edgar/data/2056634/000121390025037771/ea0230984-04.htm
2025-05-24 23:32:22,288 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2056634/000121390025037771/ea0230984-04.htm
2025-05-24 23:32:25,954 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2056634/000121390025037771/ea0230984-04.htm...
2025-05-24 23:32:25,957 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 3839608) for ProCap Acquisition Corp
2025-05-24 23:32:26,565 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:32:41,755 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for ProCap Acquisition Corp
2025-05-24 23:32:42,341 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: ProCap Acquisition Corp (Analysis ID: 9)
2025-05-24 23:32:50,344 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Armada Acquisition Corp. II from source Finnhub
2025-05-24 23:32:50,517 - root - INFO - ipo_analyzer:779 - IPO 'Armada Acquisition Corp. II' not found in DB, creating new entry.
2025-05-24 23:32:50,936 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Armada Acquisition Corp. II' (ID: 10, CIK: None)
2025-05-24 23:32:51,022 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol AACIU for IPO 'Armada Acquisition Corp. II'.
2025-05-24 23:33:07,534 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Armada Acquisition Corp. II
2025-05-24 23:33:08,042 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Armada Acquisition Corp. II (Analysis ID: 10)
2025-05-24 23:33:16,044 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: OFA Group from source Finnhub
2025-05-24 23:33:16,216 - root - INFO - ipo_analyzer:779 - IPO 'OFA Group' not found in DB, creating new entry.
2025-05-24 23:33:16,635 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'OFA Group' (ID: 11, CIK: 0002036307)
2025-05-24 23:33:16,723 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for OFA Group (CIK: 0002036307)
2025-05-24 23:33:19,003 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002036307.json?...
2025-05-24 23:33:19,004 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002036307 matching criteria.
2025-05-24 23:33:19,258 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002036307.json?...
2025-05-24 23:33:19,344 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002036307 matching criteria.
2025-05-24 23:33:19,595 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002036307.json?...
2025-05-24 23:33:19,680 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for OFA Group: https://www.sec.gov/Archives/edgar/data/2036307/000149315225007774/formf-1.htm
2025-05-24 23:33:19,682 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2036307/000149315225007774/formf-1.htm
2025-05-24 23:33:22,743 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2036307/000149315225007774/formf-1.htm...
2025-05-24 23:33:22,746 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2850883) for OFA Group
2025-05-24 23:33:23,567 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:33:38,250 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for OFA Group
2025-05-24 23:33:38,828 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: OFA Group (Analysis ID: 11)
2025-05-24 23:33:46,830 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: FIGX Capital Acquisition Corp. from source Finnhub
2025-05-24 23:33:46,997 - root - INFO - ipo_analyzer:779 - IPO 'FIGX Capital Acquisition Corp.' not found in DB, creating new entry.
2025-05-24 23:33:47,410 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'FIGX Capital Acquisition Corp.' (ID: 12, CIK: None)
2025-05-24 23:33:47,495 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol FIGXU for IPO 'FIGX Capital Acquisition Corp.'.
2025-05-24 23:34:02,199 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for FIGX Capital Acquisition Corp.
2025-05-24 23:34:02,698 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: FIGX Capital Acquisition Corp. (Analysis ID: 12)
2025-05-24 23:34:10,699 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Jefferson Capital, Inc. / DE from source Finnhub
2025-05-24 23:34:10,866 - root - INFO - ipo_analyzer:779 - IPO 'Jefferson Capital, Inc. / DE' not found in DB, creating new entry.
2025-05-24 23:34:11,278 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Jefferson Capital, Inc. / DE' (ID: 13, CIK: None)
2025-05-24 23:34:11,362 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol JCAP for IPO 'Jefferson Capital, Inc. / DE'.
2025-05-24 23:34:24,367 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Jefferson Capital, Inc. / DE
2025-05-24 23:34:24,864 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Jefferson Capital, Inc. / DE (Analysis ID: 13)
2025-05-24 23:34:32,865 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: INFINT Acquisition Corp 2 from source Finnhub
2025-05-24 23:34:32,948 - root - INFO - ipo_analyzer:779 - IPO 'INFINT Acquisition Corp 2' not found in DB, creating new entry.
2025-05-24 23:34:33,363 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'INFINT Acquisition Corp 2' (ID: 14, CIK: None)
2025-05-24 23:34:33,447 - root - WARNING - ipo_analyzer:849 - No CIK or symbol for IPO 'INFINT Acquisition Corp 2'. Cannot fetch S-1/F-1.
2025-05-24 23:34:48,803 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for INFINT Acquisition Corp 2
2025-05-24 23:34:49,303 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: INFINT Acquisition Corp 2 (Analysis ID: 14)
2025-05-24 23:34:57,305 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Uptrend Holdings Ltd from source Finnhub
2025-05-24 23:34:57,473 - root - INFO - ipo_analyzer:779 - IPO 'Uptrend Holdings Ltd' not found in DB, creating new entry.
2025-05-24 23:34:57,888 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Uptrend Holdings Ltd' (ID: 15, CIK: None)
2025-05-24 23:34:57,973 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol UPX for IPO 'Uptrend Holdings Ltd'.
2025-05-24 23:35:12,997 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Uptrend Holdings Ltd
2025-05-24 23:35:13,493 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Uptrend Holdings Ltd (Analysis ID: 15)
2025-05-24 23:35:21,494 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: LightWave Acquisition Corp. from source Finnhub
2025-05-24 23:35:21,661 - root - INFO - ipo_analyzer:779 - IPO 'LightWave Acquisition Corp.' not found in DB, creating new entry.
2025-05-24 23:35:22,077 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'LightWave Acquisition Corp.' (ID: 16, CIK: None)
2025-05-24 23:35:22,162 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol LWACU for IPO 'LightWave Acquisition Corp.'.
2025-05-24 23:35:37,533 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for LightWave Acquisition Corp.
2025-05-24 23:35:38,033 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: LightWave Acquisition Corp. (Analysis ID: 16)
2025-05-24 23:35:46,033 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Ultra High Point Holdings Ltd from source Finnhub
2025-05-24 23:35:46,205 - root - INFO - ipo_analyzer:779 - IPO 'Ultra High Point Holdings Ltd' not found in DB, creating new entry.
2025-05-24 23:35:46,622 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Ultra High Point Holdings Ltd' (ID: 17, CIK: None)
2025-05-24 23:35:46,709 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol UHP for IPO 'Ultra High Point Holdings Ltd'.
2025-05-24 23:36:02,253 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Ultra High Point Holdings Ltd
2025-05-24 23:36:02,759 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Ultra High Point Holdings Ltd (Analysis ID: 17)
2025-05-24 23:36:18,761 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: K-TECH SOLUTIONS CO LTD from source Finnhub
2025-05-24 23:36:18,930 - root - INFO - ipo_analyzer:779 - IPO 'K-TECH SOLUTIONS CO LTD' not found in DB, creating new entry.
2025-05-24 23:36:19,342 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'K-TECH SOLUTIONS CO LTD' (ID: 18, CIK: 0002049187)
2025-05-24 23:36:19,429 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for K-TECH SOLUTIONS CO LTD (CIK: 0002049187)
2025-05-24 23:36:20,398 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002049187.json?...
2025-05-24 23:36:20,400 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002049187 matching criteria.
2025-05-24 23:36:20,655 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002049187.json?...
2025-05-24 23:36:20,741 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002049187 matching criteria.
2025-05-24 23:36:20,990 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002049187.json?...
2025-05-24 23:36:21,075 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for K-TECH SOLUTIONS CO LTD: https://www.sec.gov/Archives/edgar/data/2049187/000121390025045262/ea0223235-08.htm
2025-05-24 23:36:21,077 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2049187/000121390025045262/ea0223235-08.htm
2025-05-24 23:36:27,407 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2049187/000121390025045262/ea0223235-08.htm...
2025-05-24 23:36:27,411 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 6686376) for K-TECH SOLUTIONS CO LTD
2025-05-24 23:36:28,341 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:36:42,886 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for K-TECH SOLUTIONS CO LTD
2025-05-24 23:36:43,474 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: K-TECH SOLUTIONS CO LTD (Analysis ID: 18)
2025-05-24 23:36:51,477 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Fitness Champs Holdings Ltd from source Finnhub
2025-05-24 23:36:51,647 - root - INFO - ipo_analyzer:779 - IPO 'Fitness Champs Holdings Ltd' not found in DB, creating new entry.
2025-05-24 23:36:52,067 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Fitness Champs Holdings Ltd' (ID: 19, CIK: 0002023796)
2025-05-24 23:36:52,154 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Fitness Champs Holdings Ltd (CIK: 0002023796)
2025-05-24 23:36:53,155 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002023796.json?...
2025-05-24 23:36:53,157 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002023796 matching criteria.
2025-05-24 23:36:53,415 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002023796.json?...
2025-05-24 23:36:53,499 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002023796 matching criteria.
2025-05-24 23:36:53,753 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002023796.json?...
2025-05-24 23:36:53,838 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for Fitness Champs Holdings Ltd: https://www.sec.gov/Archives/edgar/data/2023796/000164117225011509/formf-1.htm
2025-05-24 23:36:53,840 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2023796/000164117225011509/formf-1.htm
2025-05-24 23:36:57,117 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2023796/000164117225011509/formf-1.htm...
2025-05-24 23:36:57,119 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2566360) for Fitness Champs Holdings Ltd
2025-05-24 23:36:57,983 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:37:14,653 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Fitness Champs Holdings Ltd
2025-05-24 23:37:15,239 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Fitness Champs Holdings Ltd (Analysis ID: 19)
2025-05-24 23:37:23,241 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: WEN Acquisition Corp from source Finnhub
2025-05-24 23:37:23,410 - root - INFO - ipo_analyzer:779 - IPO 'WEN Acquisition Corp' not found in DB, creating new entry.
2025-05-24 23:37:23,827 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'WEN Acquisition Corp' (ID: 20, CIK: 0002057043)
2025-05-24 23:37:23,913 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for WEN Acquisition Corp (CIK: 0002057043)
2025-05-24 23:37:24,872 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002057043.json?...
2025-05-24 23:37:24,874 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for WEN Acquisition Corp: https://www.sec.gov/Archives/edgar/data/2057043/000121390025037753/ea0231326-04.htm
2025-05-24 23:37:24,875 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2057043/000121390025037753/ea0231326-04.htm
2025-05-24 23:37:28,688 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2057043/000121390025037753/ea0231326-04.htm...
2025-05-24 23:37:28,691 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 4064362) for WEN Acquisition Corp
2025-05-24 23:37:29,340 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:37:44,780 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for WEN Acquisition Corp
2025-05-24 23:37:45,372 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: WEN Acquisition Corp (Analysis ID: 20)
2025-05-24 23:37:53,376 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Columbus Circle Capital Corp. I from source Finnhub
2025-05-24 23:37:53,547 - root - INFO - ipo_analyzer:779 - IPO 'Columbus Circle Capital Corp. I' not found in DB, creating new entry.
2025-05-24 23:37:53,965 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Columbus Circle Capital Corp. I' (ID: 21, CIK: 0002056263)
2025-05-24 23:37:54,053 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Columbus Circle Capital Corp. I (CIK: 0002056263)
2025-05-24 23:37:54,994 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002056263.json?...
2025-05-24 23:37:54,996 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Columbus Circle Capital Corp. I: https://www.sec.gov/Archives/edgar/data/2056263/000121390025035663/ea0232327-02.htm
2025-05-24 23:37:54,997 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2056263/000121390025035663/ea0232327-02.htm
2025-05-24 23:37:59,239 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2056263/000121390025035663/ea0232327-02.htm...
2025-05-24 23:37:59,241 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 4532236) for Columbus Circle Capital Corp. I
2025-05-24 23:37:59,944 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:38:15,339 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Columbus Circle Capital Corp. I
2025-05-24 23:38:15,926 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Columbus Circle Capital Corp. I (Analysis ID: 21)
2025-05-24 23:38:23,930 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Voyager Technologies, Inc./DE from source Finnhub
2025-05-24 23:38:24,099 - root - INFO - ipo_analyzer:779 - IPO 'Voyager Technologies, Inc./DE' not found in DB, creating new entry.
2025-05-24 23:38:24,519 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Voyager Technologies, Inc./DE' (ID: 22, CIK: 0001788060)
2025-05-24 23:38:24,606 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Voyager Technologies, Inc./DE (CIK: 0001788060)
2025-05-24 23:38:25,555 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001788060.json?...
2025-05-24 23:38:25,556 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Voyager Technologies, Inc./DE: https://www.sec.gov/Archives/edgar/data/1788060/000162828025026244/voyager-sx1.htm
2025-05-24 23:38:25,557 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1788060/000162828025026244/voyager-sx1.htm
2025-05-24 23:38:29,318 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1788060/000162828025026244/voyager-sx1.htm...
2025-05-24 23:38:29,321 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 3888886) for Voyager Technologies, Inc./DE
2025-05-24 23:38:30,113 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:38:47,199 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Voyager Technologies, Inc./DE
2025-05-24 23:38:47,784 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Voyager Technologies, Inc./DE (Analysis ID: 22)
2025-05-24 23:38:55,787 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: World Road Inc. from source Finnhub
2025-05-24 23:38:55,960 - root - INFO - ipo_analyzer:779 - IPO 'World Road Inc.' not found in DB, creating new entry.
2025-05-24 23:38:56,379 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'World Road Inc.' (ID: 23, CIK: 0002049348)
2025-05-24 23:38:56,466 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for World Road Inc. (CIK: 0002049348)
2025-05-24 23:38:57,456 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002049348.json?...
2025-05-24 23:38:57,458 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002049348 matching criteria.
2025-05-24 23:38:57,714 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002049348.json?...
2025-05-24 23:38:57,798 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002049348 matching criteria.
2025-05-24 23:38:58,048 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002049348.json?...
2025-05-24 23:38:58,134 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for World Road Inc.: https://www.sec.gov/Archives/edgar/data/2049348/000182912625003770/worldroad_f1.htm
2025-05-24 23:38:58,135 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2049348/000182912625003770/worldroad_f1.htm
2025-05-24 23:39:01,119 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2049348/000182912625003770/worldroad_f1.htm...
2025-05-24 23:39:01,120 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2800467) for World Road Inc.
2025-05-24 23:39:02,077 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:39:18,331 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for World Road Inc.
2025-05-24 23:39:18,915 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: World Road Inc. (Analysis ID: 23)
2025-05-24 23:39:26,917 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Zi Yun Dong Fang Ltd from source Finnhub
2025-05-24 23:39:27,084 - root - INFO - ipo_analyzer:779 - IPO 'Zi Yun Dong Fang Ltd' not found in DB, creating new entry.
2025-05-24 23:39:27,496 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Zi Yun Dong Fang Ltd' (ID: 24, CIK: 0002053115)
2025-05-24 23:39:27,582 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Zi Yun Dong Fang Ltd (CIK: 0002053115)
2025-05-24 23:39:28,622 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002053115.json?...
2025-05-24 23:39:28,623 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002053115 matching criteria.
2025-05-24 23:39:28,879 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002053115.json?...
2025-05-24 23:39:28,969 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002053115 matching criteria.
2025-05-24 23:39:29,216 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002053115.json?...
2025-05-24 23:39:29,299 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for Zi Yun Dong Fang Ltd: https://www.sec.gov/Archives/edgar/data/2053115/000164117225011292/formf-1.htm
2025-05-24 23:39:29,299 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2053115/000164117225011292/formf-1.htm
2025-05-24 23:39:32,277 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2053115/000164117225011292/formf-1.htm...
2025-05-24 23:39:32,280 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2782027) for Zi Yun Dong Fang Ltd
2025-05-24 23:39:33,264 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:39:47,795 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Zi Yun Dong Fang Ltd
2025-05-24 23:39:48,379 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Zi Yun Dong Fang Ltd (Analysis ID: 24)
2025-05-24 23:39:56,380 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Thayer Ventures Acquisition Corp II from source Finnhub
2025-05-24 23:39:56,551 - root - INFO - ipo_analyzer:779 - IPO 'Thayer Ventures Acquisition Corp II' not found in DB, creating new entry.
2025-05-24 23:39:56,973 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Thayer Ventures Acquisition Corp II' (ID: 25, CIK: 0001872228)
2025-05-24 23:39:57,059 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Thayer Ventures Acquisition Corp II (CIK: 0001872228)
2025-05-24 23:39:57,993 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001872228.json?...
2025-05-24 23:39:57,995 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Thayer Ventures Acquisition Corp II: https://www.sec.gov/Archives/edgar/data/1872228/000119312525054865/d771523ds1.htm
2025-05-24 23:39:57,996 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1872228/000119312525054865/d771523ds1.htm
2025-05-24 23:40:00,081 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1872228/000119312525054865/d771523ds1.htm...
2025-05-24 23:40:00,083 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 1555800) for Thayer Ventures Acquisition Corp II
2025-05-24 23:40:00,519 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:40:16,076 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Thayer Ventures Acquisition Corp II
2025-05-24 23:40:16,663 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Thayer Ventures Acquisition Corp II (Analysis ID: 25)
2025-05-24 23:40:24,666 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Renatus Tactical Acquisition Corp I from source Finnhub
2025-05-24 23:40:24,837 - root - INFO - ipo_analyzer:779 - IPO 'Renatus Tactical Acquisition Corp I' not found in DB, creating new entry.
2025-05-24 23:40:25,256 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Renatus Tactical Acquisition Corp I' (ID: 26, CIK: 0002035173)
2025-05-24 23:40:25,343 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Renatus Tactical Acquisition Corp I (CIK: 0002035173)
2025-05-24 23:40:26,356 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002035173.json?...
2025-05-24 23:40:26,358 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Renatus Tactical Acquisition Corp I: https://www.sec.gov/Archives/edgar/data/2035173/000114036125008858/ny20045296x1_s1.htm
2025-05-24 23:40:26,359 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2035173/000114036125008858/ny20045296x1_s1.htm
2025-05-24 23:40:29,527 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2035173/000114036125008858/ny20045296x1_s1.htm...
2025-05-24 23:40:29,529 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2346135) for Renatus Tactical Acquisition Corp I
2025-05-24 23:40:30,100 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:40:44,190 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Renatus Tactical Acquisition Corp I
2025-05-24 23:40:44,774 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Renatus Tactical Acquisition Corp I (Analysis ID: 26)
2025-05-24 23:40:52,777 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Arrive AI Inc. from source Finnhub
2025-05-24 23:40:52,943 - root - INFO - ipo_analyzer:779 - IPO 'Arrive AI Inc.' not found in DB, creating new entry.
2025-05-24 23:40:53,363 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Arrive AI Inc.' (ID: 27, CIK: 0001818274)
2025-05-24 23:40:53,450 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Arrive AI Inc. (CIK: 0001818274)
2025-05-24 23:40:54,417 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001818274.json?...
2025-05-24 23:40:54,419 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Arrive AI Inc.: https://www.sec.gov/Archives/edgar/data/1818274/000149315224051854/forms-1.htm
2025-05-24 23:40:54,420 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1818274/000149315224051854/forms-1.htm
2025-05-24 23:40:58,248 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1818274/000149315224051854/forms-1.htm...
2025-05-24 23:40:58,251 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2790064) for Arrive AI Inc.
2025-05-24 23:40:59,093 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:41:14,638 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Arrive AI Inc.
2025-05-24 23:41:15,223 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Arrive AI Inc. (Analysis ID: 27)
2025-05-24 23:41:23,225 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Hang Feng Technology Innovation Co., Ltd. from source Finnhub
2025-05-24 23:41:23,400 - root - INFO - ipo_analyzer:779 - IPO 'Hang Feng Technology Innovation Co., Ltd.' not found in DB, creating new entry.
2025-05-24 23:41:23,817 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Hang Feng Technology Innovation Co., Ltd.' (ID: 28, CIK: 0002060083)
2025-05-24 23:41:23,904 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Hang Feng Technology Innovation Co., Ltd. (CIK: 0002060083)
2025-05-24 23:41:24,865 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002060083.json?...
2025-05-24 23:41:24,866 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002060083 matching criteria.
2025-05-24 23:41:25,124 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002060083.json?...
2025-05-24 23:41:25,211 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002060083 matching criteria.
2025-05-24 23:41:25,466 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002060083.json?...
2025-05-24 23:41:25,550 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for Hang Feng Technology Innovation Co., Ltd.: https://www.sec.gov/Archives/edgar/data/2060083/000121390025043480/ea0233710-06.htm
2025-05-24 23:41:25,552 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2060083/000121390025043480/ea0233710-06.htm
2025-05-24 23:41:31,166 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2060083/000121390025043480/ea0233710-06.htm...
2025-05-24 23:41:31,169 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 5421133) for Hang Feng Technology Innovation Co., Ltd.
2025-05-24 23:41:31,954 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:41:48,740 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Hang Feng Technology Innovation Co., Ltd.
2025-05-24 23:41:49,323 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Hang Feng Technology Innovation Co., Ltd. (Analysis ID: 28)
2025-05-24 23:41:57,327 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Thrive Capital Group Co., LTD from source Finnhub
2025-05-24 23:41:57,496 - root - INFO - ipo_analyzer:779 - IPO 'Thrive Capital Group Co., LTD' not found in DB, creating new entry.
2025-05-24 23:41:57,918 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Thrive Capital Group Co., LTD' (ID: 29, CIK: 0002058349)
2025-05-24 23:41:58,003 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Thrive Capital Group Co., LTD (CIK: 0002058349)
2025-05-24 23:41:58,962 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002058349.json?...
2025-05-24 23:41:58,964 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002058349 matching criteria.
2025-05-24 23:41:59,220 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002058349.json?...
2025-05-24 23:41:59,306 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002058349 matching criteria.
2025-05-24 23:41:59,557 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002058349.json?...
2025-05-24 23:41:59,642 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for Thrive Capital Group Co., LTD: https://www.sec.gov/Archives/edgar/data/2058349/000121390025043926/ea0227995-04.htm
2025-05-24 23:41:59,643 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2058349/000121390025043926/ea0227995-04.htm
2025-05-24 23:42:03,797 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2058349/000121390025043926/ea0227995-04.htm...
2025-05-24 23:42:03,800 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 4221508) for Thrive Capital Group Co., LTD
2025-05-24 23:42:04,410 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:42:20,154 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Thrive Capital Group Co., LTD
2025-05-24 23:42:20,749 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Thrive Capital Group Co., LTD (Analysis ID: 29)
2025-05-24 23:42:28,753 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: CTW Cayman from source Finnhub
2025-05-24 23:42:28,926 - root - INFO - ipo_analyzer:779 - IPO 'CTW Cayman' not found in DB, creating new entry.
2025-05-24 23:42:29,346 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'CTW Cayman' (ID: 30, CIK: 0002047148)
2025-05-24 23:42:29,431 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for CTW Cayman (CIK: 0002047148)
2025-05-24 23:42:30,381 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002047148.json?...
2025-05-24 23:42:30,383 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002047148 matching criteria.
2025-05-24 23:42:30,637 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002047148.json?...
2025-05-24 23:42:30,723 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002047148 matching criteria.
2025-05-24 23:42:30,974 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002047148.json?...
2025-05-24 23:42:31,058 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for CTW Cayman: https://www.sec.gov/Archives/edgar/data/2047148/000121390025044061/ea0229974-05.htm
2025-05-24 23:42:31,059 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2047148/000121390025044061/ea0229974-05.htm
2025-05-24 23:42:37,875 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2047148/000121390025044061/ea0229974-05.htm...
2025-05-24 23:42:37,879 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 7538291) for CTW Cayman
2025-05-24 23:42:38,913 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:42:54,700 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for CTW Cayman
2025-05-24 23:42:55,279 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: CTW Cayman (Analysis ID: 30)
2025-05-24 23:43:03,280 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Linkhome Holdings Inc. from source Finnhub
2025-05-24 23:43:03,449 - root - INFO - ipo_analyzer:779 - IPO 'Linkhome Holdings Inc.' not found in DB, creating new entry.
2025-05-24 23:43:03,864 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Linkhome Holdings Inc.' (ID: 31, CIK: 0002017758)
2025-05-24 23:43:03,949 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Linkhome Holdings Inc. (CIK: 0002017758)
2025-05-24 23:43:04,884 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002017758.json?...
2025-05-24 23:43:04,885 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Linkhome Holdings Inc.: https://www.sec.gov/Archives/edgar/data/2017758/000121390024054736/ea0203553-04.htm
2025-05-24 23:43:04,885 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2017758/000121390024054736/ea0203553-04.htm
2025-05-24 23:43:09,682 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2017758/000121390024054736/ea0203553-04.htm...
2025-05-24 23:43:09,685 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 4853561) for Linkhome Holdings Inc.
2025-05-24 23:43:10,384 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:43:25,376 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Linkhome Holdings Inc.
2025-05-24 23:43:25,959 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Linkhome Holdings Inc. (Analysis ID: 31)
2025-05-24 23:43:33,961 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Churchill Capital Corp X/Cayman from source Finnhub
2025-05-24 23:43:34,128 - root - INFO - ipo_analyzer:779 - IPO 'Churchill Capital Corp X/Cayman' not found in DB, creating new entry.
2025-05-24 23:43:34,547 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Churchill Capital Corp X/Cayman' (ID: 32, CIK: 0002007825)
2025-05-24 23:43:34,631 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Churchill Capital Corp X/Cayman (CIK: 0002007825)
2025-05-24 23:43:35,544 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002007825.json?...
2025-05-24 23:43:35,545 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Churchill Capital Corp X/Cayman: https://www.sec.gov/Archives/edgar/data/2007825/000119312525100727/d698385ds1.htm
2025-05-24 23:43:35,545 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2007825/000119312525100727/d698385ds1.htm
2025-05-24 23:43:38,193 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2007825/000119312525100727/d698385ds1.htm...
2025-05-24 23:43:38,195 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 1896916) for Churchill Capital Corp X/Cayman
2025-05-24 23:43:38,843 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:43:54,471 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Churchill Capital Corp X/Cayman
2025-05-24 23:43:55,059 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Churchill Capital Corp X/Cayman (Analysis ID: 32)
2025-05-24 23:44:03,061 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Antalpha Platform Holding Co from source Finnhub
2025-05-24 23:44:03,229 - root - INFO - ipo_analyzer:779 - IPO 'Antalpha Platform Holding Co' not found in DB, creating new entry.
2025-05-24 23:44:03,645 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Antalpha Platform Holding Co' (ID: 33, CIK: 0002044255)
2025-05-24 23:44:03,729 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Antalpha Platform Holding Co (CIK: 0002044255)
2025-05-24 23:44:04,665 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002044255.json?...
2025-05-24 23:44:04,665 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002044255 matching criteria.
2025-05-24 23:44:04,918 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002044255.json?...
2025-05-24 23:44:05,005 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002044255 matching criteria.
2025-05-24 23:44:05,253 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002044255.json?...
2025-05-24 23:44:05,339 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for Antalpha Platform Holding Co: https://www.sec.gov/Archives/edgar/data/2044255/000119312525085747/d849162df1.htm
2025-05-24 23:44:05,340 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2044255/000119312525085747/d849162df1.htm
2025-05-24 23:44:07,900 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2044255/000119312525085747/d849162df1.htm...
2025-05-24 23:44:07,901 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2141759) for Antalpha Platform Holding Co
2025-05-24 23:44:08,832 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:44:23,698 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Antalpha Platform Holding Co
2025-05-24 23:44:24,279 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Antalpha Platform Holding Co (Analysis ID: 33)
2025-05-24 23:44:32,281 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: eToro Group Ltd. from source Finnhub
2025-05-24 23:44:32,448 - root - INFO - ipo_analyzer:779 - IPO 'eToro Group Ltd.' not found in DB, creating new entry.
2025-05-24 23:44:32,864 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'eToro Group Ltd.' (ID: 34, CIK: 0001493318)
2025-05-24 23:44:32,949 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for eToro Group Ltd. (CIK: 0001493318)
2025-05-24 23:44:33,888 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001493318.json?...
2025-05-24 23:44:33,889 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0001493318 matching criteria.
2025-05-24 23:44:34,143 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0001493318.json?...
2025-05-24 23:44:34,236 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0001493318 matching criteria.
2025-05-24 23:44:34,496 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0001493318.json?...
2025-05-24 23:44:34,582 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for eToro Group Ltd.: https://www.sec.gov/Archives/edgar/data/1493318/000101376225001589/ea0223534-08.htm
2025-05-24 23:44:34,583 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1493318/000101376225001589/ea0223534-08.htm
2025-05-24 23:44:41,486 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1493318/000101376225001589/ea0223534-08.htm...
2025-05-24 23:44:41,495 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 8512546) for eToro Group Ltd.
2025-05-24 23:44:43,073 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:44:58,061 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for eToro Group Ltd.
2025-05-24 23:44:58,942 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: eToro Group Ltd. (Analysis ID: 34)
2025-05-24 23:45:06,946 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Charming Medical Ltd from source Finnhub
2025-05-24 23:45:07,124 - root - INFO - ipo_analyzer:779 - IPO 'Charming Medical Ltd' not found in DB, creating new entry.
2025-05-24 23:45:07,546 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Charming Medical Ltd' (ID: 35, CIK: 0002035992)
2025-05-24 23:45:07,632 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Charming Medical Ltd (CIK: 0002035992)
2025-05-24 23:45:08,579 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002035992.json?...
2025-05-24 23:45:08,580 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002035992 matching criteria.
2025-05-24 23:45:08,829 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002035992.json?...
2025-05-24 23:45:08,915 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002035992 matching criteria.
2025-05-24 23:45:09,164 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002035992.json?...
2025-05-24 23:45:09,253 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for Charming Medical Ltd: https://www.sec.gov/Archives/edgar/data/2035992/000121390025043185/ea0213012-06.htm
2025-05-24 23:45:09,254 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2035992/000121390025043185/ea0213012-06.htm
2025-05-24 23:45:15,520 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2035992/000121390025043185/ea0213012-06.htm...
2025-05-24 23:45:15,521 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 6951619) for Charming Medical Ltd
2025-05-24 23:45:16,523 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:45:33,680 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Charming Medical Ltd
2025-05-24 23:45:34,279 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Charming Medical Ltd (Analysis ID: 35)
2025-05-24 23:45:42,282 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Axiom Intelligence Acquisition Corp 1 from source Finnhub
2025-05-24 23:45:42,448 - root - INFO - ipo_analyzer:779 - IPO 'Axiom Intelligence Acquisition Corp 1' not found in DB, creating new entry.
2025-05-24 23:45:42,867 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Axiom Intelligence Acquisition Corp 1' (ID: 36, CIK: None)
2025-05-24 23:45:42,951 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol AXINU for IPO 'Axiom Intelligence Acquisition Corp 1'.
2025-05-24 23:45:58,502 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Axiom Intelligence Acquisition Corp 1
2025-05-24 23:45:59,010 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Axiom Intelligence Acquisition Corp 1 (Analysis ID: 36)
2025-05-24 23:46:07,011 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Blue Acquisition Corp/Cayman from source Finnhub
2025-05-24 23:46:07,191 - root - INFO - ipo_analyzer:779 - IPO 'Blue Acquisition Corp/Cayman' not found in DB, creating new entry.
2025-05-24 23:46:07,602 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Blue Acquisition Corp/Cayman' (ID: 37, CIK: None)
2025-05-24 23:46:07,687 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol BACCU for IPO 'Blue Acquisition Corp/Cayman'.
2025-05-24 23:46:23,997 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Blue Acquisition Corp/Cayman
2025-05-24 23:46:24,500 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Blue Acquisition Corp/Cayman (Analysis ID: 37)
2025-05-24 23:46:32,502 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Perimeter Acquisition Corp. I from source Finnhub
2025-05-24 23:46:32,672 - root - INFO - ipo_analyzer:779 - IPO 'Perimeter Acquisition Corp. I' not found in DB, creating new entry.
2025-05-24 23:46:33,093 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Perimeter Acquisition Corp. I' (ID: 38, CIK: 0002061473)
2025-05-24 23:46:33,177 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Perimeter Acquisition Corp. I (CIK: 0002061473)
2025-05-24 23:46:34,200 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002061473.json?...
2025-05-24 23:46:34,200 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Perimeter Acquisition Corp. I: https://www.sec.gov/Archives/edgar/data/2061473/000164117225000074/forms-1.htm
2025-05-24 23:46:34,201 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2061473/000164117225000074/forms-1.htm
2025-05-24 23:46:37,068 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2061473/000164117225000074/forms-1.htm...
2025-05-24 23:46:37,068 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2639976) for Perimeter Acquisition Corp. I
2025-05-24 23:46:37,785 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:46:53,203 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Perimeter Acquisition Corp. I
2025-05-24 23:46:53,784 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Perimeter Acquisition Corp. I (Analysis ID: 38)
2025-05-24 23:47:01,787 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: OMS Energy Technologies Inc. from source Finnhub
2025-05-24 23:47:01,955 - root - INFO - ipo_analyzer:779 - IPO 'OMS Energy Technologies Inc.' not found in DB, creating new entry.
2025-05-24 23:47:02,370 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'OMS Energy Technologies Inc.' (ID: 39, CIK: 0002012219)
2025-05-24 23:47:02,456 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for OMS Energy Technologies Inc. (CIK: 0002012219)
2025-05-24 23:47:03,368 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002012219.json?...
2025-05-24 23:47:03,369 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002012219 matching criteria.
2025-05-24 23:47:03,622 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002012219.json?...
2025-05-24 23:47:03,707 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002012219 matching criteria.
2025-05-24 23:47:03,956 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002012219.json?...
2025-05-24 23:47:04,042 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for OMS Energy Technologies Inc.: https://www.sec.gov/Archives/edgar/data/2012219/000121390024094183/ea0200016-06.htm
2025-05-24 23:47:04,042 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2012219/000121390024094183/ea0200016-06.htm
2025-05-24 23:47:11,401 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2012219/000121390024094183/ea0200016-06.htm...
2025-05-24 23:47:11,403 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 9481144) for OMS Energy Technologies Inc.
2025-05-24 23:47:12,741 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:47:29,118 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for OMS Energy Technologies Inc.
2025-05-24 23:47:29,706 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: OMS Energy Technologies Inc. (Analysis ID: 39)
2025-05-24 23:47:37,707 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Chime Financial, Inc. from source Finnhub
2025-05-24 23:47:37,874 - root - INFO - ipo_analyzer:779 - IPO 'Chime Financial, Inc.' not found in DB, creating new entry.
2025-05-24 23:47:38,290 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Chime Financial, Inc.' (ID: 40, CIK: 0001795586)
2025-05-24 23:47:38,374 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Chime Financial, Inc. (CIK: 0001795586)
2025-05-24 23:47:39,308 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001795586.json?...
2025-05-24 23:47:39,309 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Chime Financial, Inc.: https://www.sec.gov/Archives/edgar/data/1795586/000162828025025059/chimefinancialinc-sx1wq1da.htm
2025-05-24 23:47:39,309 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1795586/000162828025025059/chimefinancialinc-sx1wq1da.htm
2025-05-24 23:47:43,848 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1795586/000162828025025059/chimefinancialinc-sx1...
2025-05-24 23:47:43,849 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 5080002) for Chime Financial, Inc.
2025-05-24 23:47:44,895 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:48:00,928 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Chime Financial, Inc.
2025-05-24 23:48:01,516 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Chime Financial, Inc. (Analysis ID: 40)
2025-05-24 23:48:09,520 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: JENA ACQUISITION CORPORARTION II from source Finnhub
2025-05-24 23:48:09,690 - root - INFO - ipo_analyzer:779 - IPO 'JENA ACQUISITION CORPORARTION II' not found in DB, creating new entry.
2025-05-24 23:48:10,106 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'JENA ACQUISITION CORPORARTION II' (ID: 41, CIK: None)
2025-05-24 23:48:10,192 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol JENAU for IPO 'JENA ACQUISITION CORPORARTION II'.
2025-05-24 23:48:24,981 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for JENA ACQUISITION CORPORARTION II
2025-05-24 23:48:25,485 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: JENA ACQUISITION CORPORARTION II (Analysis ID: 41)
2025-05-24 23:48:49,488 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: EGH Acquisition Corp. from source Finnhub
2025-05-24 23:48:49,655 - root - INFO - ipo_analyzer:779 - IPO 'EGH Acquisition Corp.' not found in DB, creating new entry.
2025-05-24 23:48:50,067 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'EGH Acquisition Corp.' (ID: 42, CIK: 0002052547)
2025-05-24 23:48:50,152 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for EGH Acquisition Corp. (CIK: 0002052547)
2025-05-24 23:48:51,375 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002052547.json?...
2025-05-24 23:48:51,377 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for EGH Acquisition Corp.: https://www.sec.gov/Archives/edgar/data/2052547/000110465925035524/tm255377-6_drsa.htm
2025-05-24 23:48:51,378 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2052547/000110465925035524/tm255377-6_drsa.htm
2025-05-24 23:48:54,114 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2052547/000110465925035524/tm255377-6_drsa.htm...
2025-05-24 23:48:54,115 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2477453) for EGH Acquisition Corp.
2025-05-24 23:48:54,879 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:49:10,557 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for EGH Acquisition Corp.
2025-05-24 23:49:11,150 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: EGH Acquisition Corp. (Analysis ID: 42)
2025-05-24 23:49:19,154 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Apimeds Pharmaceuticals US, Inc. from source Finnhub
2025-05-24 23:49:19,325 - root - INFO - ipo_analyzer:779 - IPO 'Apimeds Pharmaceuticals US, Inc.' not found in DB, creating new entry.
2025-05-24 23:49:19,745 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Apimeds Pharmaceuticals US, Inc.' (ID: 43, CIK: 0001894525)
2025-05-24 23:49:19,833 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Apimeds Pharmaceuticals US, Inc. (CIK: 0001894525)
2025-05-24 23:49:20,847 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001894525.json?...
2025-05-24 23:49:20,849 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Apimeds Pharmaceuticals US, Inc.: https://www.sec.gov/Archives/edgar/data/1894525/000121390024081766/ea0201124-07.htm
2025-05-24 23:49:20,850 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1894525/000121390024081766/ea0201124-07.htm
2025-05-24 23:49:24,680 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1894525/000121390024081766/ea0201124-07.htm...
2025-05-24 23:49:24,683 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 3912840) for Apimeds Pharmaceuticals US, Inc.
2025-05-24 23:49:25,255 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:49:40,902 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Apimeds Pharmaceuticals US, Inc.
2025-05-24 23:49:41,500 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Apimeds Pharmaceuticals US, Inc. (Analysis ID: 43)
2025-05-24 23:49:49,503 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: HW Electro Co., Ltd. from source Finnhub
2025-05-24 23:49:49,672 - root - INFO - ipo_analyzer:779 - IPO 'HW Electro Co., Ltd.' not found in DB, creating new entry.
2025-05-24 23:49:50,086 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'HW Electro Co., Ltd.' (ID: 44, CIK: 0001980262)
2025-05-24 23:49:50,171 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for HW Electro Co., Ltd. (CIK: 0001980262)
2025-05-24 23:49:51,106 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001980262.json?...
2025-05-24 23:49:51,106 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0001980262 matching criteria.
2025-05-24 23:49:51,360 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0001980262.json?...
2025-05-24 23:49:51,446 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0001980262 matching criteria.
2025-05-24 23:49:51,696 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0001980262.json?...
2025-05-24 23:49:51,781 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for HW Electro Co., Ltd.: https://www.sec.gov/Archives/edgar/data/1980262/000121390025041169/ea0200164-14.htm
2025-05-24 23:49:51,782 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1980262/000121390025041169/ea0200164-14.htm
2025-05-24 23:49:57,899 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1980262/000121390025041169/ea0200164-14.htm...
2025-05-24 23:49:57,900 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 6866436) for HW Electro Co., Ltd.
2025-05-24 23:49:58,834 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:50:16,519 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for HW Electro Co., Ltd.
2025-05-24 23:50:17,103 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: HW Electro Co., Ltd. (Analysis ID: 44)
2025-05-24 23:50:25,104 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Lake Superior Acquisition Corp from source Finnhub
2025-05-24 23:50:25,272 - root - INFO - ipo_analyzer:779 - IPO 'Lake Superior Acquisition Corp' not found in DB, creating new entry.
2025-05-24 23:50:25,691 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Lake Superior Acquisition Corp' (ID: 45, CIK: None)
2025-05-24 23:50:25,774 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol LKSPU for IPO 'Lake Superior Acquisition Corp'.
2025-05-24 23:50:41,325 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Lake Superior Acquisition Corp
2025-05-24 23:50:41,840 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Lake Superior Acquisition Corp (Analysis ID: 45)
2025-05-24 23:50:49,841 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Bonus Biogroup Ltd. from source Finnhub
2025-05-24 23:50:50,013 - root - INFO - ipo_analyzer:779 - IPO 'Bonus Biogroup Ltd.' not found in DB, creating new entry.
2025-05-24 23:50:50,432 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Bonus Biogroup Ltd.' (ID: 46, CIK: 0001710531)
2025-05-24 23:50:50,519 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Bonus Biogroup Ltd. (CIK: 0001710531)
2025-05-24 23:50:51,507 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001710531.json?...
2025-05-24 23:50:51,509 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0001710531 matching criteria.
2025-05-24 23:50:51,763 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0001710531.json?...
2025-05-24 23:50:51,850 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0001710531 matching criteria.
2025-05-24 23:50:52,099 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0001710531.json?...
2025-05-24 23:50:52,185 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for Bonus Biogroup Ltd.: https://www.sec.gov/Archives/edgar/data/1710531/000121390025041347/ea0241191-f1_bonus.htm
2025-05-24 23:50:52,186 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1710531/000121390025041347/ea0241191-f1_bonus.htm
2025-05-24 23:50:54,630 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1710531/000121390025041347/ea0241191-f1_bonus.ht...
2025-05-24 23:50:54,633 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2191968) for Bonus Biogroup Ltd.
2025-05-24 23:50:55,341 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:51:11,618 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Bonus Biogroup Ltd.
2025-05-24 23:51:12,207 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Bonus Biogroup Ltd. (Analysis ID: 46)
2025-05-24 23:51:20,212 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Platinum Analytics Cayman Ltd from source Finnhub
2025-05-24 23:51:20,380 - root - INFO - ipo_analyzer:779 - IPO 'Platinum Analytics Cayman Ltd' not found in DB, creating new entry.
2025-05-24 23:51:20,805 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Platinum Analytics Cayman Ltd' (ID: 47, CIK: 0002053033)
2025-05-24 23:51:20,892 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Platinum Analytics Cayman Ltd (CIK: 0002053033)
2025-05-24 23:51:21,823 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002053033.json?...
2025-05-24 23:51:21,825 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0002053033 matching criteria.
2025-05-24 23:51:22,078 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002053033.json?...
2025-05-24 23:51:22,163 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0002053033 matching criteria.
2025-05-24 23:51:22,414 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0002053033.json?...
2025-05-24 23:51:22,498 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for Platinum Analytics Cayman Ltd: https://www.sec.gov/Archives/edgar/data/2053033/000164117225009388/formf-1.htm
2025-05-24 23:51:22,500 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2053033/000164117225009388/formf-1.htm
2025-05-24 23:51:24,778 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2053033/000164117225009388/formf-1.htm...
2025-05-24 23:51:24,780 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 1843579) for Platinum Analytics Cayman Ltd
2025-05-24 23:51:25,330 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:51:40,986 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Platinum Analytics Cayman Ltd
2025-05-24 23:51:41,577 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Platinum Analytics Cayman Ltd (Analysis ID: 47)
2025-05-24 23:51:49,579 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: CapsoVision, Inc from source Finnhub
2025-05-24 23:51:49,750 - root - INFO - ipo_analyzer:779 - IPO 'CapsoVision, Inc' not found in DB, creating new entry.
2025-05-24 23:51:50,167 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'CapsoVision, Inc' (ID: 48, CIK: 0001378325)
2025-05-24 23:51:50,254 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for CapsoVision, Inc (CIK: 0001378325)
2025-05-24 23:51:51,258 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001378325.json?...
2025-05-24 23:51:51,260 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for CapsoVision, Inc: https://www.sec.gov/Archives/edgar/data/1378325/000119312525116894/d916440ds1.htm
2025-05-24 23:51:51,261 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1378325/000119312525116894/d916440ds1.htm
2025-05-24 23:51:53,788 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1378325/000119312525116894/d916440ds1.htm...
2025-05-24 23:51:53,791 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2145092) for CapsoVision, Inc
2025-05-24 23:51:54,667 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:52:12,439 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for CapsoVision, Inc
2025-05-24 23:52:13,029 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: CapsoVision, Inc (Analysis ID: 48)
2025-05-24 23:52:21,031 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Omada Health, Inc. from source Finnhub
2025-05-24 23:52:21,203 - root - INFO - ipo_analyzer:779 - IPO 'Omada Health, Inc.' not found in DB, creating new entry.
2025-05-24 23:52:21,620 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Omada Health, Inc.' (ID: 49, CIK: 0001611115)
2025-05-24 23:52:21,706 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Omada Health, Inc. (CIK: 0001611115)
2025-05-24 23:52:22,687 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001611115.json?...
2025-05-24 23:52:22,689 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Omada Health, Inc.: https://www.sec.gov/Archives/edgar/data/1611115/000119312525116907/d785770ds1.htm
2025-05-24 23:52:22,690 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1611115/000119312525116907/d785770ds1.htm
2025-05-24 23:52:27,782 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1611115/000119312525116907/d785770ds1.htm...
2025-05-24 23:52:27,785 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 3821286) for Omada Health, Inc.
2025-05-24 23:52:29,475 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:52:46,434 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Omada Health, Inc.
2025-05-24 23:52:47,020 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Omada Health, Inc. (Analysis ID: 49)
2025-05-24 23:52:55,023 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: American Integrity Insurance Group, Inc. from source Finnhub
2025-05-24 23:52:55,189 - root - INFO - ipo_analyzer:779 - IPO 'American Integrity Insurance Group, Inc.' not found in DB, creating new entry.
2025-05-24 23:52:55,603 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'American Integrity Insurance Group, Inc.' (ID: 50, CIK: 0002007587)
2025-05-24 23:52:55,688 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for American Integrity Insurance Group, Inc. (CIK: 0002007587)
2025-05-24 23:52:56,631 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002007587.json?...
2025-05-24 23:52:56,632 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for American Integrity Insurance Group, Inc.: https://www.sec.gov/Archives/edgar/data/2007587/000119312525080175/d63061ds1.htm
2025-05-24 23:52:56,632 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2007587/000119312525080175/d63061ds1.htm
2025-05-24 23:52:59,610 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2007587/000119312525080175/d63061ds1.htm...
2025-05-24 23:52:59,612 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 2235575) for American Integrity Insurance Group, Inc.
2025-05-24 23:53:00,782 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:53:15,570 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for American Integrity Insurance Group, Inc.
2025-05-24 23:53:16,153 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: American Integrity Insurance Group, Inc. (Analysis ID: 50)
2025-05-24 23:53:24,155 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: ASPEN INSURANCE HOLDINGS LTD from source Finnhub
2025-05-24 23:53:24,323 - root - INFO - ipo_analyzer:779 - IPO 'ASPEN INSURANCE HOLDINGS LTD' not found in DB, creating new entry.
2025-05-24 23:53:24,735 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'ASPEN INSURANCE HOLDINGS LTD' (ID: 51, CIK: 0001267395)
2025-05-24 23:53:24,821 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for ASPEN INSURANCE HOLDINGS LTD (CIK: 0001267395)
2025-05-24 23:53:26,224 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0001267395.json?...
2025-05-24 23:53:26,225 - root - INFO - api_clients:347 - No 'S-1' filings found for CIK 0001267395 matching criteria.
2025-05-24 23:53:26,736 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0001267395.json?...
2025-05-24 23:53:26,821 - root - INFO - api_clients:347 - No 'S-1/A' filings found for CIK 0001267395 matching criteria.
2025-05-24 23:53:27,074 - root - INFO - api_clients:41 - Cache hit for: GET:https://data.sec.gov/submissions/CIK0001267395.json?...
2025-05-24 23:53:27,160 - root - INFO - ipo_analyzer:859 - Found F-1 filing URL for ASPEN INSURANCE HOLDINGS LTD: https://www.sec.gov/Archives/edgar/data/1267395/000162828023042245/ahl-20231220.htm
2025-05-24 23:53:27,161 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/1267395/000162828023042245/ahl-20231220.htm
2025-05-24 23:53:36,196 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/1267395/000162828023042245/ahl-20231220.htm...
2025-05-24 23:53:36,198 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 10235843) for ASPEN INSURANCE HOLDINGS LTD
2025-05-24 23:53:38,555 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:53:55,286 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for ASPEN INSURANCE HOLDINGS LTD
2025-05-24 23:53:55,869 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: ASPEN INSURANCE HOLDINGS LTD (Analysis ID: 51)
2025-05-24 23:54:11,872 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Dune Acquisition Corp II from source Finnhub
2025-05-24 23:54:12,039 - root - INFO - ipo_analyzer:779 - IPO 'Dune Acquisition Corp II' not found in DB, creating new entry.
2025-05-24 23:54:12,466 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Dune Acquisition Corp II' (ID: 52, CIK: 0002041047)
2025-05-24 23:54:12,551 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Dune Acquisition Corp II (CIK: 0002041047)
2025-05-24 23:54:13,462 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002041047.json?...
2025-05-24 23:54:13,463 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Dune Acquisition Corp II: https://www.sec.gov/Archives/edgar/data/2041047/000121390025021721/ea0218230-04.htm
2025-05-24 23:54:13,465 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2041047/000121390025021721/ea0218230-04.htm
2025-05-24 23:54:17,949 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2041047/000121390025021721/ea0218230-04.htm...
2025-05-24 23:54:17,950 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 4997385) for Dune Acquisition Corp II
2025-05-24 23:54:18,778 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:54:33,397 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Dune Acquisition Corp II
2025-05-24 23:54:33,980 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Dune Acquisition Corp II (Analysis ID: 52)
2025-05-24 23:54:49,983 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: NMP Acquisition Corp. from source Finnhub
2025-05-24 23:54:50,150 - root - INFO - ipo_analyzer:779 - IPO 'NMP Acquisition Corp.' not found in DB, creating new entry.
2025-05-24 23:54:50,566 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'NMP Acquisition Corp.' (ID: 53, CIK: None)
2025-05-24 23:54:50,655 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol NMPU for IPO 'NMP Acquisition Corp.'.
2025-05-24 23:55:05,805 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for NMP Acquisition Corp.
2025-05-24 23:55:06,309 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: NMP Acquisition Corp. (Analysis ID: 53)
2025-05-24 23:55:22,312 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: Elite Express Holding Inc. from source Finnhub
2025-05-24 23:55:22,483 - root - INFO - ipo_analyzer:779 - IPO 'Elite Express Holding Inc.' not found in DB, creating new entry.
2025-05-24 23:55:22,900 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'Elite Express Holding Inc.' (ID: 54, CIK: 0002053641)
2025-05-24 23:55:22,985 - root - INFO - ipo_analyzer:852 - Attempting to fetch S-1/F-1 filing for Elite Express Holding Inc. (CIK: 0002053641)
2025-05-24 23:55:23,923 - root - INFO - api_clients:70 - Cached response for: GET:https://data.sec.gov/submissions/CIK0002053641.json?...
2025-05-24 23:55:23,925 - root - INFO - ipo_analyzer:859 - Found S-1 filing URL for Elite Express Holding Inc.: https://www.sec.gov/Archives/edgar/data/2053641/000110465925044357/tm254980d4_s1.htm
2025-05-24 23:55:23,925 - root - INFO - api_clients:360 - Fetching filing text from: https://www.sec.gov/Archives/edgar/data/2053641/000110465925044357/tm254980d4_s1.htm
2025-05-24 23:55:26,148 - root - INFO - api_clients:70 - Cached response for: GET_SEC_DOC:https://www.sec.gov/Archives/edgar/data/2053641/000110465925044357/tm254980d4_s1.htm...
2025-05-24 23:55:26,152 - root - INFO - ipo_analyzer:870 - Fetched S-1/F-1 text content (length: 1654640) for Elite Express Holding Inc.
2025-05-24 23:55:26,998 - root - WARNING - api_clients:633 - No primary section headers (ITEM X.) found in SEC filing. Extraction might be limited.
2025-05-24 23:55:43,012 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for Elite Express Holding Inc.
2025-05-24 23:55:43,600 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: Elite Express Holding Inc. (Analysis ID: 54)
2025-05-24 23:55:51,602 - root - INFO - ipo_analyzer:884 - Starting analysis for IPO: StoneBridge Acquisition II Corp from source Finnhub
2025-05-24 23:55:51,770 - root - INFO - ipo_analyzer:779 - IPO 'StoneBridge Acquisition II Corp' not found in DB, creating new entry.
2025-05-24 23:55:52,189 - root - INFO - ipo_analyzer:797 - Created IPO entry for 'StoneBridge Acquisition II Corp' (ID: 55, CIK: None)
2025-05-24 23:55:52,275 - root - WARNING - ipo_analyzer:846 - No CIK found via symbol APACU for IPO 'StoneBridge Acquisition II Corp'.
2025-05-24 23:56:08,564 - root - INFO - ipo_analyzer:1026 - Creating new IPO analysis entry for StoneBridge Acquisition II Corp
2025-05-24 23:56:09,068 - root - INFO - ipo_analyzer:1038 - Successfully analyzed and saved IPO: StoneBridge Acquisition II Corp (Analysis ID: 55)

---------- END app_analysis.log ----------


---------- config.py ----------
# config.py

# API Keys
# It's recommended to load sensitive keys from environment variables or a secure vault in production.

GOOGLE_API_KEYS = [
    "AIzaSyDLkwkVYBTUjabShS7VfdLkQTe7vZkxcjY",
    "AIzaSyAjECAJZVZz6PzDaUVaAkgfcOeLXCPFA6Y",
    "AIzaSyBRDIgN7ffBvoqAgaizQfuWRQExKc_oVig",
    "AIzaSyC4XLSmSX4U2iuAqW_pvQ87eNyPaJwQpDo",
]

FINNHUB_API_KEY = "d0o7hphr01qqr9alj38gd0o7hphr01qqr9alj390"  # Replace with your actual key
FINANCIAL_MODELING_PREP_API_KEY = "62ERGmJoqQgGD0nSGxRZS91TVzfz61uB"  # Replace with your actual key
EODHD_API_KEY = "683079df749c42.21476005"  # Replace with your actual key or "demo"
RAPIDAPI_UPCOMING_IPO_KEY = "0bd9b5144cmsh50c0e6d95c0b662p1cbdefjsn2d1cb0104cde"  # Replace with your actual key

# SEC EDGAR Configuration
EDGAR_USER_AGENT = "YourAppName YourContactEmail@example.com"  # SEC requests a user agent

# Database Configuration
DATABASE_URL = "postgresql://avnadmin:AVNS_IeMYS-rv46Au9xqkza2@pg-4d810ff-daxiake-7258.d.aivencloud.com:26922/stock-alarm?sslmode=require"

# Email Configuration
EMAIL_HOST = "smtp-relay.brevo.com"
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_HOST_USER = "8dca1d001@smtp-brevo.com" # Replace with your actual email user
EMAIL_HOST_PASSWORD = "VrNUkDdcR5G9AL8P"    # Replace with your actual email password
EMAIL_SENDER = "testypesty54@gmail.com" # Replace with your sender email
EMAIL_RECIPIENT = "daniprav@gmail.com"      # The user who receives the summary

# Logging Configuration
LOG_FILE_PATH = "app_analysis.log"
LOG_LEVEL = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# API Client Settings
API_REQUEST_TIMEOUT = 45  # seconds, increased for potentially larger data
API_RETRY_ATTEMPTS = 3
API_RETRY_DELAY = 10  # seconds, increased
MAX_GEMINI_TEXT_LENGTH = 15000 # Max characters to send to Gemini for summaries to avoid hitting limits

# Analysis Settings
MAX_NEWS_ARTICLES_PER_QUERY = 10
MAX_NEWS_TO_ANALYZE_PER_RUN = 5 # Control how many new news items are analyzed in one script run
MIN_MARKET_CAP = 1000000000  # 1 Billion (example, not currently used but good for future filters)
STOCK_FINANCIAL_YEARS = 7 # Number of years for financial statement analysis
IPO_ANALYSIS_REANALYZE_DAYS = 7 # Re-analyze IPO if last analysis is older than this

# Path to store cached API responses
CACHE_DIR = "api_cache" # Currently using DB cache, this is for potential file cache fallback
CACHE_EXPIRY_SECONDS = 3600 * 6 # 6 hours for general data

# DCF Analysis Defaults (Stock Analyzer)
DEFAULT_DISCOUNT_RATE = 0.09  # WACC estimate
DEFAULT_PERPETUAL_GROWTH_RATE = 0.025
DEFAULT_FCF_PROJECTION_YEARS = 5

# News Analysis
NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI = 25000 # Max characters of full news article to send to Gemini

# IPO Analysis
# Define keywords to identify sections in S-1/F-1 filings (very basic)
S1_KEY_SECTIONS = {
    "business": ["Item 1.", "Business"],
    "risk_factors": ["Item 1A.", "Risk Factors"],
    "mda": ["Item 7.", "Management's Discussion and Analysis"],
    "financial_statements": ["Item 8.", "Financial Statements and Supplementary Data"]
}
MAX_S1_SECTION_LENGTH_FOR_GEMINI = 5000 # Max characters per S-1 section to send to Gemini

# Stock Analysis (10-K sections, similar to S-1)
TEN_K_KEY_SECTIONS = S1_KEY_SECTIONS # Often similar item numbers
MAX_10K_SECTION_LENGTH_FOR_GEMINI = MAX_S1_SECTION_LENGTH_FOR_GEMINI
---------- END config.py ----------


---------- database.py ----------
# database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, scoped_session # scoped_session helps manage sessions in web apps or threads
from config import DATABASE_URL
from error_handler import logger

try:
    engine = create_engine(DATABASE_URL, pool_pre_ping=True) # pool_pre_ping can help with stale connections
    # Use scoped_session for thread safety if different parts of app might access DB concurrently.
    # For a sequential script, simple sessionmaker might be enough, but scoped_session is more robust.
    SessionFactory = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    SessionLocal = scoped_session(SessionFactory) # Creates a thread-local session

    Base = declarative_base()
    Base.query = SessionLocal.query_property() # For convenience if using Flask-SQLAlchemy style queries, less common now

    def init_db():
        """Initializes the database and creates tables if they don't exist."""
        try:
            logger.info("Initializing database and creating tables...")
            # Import all modules here that define models so that
            # they are registered with the Base meta-data when Base.metadata.create_all is called.
            import models # noqa F401 (flake8 ignore 'imported but unused')
            Base.metadata.create_all(bind=engine)
            logger.info("Database tables created successfully (if they didn't exist).")
        except Exception as e:
            logger.critical(f"CRITICAL Error initializing database: {e}", exc_info=True)
            raise # Re-raise to halt execution if DB init fails

    def get_db_session():
        """Provides a database session. Caller is responsible for closing."""
        db = SessionLocal()
        try:
            yield db
        finally:
            # The session is closed by the context manager (StockAnalyzer, IPOAnalyzer, etc.)
            # or here if used in a `with get_db_session() as session:` block in main.
            # However, analyzers now manage their own sessions.
            # For main.py's email summary, it will close its own session.
            # If this function is directly used elsewhere, ensure closure.
            # Scoped session handles removal, but explicit close is good practice for non-scoped use.
            # SessionLocal.remove() is called automatically when the scope (e.g. thread) ends.
            # For a script, direct SessionLocal() then .close() is fine.
            # This generator pattern is more common for web request scopes.
            # For the analyzers, they call SessionLocal() directly.
            # For main.py's direct usage:
            if db.is_active:
                db.close()


except Exception as e:
    logger.critical(f"CRITICAL Failed to connect to database or setup SQLAlchemy: {e}", exc_info=True)
    raise # Re-raise as DB is critical
---------- END database.py ----------


---------- email_generator.py ----------
# email_generator.py
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from config import (
    EMAIL_HOST, EMAIL_PORT, EMAIL_USE_TLS, EMAIL_HOST_USER,
    EMAIL_HOST_PASSWORD, EMAIL_SENDER, EMAIL_RECIPIENT
)
from error_handler import logger
from models import StockAnalysis, IPOAnalysis, NewsEventAnalysis  # Ensure these are imported
from datetime import datetime, timezone  # Added timezone
import json
from markdown2 import Markdown


class EmailGenerator:
    def __init__(self):
        self.markdowner = Markdown(extras=["tables", "fenced-code-blocks", "break-on-newline"])

    def _md_to_html(self, md_text):
        if md_text is None: return "<p>N/A</p>"
        if isinstance(md_text, (dict, list)):  # Handle JSON data directly if passed
            return f"<pre>{json.dumps(md_text, indent=2)}</pre>"
        if not isinstance(md_text, str): md_text = str(md_text)

        # Basic check for existing HTML tags
        if "<" in md_text and ">" in md_text and ("<p>" in md_text.lower() or "<div>" in md_text.lower()):
            return md_text  # Assume it's already HTMLish
        return self.markdowner.convert(md_text)

    def _format_stock_analysis_html(self, analysis: StockAnalysis):
        if not analysis: return ""
        stock = analysis.stock

        # Helper for formatting numbers (percentage, decimal)
        def fmt_num(val, type="decimal", na_val="N/A"):
            if val is None or (isinstance(val, float) and (math.isnan(val) or math.isinf(val))): return na_val
            if type == "percent": return f"{val * 100:.2f}%"
            if type == "decimal": return f"{val:.2f}"
            return str(val)

        # Qualitative summaries from potentially Markdown content
        business_summary_html = self._md_to_html(analysis.business_summary)
        economic_moat_html = self._md_to_html(analysis.economic_moat_summary)
        industry_trends_html = self._md_to_html(analysis.industry_trends_summary)
        competitive_landscape_html = self._md_to_html(analysis.competitive_landscape_summary)
        management_assessment_html = self._md_to_html(analysis.management_assessment_summary)
        risk_factors_html = self._md_to_html(analysis.risk_factors_summary)
        investment_thesis_html = self._md_to_html(analysis.investment_thesis_full)  # Full thesis
        reasoning_points_html = self._md_to_html(analysis.reasoning)  # Key reasoning points

        html = f"""
        <div class="analysis-block stock-analysis">
            <h2>Stock Analysis: {stock.company_name} ({stock.ticker})</h2>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Industry:</strong> {stock.industry or 'N/A'}, <strong>Sector:</strong> {stock.sector or 'N/A'}</p>
            <p><strong>Investment Decision:</strong> {analysis.investment_decision or 'N/A'}</p>
            <p><strong>Strategy Type:</strong> {analysis.strategy_type or 'N/A'}</p>
            <p><strong>Confidence Level:</strong> {analysis.confidence_level or 'N/A'}</p>

            <details>
                <summary><strong>Investment Thesis & Reasoning (Click to expand)</strong></summary>
                <h4>Full Thesis:</h4>
                <div class="markdown-content">{investment_thesis_html}</div>
                <h4>Key Reasoning Points:</h4>
                <div class="markdown-content">{reasoning_points_html}</div>
            </details>

            <details>
                <summary><strong>Key Financial Metrics (Click to expand)</strong></summary>
                <ul>
                    <li>P/E Ratio: {fmt_num(analysis.pe_ratio)}</li>
                    <li>P/B Ratio: {fmt_num(analysis.pb_ratio)}</li>
                    <li>P/S Ratio: {fmt_num(analysis.ps_ratio)}</li>
                    <li>EV/Sales: {fmt_num(analysis.ev_to_sales)}</li>
                    <li>EV/EBITDA: {fmt_num(analysis.ev_to_ebitda)}</li>
                    <li>EPS: {fmt_num(analysis.eps)}</li>
                    <li>ROE: {fmt_num(analysis.roe, 'percent')}</li>
                    <li>ROA: {fmt_num(analysis.roa, 'percent')}</li>
                    <li>ROIC: {fmt_num(analysis.roic, 'percent')}</li>
                    <li>Dividend Yield: {fmt_num(analysis.dividend_yield, 'percent')}</li>
                    <li>Debt-to-Equity: {fmt_num(analysis.debt_to_equity)}</li>
                    <li>Debt-to-EBITDA: {fmt_num(analysis.debt_to_ebitda)}</li>
                    <li>Interest Coverage: {fmt_num(analysis.interest_coverage_ratio)}x</li>
                    <li>Current Ratio: {fmt_num(analysis.current_ratio)}</li>
                    <li>Quick Ratio: {fmt_num(analysis.quick_ratio)}</li>
                    <li>Gross Profit Margin: {fmt_num(analysis.gross_profit_margin, 'percent')}</li>
                    <li>Operating Profit Margin: {fmt_num(analysis.operating_profit_margin, 'percent')}</li>
                    <li>Net Profit Margin: {fmt_num(analysis.net_profit_margin, 'percent')}</li>
                    <li>Revenue Growth YoY: {fmt_num(analysis.revenue_growth_yoy, 'percent')} (QoQ: {fmt_num(analysis.revenue_growth_qoq, 'percent')})</li>
                    <li>Revenue Growth CAGR (3yr/5yr): {fmt_num(analysis.revenue_growth_cagr_3yr, 'percent')} / {fmt_num(analysis.revenue_growth_cagr_5yr, 'percent')}</li>
                    <li>EPS Growth YoY: {fmt_num(analysis.eps_growth_yoy, 'percent')}</li>
                    <li>EPS Growth CAGR (3yr/5yr): {fmt_num(analysis.eps_growth_cagr_3yr, 'percent')} / {fmt_num(analysis.eps_growth_cagr_5yr, 'percent')}</li>
                    <li>FCF per Share: {fmt_num(analysis.free_cash_flow_per_share)}</li>
                    <li>FCF Yield: {fmt_num(analysis.free_cash_flow_yield, 'percent')}</li>
                    <li>FCF Trend: {analysis.free_cash_flow_trend or 'N/A'}</li>
                    <li>Retained Earnings Trend: {analysis.retained_earnings_trend or 'N/A'}</li>
                </ul>
            </details>

            <details>
                <summary><strong>DCF Analysis (Simplified) (Click to expand)</strong></summary>
                <ul>
                    <li>Intrinsic Value per Share: {fmt_num(analysis.dcf_intrinsic_value)}</li>
                    <li>Upside/Downside: {fmt_num(analysis.dcf_upside_percentage, 'percent')}</li>
                </ul>
                <p><em>Assumptions:</em></p>
                <div class="markdown-content">{self._md_to_html(analysis.dcf_assumptions)}</div>
            </details>

            <details>
                <summary><strong>Qualitative Analysis (from 10-K/Profile & AI) (Click to expand)</strong></summary>
                <p><strong>Business Summary:</strong></p><div class="markdown-content">{business_summary_html}</div>
                <p><strong>Economic Moat:</strong></p><div class="markdown-content">{economic_moat_html}</div>
                <p><strong>Industry Trends & Position:</strong></p><div class="markdown-content">{industry_trends_html}</div>
                <p><strong>Competitive Landscape:</strong></p><div class="markdown-content">{competitive_landscape_html}</div>
                <p><strong>Management Discussion Highlights (MD&A/Assessment):</strong></p><div class="markdown-content">{management_assessment_html}</div>
                <p><strong>Key Risk Factors:</strong></p><div class="markdown-content">{risk_factors_html}</div>
            </details>

            <details>
                <summary><strong>Supporting Data Snapshots (Click to expand)</strong></summary>
                <p><em>Key Metrics Data Points Used:</em></p>
                <div class="markdown-content">{self._md_to_html(analysis.key_metrics_snapshot)}</div>
                <p><em>Qualitative Analysis Sources Summary:</em></p>
                <div class="markdown-content">{self._md_to_html(analysis.qualitative_sources_summary)}</div>
            </details>
        </div>
        """
        return html

    def _format_ipo_analysis_html(self, analysis: IPOAnalysis):
        if not analysis: return ""
        ipo = analysis.ipo

        # Helper for formatting numbers/price ranges
        def fmt_price(val_low, val_high, currency="USD"):
            if val_low is None and val_high is None: return "N/A"
            if val_low is not None and val_high is not None:
                if val_low == val_high: return f"{val_low:.2f} {currency}"
                return f"{val_low:.2f} - {val_high:.2f} {currency}"
            if val_low is not None: return f"{val_low:.2f} {currency}"
            if val_high is not None: return f"{val_high:.2f} {currency}"  # Should ideally have low if high exists
            return "N/A"

        reasoning_html = self._md_to_html(analysis.reasoning)
        s1_business_summary_html = self._md_to_html(
            analysis.s1_business_summary or analysis.business_model_summary)  # Fallback to old field
        s1_risk_factors_summary_html = self._md_to_html(
            analysis.s1_risk_factors_summary or analysis.risk_factors_summary)
        s1_mda_summary_html = self._md_to_html(analysis.s1_mda_summary)
        s1_financial_health_summary_html = self._md_to_html(
            analysis.s1_financial_health_summary or analysis.pre_ipo_financials_summary)
        competitive_landscape_html = self._md_to_html(analysis.competitive_landscape_summary)
        industry_outlook_html = self._md_to_html(analysis.industry_outlook_summary)  # was industry_health_summary
        use_of_proceeds_html = self._md_to_html(analysis.use_of_proceeds_summary)
        management_team_html = self._md_to_html(analysis.management_team_assessment)
        underwriter_html = self._md_to_html(analysis.underwriter_quality_assessment)
        valuation_html = self._md_to_html(analysis.valuation_comparison_summary)

        html = f"""
        <div class="analysis-block ipo-analysis">
            <h2>IPO Analysis: {ipo.company_name} ({ipo.symbol or 'N/A'})</h2>
            <p><strong>Expected IPO Date:</strong> {ipo.ipo_date.strftime('%Y-%m-%d') if ipo.ipo_date else ipo.ipo_date_str or 'N/A'}</p>
            <p><strong>Expected Price Range:</strong> {fmt_price(ipo.expected_price_range_low, ipo.expected_price_range_high, ipo.expected_price_currency)}</p>
            <p><strong>Exchange:</strong> {ipo.exchange or 'N/A'}, <strong>Status:</strong> {ipo.status or 'N/A'}</p>
            <p><strong>S-1 Filing URL:</strong> {f'<a href="{ipo.s1_filing_url}">{ipo.s1_filing_url}</a>' if ipo.s1_filing_url else 'Not Found'}</p>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>
            <p><strong>Preliminary Stance:</strong> {analysis.investment_decision or 'N/A'}</p>

            <details>
                <summary><strong>AI Synthesized Reasoning & Critical Verification Points (Click to expand)</strong></summary>
                <div class="markdown-content">{reasoning_html}</div>
            </details>

            <details>
                <summary><strong>S-1 Based Summaries (if available) & AI Analysis (Click to expand)</strong></summary>
                <p><strong>Business Summary (from S-1 or inferred):</strong></p><div class="markdown-content">{s1_business_summary_html}</div>
                <p><strong>Competitive Landscape:</strong></p><div class="markdown-content">{competitive_landscape_html}</div>
                <p><strong>Industry Outlook:</strong></p><div class="markdown-content">{industry_outlook_html}</div>
                <p><strong>Risk Factors Summary (from S-1 or inferred):</strong></p><div class="markdown-content">{s1_risk_factors_summary_html}</div>
                <p><strong>Use of Proceeds (from S-1 or inferred):</strong></p><div class="markdown-content">{use_of_proceeds_html}</div>
                <p><strong>MD&A / Financial Health Summary (from S-1 or inferred):</strong></p><div class="markdown-content">{s1_mda_summary_html if s1_mda_summary_html and not s1_mda_summary_html.startswith("Section not found") else s1_financial_health_summary_html}</div>
                <p><strong>Management Team Assessment (Placeholder):</strong></p><div class="markdown-content">{management_team_html}</div>
                <p><strong>Underwriter Quality Assessment (Placeholder):</strong></p><div class="markdown-content">{underwriter_html}</div>
                <p><strong>Valuation Comparison Guidance (Generic):</strong></p><div class="markdown-content">{valuation_html}</div>
            </details>

            <details>
                <summary><strong>Supporting Data (Click to expand)</strong></summary>
                <p><em>Raw data from IPO calendar API:</em></p>
                <div class="markdown-content">{self._md_to_html(analysis.key_data_snapshot)}</div>
                <p><em>S-1 Sections Used for Analysis (True if found & used):</em></p>
                <div class="markdown-content">{self._md_to_html(analysis.s1_sections_used)}</div>
            </details>
        </div>
        """
        return html

    def _format_news_event_analysis_html(self, analysis: NewsEventAnalysis):
        if not analysis: return ""
        news_event = analysis.news_event

        sentiment_html = self._md_to_html(
            f"**Sentiment:** {analysis.sentiment or 'N/A'}\n**Reasoning:** {analysis.sentiment_reasoning or 'N/A'}")
        news_summary_detailed_html = self._md_to_html(analysis.news_summary_detailed)
        impact_companies_html = self._md_to_html(analysis.potential_impact_on_companies)
        impact_sectors_html = self._md_to_html(analysis.potential_impact_on_sectors)
        mechanism_html = self._md_to_html(analysis.mechanism_of_impact)
        timing_duration_html = self._md_to_html(analysis.estimated_timing_duration)
        magnitude_direction_html = self._md_to_html(analysis.estimated_magnitude_direction)
        confidence_html = self._md_to_html(analysis.confidence_of_assessment)
        investor_summary_html = self._md_to_html(analysis.summary_for_email)

        html = f"""
        <div class="analysis-block news-analysis">
            <h2>News/Event Analysis: {news_event.event_title}</h2>
            <p><strong>Event Date:</strong> {news_event.event_date.strftime('%Y-%m-%d %H:%M %Z') if news_event.event_date else 'N/A'}</p>
            <p><strong>Source:</strong> <a href="{news_event.source_url}">{news_event.source_name or news_event.source_url}</a></p>
            <p><strong>Full Article Scraped:</strong> {'Yes' if news_event.full_article_text else 'No (Analysis based on headline/summary if available)'}</p>
            <p><strong>Analysis Date:</strong> {analysis.analysis_date.strftime('%Y-%m-%d %H:%M %Z')}</p>

            <p><strong>Investor Summary:</strong></p>
            <div class="markdown-content">{investor_summary_html}</div>

            <details>
                <summary><strong>Detailed AI Analysis (Click to expand)</strong></summary>
                <p><strong>Sentiment Analysis:</strong></p>
                <div class="markdown-content">{sentiment_html}</div>
                <p><strong>Detailed News Summary:</strong></p>
                <div class="markdown-content">{news_summary_detailed_html}</div>
                <p><strong>Potentially Affected Companies/Stocks:</strong></p>
                <div class="markdown-content">{impact_companies_html}</div>
                <p><strong>Potentially Affected Sectors:</strong></p>
                <div class="markdown-content">{impact_sectors_html}</div>
                <p><strong>Mechanism of Impact:</strong></p>
                <div class="markdown-content">{mechanism_html}</div>
                <p><strong>Estimated Timing & Duration:</strong></p>
                <div class="markdown-content">{timing_duration_html}</div>
                <p><strong>Estimated Magnitude & Direction:</strong></p>
                <div class="markdown-content">{magnitude_direction_html}</div>
                <p><strong>Confidence of Assessment:</strong></p>
                <div class="markdown-content">{confidence_html}</div>
            </details>
             <details>
                <summary><strong>Key Snippets Used for Analysis (Click to expand)</strong></summary>
                <div class="markdown-content">{self._md_to_html(analysis.key_news_snippets)}</div>
            </details>
        </div>
        """
        return html

    def create_summary_email(self, stock_analyses=None, ipo_analyses=None, news_analyses=None):
        if not any([stock_analyses, ipo_analyses, news_analyses]):
            logger.info("No analyses provided to create an email.")
            return None

        subject_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        subject = f"Financial Analysis Summary - {subject_date}"

        html_body = f"""
        <html>
            <head>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; background-color: #f4f4f4; line-height: 1.6; color: #333; }}
                    .container {{ background-color: #ffffff; padding: 20px; border-radius: 8px; box-shadow: 0 0 15px rgba(0,0,0,0.1); max-width: 900px; margin: auto; }}
                    .analysis-block {{ border: 1px solid #ddd; padding: 15px; margin-bottom: 25px; border-radius: 5px; background-color: #fdfdfd; box-shadow: 0 2px 4px rgba(0,0,0,0.05);}}
                    .stock-analysis {{ border-left: 5px solid #4CAF50; }} /* Green for stocks */
                    .ipo-analysis {{ border-left: 5px solid #2196F3; }}   /* Blue for IPOs */
                    .news-analysis {{ border-left: 5px solid #FFC107; }} /* Yellow for News */
                    h1 {{ color: #2c3e50; text-align: center; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
                    h2 {{ color: #34495e; border-bottom: 1px solid #eee; padding-bottom: 5px; margin-top: 0; }}
                    h4 {{ color: #555; margin-top: 15px; margin-bottom: 5px; }}
                    details > summary {{ cursor: pointer; font-weight: bold; margin-bottom: 10px; color: #2980b9; padding: 5px; background-color: #ecf0f1; border-radius:3px; }}
                    details[open] > summary {{ background-color: #dde5e8; }}
                    pre {{ background-color: #eee; padding: 10px; border-radius: 4px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; font-size: 0.85em; border: 1px solid #ccc; }}
                    ul {{ list-style-type: disc; margin-left: 20px; padding-left: 5px; }}
                    li {{ margin-bottom: 8px; }}
                    .markdown-content {{ padding: 5px 0; }}
                    .markdown-content p {{ margin: 0.5em 0; }}
                    .markdown-content ul, .markdown-content ol {{ margin-left: 20px; }}
                    .markdown-content table {{ border-collapse: collapse; width: 100%; margin-bottom: 1em;}}
                    .markdown-content th, .markdown-content td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                    .markdown-content th {{ background-color: #f2f2f2; }}
                    .report-footer {{ text-align: center; font-size: 0.9em; color: #777; margin-top: 30px; }}
                </style>
            </head>
            <body>
                <div class="container">
                    <h1>Financial Analysis Report</h1>
                    <p style="text-align:center; font-style:italic; color:#555;">Generated: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z')}</p>
                    <p style="text-align:center; font-style:italic; color:#7f8c8d;"><em>This email contains automated analysis. Always do your own research before making investment decisions.</em></p>
        """

        if stock_analyses:
            html_body += "<h2>Individual Stock Analyses</h2>"
            for sa in stock_analyses:
                html_body += self._format_stock_analysis_html(sa)

        if ipo_analyses:
            html_body += "<h2>Upcoming IPO Analyses</h2>"
            for ia in ipo_analyses:
                html_body += self._format_ipo_analysis_html(ia)

        if news_analyses:
            html_body += "<h2>Recent News & Event Analyses</h2>"
            for na in news_analyses:
                html_body += self._format_news_event_analysis_html(na)

        html_body += """
                    <div class="report-footer">
                        <p>© Automated Financial Analysis System</p>
                    </div>
                </div>
            </body>
        </html>
        """

        msg = MIMEMultipart('alternative')
        msg['Subject'] = subject
        msg['From'] = EMAIL_SENDER
        msg['To'] = EMAIL_RECIPIENT

        msg.attach(MIMEText(html_body, 'html', 'utf-8'))  # Ensure UTF-8
        return msg

    def send_email(self, message: MIMEMultipart):
        if not message:
            logger.error("No message object provided to send_email.")
            return False
        try:
            # Ensure SMTP settings are correctly fetched from config
            smtp_server = smtplib.SMTP(EMAIL_HOST, EMAIL_PORT, timeout=20)  # Added timeout
            if EMAIL_USE_TLS:
                smtp_server.starttls()
            smtp_server.login(EMAIL_HOST_USER, EMAIL_HOST_PASSWORD)
            smtp_server.sendmail(EMAIL_SENDER, EMAIL_RECIPIENT, message.as_string())
            smtp_server.quit()
            logger.info(f"Email sent successfully to {EMAIL_RECIPIENT}")
            return True
        except smtplib.SMTPException as e_smtp:
            logger.error(f"SMTP error sending email: {e_smtp}", exc_info=True)
            return False
        except Exception as e:
            logger.error(f"General error sending email: {e}", exc_info=True)
            return False


if __name__ == '__main__':
    import math  # For mock data with potential NaN/inf

    logger.info("Starting email generator test with new model fields...")


    # --- Mock Stock Data ---
    class MockStock:
        def __init__(self, ticker, company_name, industry="Tech", sector="Software"):
            self.ticker = ticker
            self.company_name = company_name
            self.industry = industry
            self.sector = sector


    class MockStockAnalysis:
        def __init__(self, stock):
            self.stock = stock
            self.analysis_date = datetime.now(timezone.utc)
            self.investment_decision = "Buy"
            self.strategy_type = "GARP (Growth at a Reasonable Price)"
            self.confidence_level = "Medium"
            self.investment_thesis_full = "This is a **compelling** investment due to *strong growth* and reasonable valuation. AI suggests `monitoring` market conditions."
            self.reasoning = "- Strong revenue growth.\n- Improving margins.\n- Reasonable valuation compared to peers."

            self.pe_ratio = 18.5;
            self.pb_ratio = 3.2;
            self.ps_ratio = 2.5;
            self.ev_to_sales = 2.8;
            self.ev_to_ebitda = 12.0
            self.eps = 2.50;
            self.roe = 0.22;
            self.roa = 0.10;
            self.roic = 0.15;
            self.dividend_yield = 0.015
            self.debt_to_equity = 0.5;
            self.debt_to_ebitda = 2.1;
            self.interest_coverage_ratio = 8.0
            self.current_ratio = 1.8;
            self.quick_ratio = 1.2;
            self.revenue_growth_yoy = 0.15;
            self.revenue_growth_qoq = 0.04;
            self.revenue_growth_cagr_3yr = 0.12;
            self.revenue_growth_cagr_5yr = 0.10
            self.eps_growth_yoy = 0.20;
            self.eps_growth_cagr_3yr = 0.18;
            self.eps_growth_cagr_5yr = 0.15
            self.net_profit_margin = 0.12;
            self.gross_profit_margin = 0.60;
            self.operating_profit_margin = 0.20
            self.free_cash_flow_per_share = 1.80;
            self.free_cash_flow_yield = 0.05;
            self.free_cash_flow_trend = "Growing"
            self.retained_earnings_trend = "Growing"

            self.dcf_intrinsic_value = 120.50;
            self.dcf_upside_percentage = 0.205
            self.dcf_assumptions = {"discount_rate": 0.09, "perpetual_growth_rate": 0.025, "start_fcf": 100000000}

            self.business_summary = "MockCorp is a leading provider of cloud solutions."
            self.economic_moat_summary = "Strong brand recognition and high switching costs."
            self.industry_trends_summary = "Industry is rapidly growing with tailwinds from AI adoption."
            self.competitive_landscape_summary = "Competitive but MockCorp has a differentiated product."
            self.management_assessment_summary = "Experienced management team with a clear vision (from MD&A)."
            self.risk_factors_summary = "Key risks include talent retention and cybersecurity threats."

            self.key_metrics_snapshot = {"price": 100, "marketCap": 1000000000, "latest_revenue_q": 25000000}
            self.qualitative_sources_summary = {"10k_filing_url_used": "http://example.com/10k",
                                                "business_10k_source_length": 5000}


    # --- Mock IPO Data ---
    class MockIPO:
        def __init__(self, company_name, symbol, ipo_date_str="2025-07-15"):
            self.company_name = company_name
            self.symbol = symbol
            self.ipo_date_str = ipo_date_str
            self.ipo_date = datetime.strptime(ipo_date_str, "%Y-%m-%d").date() if ipo_date_str else None
            self.expected_price_range_low = 18.00
            self.expected_price_range_high = 22.00
            self.expected_price_currency = "USD"
            self.exchange = "NASDAQ"
            self.status = "Filed"
            self.s1_filing_url = "http://example.com/s1_filing"


    class MockIPOAnalysis:
        def __init__(self, ipo):
            self.ipo = ipo
            self.analysis_date = datetime.now(timezone.utc)
            self.investment_decision = "Potentially Interesting, S-1 Review Critical"
            self.reasoning = "Promising industry, but financial details from S-1 are key. S-1 summary indicates good initial traction."
            self.s1_business_summary = "NewIPO Inc. is a disruptive player in the fintech space, offering innovative payment solutions."
            self.s1_risk_factors_summary = "Primary risks include regulatory changes and competition from established banks."
            self.s1_mda_summary = "MD&A shows rapid revenue growth but increasing operating losses due to R&D and S&M."
            self.s1_financial_health_summary = "Strong top-line growth, negative FCF, well-funded post-IPO."
            self.competitive_landscape_summary = "Competes with traditional banks and other fintech startups."
            self.industry_outlook_summary = "Fintech industry has strong tailwinds but is becoming crowded."
            self.use_of_proceeds_summary = "Proceeds to be used for product development and market expansion."
            self.management_team_assessment = "Founders have prior startup success. Full team review in S-1 needed."
            self.underwriter_quality_assessment = "Lead underwriters are reputable (e.g., Goldman Sachs, Morgan Stanley - from S-1)."
            self.key_data_snapshot = {"name": ipo.company_name, "symbol": ipo.symbol, "price": "18.00-22.00"}
            self.s1_sections_used = {"business": True, "risk_factors": True, "mda": True}
            # Fallback fields for demonstration
            self.business_model_summary = self.s1_business_summary
            self.risk_factors_summary = self.s1_risk_factors_summary
            self.pre_ipo_financials_summary = self.s1_financial_health_summary
            self.valuation_comparison_summary = "Peer valuation suggests a range of X to Y based on P/S multiples."


    # --- Mock News Data ---
    class MockNewsEvent:
        def __init__(self, title, url, event_date_str="2025-05-25 10:00:00"):
            self.event_title = title
            self.source_url = url
            self.source_name = "Mock News Source"
            self.event_date = datetime.strptime(event_date_str, "%Y-%m-%d %H:%M:%S").replace(
                tzinfo=timezone.utc) if event_date_str else datetime.now(timezone.utc)
            self.full_article_text = "This is the full article text which is much longer and provides more context than just a summary. It discusses market trends and company X's new product."


    class MockNewsEventAnalysis:
        def __init__(self, news_event):
            self.news_event = news_event
            self.analysis_date = datetime.now(timezone.utc)
            self.sentiment = "Positive"
            self.sentiment_reasoning = "The article highlights strong growth and innovation."
            self.news_summary_detailed = "A detailed summary of the news focusing on key impacts."
            self.potential_impact_on_companies = "Company X (TICK) likely to benefit from new product launch. Competitor Y (COMP) may face pressure."
            self.potential_impact_on_sectors = "Technology sector, specifically software services, will see increased activity."
            self.mechanism_of_impact = "New product addresses a key market need, potentially increasing revenue for Company X."
            self.estimated_timing_duration = "Short-term positive sentiment, Medium-term revenue impact."
            self.estimated_magnitude_direction = "Medium Positive for Company X."
            self.confidence_of_assessment = "High"
            self.summary_for_email = "Company X launched a new product, expecting positive revenue impact in the tech sector."
            self.key_news_snippets = {"headline": news_event.event_title, "snippet_used": "Company X's new product..."}


    # Create mock instances
    mock_sa = MockStockAnalysis(MockStock("MOCK", "MockCorp Inc."))
    mock_ipo_a = MockIPOAnalysis(MockIPO("NewIPO Inc.", "NIPO"))
    mock_news_a = MockNewsEventAnalysis(MockNewsEvent("Major Tech Breakthrough Announced", "http://example.com/news1"))

    email_gen = EmailGenerator()
    email_message = email_gen.create_summary_email(
        stock_analyses=[mock_sa],
        ipo_analyses=[mock_ipo_a],
        news_analyses=[mock_news_a]
    )

    if email_message:
        logger.info("Email message created successfully with new fields.")
        # Save to file for inspection
        output_filename = f"test_email_summary_refactored_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(output_filename, "w", encoding="utf-8") as f:
            # MIMEText part is the first payload if it's html only, or might be nested.
            # For 'alternative', payload is a list of MIMEMultipart/MIMEText.
            # We expect the HTML part.
            payload_html = ""
            if email_message.is_multipart():
                for part in email_message.get_payload():
                    if part.get_content_type() == "text/html":
                        payload_html = part.get_payload(decode=True).decode(part.get_content_charset() or 'utf-8')
                        break
            else:  # Not multipart, should be text/html directly
                payload_html = email_message.get_payload(decode=True).decode(
                    email_message.get_content_charset() or 'utf-8')

            if payload_html:
                f.write(payload_html)
                logger.info(f"Test email HTML saved to {output_filename}")
            else:
                logger.error("Could not extract HTML payload from email message.")

        # To actually send, uncomment and ensure config.py is correct:
        # logger.info("Attempting to send test email...")
        # if email_gen.send_email(email_message):
        #     logger.info("Test email sent successfully (check your inbox).")
        # else:
        #     logger.error("Failed to send test email.")

    else:
        logger.error("Failed to create email message.")
---------- END email_generator.py ----------


---------- error_handler.py ----------
# error_handler.py
import logging
import sys
from config import LOG_FILE_PATH, LOG_LEVEL # Ensure these are imported

# Global flag to prevent multiple handler additions if setup_logging is called more than once
# though with import-time setup, this is less of an issue.
_logging_configured = False

def setup_logging():
    """Configures logging for the application."""
    global _logging_configured
    if _logging_configured:
        return logging.getLogger() # Return existing root logger if already configured

    numeric_level = getattr(logging, LOG_LEVEL.upper(), None)
    if not isinstance(numeric_level, int):
        # Fallback to INFO if invalid level is provided in config
        logging.warning(f"Invalid log level: {LOG_LEVEL} in config. Defaulting to INFO.")
        numeric_level = logging.INFO

    # Get the root logger
    logger_obj = logging.getLogger() # Get the root logger
    logger_obj.setLevel(numeric_level) # Set its level

    # Create formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')

    # Console Handler
    if not any(isinstance(h, logging.StreamHandler) for h in logger_obj.handlers):
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        logger_obj.addHandler(console_handler)

    # File Handler
    # Check if a file handler for the specific LOG_FILE_PATH is already attached
    if not any(isinstance(h, logging.FileHandler) and getattr(h, 'baseFilename', None) == LOG_FILE_PATH for h in logger_obj.handlers):
        try:
            file_handler = logging.FileHandler(LOG_FILE_PATH, mode='a') # Append mode
            file_handler.setFormatter(formatter)
            logger_obj.addHandler(file_handler)
        except Exception as e:
            # If file handler fails, log to console and continue
            logging.error(f"Failed to set up file handler for {LOG_FILE_PATH}: {e}", exc_info=True)


    _logging_configured = True
    return logger_obj # Return the configured root logger

# Initialize logger when this module is imported
logger = setup_logging()

def handle_global_exception(exc_type, exc_value, exc_traceback):
    """Custom global exception handler to log unhandled exceptions via the root logger."""
    if issubclass(exc_type, KeyboardInterrupt):
        # Call the default hook for KeyboardInterrupt so it behaves as expected (exit).
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    # Log other unhandled exceptions.
    logger.critical("Unhandled global exception:", exc_info=(exc_type, exc_value, exc_traceback))

# Set the custom global exception hook.
# This will catch any exceptions not caught elsewhere in the application.
# sys.excepthook = handle_global_exception # Uncomment this in main.py or at the top of your script if desired.
---------- END error_handler.py ----------


---------- ipo_analyzer.py ----------
# ipo_analyzer.py
import time
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timedelta, timezone
from dateutil import parser as date_parser  # For flexible date parsing

from api_clients import FinnhubClient, GeminiAPIClient, SECEDGARClient, extract_S1_text_sections
from database import SessionLocal, get_db_session
from models import IPO, IPOAnalysis
from error_handler import logger
from sqlalchemy.exc import SQLAlchemyError
from config import (
    S1_KEY_SECTIONS, MAX_S1_SECTION_LENGTH_FOR_GEMINI,
    IPO_ANALYSIS_REANALYZE_DAYS, MAX_GEMINI_TEXT_LENGTH
)


class IPOAnalyzer:
    def __init__(self):
        self.finnhub = FinnhubClient()
        self.gemini = GeminiAPIClient()
        self.sec_edgar = SECEDGARClient()
        self.db_session = next(get_db_session())

    def _close_session_if_active(self):
        if self.db_session and self.db_session.is_active:
            try:
                self.db_session.close()
                logger.debug("DB session closed in IPOAnalyzer.")
            except Exception as e_close:
                logger.warning(f"Error closing session in IPOAnalyzer: {e_close}")

    def _parse_ipo_date(self, date_str):
        if not date_str:
            return None
        try:
            # Handle various common date formats, make it timezone naive for DB storage as Date
            return date_parser.parse(date_str).date()
        except (ValueError, TypeError) as e:
            logger.warning(f"Could not parse IPO date string '{date_str}': {e}")
            return None

    def fetch_upcoming_ipos(self):
        logger.info("Fetching upcoming IPOs using Finnhub...")
        ipos_data_to_process = []
        today = datetime.now(timezone.utc)
        from_date = (today - timedelta(days=60)).strftime('%Y-%m-%d')  # Look back for recently filed/priced
        to_date = (today + timedelta(days=180)).strftime('%Y-%m-%d')  # Look further ahead

        finnhub_response = self.finnhub.get_ipo_calendar(from_date=from_date, to_date=to_date)
        actual_ipo_list = []

        if finnhub_response and isinstance(finnhub_response, dict) and "ipoCalendar" in finnhub_response:
            actual_ipo_list = finnhub_response["ipoCalendar"]
            if not isinstance(actual_ipo_list, list):
                logger.warning(f"Finnhub response 'ipoCalendar' field is not a list. Found: {type(actual_ipo_list)}")
                actual_ipo_list = []
            elif not actual_ipo_list:
                logger.info("Finnhub 'ipoCalendar' list is empty for the current period.")
        elif finnhub_response is None:  # API call itself failed
            logger.error("Failed to fetch IPOs from Finnhub (API call failed or returned None).")
        else:  # Unexpected format or just no IPOs
            logger.info(f"No IPOs found or unexpected format from Finnhub. Response: {str(finnhub_response)[:200]}")

        if actual_ipo_list:
            for ipo_api_data in actual_ipo_list:
                if not isinstance(ipo_api_data, dict):  # Skip non-dict items
                    logger.warning(f"Skipping non-dictionary item in Finnhub IPO calendar: {ipo_api_data}")
                    continue

                price_range_raw = ipo_api_data.get("price")
                price_low, price_high = None, None

                if isinstance(price_range_raw, str) and price_range_raw.strip():
                    if '-' in price_range_raw:
                        parts = price_range_raw.split('-', 1)
                        try:
                            price_low = float(parts[0].strip())
                        except (ValueError, TypeError):
                            price_low = None
                        try:
                            price_high = float(parts[1].strip()) if len(parts) > 1 and parts[1].strip() else price_low
                        except (ValueError, TypeError):
                            price_high = price_low if price_low is not None else None
                    else:  # Single price
                        try:
                            price_low = float(price_range_raw.strip())
                            price_high = price_low
                        except (ValueError, TypeError):
                            price_low = price_high = None
                elif isinstance(price_range_raw, (float, int)):
                    price_low = float(price_range_raw)
                    price_high = float(price_range_raw)

                parsed_date = self._parse_ipo_date(ipo_api_data.get("date"))

                ipos_data_to_process.append({
                    "company_name": ipo_api_data.get("name"),
                    "symbol": ipo_api_data.get("symbol"),
                    "ipo_date_str": ipo_api_data.get("date"),
                    "ipo_date": parsed_date,
                    "expected_price_range_low": price_low,
                    "expected_price_range_high": price_high,
                    "exchange": ipo_api_data.get("exchange"),
                    "status": ipo_api_data.get("status"),
                    "offered_shares": ipo_api_data.get("numberOfShares"),
                    "total_shares_value": ipo_api_data.get("totalSharesValue"),
                    "source_api": "Finnhub",
                    "raw_data": ipo_api_data
                })
            logger.info(f"Successfully parsed {len(ipos_data_to_process)} IPOs from Finnhub API response.")

        unique_ipos = []
        seen_keys = set()
        for ipo_info in ipos_data_to_process:
            key_name = ipo_info.get("company_name", "").strip().lower() if ipo_info.get(
                "company_name") else "unknown_company"
            key_symbol = ipo_info.get("symbol", "").strip().upper() if ipo_info.get("symbol") else "NO_SYMBOL"
            key_date = ipo_info.get("ipo_date_str", "")  # Use original string date for precise API matching

            unique_tuple = (key_name, key_symbol, key_date)

            if unique_tuple not in seen_keys:
                unique_ipos.append(ipo_info)
                seen_keys.add(unique_tuple)
            else:
                logger.debug(
                    f"Duplicate IPO based on key '{unique_tuple}' found, skipping: {ipo_info.get('company_name')}")

        logger.info(f"Total unique IPOs fetched after deduplication: {len(unique_ipos)}")
        return unique_ipos

    def _get_or_create_ipo_db_entry(self, ipo_data_from_fetch):
        self._ensure_ipo_db_entry_session_active(ipo_data_from_fetch.get('company_name', 'Unknown IPO'))

        ipo_db_entry = None
        # Try finding by symbol first, as it's more unique if present for listed/priced IPOs
        if ipo_data_from_fetch.get("symbol"):
            ipo_db_entry = self.db_session.query(IPO).filter(IPO.symbol == ipo_data_from_fetch["symbol"]).first()

        # If not found by symbol, or no symbol, try by name and original date string (calendar entries)
        if not ipo_db_entry and ipo_data_from_fetch.get("company_name") and ipo_data_from_fetch.get("ipo_date_str"):
            ipo_db_entry = self.db_session.query(IPO).filter(
                IPO.company_name == ipo_data_from_fetch["company_name"],
                IPO.ipo_date_str == ipo_data_from_fetch["ipo_date_str"]
            ).first()

        cik_to_store = ipo_data_from_fetch.get("cik")  # CIK might be passed if pre-fetched
        if not cik_to_store and ipo_data_from_fetch.get("symbol"):
            cik_to_store = self.sec_edgar.get_cik_by_ticker(ipo_data_from_fetch["symbol"])
        elif not cik_to_store and ipo_db_entry and ipo_db_entry.symbol and not ipo_db_entry.cik:  # If updating and CIK was missing
            cik_to_store = self.sec_edgar.get_cik_by_ticker(ipo_db_entry.symbol)

        if not ipo_db_entry:
            logger.info(f"IPO '{ipo_data_from_fetch.get('company_name')}' not found in DB, creating new entry.")
            ipo_db_entry = IPO(
                company_name=ipo_data_from_fetch.get("company_name"),
                symbol=ipo_data_from_fetch.get("symbol"),
                ipo_date_str=ipo_data_from_fetch.get("ipo_date_str"),
                ipo_date=ipo_data_from_fetch.get("ipo_date"),
                expected_price_range_low=ipo_data_from_fetch.get("expected_price_range_low"),
                expected_price_range_high=ipo_data_from_fetch.get("expected_price_range_high"),
                offered_shares=ipo_data_from_fetch.get("offered_shares"),
                total_shares_value=ipo_data_from_fetch.get("total_shares_value"),
                exchange=ipo_data_from_fetch.get("exchange"),
                status=ipo_data_from_fetch.get("status"),
                cik=cik_to_store
            )
            self.db_session.add(ipo_db_entry)
            try:
                self.db_session.commit()
                self.db_session.refresh(ipo_db_entry)
                logger.info(
                    f"Created IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}, CIK: {ipo_db_entry.cik})")
            except SQLAlchemyError as e:
                self.db_session.rollback()
                logger.error(f"Error creating IPO entry for '{ipo_data_from_fetch.get('company_name')}': {e}",
                             exc_info=True)
                return None  # Critical failure
        else:  # Update existing
            logger.info(
                f"Found existing IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}). Checking for updates.")
            updated = False
            fields_to_update = [
                "company_name", "symbol", "ipo_date_str", "ipo_date", "expected_price_range_low",
                "expected_price_range_high", "offered_shares", "total_shares_value", "exchange", "status"
            ]
            for field in fields_to_update:
                new_value = ipo_data_from_fetch.get(field)
                current_value = getattr(ipo_db_entry, field)
                if new_value is not None and current_value != new_value:  # Only update if new value exists and is different
                    setattr(ipo_db_entry, field, new_value)
                    updated = True

            if cik_to_store and ipo_db_entry.cik != cik_to_store:
                ipo_db_entry.cik = cik_to_store
                updated = True

            if updated:
                try:
                    self.db_session.commit()
                    self.db_session.refresh(ipo_db_entry)
                    logger.info(
                        f"Updated existing IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}).")
                except SQLAlchemyError as e:
                    self.db_session.rollback()
                    logger.error(f"Error updating IPO entry for '{ipo_db_entry.company_name}': {e}", exc_info=True)
        return ipo_db_entry

    def _fetch_s1_data(self, ipo_db_entry):
        if not ipo_db_entry: return None, None
        target_cik = ipo_db_entry.cik  # Use CIK from DB entry

        if not target_cik:  # If CIK not in DB, try to get it via symbol
            if ipo_db_entry.symbol:
                target_cik = self.sec_edgar.get_cik_by_ticker(ipo_db_entry.symbol)
                if target_cik:
                    ipo_db_entry.cik = target_cik  # Update DB
                    try:
                        self.db_session.commit()
                        logger.info(f"Found and updated CIK for {ipo_db_entry.company_name} to {target_cik}.")
                    except SQLAlchemyError as e:
                        self.db_session.rollback()
                        logger.error(f"Failed to update CIK for {ipo_db_entry.company_name}: {e}")
                else:
                    logger.warning(
                        f"No CIK found via symbol {ipo_db_entry.symbol} for IPO '{ipo_db_entry.company_name}'.")
                    return None, None
            else:  # No CIK and no symbol
                logger.warning(f"No CIK or symbol for IPO '{ipo_db_entry.company_name}'. Cannot fetch S-1/F-1.")
                return None, None

        logger.info(f"Attempting to fetch S-1/F-1 filing for {ipo_db_entry.company_name} (CIK: {target_cik})")
        # Try S-1, S-1/A, F-1, F-1/A in order of preference for original/latest comprehensive filing
        filing_types_to_try = ["S-1", "S-1/A", "F-1", "F-1/A"]
        s1_url = None
        for form_type in filing_types_to_try:
            s1_url = self.sec_edgar.get_filing_document_url(cik=target_cik, form_type=form_type)
            if s1_url:
                logger.info(f"Found {form_type} filing URL for {ipo_db_entry.company_name}: {s1_url}")
                break

        if s1_url:
            if ipo_db_entry.s1_filing_url != s1_url:  # Store/update the found URL
                ipo_db_entry.s1_filing_url = s1_url
                try:
                    self.db_session.commit()
                except SQLAlchemyError as e:
                    self.db_session.rollback()

            filing_text = self.sec_edgar.get_filing_text(s1_url)
            if filing_text:
                logger.info(
                    f"Fetched S-1/F-1 text content (length: {len(filing_text)}) for {ipo_db_entry.company_name}")
                return filing_text, s1_url
            else:
                logger.warning(f"Failed to fetch S-1/F-1 text content from {s1_url}")
        else:
            logger.warning(f"No S-1 or F-1 type filing URL found for {ipo_db_entry.company_name} (CIK: {target_cik}).")
        return None, None

    def analyze_single_ipo(self, ipo_data_from_fetch):
        ipo_identifier = ipo_data_from_fetch.get("company_name") or ipo_data_from_fetch.get("symbol")
        if not ipo_identifier:
            logger.error(f"Cannot analyze IPO, missing company_name and symbol: {ipo_data_from_fetch}")
            return None  # Cannot proceed
        logger.info(f"Starting analysis for IPO: {ipo_identifier} from source {ipo_data_from_fetch.get('source_api')}")

        self._ensure_ipo_db_entry_session_active(ipo_identifier)
        ipo_db_entry = self._get_or_create_ipo_db_entry(ipo_data_from_fetch)

        if not ipo_db_entry:
            logger.error(f"Could not get or create DB entry for IPO {ipo_identifier}. Aborting analysis.")
            return None

        # Ensure the db entry is properly bound to the current session
        ipo_db_entry = self._ensure_ipo_db_entry_is_bound(ipo_db_entry, ipo_identifier)
        if not ipo_db_entry:  # If binding failed critically
            return None

        # Check for existing recent analysis
        # Datetime comparison fix: Ensure all comparisons are between offset-aware or offset-naive.
        # DB stores aware (UTC), so use aware for threshold.
        reanalyze_threshold_date = datetime.now(timezone.utc) - timedelta(days=IPO_ANALYSIS_REANALYZE_DAYS)

        existing_analysis = self.db_session.query(IPOAnalysis) \
            .filter(IPOAnalysis.ipo_id == ipo_db_entry.id) \
            .order_by(IPOAnalysis.analysis_date.desc()) \
            .first()

        significant_change = False
        if existing_analysis:
            # Compare key calendar data points if they exist in snapshot
            snap = existing_analysis.key_data_snapshot or {}
            parsed_snap_date = self._parse_ipo_date(snap.get("date"))

            # Check for significant changes in calendar data that warrant re-analysis
            if ipo_db_entry.ipo_date != parsed_snap_date or \
                    ipo_db_entry.status != snap.get("status") or \
                    ipo_db_entry.expected_price_range_low != snap.get("price_range_low") or \
                    ipo_db_entry.expected_price_range_high != snap.get("price_range_high"):
                significant_change = True
                logger.info(f"Significant calendar data change detected for IPO {ipo_identifier}.")

            if not significant_change and existing_analysis.analysis_date >= reanalyze_threshold_date:
                logger.info(
                    f"Recent analysis for IPO {ipo_identifier} (ID: {existing_analysis.id}, Date: {existing_analysis.analysis_date}) exists, no significant calendar changes. Skipping full re-analysis.")
                return existing_analysis  # Return existing if recent and no major changes
            else:
                logger.info(
                    f"Re-analyzing IPO {ipo_identifier}. Change: {significant_change}, Analysis Date: {existing_analysis.analysis_date} vs Threshold: {reanalyze_threshold_date}")

        s1_text_content, s1_filing_url_found = self._fetch_s1_data(ipo_db_entry)
        s1_extracted_sections = {}
        if s1_text_content:
            s1_extracted_sections = extract_S1_text_sections(s1_text_content, S1_KEY_SECTIONS)

        analysis_payload = {
            "key_data_snapshot": ipo_data_from_fetch.get("raw_data", {}),  # Store the latest calendar data
            "s1_sections_used": {k: bool(v) for k, v in s1_extracted_sections.items()}
            # Track what S1 sections were found
        }

        company_name_for_prompt = ipo_db_entry.company_name
        symbol_for_prompt = f" (Proposed Ticker: {ipo_db_entry.symbol})" if ipo_db_entry.symbol else ""

        business_text = s1_extracted_sections.get("business", "Not Available from S-1.")[
                        :MAX_S1_SECTION_LENGTH_FOR_GEMINI]
        risk_text = s1_extracted_sections.get("risk_factors", "Not Available from S-1.")[
                    :MAX_S1_SECTION_LENGTH_FOR_GEMINI]
        mda_text = s1_extracted_sections.get("mda", "Not Available from S-1.")[:MAX_S1_SECTION_LENGTH_FOR_GEMINI]

        # Prompts for Gemini based on S-1 if available
        prompt_context = (
            f"Company: {company_name_for_prompt}{symbol_for_prompt}. IPO Status: {ipo_db_entry.status}.\n"
            f"S-1 'Business' Snippet: \"{business_text}\"\n"
            f"S-1 'Risk Factors' Snippet: \"{risk_text}\"\n"
            f"S-1 'MD&A' Snippet: \"{mda_text}\"\n\n"
            f"Instructions: Based *primarily* on the provided S-1 snippets (if available and informative, otherwise infer cautiously based on company name/sector for SPACs or general knowledge):\n"
        )

        # 1. Business Model, Competitive Landscape, Industry Outlook
        prompt1 = prompt_context + (
            f"1. Business Model: Describe core products/services and target market.\n"
            f"2. Competitive Landscape: Identify key competitors and unique selling propositions.\n"
            f"3. Industry Outlook: Summarize key trends, growth drivers, and challenges for its industry.\n"
            f"Provide distinct, concise summaries for each (Business Model, Competitive Landscape, Industry Outlook)."
        )
        response1 = self.gemini.generate_text(prompt1[:MAX_GEMINI_TEXT_LENGTH])
        time.sleep(2)
        analysis_payload["s1_business_summary"] = self._parse_ai_section(response1, "Business Model")
        analysis_payload["competitive_landscape_summary"] = self._parse_ai_section(response1, "Competitive Landscape")
        analysis_payload["industry_outlook_summary"] = self._parse_ai_section(response1, "Industry Outlook")

        # 2. Risk Factors, Use of Proceeds, Financial Health
        prompt2 = prompt_context + (
            f"1. Significant Risk Factors: Summarize the 3-5 most material risks mentioned.\n"
            f"2. Use of IPO Proceeds: Describe the intended primary uses.\n"
            f"3. Financial Health Summary (from MD&A): Summarize recent financial performance, key financial health indicators (revenue growth, profitability/loss trends, cash flow), and outlook.\n"
            f"Provide distinct, concise summaries for each (Risk Factors, Use of Proceeds, Financial Health Summary)."
        )
        response2 = self.gemini.generate_text(prompt2[:MAX_GEMINI_TEXT_LENGTH])
        time.sleep(2)
        analysis_payload["s1_risk_factors_summary"] = self._parse_ai_section(response2, "Significant Risk Factors")
        analysis_payload["use_of_proceeds_summary"] = self._parse_ai_section(response2, "Use of IPO Proceeds")
        analysis_payload["s1_financial_health_summary"] = self._parse_ai_section(response2, "Financial Health Summary")

        # Fallbacks for older model fields if new s1_ fields are empty
        if not analysis_payload.get("s1_business_summary") or analysis_payload.get("s1_business_summary",
                                                                                   "").startswith("Section not found"):
            analysis_payload["business_model_summary"] = analysis_payload[
                "s1_business_summary"]  # Copy if parsed under old name
        if not analysis_payload.get("s1_risk_factors_summary") or analysis_payload.get("s1_risk_factors_summary",
                                                                                       "").startswith(
                "Section not found"):
            analysis_payload["risk_factors_summary"] = analysis_payload["s1_risk_factors_summary"]

        # Management & Underwriter assessments (placeholder - complex from S-1 text)
        analysis_payload[
            "management_team_assessment"] = "Review 'Directors and Executive Officers' section in S-1. AI summary not implemented for this."
        analysis_payload[
            "underwriter_quality_assessment"] = "Review 'Underwriting' section in S-1 for lead underwriters. AI summary not implemented."

        # Final Synthesis Prompt
        synthesis_prompt = (
            f"Synthesize a cautious, preliminary investment perspective for the IPO of {company_name_for_prompt}{symbol_for_prompt}.\n"
            f"IPO Price Range: {ipo_db_entry.expected_price_range_low} - {ipo_db_entry.expected_price_range_high} {ipo_db_entry.expected_price_currency or 'USD'}\n"
            f"S-1 Based Summaries (if available):\n"
            f"  Business: {analysis_payload.get('s1_business_summary', 'N/A')[:250]}...\n"
            f"  Industry: {analysis_payload.get('industry_outlook_summary', 'N/A')[:250]}...\n"
            f"  Competition: {analysis_payload.get('competitive_landscape_summary', 'N/A')[:250]}...\n"
            f"  Risks: {analysis_payload.get('s1_risk_factors_summary', 'N/A')[:250]}...\n"
            f"  Financials (MD&A view): {analysis_payload.get('s1_financial_health_summary', 'N/A')[:250]}...\n"
            f"  Use of Proceeds: {analysis_payload.get('use_of_proceeds_summary', 'N/A')[:150]}...\n\n"
            f"Based on the available information (prioritizing S-1 summaries if present): \n"
            f"1. Investment Stance: Provide a preliminary stance (e.g., 'Potentially Interesting, S-1 Review Critical', 'High Caution Advised', 'Avoid - High Risk / Low Info', 'SPAC - Monitor Target').\n"
            f"2. Reasoning: Briefly explain this stance (3-4 sentences), highlighting key positive/negative factors from the summaries.\n"
            f"3. Critical Verification Points: List 2-3 *critical items* an investor *must* further verify or scrutinize deeply in the full S-1 filing (or await in future filings for SPACs) before any investment decision.\n"
            f"This is a preliminary assessment for research guidance, not financial advice."
        )

        gemini_synthesis = self.gemini.generate_text(synthesis_prompt[:MAX_GEMINI_TEXT_LENGTH])
        time.sleep(2)

        parsed_synthesis = self._parse_ai_synthesis(gemini_synthesis)
        analysis_payload["investment_decision"] = parsed_synthesis.get("decision", "Research Further / Cautious")
        analysis_payload["reasoning"] = parsed_synthesis.get("reasoning_detail", gemini_synthesis)

        current_time_utc = datetime.now(timezone.utc)
        if existing_analysis:
            logger.info(f"Updating existing IPO analysis for {ipo_identifier} (ID: {existing_analysis.id})")
            for key, value in analysis_payload.items():
                setattr(existing_analysis, key, value)
            existing_analysis.analysis_date = current_time_utc
            ipo_analysis_entry_to_save = existing_analysis
        else:
            logger.info(f"Creating new IPO analysis entry for {ipo_identifier}")
            ipo_analysis_entry_to_save = IPOAnalysis(
                ipo_id=ipo_db_entry.id,
                analysis_date=current_time_utc,
                **analysis_payload
            )
            self.db_session.add(ipo_analysis_entry_to_save)

        ipo_db_entry.last_analysis_date = current_time_utc

        try:
            self.db_session.commit()
            logger.info(
                f"Successfully analyzed and saved IPO: {ipo_identifier} (Analysis ID: {ipo_analysis_entry_to_save.id})")
        except SQLAlchemyError as e:
            self.db_session.rollback()
            logger.error(f"Database error saving IPO analysis for {ipo_identifier}: {e}", exc_info=True)
            return None

        return ipo_analysis_entry_to_save

    def _parse_ai_section(self, ai_text, section_header_keywords):
        # Same helper as in stock_analyzer (can be moved to a common utility)
        if not ai_text or ai_text.startswith("Error:"): return "AI Error or No Text"

        if isinstance(section_header_keywords, str):
            keywords_to_check = [section_header_keywords.lower()]
        else:
            keywords_to_check = [k.lower() for k in section_header_keywords]

        lines = ai_text.split('\n')
        capture = False
        section_content = []
        all_known_headers_lower = [
            "business model:", "competitive landscape:", "industry outlook:",
            "significant risk factors:", "use of ipo proceeds:", "financial health summary:",
            "investment stance:", "reasoning:", "critical verification points:"  # From synthesis prompt
        ]

        for i, line in enumerate(lines):
            normalized_line_start_raw = line.strip().lower()

            # Try to match if line STARTS with one of the keywords followed by a colon
            matched_keyword = None
            for kw in keywords_to_check:
                if normalized_line_start_raw.startswith(
                        kw + ":") or normalized_line_start_raw == kw:  # Exact match or with colon
                    matched_keyword = kw
                    break

            if matched_keyword:
                capture = True
                content_on_header_line = line.strip()[len(matched_keyword):].strip()
                if content_on_header_line.startswith(":"): content_on_header_line = content_on_header_line[1:].strip()
                if content_on_header_line: section_content.append(content_on_header_line)
                continue

            if capture:
                # Check if the current line is another known major header, indicating end of current section
                is_another_header = False
                for kh_lower in all_known_headers_lower:
                    if normalized_line_start_raw.startswith(kh_lower) and kh_lower not in keywords_to_check:
                        is_another_header = True
                        break
                if is_another_header:
                    break
                section_content.append(line)  # Append the original line, not normalized

        return "\n".join(section_content).strip() if section_content else "Section not found or empty."

    def _parse_ai_synthesis(self, ai_response):
        # Same helper as in stock_analyzer (can be moved to a common utility)
        parsed = {}
        if ai_response.startswith("Error:") or not ai_response:
            parsed["decision"] = "AI Error"
            parsed["reasoning_detail"] = ai_response
            return parsed

        # Use the more general _parse_ai_section logic
        parsed["decision"] = self._parse_ai_section(ai_response, "Investment Stance")
        parsed["reasoning_detail"] = self._parse_ai_section(ai_response, ["Reasoning",
                                                                          "Critical Verification Points"])  # Combine these for overall reasoning

        if parsed["decision"].startswith("Section not found") or not parsed["decision"]:
            parsed["decision"] = "Review AI Output"  # Fallback
        if parsed["reasoning_detail"].startswith("Section not found") or not parsed["reasoning_detail"]:
            parsed["reasoning_detail"] = ai_response  # Fallback to full response

        return parsed

    def run_ipo_analysis_pipeline(self):
        all_upcoming_ipos = self.fetch_upcoming_ipos()
        analyzed_ipos_results = []
        if not all_upcoming_ipos:
            logger.info("No upcoming IPOs found to analyze.")
            self._close_session_if_active()
            return []

        for ipo_data in all_upcoming_ipos:
            try:
                status = ipo_data.get("status", "").lower()
                # Focus on 'filed', 'expected', 'priced' as these are most actionable for S-1 review
                # 'upcoming' is also fine. 'withdrawn' should be skipped.
                relevant_statuses = ["expected", "filed", "priced", "upcoming", "active"]  # 'active' can be ambiguous
                if status not in relevant_statuses:
                    logger.debug(f"Skipping IPO '{ipo_data.get('company_name')}' with status '{status}'.")
                    continue

                if not ipo_data.get("company_name"):
                    logger.warning(f"Skipping IPO due to missing company name: {ipo_data}")
                    continue

                result = self.analyze_single_ipo(ipo_data)  # This now handles DB session internally for the item
                if result:
                    analyzed_ipos_results.append(result)
            except Exception as e:
                logger.error(f"CRITICAL error in IPO analysis pipeline for item '{ipo_data.get('company_name')}': {e}",
                             exc_info=True)
                # Ensure session is robust for the next IPO if an error occurred
                if self.db_session and not self.db_session.is_active:
                    self.db_session = next(get_db_session())
                elif self.db_session:  # If active but transaction might be bad due to unhandled exception in analyze_single_ipo
                    self.db_session.rollback()
            finally:
                time.sleep(8)  # Increased delay for more intensive S-1 processing and multiple Gemini calls

        logger.info(f"IPO analysis pipeline completed. Analyzed/Updated {len(analyzed_ipos_results)} IPOs.")
        self._close_session_if_active()
        return analyzed_ipos_results

    def _ensure_ipo_db_entry_session_active(self, ipo_identifier_for_log):
        if not self.db_session.is_active:
            logger.warning(f"Session for IPO {ipo_identifier_for_log} was inactive. Re-establishing.")
            self._close_session_if_active()
            self.db_session = next(get_db_session())

    def _ensure_ipo_db_entry_is_bound(self, ipo_db_entry_obj, ipo_identifier_for_log):
        if not ipo_db_entry_obj:  # Should not happen if called after _get_or_create
            logger.error(f"IPO DB entry object is None for {ipo_identifier_for_log} before session binding check.")
            return None

        self._ensure_ipo_db_entry_session_active(ipo_identifier_for_log)  # Ensure session itself is active

        instance_state = sa_inspect(ipo_db_entry_obj)
        if not instance_state.session or instance_state.session is not self.db_session:
            logger.warning(
                f"IPO DB entry {ipo_identifier_for_log} (ID: {ipo_db_entry_obj.id if instance_state.has_identity else 'Transient'}) is not bound to current session {id(self.db_session)} or bound to {id(instance_state.session) if instance_state.session else 'None'}. Attempting merge.")
            try:
                # If object is transient (no ID yet) and session is different, it might be from a failed previous transaction.
                # Re-querying is safer if identity is uncertain or if it's a new object for this session.
                if not instance_state.has_identity and ipo_db_entry_obj.id is None:
                    # Try to find it in the current session by presumed unique keys before merging a new transient one.
                    existing_in_session = self.db_session.query(IPO).filter_by(
                        company_name=ipo_db_entry_obj.company_name,
                        ipo_date_str=ipo_db_entry_obj.ipo_date_str,
                        symbol=ipo_db_entry_obj.symbol
                    ).first()
                    if existing_in_session:
                        ipo_db_entry_obj = existing_in_session  # Use the one from the current session
                        logger.info(
                            f"Replaced transient IPO entry for {ipo_identifier_for_log} with instance from current session (ID: {ipo_db_entry_obj.id}).")
                        return ipo_db_entry_obj  # Return the session-bound object
                    # If not found, it's genuinely new to this session's context, merge will add it.

                merged_ipo = self.db_session.merge(ipo_db_entry_obj)
                # self.db_session.flush() # Optional: ensure it's in identity map. Commit will do this.
                logger.info(
                    f"Successfully merged/re-associated IPO {ipo_identifier_for_log} (ID: {merged_ipo.id}) into current session.")
                return merged_ipo
            except Exception as e_merge:
                logger.error(
                    f"Failed to merge IPO {ipo_identifier_for_log} into session: {e_merge}. Re-fetching as fallback.",
                    exc_info=True)
                # Fallback: try to get by ID if it exists, or by unique constraint if transient and merge failed
                pk_id = ipo_db_entry_obj.id if instance_state.has_identity and ipo_db_entry_obj.id else None
                fallback_ipo = None
                if pk_id:
                    fallback_ipo = self.db_session.query(IPO).get(pk_id)

                if not fallback_ipo:  # If no ID or get by ID failed
                    fallback_ipo = self.db_session.query(IPO).filter_by(
                        company_name=ipo_db_entry_obj.company_name,
                        ipo_date_str=ipo_db_entry_obj.ipo_date_str,
                        symbol=ipo_db_entry_obj.symbol
                    ).first()

                if not fallback_ipo:
                    logger.critical(
                        f"CRITICAL: Failed to re-associate IPO {ipo_identifier_for_log} with current session after merge failure and could not re-fetch.")
                    # Depending on strictness, could raise RuntimeError here.
                    # For now, allow to proceed, but it might fail later if ipo_db_entry_obj is needed for FK.
                    return None  # Indicate critical failure to bind
                logger.info(f"Successfully re-fetched IPO {ipo_identifier_for_log} after merge failure.")
                return fallback_ipo
        return ipo_db_entry_obj  # Already bound or successfully merged/re-fetched


if __name__ == '__main__':
    from database import init_db

    # init_db() # Ensure DB is initialized with new IPO model fields if changed

    logger.info("Starting standalone IPO analysis pipeline test...")
    analyzer = IPOAnalyzer()
    results = analyzer.run_ipo_analysis_pipeline()
    if results:
        logger.info(f"Processed {len(results)} IPOs.")
        for res in results:
            if hasattr(res, 'ipo') and res.ipo:  # Ensure result has 'ipo' attribute
                ipo_info = res.ipo
                logger.info(
                    f"IPO: {ipo_info.company_name} ({ipo_info.symbol}), Decision: {res.investment_decision}, IPO Date: {ipo_info.ipo_date}, Status: {ipo_info.status}, S-1 URL: {ipo_info.s1_filing_url if ipo_info.s1_filing_url else 'Not Found'}")
            else:
                logger.warning(f"Processed IPO result item missing 'ipo' attribute or ipo is None. Result: {res}")

    else:
        logger.info("No IPOs were processed or found by the pipeline.")# ipo_analyzer.py
import time
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timedelta, timezone
from dateutil import parser as date_parser # For flexible date parsing

from api_clients import FinnhubClient, GeminiAPIClient, SECEDGARClient, extract_S1_text_sections
from database import SessionLocal, get_db_session
from models import IPO, IPOAnalysis
from error_handler import logger
from sqlalchemy.exc import SQLAlchemyError
from config import (
    S1_KEY_SECTIONS, MAX_S1_SECTION_LENGTH_FOR_GEMINI,
    IPO_ANALYSIS_REANALYZE_DAYS, MAX_GEMINI_TEXT_LENGTH
)

class IPOAnalyzer:
    def __init__(self):
        self.finnhub = FinnhubClient()
        self.gemini = GeminiAPIClient()
        self.sec_edgar = SECEDGARClient()
        self.db_session = next(get_db_session())

    def _close_session_if_active(self):
        if self.db_session and self.db_session.is_active:
            try:
                self.db_session.close()
                logger.debug("DB session closed in IPOAnalyzer.")
            except Exception as e_close:
                logger.warning(f"Error closing session in IPOAnalyzer: {e_close}")

    def _parse_ipo_date(self, date_str):
        if not date_str:
            return None
        try:
            # Handle various common date formats, make it timezone naive for DB storage as Date
            return date_parser.parse(date_str).date()
        except (ValueError, TypeError) as e:
            logger.warning(f"Could not parse IPO date string '{date_str}': {e}")
            return None

    def fetch_upcoming_ipos(self):
        logger.info("Fetching upcoming IPOs using Finnhub...")
        ipos_data_to_process = []
        today = datetime.now(timezone.utc)
        from_date = (today - timedelta(days=60)).strftime('%Y-%m-%d') # Look back for recently filed/priced
        to_date = (today + timedelta(days=180)).strftime('%Y-%m-%d') # Look further ahead

        finnhub_response = self.finnhub.get_ipo_calendar(from_date=from_date, to_date=to_date)
        actual_ipo_list = []

        if finnhub_response and isinstance(finnhub_response, dict) and "ipoCalendar" in finnhub_response:
            actual_ipo_list = finnhub_response["ipoCalendar"]
            if not isinstance(actual_ipo_list, list):
                logger.warning(f"Finnhub response 'ipoCalendar' field is not a list. Found: {type(actual_ipo_list)}")
                actual_ipo_list = []
            elif not actual_ipo_list:
                logger.info("Finnhub 'ipoCalendar' list is empty for the current period.")
        elif finnhub_response is None: # API call itself failed
            logger.error("Failed to fetch IPOs from Finnhub (API call failed or returned None).")
        else: # Unexpected format or just no IPOs
            logger.info(f"No IPOs found or unexpected format from Finnhub. Response: {str(finnhub_response)[:200]}")

        if actual_ipo_list:
            for ipo_api_data in actual_ipo_list:
                if not isinstance(ipo_api_data, dict): # Skip non-dict items
                    logger.warning(f"Skipping non-dictionary item in Finnhub IPO calendar: {ipo_api_data}")
                    continue

                price_range_raw = ipo_api_data.get("price")
                price_low, price_high = None, None

                if isinstance(price_range_raw, str) and price_range_raw.strip():
                    if '-' in price_range_raw:
                        parts = price_range_raw.split('-', 1)
                        try: price_low = float(parts[0].strip())
                        except (ValueError, TypeError): price_low = None
                        try: price_high = float(parts[1].strip()) if len(parts) > 1 and parts[1].strip() else price_low
                        except (ValueError, TypeError): price_high = price_low if price_low is not None else None
                    else: # Single price
                        try:
                            price_low = float(price_range_raw.strip())
                            price_high = price_low
                        except (ValueError, TypeError): price_low = price_high = None
                elif isinstance(price_range_raw, (float, int)):
                    price_low = float(price_range_raw)
                    price_high = float(price_range_raw)

                parsed_date = self._parse_ipo_date(ipo_api_data.get("date"))

                ipos_data_to_process.append({
                    "company_name": ipo_api_data.get("name"),
                    "symbol": ipo_api_data.get("symbol"),
                    "ipo_date_str": ipo_api_data.get("date"),
                    "ipo_date": parsed_date,
                    "expected_price_range_low": price_low,
                    "expected_price_range_high": price_high,
                    "exchange": ipo_api_data.get("exchange"),
                    "status": ipo_api_data.get("status"),
                    "offered_shares": ipo_api_data.get("numberOfShares"),
                    "total_shares_value": ipo_api_data.get("totalSharesValue"),
                    "source_api": "Finnhub",
                    "raw_data": ipo_api_data
                })
            logger.info(f"Successfully parsed {len(ipos_data_to_process)} IPOs from Finnhub API response.")

        unique_ipos = []
        seen_keys = set()
        for ipo_info in ipos_data_to_process:
            key_name = ipo_info.get("company_name", "").strip().lower() if ipo_info.get("company_name") else "unknown_company"
            key_symbol = ipo_info.get("symbol", "").strip().upper() if ipo_info.get("symbol") else "NO_SYMBOL"
            key_date = ipo_info.get("ipo_date_str", "") # Use original string date for precise API matching

            unique_tuple = (key_name, key_symbol, key_date)

            if unique_tuple not in seen_keys:
                unique_ipos.append(ipo_info)
                seen_keys.add(unique_tuple)
            else:
                logger.debug(f"Duplicate IPO based on key '{unique_tuple}' found, skipping: {ipo_info.get('company_name')}")

        logger.info(f"Total unique IPOs fetched after deduplication: {len(unique_ipos)}")
        return unique_ipos

    def _get_or_create_ipo_db_entry(self, ipo_data_from_fetch):
        self._ensure_ipo_db_entry_session_active(ipo_data_from_fetch.get('company_name', 'Unknown IPO'))

        ipo_db_entry = None
        # Try finding by symbol first, as it's more unique if present for listed/priced IPOs
        if ipo_data_from_fetch.get("symbol"):
            ipo_db_entry = self.db_session.query(IPO).filter(IPO.symbol == ipo_data_from_fetch["symbol"]).first()

        # If not found by symbol, or no symbol, try by name and original date string (calendar entries)
        if not ipo_db_entry and ipo_data_from_fetch.get("company_name") and ipo_data_from_fetch.get("ipo_date_str"):
             ipo_db_entry = self.db_session.query(IPO).filter(
                IPO.company_name == ipo_data_from_fetch["company_name"],
                IPO.ipo_date_str == ipo_data_from_fetch["ipo_date_str"]
            ).first()

        cik_to_store = ipo_data_from_fetch.get("cik") # CIK might be passed if pre-fetched
        if not cik_to_store and ipo_data_from_fetch.get("symbol"):
            cik_to_store = self.sec_edgar.get_cik_by_ticker(ipo_data_from_fetch["symbol"])
        elif not cik_to_store and ipo_db_entry and ipo_db_entry.symbol and not ipo_db_entry.cik: # If updating and CIK was missing
            cik_to_store = self.sec_edgar.get_cik_by_ticker(ipo_db_entry.symbol)

        if not ipo_db_entry:
            logger.info(f"IPO '{ipo_data_from_fetch.get('company_name')}' not found in DB, creating new entry.")
            ipo_db_entry = IPO(
                company_name=ipo_data_from_fetch.get("company_name"),
                symbol=ipo_data_from_fetch.get("symbol"),
                ipo_date_str=ipo_data_from_fetch.get("ipo_date_str"),
                ipo_date=ipo_data_from_fetch.get("ipo_date"),
                expected_price_range_low=ipo_data_from_fetch.get("expected_price_range_low"),
                expected_price_range_high=ipo_data_from_fetch.get("expected_price_range_high"),
                offered_shares=ipo_data_from_fetch.get("offered_shares"),
                total_shares_value=ipo_data_from_fetch.get("total_shares_value"),
                exchange=ipo_data_from_fetch.get("exchange"),
                status=ipo_data_from_fetch.get("status"),
                cik=cik_to_store
            )
            self.db_session.add(ipo_db_entry)
            try:
                self.db_session.commit()
                self.db_session.refresh(ipo_db_entry)
                logger.info(f"Created IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}, CIK: {ipo_db_entry.cik})")
            except SQLAlchemyError as e:
                self.db_session.rollback()
                logger.error(f"Error creating IPO entry for '{ipo_data_from_fetch.get('company_name')}': {e}", exc_info=True)
                return None # Critical failure
        else: # Update existing
            logger.info(f"Found existing IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}). Checking for updates.")
            updated = False
            fields_to_update = [
                "company_name", "symbol", "ipo_date_str", "ipo_date", "expected_price_range_low",
                "expected_price_range_high", "offered_shares", "total_shares_value", "exchange", "status"
            ]
            for field in fields_to_update:
                new_value = ipo_data_from_fetch.get(field)
                current_value = getattr(ipo_db_entry, field)
                if new_value is not None and current_value != new_value: # Only update if new value exists and is different
                    setattr(ipo_db_entry, field, new_value)
                    updated = True

            if cik_to_store and ipo_db_entry.cik != cik_to_store:
                ipo_db_entry.cik = cik_to_store
                updated = True

            if updated:
                try:
                    self.db_session.commit()
                    self.db_session.refresh(ipo_db_entry)
                    logger.info(f"Updated existing IPO entry for '{ipo_db_entry.company_name}' (ID: {ipo_db_entry.id}).")
                except SQLAlchemyError as e:
                    self.db_session.rollback()
                    logger.error(f"Error updating IPO entry for '{ipo_db_entry.company_name}': {e}", exc_info=True)
        return ipo_db_entry

    def _fetch_s1_data(self, ipo_db_entry):
        if not ipo_db_entry: return None, None
        target_cik = ipo_db_entry.cik # Use CIK from DB entry

        if not target_cik: # If CIK not in DB, try to get it via symbol
            if ipo_db_entry.symbol:
                target_cik = self.sec_edgar.get_cik_by_ticker(ipo_db_entry.symbol)
                if target_cik:
                    ipo_db_entry.cik = target_cik # Update DB
                    try:
                        self.db_session.commit()
                        logger.info(f"Found and updated CIK for {ipo_db_entry.company_name} to {target_cik}.")
                    except SQLAlchemyError as e:
                        self.db_session.rollback()
                        logger.error(f"Failed to update CIK for {ipo_db_entry.company_name}: {e}")
                else:
                    logger.warning(f"No CIK found via symbol {ipo_db_entry.symbol} for IPO '{ipo_db_entry.company_name}'.")
                    return None, None
            else: # No CIK and no symbol
                logger.warning(f"No CIK or symbol for IPO '{ipo_db_entry.company_name}'. Cannot fetch S-1/F-1.")
                return None, None

        logger.info(f"Attempting to fetch S-1/F-1 filing for {ipo_db_entry.company_name} (CIK: {target_cik})")
        # Try S-1, S-1/A, F-1, F-1/A in order of preference for original/latest comprehensive filing
        filing_types_to_try = ["S-1", "S-1/A", "F-1", "F-1/A"]
        s1_url = None
        for form_type in filing_types_to_try:
            s1_url = self.sec_edgar.get_filing_document_url(cik=target_cik, form_type=form_type)
            if s1_url:
                logger.info(f"Found {form_type} filing URL for {ipo_db_entry.company_name}: {s1_url}")
                break

        if s1_url:
            if ipo_db_entry.s1_filing_url != s1_url: # Store/update the found URL
                ipo_db_entry.s1_filing_url = s1_url
                try: self.db_session.commit()
                except SQLAlchemyError as e: self.db_session.rollback()

            filing_text = self.sec_edgar.get_filing_text(s1_url)
            if filing_text:
                logger.info(f"Fetched S-1/F-1 text content (length: {len(filing_text)}) for {ipo_db_entry.company_name}")
                return filing_text, s1_url
            else:
                logger.warning(f"Failed to fetch S-1/F-1 text content from {s1_url}")
        else:
            logger.warning(f"No S-1 or F-1 type filing URL found for {ipo_db_entry.company_name} (CIK: {target_cik}).")
        return None, None


    def analyze_single_ipo(self, ipo_data_from_fetch):
        ipo_identifier = ipo_data_from_fetch.get("company_name") or ipo_data_from_fetch.get("symbol")
        if not ipo_identifier:
            logger.error(f"Cannot analyze IPO, missing company_name and symbol: {ipo_data_from_fetch}")
            return None # Cannot proceed
        logger.info(f"Starting analysis for IPO: {ipo_identifier} from source {ipo_data_from_fetch.get('source_api')}")

        self._ensure_ipo_db_entry_session_active(ipo_identifier)
        ipo_db_entry = self._get_or_create_ipo_db_entry(ipo_data_from_fetch)

        if not ipo_db_entry:
            logger.error(f"Could not get or create DB entry for IPO {ipo_identifier}. Aborting analysis.")
            return None

        # Ensure the db entry is properly bound to the current session
        ipo_db_entry = self._ensure_ipo_db_entry_is_bound(ipo_db_entry, ipo_identifier)
        if not ipo_db_entry: # If binding failed critically
             return None


        # Check for existing recent analysis
        # Datetime comparison fix: Ensure all comparisons are between offset-aware or offset-naive.
        # DB stores aware (UTC), so use aware for threshold.
        reanalyze_threshold_date = datetime.now(timezone.utc) - timedelta(days=IPO_ANALYSIS_REANALYZE_DAYS)

        existing_analysis = self.db_session.query(IPOAnalysis) \
            .filter(IPOAnalysis.ipo_id == ipo_db_entry.id) \
            .order_by(IPOAnalysis.analysis_date.desc()) \
            .first()

        significant_change = False
        if existing_analysis:
            # Compare key calendar data points if they exist in snapshot
            snap = existing_analysis.key_data_snapshot or {}
            parsed_snap_date = self._parse_ipo_date(snap.get("date"))

            # Check for significant changes in calendar data that warrant re-analysis
            if ipo_db_entry.ipo_date != parsed_snap_date or \
               ipo_db_entry.status != snap.get("status") or \
               ipo_db_entry.expected_price_range_low != snap.get("price_range_low") or \
               ipo_db_entry.expected_price_range_high != snap.get("price_range_high"):
                significant_change = True
                logger.info(f"Significant calendar data change detected for IPO {ipo_identifier}.")

            if not significant_change and existing_analysis.analysis_date >= reanalyze_threshold_date:
                logger.info(f"Recent analysis for IPO {ipo_identifier} (ID: {existing_analysis.id}, Date: {existing_analysis.analysis_date}) exists, no significant calendar changes. Skipping full re-analysis.")
                return existing_analysis # Return existing if recent and no major changes
            else:
                 logger.info(f"Re-analyzing IPO {ipo_identifier}. Change: {significant_change}, Analysis Date: {existing_analysis.analysis_date} vs Threshold: {reanalyze_threshold_date}")


        s1_text_content, s1_filing_url_found = self._fetch_s1_data(ipo_db_entry)
        s1_extracted_sections = {}
        if s1_text_content:
            s1_extracted_sections = extract_S1_text_sections(s1_text_content, S1_KEY_SECTIONS)

        analysis_payload = {
            "key_data_snapshot": ipo_data_from_fetch.get("raw_data", {}), # Store the latest calendar data
            "s1_sections_used": {k: bool(v) for k,v in s1_extracted_sections.items()} # Track what S1 sections were found
        }

        company_name_for_prompt = ipo_db_entry.company_name
        symbol_for_prompt = f" (Proposed Ticker: {ipo_db_entry.symbol})" if ipo_db_entry.symbol else ""

        business_text = s1_extracted_sections.get("business", "Not Available from S-1.")[:MAX_S1_SECTION_LENGTH_FOR_GEMINI]
        risk_text = s1_extracted_sections.get("risk_factors", "Not Available from S-1.")[:MAX_S1_SECTION_LENGTH_FOR_GEMINI]
        mda_text = s1_extracted_sections.get("mda", "Not Available from S-1.")[:MAX_S1_SECTION_LENGTH_FOR_GEMINI]

        # Prompts for Gemini based on S-1 if available
        prompt_context = (
            f"Company: {company_name_for_prompt}{symbol_for_prompt}. IPO Status: {ipo_db_entry.status}.\n"
            f"S-1 'Business' Snippet: \"{business_text}\"\n"
            f"S-1 'Risk Factors' Snippet: \"{risk_text}\"\n"
            f"S-1 'MD&A' Snippet: \"{mda_text}\"\n\n"
            f"Instructions: Based *primarily* on the provided S-1 snippets (if available and informative, otherwise infer cautiously based on company name/sector for SPACs or general knowledge):\n"
        )

        # 1. Business Model, Competitive Landscape, Industry Outlook
        prompt1 = prompt_context + (
            f"1. Business Model: Describe core products/services and target market.\n"
            f"2. Competitive Landscape: Identify key competitors and unique selling propositions.\n"
            f"3. Industry Outlook: Summarize key trends, growth drivers, and challenges for its industry.\n"
            f"Provide distinct, concise summaries for each (Business Model, Competitive Landscape, Industry Outlook)."
        )
        response1 = self.gemini.generate_text(prompt1[:MAX_GEMINI_TEXT_LENGTH])
        time.sleep(2)
        analysis_payload["s1_business_summary"] = self._parse_ai_section(response1, "Business Model")
        analysis_payload["competitive_landscape_summary"] = self._parse_ai_section(response1, "Competitive Landscape")
        analysis_payload["industry_outlook_summary"] = self._parse_ai_section(response1, "Industry Outlook")

        # 2. Risk Factors, Use of Proceeds, Financial Health
        prompt2 = prompt_context + (
            f"1. Significant Risk Factors: Summarize the 3-5 most material risks mentioned.\n"
            f"2. Use of IPO Proceeds: Describe the intended primary uses.\n"
            f"3. Financial Health Summary (from MD&A): Summarize recent financial performance, key financial health indicators (revenue growth, profitability/loss trends, cash flow), and outlook.\n"
            f"Provide distinct, concise summaries for each (Risk Factors, Use of Proceeds, Financial Health Summary)."
        )
        response2 = self.gemini.generate_text(prompt2[:MAX_GEMINI_TEXT_LENGTH])
        time.sleep(2)
        analysis_payload["s1_risk_factors_summary"] = self._parse_ai_section(response2, "Significant Risk Factors")
        analysis_payload["use_of_proceeds_summary"] = self._parse_ai_section(response2, "Use of IPO Proceeds")
        analysis_payload["s1_financial_health_summary"] = self._parse_ai_section(response2, "Financial Health Summary")

        # Fallbacks for older model fields if new s1_ fields are empty
        if not analysis_payload.get("s1_business_summary") or analysis_payload.get("s1_business_summary", "").startswith("Section not found"):
            analysis_payload["business_model_summary"] = analysis_payload["s1_business_summary"] # Copy if parsed under old name
        if not analysis_payload.get("s1_risk_factors_summary") or analysis_payload.get("s1_risk_factors_summary", "").startswith("Section not found"):
            analysis_payload["risk_factors_summary"] = analysis_payload["s1_risk_factors_summary"]


        # Management & Underwriter assessments (placeholder - complex from S-1 text)
        analysis_payload["management_team_assessment"] = "Review 'Directors and Executive Officers' section in S-1. AI summary not implemented for this."
        analysis_payload["underwriter_quality_assessment"] = "Review 'Underwriting' section in S-1 for lead underwriters. AI summary not implemented."

        # Final Synthesis Prompt
        synthesis_prompt = (
            f"Synthesize a cautious, preliminary investment perspective for the IPO of {company_name_for_prompt}{symbol_for_prompt}.\n"
            f"IPO Price Range: {ipo_db_entry.expected_price_range_low} - {ipo_db_entry.expected_price_range_high} {ipo_db_entry.expected_price_currency or 'USD'}\n"
            f"S-1 Based Summaries (if available):\n"
            f"  Business: {analysis_payload.get('s1_business_summary', 'N/A')[:250]}...\n"
            f"  Industry: {analysis_payload.get('industry_outlook_summary', 'N/A')[:250]}...\n"
            f"  Competition: {analysis_payload.get('competitive_landscape_summary', 'N/A')[:250]}...\n"
            f"  Risks: {analysis_payload.get('s1_risk_factors_summary', 'N/A')[:250]}...\n"
            f"  Financials (MD&A view): {analysis_payload.get('s1_financial_health_summary', 'N/A')[:250]}...\n"
            f"  Use of Proceeds: {analysis_payload.get('use_of_proceeds_summary', 'N/A')[:150]}...\n\n"
            f"Based on the available information (prioritizing S-1 summaries if present): \n"
            f"1. Investment Stance: Provide a preliminary stance (e.g., 'Potentially Interesting, S-1 Review Critical', 'High Caution Advised', 'Avoid - High Risk / Low Info', 'SPAC - Monitor Target').\n"
            f"2. Reasoning: Briefly explain this stance (3-4 sentences), highlighting key positive/negative factors from the summaries.\n"
            f"3. Critical Verification Points: List 2-3 *critical items* an investor *must* further verify or scrutinize deeply in the full S-1 filing (or await in future filings for SPACs) before any investment decision.\n"
            f"This is a preliminary assessment for research guidance, not financial advice."
        )

        gemini_synthesis = self.gemini.generate_text(synthesis_prompt[:MAX_GEMINI_TEXT_LENGTH])
        time.sleep(2)

        parsed_synthesis = self._parse_ai_synthesis(gemini_synthesis)
        analysis_payload["investment_decision"] = parsed_synthesis.get("decision", "Research Further / Cautious")
        analysis_payload["reasoning"] = parsed_synthesis.get("reasoning_detail", gemini_synthesis)

        current_time_utc = datetime.now(timezone.utc)
        if existing_analysis:
            logger.info(f"Updating existing IPO analysis for {ipo_identifier} (ID: {existing_analysis.id})")
            for key, value in analysis_payload.items():
                setattr(existing_analysis, key, value)
            existing_analysis.analysis_date = current_time_utc
            ipo_analysis_entry_to_save = existing_analysis
        else:
            logger.info(f"Creating new IPO analysis entry for {ipo_identifier}")
            ipo_analysis_entry_to_save = IPOAnalysis(
                ipo_id=ipo_db_entry.id,
                analysis_date=current_time_utc,
                **analysis_payload
            )
            self.db_session.add(ipo_analysis_entry_to_save)

        ipo_db_entry.last_analysis_date = current_time_utc

        try:
            self.db_session.commit()
            logger.info(f"Successfully analyzed and saved IPO: {ipo_identifier} (Analysis ID: {ipo_analysis_entry_to_save.id})")
        except SQLAlchemyError as e:
            self.db_session.rollback()
            logger.error(f"Database error saving IPO analysis for {ipo_identifier}: {e}", exc_info=True)
            return None

        return ipo_analysis_entry_to_save

    def _parse_ai_section(self, ai_text, section_header_keywords):
        # Same helper as in stock_analyzer (can be moved to a common utility)
        if not ai_text or ai_text.startswith("Error:"): return "AI Error or No Text"

        if isinstance(section_header_keywords, str):
            keywords_to_check = [section_header_keywords.lower()]
        else:
            keywords_to_check = [k.lower() for k in section_header_keywords]

        lines = ai_text.split('\n')
        capture = False
        section_content = []
        all_known_headers_lower = [
            "business model:", "competitive landscape:", "industry outlook:",
            "significant risk factors:", "use of ipo proceeds:", "financial health summary:",
            "investment stance:", "reasoning:", "critical verification points:" # From synthesis prompt
        ]


        for i, line in enumerate(lines):
            normalized_line_start_raw = line.strip().lower()

            # Try to match if line STARTS with one of the keywords followed by a colon
            matched_keyword = None
            for kw in keywords_to_check:
                if normalized_line_start_raw.startswith(kw + ":") or normalized_line_start_raw == kw : # Exact match or with colon
                    matched_keyword = kw
                    break

            if matched_keyword:
                capture = True
                content_on_header_line = line.strip()[len(matched_keyword):].strip()
                if content_on_header_line.startswith(":"): content_on_header_line = content_on_header_line[1:].strip()
                if content_on_header_line: section_content.append(content_on_header_line)
                continue

            if capture:
                # Check if the current line is another known major header, indicating end of current section
                is_another_header = False
                for kh_lower in all_known_headers_lower:
                    if normalized_line_start_raw.startswith(kh_lower) and kh_lower not in keywords_to_check:
                        is_another_header = True
                        break
                if is_another_header:
                    break
                section_content.append(line) # Append the original line, not normalized

        return "\n".join(section_content).strip() if section_content else "Section not found or empty."

    def _parse_ai_synthesis(self, ai_response):
        # Same helper as in stock_analyzer (can be moved to a common utility)
        parsed = {}
        if ai_response.startswith("Error:") or not ai_response:
            parsed["decision"] = "AI Error"
            parsed["reasoning_detail"] = ai_response
            return parsed

        # Use the more general _parse_ai_section logic
        parsed["decision"] = self._parse_ai_section(ai_response, "Investment Stance")
        parsed["reasoning_detail"] = self._parse_ai_section(ai_response, ["Reasoning", "Critical Verification Points"]) # Combine these for overall reasoning

        if parsed["decision"].startswith("Section not found") or not parsed["decision"]:
             parsed["decision"] = "Review AI Output" # Fallback
        if parsed["reasoning_detail"].startswith("Section not found") or not parsed["reasoning_detail"]:
            parsed["reasoning_detail"] = ai_response # Fallback to full response

        return parsed

    def run_ipo_analysis_pipeline(self):
        all_upcoming_ipos = self.fetch_upcoming_ipos()
        analyzed_ipos_results = []
        if not all_upcoming_ipos:
            logger.info("No upcoming IPOs found to analyze.")
            self._close_session_if_active()
            return []

        for ipo_data in all_upcoming_ipos:
            try:
                status = ipo_data.get("status", "").lower()
                # Focus on 'filed', 'expected', 'priced' as these are most actionable for S-1 review
                # 'upcoming' is also fine. 'withdrawn' should be skipped.
                relevant_statuses = ["expected", "filed", "priced", "upcoming", "active"] # 'active' can be ambiguous
                if status not in relevant_statuses:
                    logger.debug(f"Skipping IPO '{ipo_data.get('company_name')}' with status '{status}'.")
                    continue

                if not ipo_data.get("company_name"):
                    logger.warning(f"Skipping IPO due to missing company name: {ipo_data}")
                    continue

                result = self.analyze_single_ipo(ipo_data) # This now handles DB session internally for the item
                if result:
                    analyzed_ipos_results.append(result)
            except Exception as e:
                logger.error(f"CRITICAL error in IPO analysis pipeline for item '{ipo_data.get('company_name')}': {e}", exc_info=True)
                # Ensure session is robust for the next IPO if an error occurred
                if self.db_session and not self.db_session.is_active:
                    self.db_session = next(get_db_session())
                elif self.db_session: # If active but transaction might be bad due to unhandled exception in analyze_single_ipo
                    self.db_session.rollback()
            finally:
                 time.sleep(8) # Increased delay for more intensive S-1 processing and multiple Gemini calls

        logger.info(f"IPO analysis pipeline completed. Analyzed/Updated {len(analyzed_ipos_results)} IPOs.")
        self._close_session_if_active()
        return analyzed_ipos_results

    def _ensure_ipo_db_entry_session_active(self, ipo_identifier_for_log):
        if not self.db_session.is_active:
            logger.warning(f"Session for IPO {ipo_identifier_for_log} was inactive. Re-establishing.")
            self._close_session_if_active()
            self.db_session = next(get_db_session())

    def _ensure_ipo_db_entry_is_bound(self, ipo_db_entry_obj, ipo_identifier_for_log):
        if not ipo_db_entry_obj: # Should not happen if called after _get_or_create
            logger.error(f"IPO DB entry object is None for {ipo_identifier_for_log} before session binding check.")
            return None

        self._ensure_ipo_db_entry_session_active(ipo_identifier_for_log) # Ensure session itself is active

        instance_state = sa_inspect(ipo_db_entry_obj)
        if not instance_state.session or instance_state.session is not self.db_session:
            logger.warning(f"IPO DB entry {ipo_identifier_for_log} (ID: {ipo_db_entry_obj.id if instance_state.has_identity else 'Transient'}) is not bound to current session {id(self.db_session)} or bound to {id(instance_state.session) if instance_state.session else 'None'}. Attempting merge.")
            try:
                # If object is transient (no ID yet) and session is different, it might be from a failed previous transaction.
                # Re-querying is safer if identity is uncertain or if it's a new object for this session.
                if not instance_state.has_identity and ipo_db_entry_obj.id is None:
                     # Try to find it in the current session by presumed unique keys before merging a new transient one.
                     existing_in_session = self.db_session.query(IPO).filter_by(
                         company_name=ipo_db_entry_obj.company_name,
                         ipo_date_str=ipo_db_entry_obj.ipo_date_str,
                         symbol=ipo_db_entry_obj.symbol
                     ).first()
                     if existing_in_session:
                         ipo_db_entry_obj = existing_in_session # Use the one from the current session
                         logger.info(f"Replaced transient IPO entry for {ipo_identifier_for_log} with instance from current session (ID: {ipo_db_entry_obj.id}).")
                         return ipo_db_entry_obj # Return the session-bound object
                     # If not found, it's genuinely new to this session's context, merge will add it.

                merged_ipo = self.db_session.merge(ipo_db_entry_obj)
                # self.db_session.flush() # Optional: ensure it's in identity map. Commit will do this.
                logger.info(f"Successfully merged/re-associated IPO {ipo_identifier_for_log} (ID: {merged_ipo.id}) into current session.")
                return merged_ipo
            except Exception as e_merge:
                logger.error(f"Failed to merge IPO {ipo_identifier_for_log} into session: {e_merge}. Re-fetching as fallback.", exc_info=True)
                # Fallback: try to get by ID if it exists, or by unique constraint if transient and merge failed
                pk_id = ipo_db_entry_obj.id if instance_state.has_identity and ipo_db_entry_obj.id else None
                fallback_ipo = None
                if pk_id:
                    fallback_ipo = self.db_session.query(IPO).get(pk_id)

                if not fallback_ipo: # If no ID or get by ID failed
                     fallback_ipo = self.db_session.query(IPO).filter_by(
                         company_name=ipo_db_entry_obj.company_name,
                         ipo_date_str=ipo_db_entry_obj.ipo_date_str,
                         symbol=ipo_db_entry_obj.symbol
                     ).first()

                if not fallback_ipo:
                    logger.critical(f"CRITICAL: Failed to re-associate IPO {ipo_identifier_for_log} with current session after merge failure and could not re-fetch.")
                    # Depending on strictness, could raise RuntimeError here.
                    # For now, allow to proceed, but it might fail later if ipo_db_entry_obj is needed for FK.
                    return None # Indicate critical failure to bind
                logger.info(f"Successfully re-fetched IPO {ipo_identifier_for_log} after merge failure.")
                return fallback_ipo
        return ipo_db_entry_obj # Already bound or successfully merged/re-fetched


if __name__ == '__main__':
    from database import init_db
    # init_db() # Ensure DB is initialized with new IPO model fields if changed

    logger.info("Starting standalone IPO analysis pipeline test...")
    analyzer = IPOAnalyzer()
    results = analyzer.run_ipo_analysis_pipeline()
    if results:
        logger.info(f"Processed {len(results)} IPOs.")
        for res in results:
            if hasattr(res, 'ipo') and res.ipo: # Ensure result has 'ipo' attribute
                ipo_info = res.ipo
                logger.info(
                    f"IPO: {ipo_info.company_name} ({ipo_info.symbol}), Decision: {res.investment_decision}, IPO Date: {ipo_info.ipo_date}, Status: {ipo_info.status}, S-1 URL: {ipo_info.s1_filing_url if ipo_info.s1_filing_url else 'Not Found'}")
            else:
                logger.warning(f"Processed IPO result item missing 'ipo' attribute or ipo is None. Result: {res}")

    else:
        logger.info("No IPOs were processed or found by the pipeline.")
---------- END ipo_analyzer.py ----------


---------- main.py ----------
# main.py
import argparse
from datetime import datetime, timezone
from sqlalchemy.orm import joinedload  # <--- ADDED THIS IMPORT
from database import init_db, get_db_session
from error_handler import logger
import time

from stock_analyzer import StockAnalyzer
from ipo_analyzer import IPOAnalyzer
from news_analyzer import NewsAnalyzer
from email_generator import EmailGenerator
from models import StockAnalysis, IPOAnalysis, NewsEventAnalysis
from config import MAX_NEWS_TO_ANALYZE_PER_RUN


def run_stock_analysis(tickers):
    logger.info(f"--- Starting Individual Stock Analysis for: {tickers} ---")
    results = []
    for ticker in tickers:
        try:
            analyzer = StockAnalyzer(ticker=ticker)
            analysis_result = analyzer.analyze()
            if analysis_result:
                results.append(analysis_result)
            else:
                logger.warning(f"Stock analysis for {ticker} did not return a result object.")
        except RuntimeError as rt_err:
            logger.error(f"Could not run stock analysis for {ticker} due to critical init error: {rt_err}")
        except Exception as e:
            logger.error(f"Error analyzing stock {ticker}: {e}", exc_info=True)
        time.sleep(2)
    return results


def run_ipo_analysis():
    logger.info("--- Starting IPO Analysis Pipeline ---")
    try:
        analyzer = IPOAnalyzer()
        results = analyzer.run_ipo_analysis_pipeline()
        return results
    except Exception as e:
        logger.error(f"Error during IPO analysis pipeline: {e}", exc_info=True)
        return []


def run_news_analysis(category="general", count_to_analyze=MAX_NEWS_TO_ANALYZE_PER_RUN):
    logger.info(f"--- Starting News Analysis Pipeline (Category: {category}, Max to Analyze: {count_to_analyze}) ---")
    try:
        analyzer = NewsAnalyzer()
        results = analyzer.run_news_analysis_pipeline(category=category, count_to_analyze_this_run=count_to_analyze)
        return results
    except Exception as e:
        logger.error(f"Error during news analysis pipeline: {e}", exc_info=True)
        return []


def generate_and_send_todays_email_summary():
    logger.info("--- Generating Today's Email Summary ---")
    db_session = next(get_db_session())
    today_start_utc = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)

    try:
        recent_stock_analyses = db_session.query(StockAnalysis).filter(StockAnalysis.analysis_date >= today_start_utc) \
            .options(joinedload(StockAnalysis.stock)).all()
        recent_ipo_analyses = db_session.query(IPOAnalysis).filter(IPOAnalysis.analysis_date >= today_start_utc) \
            .options(joinedload(IPOAnalysis.ipo)).all()
        recent_news_analyses = db_session.query(NewsEventAnalysis).filter(
            NewsEventAnalysis.analysis_date >= today_start_utc) \
            .options(joinedload(NewsEventAnalysis.news_event)).all()

        logger.info(
            f"Found {len(recent_stock_analyses)} stock analyses, {len(recent_ipo_analyses)} IPO analyses, "
            f"{len(recent_news_analyses)} news analyses since {today_start_utc.strftime('%Y-%m-%d %H:%M:%S %Z')} for email."
        )

        if not any([recent_stock_analyses, recent_ipo_analyses, recent_news_analyses]):
            logger.info("No new analyses performed recently to include in the email summary.")
            return

        email_gen = EmailGenerator()
        email_message = email_gen.create_summary_email(
            stock_analyses=recent_stock_analyses,
            ipo_analyses=recent_ipo_analyses,
            news_analyses=recent_news_analyses
        )

        if email_message:
            email_gen.send_email(email_message)
        else:
            logger.error("Failed to create the email message (returned None).")

    except Exception as e:
        logger.error(f"Error generating or sending email summary: {e}", exc_info=True)
    finally:
        if db_session.is_active:  # Ensure session is closed
            db_session.close()


def main():
    parser = argparse.ArgumentParser(description="Financial Analysis and Reporting Tool")
    parser.add_argument("--analyze-stocks", nargs="+", metavar="TICKER",
                        help="List of stock tickers to analyze (e.g., AAPL MSFT)")
    parser.add_argument("--analyze-ipos", action="store_true", help="Run IPO analysis pipeline.")
    parser.add_argument("--analyze-news", action="store_true", help="Run news analysis pipeline.")
    parser.add_argument("--news-category", default="general",
                        help="Category for news analysis (e.g., general, forex, crypto, merger).")
    parser.add_argument("--news-count-analyze", type=int, default=MAX_NEWS_TO_ANALYZE_PER_RUN,
                        help=f"Max number of new news items to analyze in this run (default from config: {MAX_NEWS_TO_ANALYZE_PER_RUN}).")
    parser.add_argument("--send-email", action="store_true",
                        help="Generate and send email summary of today's/recent analyses.")
    parser.add_argument("--init-db", action="store_true", help="Initialize the database (create tables).")
    parser.add_argument("--all", action="store_true",
                        help="Run all analyses (stocks from a predefined list, IPOs, News) and send email. Define stock list below.")

    args = parser.parse_args()

    if args.init_db:
        logger.info("Initializing database as per command line argument...")
        try:
            init_db()
            logger.info("Database initialization complete.")
        except Exception as e:
            logger.critical(f"Database initialization failed: {e}", exc_info=True)
            return

    if args.all:
        default_stocks_for_all = ["AAPL", "MSFT", "GOOGL", "NVDA", "JPM"]
        logger.info(
            f"Running all analyses for default stocks: {default_stocks_for_all}, IPOs, and News (max {args.news_count_analyze} items).")
        if default_stocks_for_all: run_stock_analysis(default_stocks_for_all)
        time.sleep(5)
        run_ipo_analysis()
        time.sleep(5)
        run_news_analysis(category=args.news_category, count_to_analyze=args.news_count_analyze)
        time.sleep(5)
        generate_and_send_todays_email_summary()
        logger.info("--- '--all' tasks finished. ---")
        return

    if args.analyze_stocks:
        run_stock_analysis(args.analyze_stocks)

    if args.analyze_ipos:
        run_ipo_analysis()

    if args.analyze_news:
        run_news_analysis(category=args.news_category, count_to_analyze=args.news_count_analyze)

    if args.send_email:
        generate_and_send_todays_email_summary()

    if not (
            args.analyze_stocks or args.analyze_ipos or args.analyze_news or args.send_email or args.init_db or args.all):
        logger.info("No action specified. Use --help for options.")
        parser.print_help()

    logger.info("--- Main script execution finished. ---")


if __name__ == "__main__":
    script_start_time = datetime.now(timezone.utc)
    logger.info("===================================================================")
    logger.info(f"Starting Financial Analysis Script at {script_start_time.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    logger.info("===================================================================")

    main()

    script_end_time = datetime.now(timezone.utc)
    logger.info(f"Financial Analysis Script finished at {script_end_time.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    logger.info(f"Total execution time: {script_end_time - script_start_time}")
    logger.info("===================================================================")
---------- END main.py ----------


---------- models.py ----------
# models.py
from sqlalchemy import Column, Integer, String, Float, DateTime, Text, JSON, ForeignKey, Boolean, Date, UniqueConstraint
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func  # For server-side SQL functions like func.now() if needed
from database import Base
from datetime import datetime, timezone


class Stock(Base):
    __tablename__ = "stocks"
    id = Column(Integer, primary_key=True, index=True)
    ticker = Column(String, unique=True, index=True, nullable=False)
    company_name = Column(String)
    industry = Column(String, nullable=True)
    sector = Column(String, nullable=True)
    # Use default for Python-side default value generation
    # Use onupdate for Python-side value generation on update
    last_analysis_date = Column(DateTime(timezone=True),
                                default=lambda: datetime.now(timezone.utc),
                                onupdate=lambda: datetime.now(timezone.utc))
    cik = Column(String, nullable=True, index=True)

    analyses = relationship("StockAnalysis", back_populates="stock", cascade="all, delete-orphan")


class StockAnalysis(Base):
    __tablename__ = "stock_analyses"
    id = Column(Integer, primary_key=True, index=True)
    stock_id = Column(Integer, ForeignKey("stocks.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    # Quantitative
    pe_ratio = Column(Float, nullable=True)
    pb_ratio = Column(Float, nullable=True)
    ps_ratio = Column(Float, nullable=True)
    ev_to_sales = Column(Float, nullable=True)
    ev_to_ebitda = Column(Float, nullable=True)
    eps = Column(Float, nullable=True)
    roe = Column(Float, nullable=True)
    roa = Column(Float, nullable=True)
    roic = Column(Float, nullable=True)
    dividend_yield = Column(Float, nullable=True)
    debt_to_equity = Column(Float, nullable=True)
    debt_to_ebitda = Column(Float, nullable=True)
    interest_coverage_ratio = Column(Float, nullable=True)
    current_ratio = Column(Float, nullable=True)
    quick_ratio = Column(Float, nullable=True)

    revenue_growth_yoy = Column(Float, nullable=True)
    revenue_growth_qoq = Column(Float, nullable=True)
    revenue_growth_cagr_3yr = Column(Float, nullable=True)
    revenue_growth_cagr_5yr = Column(Float, nullable=True)
    eps_growth_yoy = Column(Float, nullable=True)
    eps_growth_cagr_3yr = Column(Float, nullable=True)
    eps_growth_cagr_5yr = Column(Float, nullable=True)
    net_profit_margin = Column(Float, nullable=True)
    gross_profit_margin = Column(Float, nullable=True)
    operating_profit_margin = Column(Float, nullable=True)
    free_cash_flow_per_share = Column(Float, nullable=True)
    free_cash_flow_yield = Column(Float, nullable=True)
    free_cash_flow_trend = Column(String, nullable=True)
    retained_earnings_trend = Column(String, nullable=True)

    dcf_intrinsic_value = Column(Float, nullable=True)
    dcf_upside_percentage = Column(Float, nullable=True)
    dcf_assumptions = Column(JSON, nullable=True)

    # Qualitative
    business_summary = Column(Text, nullable=True)
    economic_moat_summary = Column(Text, nullable=True)
    industry_trends_summary = Column(Text, nullable=True)
    competitive_landscape_summary = Column(Text, nullable=True)
    management_assessment_summary = Column(Text, nullable=True)
    risk_factors_summary = Column(Text, nullable=True)

    # Conclusion
    investment_thesis_full = Column(Text, nullable=True)
    investment_decision = Column(String, nullable=True)
    reasoning = Column(Text, nullable=True)
    strategy_type = Column(String, nullable=True)
    confidence_level = Column(String, nullable=True)

    key_metrics_snapshot = Column(JSON, nullable=True)
    qualitative_sources_summary = Column(JSON, nullable=True)

    stock = relationship("Stock", back_populates="analyses")


class IPO(Base):
    __tablename__ = "ipos"
    id = Column(Integer, primary_key=True, index=True)
    company_name = Column(String, index=True, nullable=False)
    symbol = Column(String, index=True, nullable=True)
    ipo_date_str = Column(String, nullable=True)
    ipo_date = Column(Date, nullable=True)
    expected_price_range_low = Column(Float, nullable=True)
    expected_price_range_high = Column(Float, nullable=True)
    expected_price_currency = Column(String, nullable=True, default="USD")
    offered_shares = Column(Float, nullable=True)
    total_shares_value = Column(Float, nullable=True)
    exchange = Column(String, nullable=True)
    status = Column(String, nullable=True)
    cik = Column(String, nullable=True, index=True)
    last_analysis_date = Column(DateTime(timezone=True),
                                default=lambda: datetime.now(timezone.utc),
                                onupdate=lambda: datetime.now(timezone.utc))
    s1_filing_url = Column(String, nullable=True)

    analyses = relationship("IPOAnalysis", back_populates="ipo", cascade="all, delete-orphan")

    __table_args__ = (UniqueConstraint('company_name', 'ipo_date_str', 'symbol', name='uq_ipo_name_date_symbol'),)


class IPOAnalysis(Base):
    __tablename__ = "ipo_analyses"
    id = Column(Integer, primary_key=True, index=True)
    ipo_id = Column(Integer, ForeignKey("ipos.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    s1_business_summary = Column(Text, nullable=True)
    s1_risk_factors_summary = Column(Text, nullable=True)
    s1_mda_summary = Column(Text, nullable=True)
    s1_financial_health_summary = Column(Text, nullable=True)

    competitive_landscape_summary = Column(Text, nullable=True)
    industry_outlook_summary = Column(Text, nullable=True)
    management_team_assessment = Column(Text, nullable=True)
    use_of_proceeds_summary = Column(Text, nullable=True)
    underwriter_quality_assessment = Column(String, nullable=True)

    business_model_summary = Column(Text, nullable=True)
    risk_factors_summary = Column(Text, nullable=True)
    pre_ipo_financials_summary = Column(Text, nullable=True)
    valuation_comparison_summary = Column(Text, nullable=True)

    investment_decision = Column(String, nullable=True)
    reasoning = Column(Text, nullable=True)
    key_data_snapshot = Column(JSON, nullable=True)
    s1_sections_used = Column(JSON, nullable=True)

    ipo = relationship("IPO", back_populates="analyses")


class NewsEvent(Base):
    __tablename__ = "news_events"
    id = Column(Integer, primary_key=True, index=True)
    event_title = Column(String, index=True)
    event_date = Column(DateTime(timezone=True), nullable=True)
    source_url = Column(String, unique=True, nullable=False, index=True)
    source_name = Column(String, nullable=True)
    category = Column(String, nullable=True)
    processed_date = Column(DateTime(timezone=True),
                            default=lambda: datetime.now(timezone.utc))
    last_analyzed_date = Column(DateTime(timezone=True), nullable=True,
                                onupdate=lambda: datetime.now(timezone.utc))
    full_article_text = Column(Text, nullable=True)

    analyses = relationship("NewsEventAnalysis", back_populates="news_event", cascade="all, delete-orphan")

    __table_args__ = (UniqueConstraint('source_url', name='uq_news_source_url'),)


class NewsEventAnalysis(Base):
    __tablename__ = "news_event_analyses"
    id = Column(Integer, primary_key=True, index=True)
    news_event_id = Column(Integer, ForeignKey("news_events.id", ondelete="CASCADE"), nullable=False)
    analysis_date = Column(DateTime(timezone=True),
                           default=lambda: datetime.now(timezone.utc),
                           onupdate=lambda: datetime.now(timezone.utc))

    sentiment = Column(String, nullable=True)
    sentiment_reasoning = Column(Text, nullable=True)

    affected_stocks_explicit = Column(JSON, nullable=True)
    affected_sectors_explicit = Column(JSON, nullable=True)

    news_summary_detailed = Column(Text, nullable=True)
    potential_impact_on_market = Column(Text, nullable=True)
    potential_impact_on_companies = Column(Text, nullable=True)
    potential_impact_on_sectors = Column(Text, nullable=True)

    mechanism_of_impact = Column(Text, nullable=True)
    estimated_timing_duration = Column(String, nullable=True)
    estimated_magnitude_direction = Column(String, nullable=True)
    confidence_of_assessment = Column(String, nullable=True)

    summary_for_email = Column(Text, nullable=True)
    key_news_snippets = Column(JSON, nullable=True)

    news_event = relationship("NewsEvent", back_populates="analyses")


class CachedAPIData(Base):
    __tablename__ = "cached_api_data"
    id = Column(Integer, primary_key=True, index=True)
    api_source = Column(String, index=True, nullable=False)
    request_url_or_params = Column(String, unique=True, nullable=False, index=True)
    response_data = Column(JSON, nullable=False)
    timestamp = Column(DateTime(timezone=True),
                       default=lambda: datetime.now(timezone.utc))  # default for Python-side timestamp
    expires_at = Column(DateTime(timezone=True), nullable=False, index=True)
---------- END models.py ----------


---------- news_analyzer.py ----------
# news_analyzer.py
import time
from sqlalchemy import inspect as sa_inspect  # For checking instance state
from sqlalchemy.orm import joinedload
from datetime import datetime, timezone, timedelta

from api_clients import FinnhubClient, GeminiAPIClient, scrape_article_content
from database import SessionLocal, get_db_session
from models import NewsEvent, NewsEventAnalysis
from error_handler import logger
from sqlalchemy.exc import SQLAlchemyError
from config import (
    MAX_NEWS_ARTICLES_PER_QUERY, MAX_NEWS_TO_ANALYZE_PER_RUN,
    NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI, MAX_GEMINI_TEXT_LENGTH
)


class NewsAnalyzer:
    def __init__(self):
        self.finnhub = FinnhubClient()
        self.gemini = GeminiAPIClient()
        self.db_session = next(get_db_session())

    def _close_session_if_active(self):
        if self.db_session and self.db_session.is_active:
            try:
                self.db_session.close()
                logger.debug("DB session closed in NewsAnalyzer.")
            except Exception as e_close:
                logger.warning(f"Error closing session in NewsAnalyzer: {e_close}")

    def fetch_market_news(self, category="general", count_to_fetch_from_api=MAX_NEWS_ARTICLES_PER_QUERY):
        logger.info(f"Fetching latest market news for category: {category} (max {count_to_fetch_from_api} from API)...")
        # Finnhub's /news endpoint doesn't have a 'count' param for items returned in one call, it returns recent news.
        # We might need to paginate or filter if we need more than one batch.
        # For now, one call and take top 'count_to_fetch_from_api'.
        news_items = self.finnhub.get_market_news(category=category)

        if news_items and isinstance(news_items, list):
            logger.info(f"Fetched {len(news_items)} news items from Finnhub.")
            return news_items[:count_to_fetch_from_api]
        else:
            logger.warning(f"Failed to fetch news or received unexpected format from Finnhub: {news_items}")
            return []

    def _get_or_create_news_event(self, news_item_from_api):
        self._ensure_news_event_session_active(news_item_from_api.get('headline', 'Unknown News'))

        source_url = news_item_from_api.get("url")
        if not source_url:
            logger.warning(f"News item missing URL, cannot process: {news_item_from_api.get('headline')}")
            return None  # Cannot reliably deduplicate or process without a URL

        event = self.db_session.query(NewsEvent).filter_by(source_url=source_url).first()

        full_article_text_scraped_now = None
        # Scrape only if event is new, or if it exists but full_article_text is missing
        if not event or (event and not event.full_article_text):
            logger.info(f"Attempting to scrape full article for: {source_url}")
            full_article_text_scraped_now = scrape_article_content(source_url)  # This is an API call (HTTP GET)
            time.sleep(1)  # Small delay after scraping
            if full_article_text_scraped_now:
                logger.info(f"Scraped ~{len(full_article_text_scraped_now)} chars for {source_url}")
            else:
                logger.warning(
                    f"Failed to scrape full article for {source_url}. Analysis will use summary if available.")

        current_time_utc = datetime.now(timezone.utc)
        if event:  # Event already exists in DB
            logger.debug(f"News event '{event.event_title[:70]}...' (URL: {source_url}) already in DB.")
            # If we just scraped text and it was missing, update the event
            if full_article_text_scraped_now and not event.full_article_text:
                logger.info(f"Updating existing event {event.id} with newly scraped full article text.")
                event.full_article_text = full_article_text_scraped_now
                event.processed_date = current_time_utc  # Update processed date as we've enhanced it
                try:
                    self.db_session.commit()
                except SQLAlchemyError as e:
                    self.db_session.rollback()
                    logger.error(f"Error updating full_article_text for existing event {source_url}: {e}")
            return event

        # Event is new, create it
        event_timestamp = news_item_from_api.get("datetime")
        event_datetime_utc = datetime.fromtimestamp(event_timestamp,
                                                    timezone.utc) if event_timestamp else current_time_utc

        new_event = NewsEvent(
            event_title=news_item_from_api.get("headline"),
            event_date=event_datetime_utc,
            source_url=source_url,
            source_name=news_item_from_api.get("source"),
            category=news_item_from_api.get("category"),
            full_article_text=full_article_text_scraped_now,  # Store scraped text
            processed_date=current_time_utc
        )
        self.db_session.add(new_event)
        try:
            self.db_session.commit()
            self.db_session.refresh(new_event)  # Get ID and other defaults loaded
            logger.info(f"Stored new news event: {new_event.event_title[:70]}... (ID: {new_event.id})")
            return new_event
        except SQLAlchemyError as e:
            self.db_session.rollback()
            logger.error(f"Database error storing new news event '{news_item_from_api.get('headline')}': {e}",
                         exc_info=True)
            return None

    def analyze_single_news_item(self, news_event_db):
        if not news_event_db:
            logger.error("analyze_single_news_item called with no NewsEvent DB object.")
            return None

        # Ensure the event object is bound to the current session
        news_event_db = self._ensure_news_event_is_bound(news_event_db)
        if not news_event_db:  # If binding failed
            return None

        headline = news_event_db.event_title
        content_for_analysis = news_event_db.full_article_text
        analysis_source_type = "full article"

        if not content_for_analysis:
            # Fallback to headline if no full article text (e.g. scraping failed or PDF)
            # A more robust system might use Finnhub summary if that was stored with NewsEvent.
            content_for_analysis = headline
            analysis_source_type = "headline only"
            logger.warning(f"No full article text for '{headline}'. Analyzing based on headline only.")

        # Truncate for Gemini if too long
        if len(content_for_analysis) > NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI:
            content_for_analysis = content_for_analysis[
                                   :NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI] + "\n... [CONTENT TRUNCATED FOR AI ANALYSIS] ..."
            logger.info(
                f"Truncated news content for '{headline}' to {NEWS_ARTICLE_MAX_LENGTH_FOR_GEMINI} chars for Gemini.")
            analysis_source_type += " (truncated)"

        logger.info(f"Analyzing news: '{headline[:70]}...' (using {analysis_source_type})")
        analysis_payload = {"key_news_snippets": {"headline": headline, "source_type_used": analysis_source_type}}

        # 1. Sentiment Analysis
        sentiment_context = f"News headline for context: {headline}"
        sentiment_response = self.gemini.analyze_sentiment_with_reasoning(content_for_analysis,
                                                                          context=sentiment_context)
        time.sleep(2)  # API courtesy
        if not sentiment_response.startswith("Error:"):
            try:  # Attempt to parse sentiment and reasoning
                parts = sentiment_response.split("Reasoning:", 1)
                if ":" in parts[0]:  # Expect "Sentiment: [Class]"
                    sentiment_class_text = parts[0].split(":", 1)[1].strip()
                    # Take first word, remove punctuation
                    analysis_payload["sentiment"] = sentiment_class_text.split(' ')[0].split('.')[0].split(',')[
                        0].strip()
                else:  # If no "Sentiment:" prefix, try to infer
                    analysis_payload["sentiment"] = parts[0].strip().split(' ')[0]  # First word as sentiment
                analysis_payload["sentiment_reasoning"] = parts[1].strip() if len(parts) > 1 else sentiment_response
            except Exception as e_parse_sent:
                logger.warning(
                    f"Could not robustly parse sentiment response for '{headline}': {sentiment_response}. Error: {e_parse_sent}. Storing raw.")
                analysis_payload["sentiment"] = "Error Parsing"
                analysis_payload["sentiment_reasoning"] = sentiment_response
        else:
            analysis_payload["sentiment"] = "AI Error"
            analysis_payload["sentiment_reasoning"] = sentiment_response

        # 2. Detailed Summary & Impact Analysis
        prompt_detailed_analysis = (
            f"News Headline: \"{headline}\"\n"
            f"News Content (may be truncated): \"{content_for_analysis}\"\n\n"
            f"Instructions for Analysis:\n"
            f"1. News Summary: Provide a comprehensive yet concise summary of this news article (3-5 key sentences).\n"
            f"2. Affected Entities: Identify specific companies (with ticker symbols if known and highly relevant) and/or specific industry sectors directly or significantly indirectly affected by this news. Explain why briefly for each.\n"
            f"3. Mechanism of Impact: For the primary affected entities, describe how this news will likely affect their fundamentals (e.g., revenue, costs, market share, customer sentiment) or market perception.\n"
            f"4. Estimated Timing & Duration: Estimate the likely timing (e.g., Immediate, Short-term <3mo, Medium-term 3-12mo, Long-term >1yr) and duration of the impact.\n"
            f"5. Estimated Magnitude & Direction: Estimate the potential magnitude (e.g., Low, Medium, High) and direction (e.g., Positive, Negative, Neutral/Mixed) of the impact on the primary affected entities.\n"
            f"6. Confidence Level: State your confidence (High, Medium, Low) in this overall impact assessment, briefly justifying it (e.g., based on clarity of news, directness of impact).\n"
            f"7. Investor Summary: Provide a final 2-sentence summary specifically for an investor, highlighting the most critical implication or takeaway.\n\n"
            f"Structure your response clearly with headings for each point (e.g., 'News Summary:', 'Affected Entities:', etc.)."
        )

        impact_analysis_response = self.gemini.generate_text(
            prompt_detailed_analysis[:MAX_GEMINI_TEXT_LENGTH])  # Ensure prompt length
        time.sleep(2)  # API courtesy

        if not impact_analysis_response.startswith("Error:"):
            analysis_payload["news_summary_detailed"] = self._parse_ai_section(impact_analysis_response,
                                                                               "News Summary:")
            analysis_payload["potential_impact_on_companies"] = self._parse_ai_section(impact_analysis_response,
                                                                                       ["Affected Entities:",
                                                                                        "Affected Companies:",
                                                                                        "Affected Stocks/Sectors:"])
            # If "Affected Sectors:" is a distinct section, try to get it too.
            sectors_text = self._parse_ai_section(impact_analysis_response, "Affected Sectors:")
            if sectors_text and not sectors_text.startswith("Section not found"):
                analysis_payload["potential_impact_on_sectors"] = sectors_text
            elif analysis_payload["potential_impact_on_companies"] and not analysis_payload.get(
                    "potential_impact_on_sectors"):  # If combined
                analysis_payload["potential_impact_on_sectors"] = analysis_payload["potential_impact_on_companies"]

            analysis_payload["mechanism_of_impact"] = self._parse_ai_section(impact_analysis_response,
                                                                             "Mechanism of Impact:")
            analysis_payload["estimated_timing_duration"] = self._parse_ai_section(impact_analysis_response,
                                                                                   ["Estimated Timing & Duration:",
                                                                                    "Estimated Timing:"])
            analysis_payload["estimated_magnitude_direction"] = self._parse_ai_section(impact_analysis_response, [
                "Estimated Magnitude & Direction:", "Estimated Magnitude/Direction:"])
            analysis_payload["confidence_of_assessment"] = self._parse_ai_section(impact_analysis_response,
                                                                                  "Confidence Level:")
            analysis_payload["summary_for_email"] = self._parse_ai_section(impact_analysis_response,
                                                                           ["Investor Summary:",
                                                                            "Final Summary for Investor:"])
        else:
            logger.error(
                f"Gemini failed to provide detailed impact analysis for '{headline}': {impact_analysis_response}")
            analysis_payload["news_summary_detailed"] = impact_analysis_response  # Store error message

        # Store analysis
        current_analysis_time = datetime.now(timezone.utc)
        news_analysis_entry = NewsEventAnalysis(
            news_event_id=news_event_db.id,
            analysis_date=current_analysis_time,
            sentiment=analysis_payload.get("sentiment"),
            sentiment_reasoning=analysis_payload.get("sentiment_reasoning"),
            news_summary_detailed=analysis_payload.get("news_summary_detailed"),
            potential_impact_on_companies=analysis_payload.get("potential_impact_on_companies"),
            potential_impact_on_sectors=analysis_payload.get("potential_impact_on_sectors"),
            mechanism_of_impact=analysis_payload.get("mechanism_of_impact"),
            estimated_timing_duration=analysis_payload.get("estimated_timing_duration"),
            estimated_magnitude_direction=analysis_payload.get("estimated_magnitude_direction"),
            confidence_of_assessment=analysis_payload.get("confidence_of_assessment"),
            summary_for_email=analysis_payload.get("summary_for_email"),
            key_news_snippets=analysis_payload.get("key_news_snippets")
            # TODO: Add logic to parse tickers/sectors from `potential_impact_on_companies` into `affected_stocks_explicit` JSON fields if needed.
        )
        self.db_session.add(news_analysis_entry)
        news_event_db.last_analyzed_date = current_analysis_time  # Update parent event

        try:
            self.db_session.commit()
            logger.info(
                f"Successfully analyzed and saved news: '{headline[:70]}...' (Analysis ID: {news_analysis_entry.id})")
        except SQLAlchemyError as e:
            self.db_session.rollback()
            logger.error(f"Database error saving news analysis for '{headline[:70]}...': {e}", exc_info=True)
            return None

        return news_analysis_entry

    def _parse_ai_section(self, ai_text, section_header_keywords):
        # This helper is now more robust, moved from IPOAnalyzer to be reusable if needed
        # (though for now it's kept within each class for simplicity of single file changes)
        if not ai_text or ai_text.startswith("Error:"): return "AI Error or No Text"

        if isinstance(section_header_keywords, str):
            keywords_to_check = [section_header_keywords.lower().strip()]
        else:
            keywords_to_check = [k.lower().strip() for k in section_header_keywords]

        lines = ai_text.split('\n')
        capture = False
        section_content = []

        # Define a list of all potential headers that could terminate a section.
        # This helps in accurately capturing multi-line content for the current section.
        all_known_headers_lower_prefixes = [
            "news summary:", "affected entities:", "affected companies:", "affected stocks/sectors:",
            "mechanism of impact:", "estimated timing & duration:", "estimated timing:",
            "estimated magnitude & direction:", "estimated magnitude/direction:",
            "confidence level:", "investor summary:", "final summary for investor:"
        ]  # Add other general section headers from other analyzers if this becomes shared utility.

        for i, line_original in enumerate(lines):
            line_stripped_lower = line_original.strip().lower()

            matched_current_keyword = None
            for kw_lower in keywords_to_check:
                # Check if the line starts with the keyword (potentially followed by a colon)
                if line_stripped_lower.startswith(kw_lower + ":") or line_stripped_lower == kw_lower:
                    matched_current_keyword = kw_lower
                    break

            if matched_current_keyword:
                capture = True
                # Get content on the same line after the header
                content_on_header_line = line_original.strip()[len(matched_current_keyword):].strip()
                if content_on_header_line.startswith(":"):
                    content_on_header_line = content_on_header_line[1:].strip()
                if content_on_header_line:
                    section_content.append(content_on_header_line)
                continue  # Move to next line

            if capture:
                # Check if the current line starts a *different* known section
                is_another_known_header = False
                for known_header_prefix in all_known_headers_lower_prefixes:
                    if line_stripped_lower.startswith(
                            known_header_prefix) and known_header_prefix not in keywords_to_check:
                        is_another_known_header = True
                        break

                if is_another_known_header:
                    break  # End capture for the current section

                section_content.append(line_original)  # Append the original line to preserve formatting

        return "\n".join(section_content).strip() if section_content else "Section not found or empty."

    def run_news_analysis_pipeline(self, category="general", count_to_fetch_from_api=MAX_NEWS_ARTICLES_PER_QUERY,
                                   count_to_analyze_this_run=MAX_NEWS_TO_ANALYZE_PER_RUN):
        fetched_news_items_api = self.fetch_market_news(category=category,
                                                        count_to_fetch_from_api=count_to_fetch_from_api)
        if not fetched_news_items_api:
            logger.info("No news items fetched from API for analysis.")
            self._close_session_if_active()
            return []

        analyzed_news_results = []
        newly_analyzed_count_this_run = 0

        reanalyze_older_than_days = 2  # Re-analyze if analysis is older, or if full text was missing and now found
        reanalyze_threshold_date = datetime.now(timezone.utc) - timedelta(days=reanalyze_older_than_days)

        for news_item_api_data in fetched_news_items_api:
            if newly_analyzed_count_this_run >= count_to_analyze_this_run:
                logger.info(
                    f"Reached analysis limit of {count_to_analyze_this_run} new/re-analyzed items for this run.")
                break

            try:
                news_event_db = self._get_or_create_news_event(
                    news_item_api_data)  # Scrapes/stores full text if new or missing
                if not news_event_db:
                    logger.warning(
                        f"Skipping news item as it could not be fetched or created in DB: {news_item_api_data.get('headline')}")
                    continue

                news_event_db = self._ensure_news_event_is_bound(news_event_db)  # Ensure session attachment
                if not news_event_db: continue

                analysis_needed = False
                # Check if analysis exists and is recent enough
                latest_analysis = None
                if news_event_db.analyses:  # Relationship is a list
                    # Sort by analysis_date descending to get the most recent
                    sorted_analyses = sorted(news_event_db.analyses, key=lambda x: x.analysis_date, reverse=True)
                    if sorted_analyses:
                        latest_analysis = sorted_analyses[0]

                if not latest_analysis:
                    analysis_needed = True
                    logger.info(
                        f"News '{news_event_db.event_title[:50]}...' (ID: {news_event_db.id}) requires new analysis (never analyzed).")
                elif latest_analysis.analysis_date < reanalyze_threshold_date:
                    analysis_needed = True
                    logger.info(
                        f"News '{news_event_db.event_title[:50]}...' (ID: {news_event_db.id}) requires re-analysis (analysis older than {reanalyze_older_than_days} days).")
                elif not news_event_db.full_article_text and latest_analysis:
                    # If it was analyzed but full text was missing (e.g. scraping failed before)
                    # _get_or_create_news_event tries to scrape again. If it succeeds now, we should re-analyze.
                    # The check `if full_article_text_scraped_now and not event.full_article_text:` in _get_or_create_news_event
                    # handles updating the event. If event.full_article_text is now populated, re-analyze.
                    if news_event_db.full_article_text:  # Check if it was successfully populated in this run
                        analysis_needed = True
                        logger.info(
                            f"News '{news_event_db.event_title[:50]}...' (ID: {news_event_db.id}) re-analyzing with newly scraped full text.")
                    else:
                        logger.info(
                            f"News '{news_event_db.event_title[:50]}...' (ID: {news_event_db.id}) already analyzed, full text still unavailable. Skipping re-analysis.")

                if analysis_needed:
                    analysis_result = self.analyze_single_news_item(news_event_db)
                    if analysis_result:
                        analyzed_news_results.append(analysis_result)
                        newly_analyzed_count_this_run += 1
                    time.sleep(3)  # API courtesy delay after each full analysis cycle
                else:
                    logger.info(
                        f"News '{news_event_db.event_title[:50]}...' (ID: {news_event_db.id}) already recently analyzed with available text. Skipping.")

            except Exception as e:
                logger.error(f"Failed to process or analyze news item '{news_item_api_data.get('headline')}': {e}",
                             exc_info=True)
                # Ensure session robustness for the next item
                if self.db_session and not self.db_session.is_active:
                    self.db_session = next(get_db_session())
                elif self.db_session:
                    self.db_session.rollback()  # Rollback current transaction if error occurred within loop item

        logger.info(
            f"News analysis pipeline completed. Newly analyzed/re-analyzed {newly_analyzed_count_this_run} items.")
        self._close_session_if_active()
        return analyzed_news_results

    def _ensure_news_event_session_active(self, news_identifier_for_log):
        if not self.db_session.is_active:
            logger.warning(f"Session for News '{news_identifier_for_log}' was inactive. Re-establishing.")
            self._close_session_if_active()
            self.db_session = next(get_db_session())

    def _ensure_news_event_is_bound(self, news_event_db_obj):
        """Ensures the news_event_db_obj is bound to the current active session."""
        if not news_event_db_obj: return None  # Should not happen

        self._ensure_news_event_session_active(
            news_event_db_obj.event_title[:50] if news_event_db_obj.event_title else 'Unknown News')

        instance_state = sa_inspect(news_event_db_obj)
        if not instance_state.session or instance_state.session is not self.db_session:
            logger.warning(
                f"NewsEvent DB entry '{news_event_db_obj.event_title[:50]}...' (ID: {news_event_db_obj.id if instance_state.has_identity else 'Transient'}) is not bound to current session. Merging.")
            try:
                if not instance_state.has_identity and news_event_db_obj.id is None:
                    # Try to find by URL if it's a new object that might already exist in this session's view due to prior ops
                    existing_in_session = self.db_session.query(NewsEvent).filter_by(
                        source_url=news_event_db_obj.source_url).first()
                    if existing_in_session:
                        news_event_db_obj = existing_in_session
                        logger.info(
                            f"Replaced transient NewsEvent for '{news_event_db_obj.source_url}' with instance from current session.")
                        return news_event_db_obj

                merged_event = self.db_session.merge(news_event_db_obj)
                logger.info(
                    f"Successfully merged/re-associated NewsEvent '{merged_event.event_title[:50]}...' (ID: {merged_event.id}) into current session.")
                return merged_event
            except Exception as e_merge:
                logger.error(
                    f"Failed to merge NewsEvent '{news_event_db_obj.event_title[:50]}...' into session: {e_merge}. Re-fetching as fallback.",
                    exc_info=True)
                fallback_event = None
                if instance_state.has_identity and news_event_db_obj.id:
                    fallback_event = self.db_session.query(NewsEvent).get(news_event_db_obj.id)
                elif news_event_db_obj.source_url:  # Try by unique URL
                    fallback_event = self.db_session.query(NewsEvent).filter_by(
                        source_url=news_event_db_obj.source_url).first()

                if not fallback_event:
                    logger.critical(
                        f"CRITICAL: Failed to re-associate NewsEvent '{news_event_db_obj.event_title[:50]}...' with current session after merge failure and could not re-fetch.")
                    return None  # Indicate critical failure
                logger.info(
                    f"Successfully re-fetched NewsEvent '{fallback_event.event_title[:50]}...' after merge failure.")
                return fallback_event
        return news_event_db_obj


if __name__ == '__main__':
    from database import init_db

    # init_db() # Ensure DB is initialized with new NewsEvent/Analysis model fields if changed

    logger.info("Starting standalone news analysis pipeline test...")
    analyzer = NewsAnalyzer()
    # Analyze a few new general news items
    results = analyzer.run_news_analysis_pipeline(category="general", count_to_fetch_from_api=10,
                                                  count_to_analyze_this_run=3)
    if results:
        logger.info(f"Pipeline processed {len(results)} news items this run.")
        for res_idx, res in enumerate(results):
            if hasattr(res, 'news_event') and res.news_event:  # Check if result object is valid
                logger.info(f"--- Result {res_idx + 1} ---")
                logger.info(f"News: {res.news_event.event_title[:100]}...")
                logger.info(f"Source: {res.news_event.source_url}")
                logger.info(f"Sentiment: {res.sentiment} - Reasoning: {res.sentiment_reasoning[:100]}...")
                logger.info(f"Investor Summary: {res.summary_for_email}")
                logger.info(f"Full Article Scraped: {'Yes' if res.news_event.full_article_text else 'No'}")
                if res.news_event.full_article_text:
                    logger.debug(f"Full Article Snippet: {res.news_event.full_article_text[:200]}...")
            else:
                logger.warning(
                    f"Processed news result item missing 'news_event' attribute or news_event is None. Result: {res}")
    else:
        logger.info("No new news items were processed in this run.")
---------- END news_analyzer.py ----------


---------- project_structure.py ----------
# project_to_file_backend.py
import os
import sys
from pathlib import Path

# --- Configuration ---

# Directories to exclude from both structure and content
EXCLUDED_DIRS = [
    '__pycache__',
    '.git',
    '.venv',        # Common virtual environment names
    'venv',
    'env',
    '.env',         # Exclude the .env file itself if it contains secrets
    'build',
    'dist',
    '*.egg-info',   # Python packaging artifacts
    '.pytest_cache',
    '.mypy_cache',
    '.vscode',
    '.idea',
    'node_modules', # If you happen to have node_modules
    'migrations',   # Often contains auto-generated code, review if needed
    'alembic'       # Alembic directory
]

# Specific files to exclude
EXCLUDED_FILES = [
    '.DS_Store',
    '*.pyc',
    '*.pyo',
    '*.pyd',
    '.env',         # Explicitly exclude again just in case
    'project_to_file_backend.py', # Exclude this script itself
    'project_structure_backend.txt', # Exclude the output file
    # Add any other specific files like local config overrides
]

# File extensions to treat as binary/media (content won't be included)
# Add more as needed (e.g., .jpg, .gif, .mp4, .db, .sqlite)
BINARY_EXTENSIONS = [
    '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.webp', '.ico',
    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
    '.zip', '.tar', '.gz', '.rar',
    '.mp3', '.wav', '.ogg',
    '.mp4', '.avi', '.mov', '.wmv',
    '.db', '.sqlite', '.sqlite3',
    '.pkl', '.joblib',
    '.pt', '.pth', '.onnx', # Model files
]

# Output file name
OUTPUT_FILE = 'project_structure_backend.txt'

# --- Script Logic ---

file_structure_tree = ""
file_contents = ""

def should_exclude(path: Path) -> bool:
    """Check if a path should be excluded based on configured lists."""
    # Check against excluded directory names
    if any(part in EXCLUDED_DIRS for part in path.parts):
        return True
    # Check against excluded directory patterns (like *.egg-info)
    if any(path.match(pattern) for pattern in EXCLUDED_DIRS if '*' in pattern):
         return True
    # Check against excluded file names and patterns
    if path.name in EXCLUDED_FILES:
        return True
    if any(path.match(pattern) for pattern in EXCLUDED_FILES if '*' in pattern):
        return True

    return False

def traverse_directory(dir_path: Path, indent: str = ''):
    """Recursively traverses directories and builds the structure/content strings."""
    global file_structure_tree
    global file_contents

    try:
        # Sort entries for consistent ordering: directories first, then files
        entries = sorted(list(dir_path.iterdir()), key=lambda p: (p.is_file(), p.name.lower()))
    except PermissionError:
        print(f"Warning: Permission denied for directory '{dir_path}'. Skipping.")
        return
    except FileNotFoundError:
         print(f"Warning: Directory '{dir_path}' not found during traversal (might have been deleted). Skipping.")
         return


    for entry in entries:
        if should_exclude(entry):
            continue

        if entry.is_dir():
            file_structure_tree += f"{indent}{entry.name}/\n"
            traverse_directory(entry, indent + '  ')
        elif entry.is_file():
            file_structure_tree += f"{indent}{entry.name}\n"
            file_extension = entry.suffix.lower()

            # Add separators for all files
            file_contents += f"\n---------- {entry.name} ----------\n"
            if file_extension in BINARY_EXTENSIONS:
                file_contents += f"(Binary file type {file_extension} - content not included)\n"
            else:
                try:
                    # Try reading with UTF-8 first
                    with entry.open('r', encoding='utf-8') as f:
                        file_contents += f.read() + "\n"
                except UnicodeDecodeError:
                    try:
                        # Fallback to latin-1 if UTF-8 fails
                        with entry.open('r', encoding='latin-1') as f:
                            file_contents += f.read() + "\n"
                        file_contents += "(Warning: Read file using latin-1 encoding due to UTF-8 decode error)\n"
                    except Exception as read_error_fallback:
                         file_contents += f"ERROR READING FILE (Fallback Failed): {read_error_fallback}\n"
                except Exception as read_error:
                    file_contents += f"ERROR READING FILE: {read_error}\n"

            file_contents += f"---------- END {entry.name} ----------\n\n"


def generate_project_structure_and_content(project_root: Path, output_file_path: Path):
    """Generates the combined structure and content file."""
    global file_structure_tree
    global file_contents
    file_structure_tree = "" # Reset global state
    file_contents = ""     # Reset global state

    print(f"Starting traversal from: {project_root}")
    traverse_directory(project_root)

    full_output = f"--- START OF FILE {output_file_path.name} ---\n\n"
    full_output += file_structure_tree
    full_output += "\n" # Separator between tree and content
    full_output += file_contents
    full_output += f"--- END OF FILE {output_file_path.name} ---\n"


    try:
        with output_file_path.open('w', encoding='utf-8') as f:
            f.write(full_output)
        print(f"Project structure and content written to '{output_file_path}'")
    except IOError as e:
        print(f"Error writing to output file '{output_file_path}': {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"An unexpected error occurred during file writing: {e}", file=sys.stderr)
        sys.exit(1)

# --- Main Execution ---
if __name__ == "__main__":
    project_root_path = Path(os.getcwd()) # Get current working directory as Path object
    output_path = project_root_path / OUTPUT_FILE

    print(f"Project Root: {project_root_path}")
    print(f"Output File: {output_path}")

    if not project_root_path.is_dir():
        print(f"Error: Project root directory '{project_root_path}' not found or is not a directory.", file=sys.stderr)
        sys.exit(1)

    try:
        generate_project_structure_and_content(project_root_path, output_path)
    except Exception as e:
        print(f"\nAn unexpected error occurred during execution: {e}", file=sys.stderr)
        sys.exit(1)
---------- END project_structure.py ----------


---------- requirements.txt ----------
# requirements.txt
sqlalchemy>=1.4,<2.0
requests>=2.32.0
psycopg2-binary>=2.8.0
pandas>=1.0.0
markdown2>=2.4.0
beautifulsoup4>=4.9.3
lxml>=4.6.3 
# Add other specific versions if needed
# python-dotenv # For managing environment variables if you choose to use .env files
---------- END requirements.txt ----------


---------- stock_analyzer.py ----------
# stock_analyzer.py
import pandas as pd
from sqlalchemy import inspect as sa_inspect
from datetime import datetime, timezone, timedelta
import math  # For DCF calculations
import time  # For API courtesy delays

from api_clients import (
    FinnhubClient, FinancialModelingPrepClient,
    EODHDClient, GeminiAPIClient, SECEDGARClient, extract_S1_text_sections  # Added SECEDGARClient and helper
)
from database import SessionLocal, get_db_session
from models import Stock, StockAnalysis
from error_handler import logger
from sqlalchemy.exc import SQLAlchemyError
from config import (
    STOCK_FINANCIAL_YEARS, DEFAULT_DISCOUNT_RATE,
    DEFAULT_PERPETUAL_GROWTH_RATE, DEFAULT_FCF_PROJECTION_YEARS,
    TEN_K_KEY_SECTIONS, MAX_10K_SECTION_LENGTH_FOR_GEMINI,
    MAX_GEMINI_TEXT_LENGTH
)


# Helper function to safely get a numeric value from a dictionary
def safe_get_float(data_dict, key, default=None):
    val = data_dict.get(key)
    if val is None: return default
    try:
        return float(val)
    except (ValueError, TypeError):
        return default


# Helper function for CAGR calculation
def calculate_cagr(end_value, start_value, years):
    if start_value is None or end_value is None or years <= 0: return None
    if start_value == 0: return None  # Avoid division by zero
    # Handle cases where start_value might be negative (e.g. for EPS)
    if start_value < 0:
        if end_value > 0:  # From loss to profit, CAGR is not straightforwardly meaningful
            return None
        elif end_value < 0:  # Both negative, growth of negative numbers (lessening loss is positive)
            return -((end_value / start_value) ** (
                        1 / years) - 1) if end_value != 0 else None  # avoid division by zero if end_value is 0
        else:  # end_value is 0 from negative start
            return 1.0  # 100% growth to zero loss
    if end_value < 0 and start_value > 0:  # From profit to loss
        return None  # Meaningful positive CAGR not possible

    # Standard CAGR for positive start/end values
    return ((end_value / start_value) ** (1 / years)) - 1


class StockAnalyzer:
    def __init__(self, ticker):
        self.ticker = ticker.upper()
        self.finnhub = FinnhubClient()
        self.fmp = FinancialModelingPrepClient()
        self.eodhd = EODHDClient()
        self.gemini = GeminiAPIClient()
        self.sec_edgar = SECEDGARClient()

        self.db_session = next(get_db_session())
        self.stock_db_entry = None
        self._financial_data_cache = {}

        try:
            self._get_or_create_stock_entry()
        except Exception as e:
            logger.error(f"CRITICAL: Failed during _get_or_create_stock_entry for {self.ticker}: {e}", exc_info=True)
            self._close_session_if_active()
            raise RuntimeError(f"StockAnalyzer for {self.ticker} could not be initialized due to DB/API issues.")

    def _close_session_if_active(self):
        if self.db_session and self.db_session.is_active:
            try:
                self.db_session.close()
                logger.debug(f"DB session closed for {self.ticker} in StockAnalyzer.")
            except Exception as e_close:
                logger.warning(f"Error closing session for {self.ticker} in StockAnalyzer: {e_close}")

    def _get_or_create_stock_entry(self):
        if not self.db_session.is_active:
            logger.warning(f"Session for {self.ticker} in _get_or_create_stock_entry was inactive. Re-establishing.")
            self._close_session_if_active()
            self.db_session = next(get_db_session())

        self.stock_db_entry = self.db_session.query(Stock).filter_by(ticker=self.ticker).first()

        company_name_from_api = None
        industry_from_api = None
        sector_from_api = None
        cik_from_api = None

        profile_fmp_list = self.fmp.get_company_profile(self.ticker)  # API Call
        profile_fmp_data = None
        if profile_fmp_list and isinstance(profile_fmp_list, list) and profile_fmp_list[0]:
            profile_fmp_data = profile_fmp_list[0]
            self._financial_data_cache['profile_fmp'] = profile_fmp_data
            company_name_from_api = profile_fmp_data.get('companyName')
            industry_from_api = profile_fmp_data.get('industry')
            sector_from_api = profile_fmp_data.get('sector')
            cik_from_api = profile_fmp_data.get('cik')
            logger.info(f"Fetched profile from FMP for {self.ticker}.")
        else:
            logger.warning(f"FMP profile fetch failed or empty for {self.ticker}. Trying Finnhub.")
            profile_finnhub = self.finnhub.get_company_profile2(self.ticker)  # API Call
            if profile_finnhub:
                self._financial_data_cache['profile_finnhub'] = profile_finnhub
                company_name_from_api = profile_finnhub.get('name')
                industry_from_api = profile_finnhub.get('finnhubIndustry')
                logger.info(f"Fetched profile from Finnhub for {self.ticker}.")
            else:
                logger.warning(f"Failed to fetch profile from FMP and Finnhub for {self.ticker}.")

        if not company_name_from_api:
            company_name_from_api = self.ticker
            logger.info(f"Using ticker '{self.ticker}' as company name due to lack of API data.")

        if not cik_from_api and self.ticker:
            logger.info(f"CIK not found from FMP/Finnhub profile for {self.ticker}. Querying SEC EDGAR.")
            cik_from_api = self.sec_edgar.get_cik_by_ticker(self.ticker)  # API Call
            if cik_from_api:
                logger.info(f"Fetched CIK {cik_from_api} from SEC EDGAR for {self.ticker}.")
            else:
                logger.warning(f"Could not fetch CIK from SEC EDGAR for {self.ticker}.")

        if not self.stock_db_entry:
            logger.info(f"Stock {self.ticker} not found in DB, creating new entry.")
            self.stock_db_entry = Stock(
                ticker=self.ticker,
                company_name=company_name_from_api,
                industry=industry_from_api,
                sector=sector_from_api,
                cik=cik_from_api
            )
            self.db_session.add(self.stock_db_entry)
            try:
                self.db_session.commit()
                self.db_session.refresh(self.stock_db_entry)
                logger.info(
                    f"Created and refreshed stock entry for {self.ticker} (ID: {self.stock_db_entry.id}). Name: {self.stock_db_entry.company_name}, CIK: {self.stock_db_entry.cik}")
            except SQLAlchemyError as e:
                self.db_session.rollback()
                logger.error(f"Error creating stock entry for {self.ticker}: {e}", exc_info=True)
                raise
        else:
            logger.info(
                f"Found existing stock entry for {self.ticker} (ID: {self.stock_db_entry.id}). Current DB CIK: {self.stock_db_entry.cik}")
            updated = False
            if company_name_from_api and self.stock_db_entry.company_name != company_name_from_api:
                logger.info(
                    f"Updating company name for {self.ticker} from '{self.stock_db_entry.company_name}' to '{company_name_from_api}'.")
                self.stock_db_entry.company_name = company_name_from_api
                updated = True
            if industry_from_api and self.stock_db_entry.industry != industry_from_api:
                self.stock_db_entry.industry = industry_from_api
                updated = True
            if sector_from_api and self.stock_db_entry.sector != sector_from_api:
                self.stock_db_entry.sector = sector_from_api
                updated = True
            if cik_from_api and self.stock_db_entry.cik != cik_from_api:
                logger.info(f"Updating CIK for {self.ticker} from '{self.stock_db_entry.cik}' to '{cik_from_api}'.")
                self.stock_db_entry.cik = cik_from_api
                updated = True

            if updated:
                try:
                    self.db_session.commit()
                    self.db_session.refresh(self.stock_db_entry)
                    logger.info(f"Successfully updated stock entry for {self.ticker} in DB.")
                except SQLAlchemyError as e:
                    self.db_session.rollback()
                    logger.error(f"Error updating stock entry for {self.ticker} in DB: {e}")

    def _fetch_financial_statements(self):
        logger.info(f"Fetching financial statements for {self.ticker} for the last {STOCK_FINANCIAL_YEARS} years.")
        statements = {"income": [], "balance": [], "cashflow": []}

        try:
            income_annual_fmp = self.fmp.get_financial_statements(self.ticker, "income-statement", period="annual",
                                                                  limit=STOCK_FINANCIAL_YEARS)
            balance_annual_fmp = self.fmp.get_financial_statements(self.ticker, "balance-sheet-statement",
                                                                   period="annual", limit=STOCK_FINANCIAL_YEARS)
            cashflow_annual_fmp = self.fmp.get_financial_statements(self.ticker, "cash-flow-statement", period="annual",
                                                                    limit=STOCK_FINANCIAL_YEARS)

            if income_annual_fmp: statements["income"].extend(income_annual_fmp)
            if balance_annual_fmp: statements["balance"].extend(balance_annual_fmp)
            if cashflow_annual_fmp: statements["cashflow"].extend(cashflow_annual_fmp)

            income_quarterly_fmp = self.fmp.get_financial_statements(self.ticker, "income-statement",
                                                                     period="quarterly", limit=8)
            if income_quarterly_fmp is None:  # API call failed (e.g. 403)
                logger.warning(
                    f"Failed to fetch FMP quarterly income statements for {self.ticker} (likely subscription issue). QoQ analysis will be limited.")
                self._financial_data_cache['income_quarterly_fmp'] = []
            else:
                self._financial_data_cache['income_quarterly_fmp'] = income_quarterly_fmp

            logger.info(
                f"Fetched from FMP: {len(statements['income'])} income, {len(statements['balance'])} balance, {len(statements['cashflow'])} cashflow annual statements. {len(self._financial_data_cache.get('income_quarterly_fmp', []))} quarterly income statements.")

        except Exception as e:
            logger.warning(
                f"Generic error during FMP financial statements fetch for {self.ticker}: {e}. This may limit trend analysis.",
                exc_info=True)

        self._financial_data_cache['financial_statements'] = statements
        return statements

    def _fetch_key_metrics_and_profile_data(self):
        logger.info(f"Fetching key metrics and profile for {self.ticker}.")

        key_metrics_annual_fmp = self.fmp.get_key_metrics(self.ticker, period="annual", limit=STOCK_FINANCIAL_YEARS + 2)
        key_metrics_quarterly_fmp = self.fmp.get_key_metrics(self.ticker, period="quarterly", limit=8)

        self._financial_data_cache['key_metrics_annual_fmp'] = key_metrics_annual_fmp or []
        if key_metrics_quarterly_fmp is None:  # API call failed (e.g. 403)
            logger.warning(
                f"Failed to fetch FMP quarterly key metrics for {self.ticker} (likely subscription issue). Latest metrics might rely on annual or Finnhub.")
            self._financial_data_cache['key_metrics_quarterly_fmp'] = []
        else:
            self._financial_data_cache['key_metrics_quarterly_fmp'] = key_metrics_quarterly_fmp

        basic_financials_finnhub = self.finnhub.get_basic_financials(self.ticker)
        self._financial_data_cache['basic_financials_finnhub'] = basic_financials_finnhub or {}

        if 'profile_fmp' not in self._financial_data_cache or not self._financial_data_cache.get(
                'profile_fmp'):  # If not fetched during init
            profile_fmp_list = self.fmp.get_company_profile(self.ticker)
            self._financial_data_cache['profile_fmp'] = profile_fmp_list[0] if profile_fmp_list and isinstance(
                profile_fmp_list, list) else {}

        logger.info(
            f"Fetched Key Metrics from FMP (Annual: {len(self._financial_data_cache['key_metrics_annual_fmp'])}, Quarterly: {len(self._financial_data_cache['key_metrics_quarterly_fmp'])}). Fetched Finnhub Basic Financials.")

    def _calculate_derived_metrics(self):
        logger.info(f"Calculating derived metrics for {self.ticker}...")
        metrics = {"key_metrics_snapshot": {}}
        statements = self._financial_data_cache.get('financial_statements',
                                                    {"income": [], "balance": [], "cashflow": []})
        key_metrics_annual = self._financial_data_cache.get('key_metrics_annual_fmp', [])
        key_metrics_quarterly = self._financial_data_cache.get('key_metrics_quarterly_fmp', [])
        basic_fin_finnhub_data = self._financial_data_cache.get('basic_financials_finnhub', {})
        basic_fin_finnhub = basic_fin_finnhub_data.get('metric',
                                                       {}) if basic_fin_finnhub_data else {}  # Ensure 'metric' key exists
        profile_fmp = self._financial_data_cache.get('profile_fmp', {})

        latest_km_q = key_metrics_quarterly[0] if key_metrics_quarterly else {}
        latest_km_a = key_metrics_annual[0] if key_metrics_annual else {}

        metrics["pe_ratio"] = safe_get_float(latest_km_q, "peRatioTTM") or safe_get_float(latest_km_a,
                                                                                          "peRatio") or safe_get_float(
            basic_fin_finnhub, "peTTM") or safe_get_float(basic_fin_finnhub, "peAnnual")
        metrics["pb_ratio"] = safe_get_float(latest_km_q, "priceToBookRatioTTM") or safe_get_float(latest_km_a,
                                                                                                   "pbRatio") or safe_get_float(
            basic_fin_finnhub, "pbAnnual")
        metrics["ps_ratio"] = safe_get_float(latest_km_q, "priceToSalesRatioTTM") or safe_get_float(latest_km_a,
                                                                                                    "priceSalesRatio")
        metrics["ev_to_sales"] = safe_get_float(latest_km_q, "enterpriseValueOverRevenueTTM") or safe_get_float(
            latest_km_a, "enterpriseValueOverRevenue")
        metrics["ev_to_ebitda"] = safe_get_float(latest_km_q, "evToEbitdaTTM") or safe_get_float(latest_km_a,
                                                                                                 "evToEbitda")

        div_yield_fmp_q = safe_get_float(latest_km_q, "dividendYieldTTM")
        div_yield_fmp_a = safe_get_float(latest_km_a, "dividendYield")
        div_yield_finnhub_val = safe_get_float(basic_fin_finnhub, "dividendYieldAnnual")
        if div_yield_finnhub_val is not None: div_yield_finnhub_val = div_yield_finnhub_val / 100
        metrics["dividend_yield"] = div_yield_fmp_q or div_yield_fmp_a or div_yield_finnhub_val

        metrics["key_metrics_snapshot"]["FMP_peRatioTTM"] = metrics["pe_ratio"]
        metrics["key_metrics_snapshot"]["FMP_pbRatioTTM"] = metrics["pb_ratio"]

        income_annual = sorted([s for s in statements.get("income", []) if s], key=lambda x: x.get("date"),
                               reverse=True)
        balance_annual = sorted([s for s in statements.get("balance", []) if s], key=lambda x: x.get("date"),
                                reverse=True)
        cashflow_annual = sorted([s for s in statements.get("cashflow", []) if s], key=lambda x: x.get("date"),
                                 reverse=True)

        if income_annual:
            latest_income_a = income_annual[0]
            metrics["eps"] = safe_get_float(latest_income_a, "eps") or safe_get_float(latest_km_a, "eps")
            metrics["net_profit_margin"] = safe_get_float(latest_income_a, "netProfitMargin")
            metrics["gross_profit_margin"] = safe_get_float(latest_income_a, "grossProfitMargin")
            metrics["operating_profit_margin"] = safe_get_float(latest_income_a, "operatingIncomeRatio")

            ebit = safe_get_float(latest_income_a, "operatingIncome")
            interest_expense = safe_get_float(latest_income_a, "interestExpense")
            if ebit is not None and interest_expense is not None and abs(
                    interest_expense) > 1e-6:  # Avoid division by near-zero
                metrics["interest_coverage_ratio"] = ebit / abs(interest_expense)

        if balance_annual:
            latest_balance_a = balance_annual[0]
            total_equity = safe_get_float(latest_balance_a, "totalStockholdersEquity")
            total_assets = safe_get_float(latest_balance_a, "totalAssets")
            latest_net_income = safe_get_float(income_annual[0], "netIncome") if income_annual else None

            if total_equity and total_equity != 0 and latest_net_income is not None:
                metrics["roe"] = latest_net_income / total_equity

            if total_assets and total_assets != 0 and latest_net_income is not None:
                metrics["roa"] = latest_net_income / total_assets

            metrics["debt_to_equity"] = safe_get_float(latest_balance_a, "debtToEquity") or safe_get_float(latest_km_a,
                                                                                                           "debtToEquity")
            if metrics["debt_to_equity"] is None:
                total_debt_val = safe_get_float(latest_balance_a, "totalDebt")
                if total_debt_val is not None and total_equity and total_equity != 0:
                    metrics["debt_to_equity"] = total_debt_val / total_equity

            current_assets = safe_get_float(latest_balance_a, "totalCurrentAssets")
            current_liabilities = safe_get_float(latest_balance_a, "totalCurrentLiabilities")
            if current_assets is not None and current_liabilities is not None and current_liabilities != 0:
                metrics["current_ratio"] = current_assets / current_liabilities

            cash_equivalents = safe_get_float(latest_balance_a, "cashAndCashEquivalents", 0.0)
            short_term_investments = safe_get_float(latest_balance_a, "shortTermInvestments", 0.0)
            accounts_receivable = safe_get_float(latest_balance_a, "netReceivables", 0.0)
            if current_liabilities is not None and current_liabilities != 0:
                metrics["quick_ratio"] = (
                                                     cash_equivalents + short_term_investments + accounts_receivable) / current_liabilities

        ebitda_for_debt_ratio = safe_get_float(latest_km_a, "ebitda")
        if not ebitda_for_debt_ratio and income_annual:
            ebitda_for_debt_ratio = safe_get_float(income_annual[0], "ebitda")

        if ebitda_for_debt_ratio and ebitda_for_debt_ratio != 0 and balance_annual:
            total_debt_val = safe_get_float(balance_annual[0], "totalDebt")
            if total_debt_val is not None:
                metrics["debt_to_ebitda"] = total_debt_val / ebitda_for_debt_ratio

        def get_value_from_statement_list(data_list, field, year_offset=0):  # More robust
            if data_list and len(data_list) > year_offset and data_list[year_offset]:
                return safe_get_float(data_list[year_offset], field)
            return None

        metrics["revenue_growth_yoy"] = calculate_growth(get_value_from_statement_list(income_annual, "revenue", 0),
                                                         get_value_from_statement_list(income_annual, "revenue", 1))
        metrics["eps_growth_yoy"] = calculate_growth(get_value_from_statement_list(income_annual, "eps", 0),
                                                     get_value_from_statement_list(income_annual, "eps", 1))

        if len(income_annual) >= 3:
            metrics["revenue_growth_cagr_3yr"] = calculate_cagr(
                get_value_from_statement_list(income_annual, "revenue", 0),
                get_value_from_statement_list(income_annual, "revenue", 2), 2)
            metrics["eps_growth_cagr_3yr"] = calculate_cagr(get_value_from_statement_list(income_annual, "eps", 0),
                                                            get_value_from_statement_list(income_annual, "eps", 2), 2)
        if len(income_annual) >= 5:  # Ensure 5 actual years of data, index 4 means 5th year
            metrics["revenue_growth_cagr_5yr"] = calculate_cagr(
                get_value_from_statement_list(income_annual, "revenue", 0),
                get_value_from_statement_list(income_annual, "revenue", 4), 4)
            metrics["eps_growth_cagr_5yr"] = calculate_cagr(get_value_from_statement_list(income_annual, "eps", 0),
                                                            get_value_from_statement_list(income_annual, "eps", 4), 4)

        income_quarterly = self._financial_data_cache.get('income_quarterly_fmp', [])
        if len(income_quarterly) >= 2:
            metrics["revenue_growth_qoq"] = calculate_growth(
                get_value_from_statement_list(income_quarterly, "revenue", 0),
                get_value_from_statement_list(income_quarterly, "revenue", 1))

        if cashflow_annual:
            fcf = get_value_from_statement_list(cashflow_annual, "freeCashFlow", 0)

            # Get shares outstanding: FMP profile "sharesOutstanding" or mktCap / price
            shares_outstanding_fcf = safe_get_float(profile_fmp, "sharesOutstanding")
            if not shares_outstanding_fcf:
                mkt_cap = safe_get_float(profile_fmp, "mktCap")
                price = safe_get_float(profile_fmp, "price")
                if mkt_cap and price and price != 0:
                    shares_outstanding_fcf = mkt_cap / price

            if fcf is not None and shares_outstanding_fcf and shares_outstanding_fcf != 0:
                metrics["free_cash_flow_per_share"] = fcf / shares_outstanding_fcf
                mkt_cap_for_yield = safe_get_float(profile_fmp, "mktCap")
                if mkt_cap_for_yield and mkt_cap_for_yield != 0:
                    metrics["free_cash_flow_yield"] = fcf / mkt_cap_for_yield

            if len(cashflow_annual) >= 3:
                fcf_curr = get_value_from_statement_list(cashflow_annual, "freeCashFlow", 0)
                fcf_prev1 = get_value_from_statement_list(cashflow_annual, "freeCashFlow", 1)
                fcf_prev2 = get_value_from_statement_list(cashflow_annual, "freeCashFlow", 2)
                if all(isinstance(x, (int, float)) for x in [fcf_curr, fcf_prev1, fcf_prev2] if
                       x is not None):  # Check for None too
                    if fcf_curr > fcf_prev1 > fcf_prev2:
                        metrics["free_cash_flow_trend"] = "Growing"
                    elif fcf_curr < fcf_prev1 < fcf_prev2:
                        metrics["free_cash_flow_trend"] = "Declining"
                    else:
                        metrics["free_cash_flow_trend"] = "Mixed/Stable"
            else:
                metrics["free_cash_flow_trend"] = "Data N/A (needs 3+ years)"

        if len(balance_annual) >= 3:
            re_curr = get_value_from_statement_list(balance_annual, "retainedEarnings", 0)
            re_prev1 = get_value_from_statement_list(balance_annual, "retainedEarnings", 1)
            re_prev2 = get_value_from_statement_list(balance_annual, "retainedEarnings", 2)
            if all(isinstance(x, (int, float)) for x in [re_curr, re_prev1, re_prev2] if x is not None):
                if re_curr > re_prev1 > re_prev2:
                    metrics["retained_earnings_trend"] = "Growing"
                elif re_curr < re_prev1 < re_prev2:
                    metrics["retained_earnings_trend"] = "Declining"
                else:
                    metrics["retained_earnings_trend"] = "Mixed/Stable"
        else:
            metrics["retained_earnings_trend"] = "Data N/A (needs 3+ years)"

        if income_annual and balance_annual:
            ebit = get_value_from_statement_list(income_annual, "operatingIncome", 0)
            tax_provision = get_value_from_statement_list(income_annual, "incomeTaxExpense", 0)
            income_before_tax = get_value_from_statement_list(income_annual, "incomeBeforeTax", 0)

            effective_tax_rate = (
                        tax_provision / income_before_tax) if income_before_tax and tax_provision and income_before_tax != 0 else 0.21
            nopat = ebit * (1 - effective_tax_rate) if ebit is not None else None

            total_debt = get_value_from_statement_list(balance_annual, "totalDebt", 0)
            total_equity = get_value_from_statement_list(balance_annual, "totalStockholdersEquity", 0)
            cash_and_equivalents = get_value_from_statement_list(balance_annual, "cashAndCashEquivalents", 0) or 0.0

            if total_debt is not None and total_equity is not None:  # Ensure they are not None
                invested_capital = total_debt + total_equity - cash_and_equivalents
                if nopat is not None and invested_capital is not None and invested_capital != 0:
                    metrics["roic"] = nopat / invested_capital

        final_metrics = {}
        for k, v in metrics.items():
            if k == "key_metrics_snapshot":
                final_metrics[k] = {sk: sv for sk, sv in v.items() if sv is not None and not (
                            isinstance(sv, float) and (math.isnan(sv) or math.isinf(sv)))}
            elif isinstance(v, (int, float)) and not math.isnan(v) and not math.isinf(v):
                final_metrics[k] = v
            elif isinstance(v, str) and v != "N/A":
                final_metrics[k] = v
            else:
                final_metrics[k] = None

        logger.info(
            f"Calculated metrics for {self.ticker}: { {k: v for k, v in final_metrics.items() if k != 'key_metrics_snapshot'} }")
        self._financial_data_cache['calculated_metrics'] = final_metrics
        return final_metrics

    def _perform_dcf_analysis(self):
        logger.info(f"Performing simplified DCF analysis for {self.ticker}...")
        dcf_results = {
            "dcf_intrinsic_value": None, "dcf_upside_percentage": None,
            "dcf_assumptions": {
                "discount_rate": DEFAULT_DISCOUNT_RATE,
                "perpetual_growth_rate": DEFAULT_PERPETUAL_GROWTH_RATE,
                "projection_years": DEFAULT_FCF_PROJECTION_YEARS,
                "start_fcf": None, "fcf_growth_rates_projection": []
            }
        }

        cashflow_annual = self._financial_data_cache.get('financial_statements', {}).get('cashflow', [])
        # Sort cashflow_annual by date descending to ensure cashflow_annual[0] is the latest.
        cashflow_annual = sorted([s for s in cashflow_annual if s], key=lambda x: x.get("date"), reverse=True)

        profile_fmp = self._financial_data_cache.get('profile_fmp', {})
        calculated_metrics = self._financial_data_cache.get('calculated_metrics', {})

        current_price = safe_get_float(profile_fmp, "price")
        # Determine shares outstanding carefully: FMP profile "sharesOutstanding" or from mktCap / price
        shares_outstanding = safe_get_float(profile_fmp, "sharesOutstanding")
        if not shares_outstanding:
            mkt_cap = safe_get_float(profile_fmp, "mktCap")
            if mkt_cap and current_price and current_price != 0:
                shares_outstanding = mkt_cap / current_price

        if not cashflow_annual or not profile_fmp or current_price is None or shares_outstanding is None or shares_outstanding == 0:
            logger.warning(
                f"Insufficient data for DCF for {self.ticker}: FCF history ({len(cashflow_annual)} years), current price ({current_price}), or shares outstanding ({shares_outstanding}) missing/invalid.")
            return dcf_results

        current_fcf = get_value_from_statement_list(cashflow_annual, "freeCashFlow", 0)
        if current_fcf is None or current_fcf <= 10000:  # Require FCF > 10k to be meaningful for DCF
            logger.warning(
                f"Current FCF for {self.ticker} is {current_fcf}. Simplified DCF requires positive & significant starting FCF.")
            return dcf_results

        dcf_results["dcf_assumptions"]["start_fcf"] = current_fcf

        fcf_growth_hist_3yr = None
        if len(cashflow_annual) >= 4:  # Need 4 years for 3 growth periods
            fcf_start_3yr = get_value_from_statement_list(cashflow_annual, "freeCashFlow",
                                                          3)  # 4th element (index 3) for 3-year period
            fcf_end_3yr = current_fcf  # current_fcf is from index 0
            if fcf_start_3yr and fcf_start_3yr > 0:
                fcf_growth_hist_3yr = calculate_cagr(fcf_end_3yr, fcf_start_3yr, 3)

        fcf_growth_initial = fcf_growth_hist_3yr \
                             or calculated_metrics.get("revenue_growth_cagr_3yr") \
                             or calculated_metrics.get("revenue_growth_yoy") \
                             or 0.05
        if not isinstance(fcf_growth_initial, (int, float)): fcf_growth_initial = 0.05  # Ensure it's a number

        fcf_growth_initial = min(fcf_growth_initial, 0.20)
        fcf_growth_initial = max(fcf_growth_initial, -0.10)

        projected_fcfs = []
        last_projected_fcf = current_fcf

        growth_decline_rate = (
                                          fcf_growth_initial - DEFAULT_PERPETUAL_GROWTH_RATE) / DEFAULT_FCF_PROJECTION_YEARS if DEFAULT_FCF_PROJECTION_YEARS > 0 else 0

        for i in range(DEFAULT_FCF_PROJECTION_YEARS):
            current_year_growth_rate = fcf_growth_initial - (growth_decline_rate * i)
            current_year_growth_rate = max(current_year_growth_rate, DEFAULT_PERPETUAL_GROWTH_RATE)

            projected_fcf = last_projected_fcf * (1 + current_year_growth_rate)
            projected_fcfs.append(projected_fcf)
            last_projected_fcf = projected_fcf
            dcf_results["dcf_assumptions"]["fcf_growth_rates_projection"].append(round(current_year_growth_rate, 4))

        if not projected_fcfs:  # Should not happen if DEFAULT_FCF_PROJECTION_YEARS > 0
            logger.error(f"DCF: No projected FCFs generated for {self.ticker}.")
            return dcf_results

        terminal_fcf_for_calc = projected_fcfs[-1] * (1 + DEFAULT_PERPETUAL_GROWTH_RATE)

        denominator = DEFAULT_DISCOUNT_RATE - DEFAULT_PERPETUAL_GROWTH_RATE
        if denominator <= 1e-6:  # Avoid division by zero or very small numbers
            logger.warning(
                f"DCF for {self.ticker}: Discount rate ({DEFAULT_DISCOUNT_RATE}) is too close to or less than perpetual growth rate ({DEFAULT_PERPETUAL_GROWTH_RATE}). Terminal value calculation is unreliable.")
            terminal_value = 0  # Or handle as error; cannot reliably calculate TV
        else:
            terminal_value = terminal_fcf_for_calc / denominator

        discounted_values_sum = 0
        for i, fcf_val in enumerate(projected_fcfs):
            discounted_values_sum += fcf_val / ((1 + DEFAULT_DISCOUNT_RATE) ** (i + 1))

        discounted_terminal_value = terminal_value / ((1 + DEFAULT_DISCOUNT_RATE) ** DEFAULT_FCF_PROJECTION_YEARS)

        intrinsic_equity_value = discounted_values_sum + discounted_terminal_value

        intrinsic_value_per_share = intrinsic_equity_value / shares_outstanding
        dcf_results["dcf_intrinsic_value"] = intrinsic_value_per_share

        if current_price and current_price != 0 and intrinsic_value_per_share is not None:
            dcf_results["dcf_upside_percentage"] = (intrinsic_value_per_share - current_price) / current_price

        logger.info(
            f"DCF for {self.ticker}: Intrinsic Value/Share: {dcf_results['dcf_intrinsic_value'] if dcf_results['dcf_intrinsic_value'] is not None else 'N/A'}, Upside: {dcf_results['dcf_upside_percentage'] if dcf_results['dcf_upside_percentage'] is not None else 'N/A'}")
        self._financial_data_cache['dcf_results'] = dcf_results
        return dcf_results

    def _fetch_and_summarize_10k(self):
        # This method structure is largely okay from previous, ensure logging and error handling is fine.
        # Key is that it uses self.stock_db_entry.cik which should now be populated more reliably.
        logger.info(f"Fetching and attempting to summarize latest 10-K for {self.ticker}")
        summary_results = {"qualitative_sources_summary": {}}

        if not self.stock_db_entry or not self.stock_db_entry.cik:
            logger.warning(f"No CIK found for {self.ticker} in DB. Cannot fetch 10-K from EDGAR directly.")
            return summary_results

        filing_url = self.sec_edgar.get_filing_document_url(cik=self.stock_db_entry.cik, form_type="10-K")
        if not filing_url:
            filing_url = self.sec_edgar.get_filing_document_url(cik=self.stock_db_entry.cik, form_type="10-K/A")

        if not filing_url:
            logger.warning(
                f"Could not retrieve 10-K (or 10-K/A) filing URL for {self.ticker} (CIK: {self.stock_db_entry.cik})")
            return summary_results

        ten_k_text_content = self.sec_edgar.get_filing_text(filing_url)
        if not ten_k_text_content:
            logger.warning(f"Failed to fetch 10-K text content from {filing_url}")
            return summary_results

        logger.info(f"Fetched 10-K text (length: {len(ten_k_text_content)}) for {self.ticker}. Extracting sections.")
        extracted_sections = extract_S1_text_sections(ten_k_text_content, TEN_K_KEY_SECTIONS)

        company_name = self.stock_db_entry.company_name or self.ticker
        summary_results["qualitative_sources_summary"]["10k_filing_url_used"] = filing_url

        # Summarize key sections using Gemini
        # Business Summary
        business_text = extracted_sections.get("business", "")
        if business_text:
            prompt_context = f"This is the 'Business' section (Item 1) from the 10-K for {company_name} ({self.ticker}). Summarize core operations, products/services, revenue streams, and target markets."
            summary = self.gemini.summarize_text_with_context(business_text, prompt_context,
                                                              MAX_10K_SECTION_LENGTH_FOR_GEMINI)
            if not summary.startswith("Error:"): summary_results["business_summary"] = summary
            summary_results["qualitative_sources_summary"]["business_10k_source_length"] = len(business_text)
            time.sleep(2)

            # Risk Factors
        risk_text = extracted_sections.get("risk_factors", "")
        if risk_text:
            prompt_context = f"This is 'Risk Factors' (Item 1A) from 10-K for {company_name} ({self.ticker}). Summarize 3-5 most material risks."
            summary = self.gemini.summarize_text_with_context(risk_text, prompt_context,
                                                              MAX_10K_SECTION_LENGTH_FOR_GEMINI)
            if not summary.startswith("Error:"): summary_results["risk_factors_summary"] = summary
            summary_results["qualitative_sources_summary"]["risk_factors_10k_source_length"] = len(risk_text)
            time.sleep(2)

        # MD&A
        mda_text = extracted_sections.get("mda", "")
        if mda_text:
            prompt_context = f"This is MD&A (Item 7) from 10-K for {company_name} ({self.ticker}). Summarize key performance drivers, financial condition, liquidity, capital resources, and management's outlook."
            summary = self.gemini.summarize_text_with_context(mda_text, prompt_context,
                                                              MAX_10K_SECTION_LENGTH_FOR_GEMINI)
            if not summary.startswith("Error:"): summary_results["management_assessment_summary"] = summary
            summary_results["qualitative_sources_summary"]["mda_10k_source_length"] = len(mda_text)
            time.sleep(2)

        # Competitive Landscape from Business & MD&A
        comp_landscape_input_text = (summary_results.get("business_summary", "") + "\n" + summary_results.get(
            "management_assessment_summary", ""))[:MAX_GEMINI_TEXT_LENGTH].strip()
        if comp_landscape_input_text:
            comp_prompt = (
                f"Based on business & MD&A for {company_name} ({self.ticker}): \"{comp_landscape_input_text}\"\n"
                f"Describe its competitive landscape, key competitors, and {company_name}'s competitive positioning.")
            summary_results["competitive_landscape_summary"] = self.gemini.generate_text(comp_prompt)
            summary_results["qualitative_sources_summary"][
                "competitive_landscape_context"] = "Derived from 10-K Business/MD&A summaries."
            time.sleep(2)

        # Economic Moat (if not directly from 10-K summaries)
        moat_input_text = (summary_results.get("business_summary", "") + "\n" + summary_results.get(
            "competitive_landscape_summary", "") + "\n" + summary_results.get("risk_factors_summary", ""))[
                          :MAX_GEMINI_TEXT_LENGTH].strip()
        if moat_input_text and not summary_results.get("economic_moat_summary"):
            moat_prompt = (f"Based on info for {company_name} ({self.ticker}): \"{moat_input_text}\"\n"
                           f"Analyze its primary economic moats (brand, network effects, switching costs, IP, cost advantages). Concise summary.")
            summary_results["economic_moat_summary"] = self.gemini.generate_text(moat_prompt)
            time.sleep(2)

        # Industry Trends (if not directly from 10-K summaries)
        industry_input_text = (summary_results.get("business_summary", "") + "\n" + (
                    self.stock_db_entry.industry or "") + "\n" + (self.stock_db_entry.sector or ""))[
                              :MAX_GEMINI_TEXT_LENGTH].strip()
        if industry_input_text and not summary_results.get("industry_trends_summary"):
            industry_prompt = (
                f"Company: {company_name} in '{self.stock_db_entry.industry}' industry, '{self.stock_db_entry.sector}' sector.\n"
                f"Context: \"{industry_input_text}\"\n"
                f"Analyze current key trends, opportunities, and challenges for this industry/sector. How is {company_name} positioned?")
            summary_results["industry_trends_summary"] = self.gemini.generate_text(industry_prompt)
            time.sleep(2)

        logger.info(f"10-K based qualitative summaries generated for {self.ticker}.")
        self._financial_data_cache['10k_summaries'] = summary_results
        return summary_results

    def _determine_investment_thesis(self):
        # This method structure is okay, ensure it uses the richer data from cache.
        logger.info(f"Synthesizing investment thesis for {self.ticker}...")
        metrics = self._financial_data_cache.get('calculated_metrics', {})
        qual_summaries = self._financial_data_cache.get('10k_summaries', {})
        dcf_results = self._financial_data_cache.get('dcf_results', {})
        company_profile = self._financial_data_cache.get('profile_fmp', {})

        company_name = self.stock_db_entry.company_name or self.ticker
        industry = self.stock_db_entry.industry or "N/A"
        sector = self.stock_db_entry.sector or "N/A"

        prompt = f"Company: {company_name} ({self.ticker})\n"
        prompt += f"Industry: {industry}, Sector: {sector}\n\n"
        prompt += "Key Financial Metrics (approximate values):\n"

        metrics_for_prompt = {
            "P/E Ratio": metrics.get("pe_ratio"), "P/B Ratio": metrics.get("pb_ratio"),
            "P/S Ratio": metrics.get("ps_ratio"), "EV/Sales": metrics.get("ev_to_sales"),
            "EV/EBITDA": metrics.get("ev_to_ebitda"), "Dividend Yield": metrics.get("dividend_yield"),
            "ROE": metrics.get("roe"), "ROIC": metrics.get("roic"),
            "Debt/Equity": metrics.get("debt_to_equity"), "Debt/EBITDA": metrics.get("debt_to_ebitda"),
            "Revenue Growth YoY": metrics.get("revenue_growth_yoy"),
            "Revenue Growth CAGR 3Yr": metrics.get("revenue_growth_cagr_3yr"),
            "EPS Growth YoY": metrics.get("eps_growth_yoy"),
            "Net Profit Margin": metrics.get("net_profit_margin"),
            "Gross Profit Margin": metrics.get("gross_profit_margin"),
            "FCF Yield": metrics.get("free_cash_flow_yield"),
            "FCF Trend": metrics.get("free_cash_flow_trend")
        }
        for name, val in metrics_for_prompt.items():
            if val is not None:
                if isinstance(val, float):
                    if name.endswith(
                            "Yield") or "Growth" in name or "Margin" in name or name == "ROE" or name == "ROIC":
                        val_str = f"{val:.2%}"
                    else:
                        val_str = f"{val:.2f}"
                else:
                    val_str = str(val)
                prompt += f"- {name}: {val_str}\n"

        dcf_val = dcf_results.get("dcf_intrinsic_value")
        dcf_upside = dcf_results.get("dcf_upside_percentage")
        current_price = company_profile.get("price")

        if dcf_val is not None: prompt += f"\nDCF Intrinsic Value per Share: {dcf_val:.2f}\n"
        if dcf_upside is not None: prompt += f"DCF Upside: {dcf_upside:.2%}\n"
        if current_price is not None: prompt += f"Current Stock Price: {current_price:.2f}\n\n"

        prompt += "Qualitative Summary (derived from 10-K, Profile, and AI analysis):\n"
        prompt += f"- Business Overview: {qual_summaries.get('business_summary', 'N/A')[:300]}...\n"
        prompt += f"- Economic Moat: {qual_summaries.get('economic_moat_summary', 'N/A')[:300]}...\n"
        prompt += f"- Competitive Landscape: {qual_summaries.get('competitive_landscape_summary', 'N/A')[:300]}...\n"
        prompt += f"- Industry Trends & Position: {qual_summaries.get('industry_trends_summary', 'N/A')[:300]}...\n"
        prompt += f"- Management Discussion Highlights (MD&A): {qual_summaries.get('management_assessment_summary', 'N/A')[:300]}...\n"
        prompt += f"- Key Risk Factors: {qual_summaries.get('risk_factors_summary', 'N/A')[:300]}...\n\n"

        prompt += (
            "Instructions for AI:\n"
            "1. Provide a comprehensive investment thesis (2-4 paragraphs) incorporating all provided quantitative and qualitative insights.\n"
            "2. State a specific Investment Decision: 'Strong Buy', 'Buy', 'Hold', 'Monitor for Entry', 'Reduce', 'Sell', or 'Avoid'.\n"
            "3. Suggest a suitable Investment Strategy Type: 'Deep Value', 'Value', 'GARP', 'Growth', 'Aggressive Growth', 'Dividend Growth', 'Special Situation', 'Speculative'.\n"
            "4. Indicate a Confidence Level for the assessment: 'High', 'Medium', or 'Low'.\n"
            "5. Detail the Key Reasoning in bullet points, covering:\n"
            "   - Valuation (relative to intrinsic value/peers, considering growth).\n"
            "   - Financial Health & Profitability (margins, debt, cash flow).\n"
            "   - Growth Prospects (historical, future drivers, industry tailwinds).\n"
            "   - Competitive Advantages/Moat.\n"
            "   - Key Risks & Concerns.\n"
            "   - Management & Strategy (if discernible from MD&A).\n"
            "Be objective, balanced, and explicitly state if data is limited or assumptions are strong."
        )

        final_prompt = prompt[:MAX_GEMINI_TEXT_LENGTH]
        if len(prompt) > MAX_GEMINI_TEXT_LENGTH:
            logger.warning(
                f"Investment thesis prompt for {self.ticker} truncated to {MAX_GEMINI_TEXT_LENGTH} chars for Gemini.")

        ai_response = self.gemini.generate_text(final_prompt)

        if ai_response.startswith("Error:"):
            logger.error(f"Gemini failed to generate investment thesis for {self.ticker}: {ai_response}")
            return {
                "investment_decision": "AI Error", "reasoning": ai_response,
                "strategy_type": "N/A", "confidence_level": "N/A",
                "investment_thesis_full": ai_response
            }

        parsed_thesis = {}
        current_section_key = None
        # Simple parser based on keywords; a more robust regex parser might be better
        # For now, ensure these keys match the StockAnalysis model fields where appropriate
        section_map = {
            "investment thesis:": "investment_thesis_full",
            "investment decision:": "investment_decision",
            "strategy type:": "strategy_type",
            "confidence level:": "confidence_level",
            "key reasoning:": "reasoning"
        }

        collected_lines = {}  # To store lines for multi-line sections

        for line in ai_response.split('\n'):
            line_l_strip = line.lower().strip()
            found_new_section = False
            for header, key_name in section_map.items():
                if line_l_strip.startswith(header):
                    current_section_key = key_name
                    # Initialize list for this section's lines
                    collected_lines[current_section_key] = [line[len(header):].strip()]
                    found_new_section = True
                    break  # Found a header, move to next line

            if not found_new_section and current_section_key:
                # If we are in a section, append the line
                collected_lines[current_section_key].append(line)

        # Join collected lines for each section
        for key, lines_list in collected_lines.items():
            parsed_thesis[key] = "\n".join(lines_list).strip()

        # Fallbacks
        if "investment_decision" not in parsed_thesis: parsed_thesis["investment_decision"] = "Review AI Output"
        if "reasoning" not in parsed_thesis: parsed_thesis["reasoning"] = ai_response  # fallback
        if "investment_thesis_full" not in parsed_thesis: parsed_thesis[
            "investment_thesis_full"] = ai_response  # fallback

        logger.info(
            f"Generated investment thesis for {self.ticker}. Decision: {parsed_thesis.get('investment_decision')}")
        return parsed_thesis

    def analyze(self):
        logger.info(f"Full analysis pipeline started for {self.ticker}...")
        final_analysis_data = {}

        try:
            if not self.stock_db_entry:
                logger.error(f"Stock entry for {self.ticker} was not properly initialized. Aborting analysis.")
                return None

            self._ensure_stock_db_entry_is_bound()

            self._fetch_financial_statements()
            self._fetch_key_metrics_and_profile_data()

            calculated_metrics = self._calculate_derived_metrics()
            final_analysis_data.update(calculated_metrics)

            dcf_results = self._perform_dcf_analysis()
            final_analysis_data.update(dcf_results)

            qual_summaries = self._fetch_and_summarize_10k()
            final_analysis_data.update(qual_summaries)

            investment_thesis_parts = self._determine_investment_thesis()
            final_analysis_data.update(investment_thesis_parts)

            # Ensure the 'reasoning' field gets the detailed bullet points if available,
            # and 'investment_thesis_full' gets the narrative thesis.
            # The parser for _determine_investment_thesis now separates these.
            # If 'reasoning' is not specifically parsed, it might default to the full response.
            # Let's ensure that "reasoning" is specifically the key points.

            analysis_entry = StockAnalysis(stock_id=self.stock_db_entry.id)

            model_fields = [col.key for col in StockAnalysis.__table__.columns if
                            col.key not in ['id', 'stock_id', 'analysis_date']]

            for field in model_fields:
                if field in final_analysis_data:
                    value_to_set = final_analysis_data[field]
                    if getattr(StockAnalysis, field).type.python_type == float:
                        if isinstance(value_to_set, str) and value_to_set.lower() == "n/a":
                            value_to_set = None
                        elif isinstance(value_to_set, str):  # Try to convert string to float if it's a number
                            try:
                                value_to_set = float(value_to_set)
                            except ValueError:
                                value_to_set = None  # If conversion fails, set to None
                        if isinstance(value_to_set, float) and (math.isnan(value_to_set) or math.isinf(value_to_set)):
                            value_to_set = None

                    setattr(analysis_entry, field, value_to_set)

            # Ensure the specific parsed reasoning is used if available
            if "reasoning" in investment_thesis_parts and investment_thesis_parts[
                "reasoning"] != "Reasoning not explicitly parsed.":
                analysis_entry.reasoning = investment_thesis_parts["reasoning"]
            elif "investment_thesis_full" in investment_thesis_parts:  # Fallback if detailed reasoning wasn't clear
                analysis_entry.reasoning = investment_thesis_parts["investment_thesis_full"]

            self.db_session.add(analysis_entry)
            self.stock_db_entry.last_analysis_date = datetime.now(timezone.utc)
            self.db_session.commit()

            logger.info(f"Successfully analyzed and saved stock: {self.ticker} (Analysis ID: {analysis_entry.id})")
            return analysis_entry

        except RuntimeError as r_err:
            logger.critical(f"Runtime error during analysis for {self.ticker}: {r_err}", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"CRITICAL error during full analysis pipeline for {self.ticker}: {e}", exc_info=True)
            if self.db_session and self.db_session.is_active:
                try:
                    self.db_session.rollback()
                    logger.info(f"Rolled back transaction for {self.ticker} due to error.")
                except Exception as e_rollback:
                    logger.error(f"Error during rollback for {self.ticker}: {e_rollback}")
            return None
        finally:
            self._close_session_if_active()

    def _ensure_stock_db_entry_is_bound(self):
        if not self.db_session.is_active:
            logger.warning(f"Session for {self.ticker} is INACTIVE. Re-establishing and re-fetching stock entry.")
            self._close_session_if_active()
            self.db_session = next(get_db_session())

            re_fetched_stock = self.db_session.query(Stock).filter(Stock.ticker == self.ticker).first()
            if not re_fetched_stock:
                logger.error(
                    f"Could not re-fetch stock {self.ticker} after session re-establishment. This may indicate a problem with initial creation or commit.")
                self._get_or_create_stock_entry()  # Attempt to fix it by re-running the creation/fetch logic
                if not self.stock_db_entry:  # If still no stock_db_entry, then critical failure.
                    raise RuntimeError(f"Failed to create or re-fetch stock {self.ticker} for active session.")
                return
            self.stock_db_entry = re_fetched_stock
            logger.info(f"Re-fetched and bound stock {self.ticker} (ID: {self.stock_db_entry.id}) to new session.")
            return

        instance_state = sa_inspect(self.stock_db_entry)
        if not instance_state.session or instance_state.session is not self.db_session:
            object_id_for_log = self.stock_db_entry.id if instance_state.has_identity else 'Unknown ID (transient)'
            current_session_id = id(self.db_session)
            instance_session_id = id(instance_state.session) if instance_state.session else 'None'

            logger.warning(
                f"Stock entry {self.ticker} (ID: {object_id_for_log}) is DETACHED or bound to a DIFFERENT session "
                f"(Expected: {current_session_id}, Actual: {instance_session_id}). Attempting to merge."
            )
            try:
                merged_stock = self.db_session.merge(self.stock_db_entry)
                self.stock_db_entry = merged_stock
                logger.info(
                    f"Successfully merged/re-associated stock {self.ticker} (ID: {self.stock_db_entry.id}) into current session {id(self.db_session)}.")
            except Exception as e_merge:
                logger.error(f"Failed to merge stock {self.ticker} into session: {e_merge}. Re-fetching as fallback.",
                             exc_info=True)
                primary_key_to_fetch = self.stock_db_entry.id if instance_state.has_identity and self.stock_db_entry.id else None

                re_fetched_stock_on_merge_fail = None
                if primary_key_to_fetch:
                    re_fetched_stock_on_merge_fail = self.db_session.query(Stock).get(primary_key_to_fetch)

                if not re_fetched_stock_on_merge_fail:
                    re_fetched_stock_on_merge_fail = self.db_session.query(Stock).filter(
                        Stock.ticker == self.ticker).first()

                if re_fetched_stock_on_merge_fail:
                    self.stock_db_entry = re_fetched_stock_on_merge_fail
                    logger.info(
                        f"Successfully re-fetched stock {self.ticker} (ID: {self.stock_db_entry.id}) after merge failure.")
                else:
                    logger.critical(
                        f"CRITICAL: Failed to re-associate stock {self.ticker} with current session after merge failure and could not re-fetch. Analysis cannot proceed reliably.")
                    raise RuntimeError(f"Failed to bind stock {self.ticker} to session for analysis.")


if __name__ == '__main__':
    from database import init_db

    # init_db()

    logger.info("Starting standalone stock analysis test...")
    tickers_to_test = ["AAPL", "MSFT", "GOOG"]  # Test with GOOG which had DCF issue in log

    for ticker_symbol in tickers_to_test:
        analysis_result = None
        try:
            logger.info(f"--- Analyzing {ticker_symbol} ---")
            analyzer = StockAnalyzer(ticker=ticker_symbol)
            analysis_result = analyzer.analyze()

            if analysis_result:
                logger.info(
                    f"Analysis for {analysis_result.stock.ticker} completed. Decision: {analysis_result.investment_decision}, Confidence: {analysis_result.confidence_level}")
                if analysis_result.dcf_intrinsic_value is not None:
                    logger.info(
                        f"DCF Value: {analysis_result.dcf_intrinsic_value:.2f}, Upside: {analysis_result.dcf_upside_percentage:.2% if analysis_result.dcf_upside_percentage is not None else 'N/A'}")
                else:
                    logger.info("DCF analysis did not yield an intrinsic value.")
                logger.info(
                    f"Reasoning highlights: {str(analysis_result.reasoning)[:300]}...")  # Ensure reasoning is string

            else:
                logger.error(f"Stock analysis pipeline FAILED for {ticker_symbol} (returned None).")
        except RuntimeError as rt_err:
            logger.error(f"Could not initialize StockAnalyzer for {ticker_symbol}: {rt_err}")
        except Exception as e:
            logger.error(f"Unhandled error analyzing {ticker_symbol} in __main__: {e}", exc_info=True)
        finally:
            logger.info(f"--- Finished processing {ticker_symbol} ---")
            if analysis_result is None: logger.info(f"No analysis result object for {ticker_symbol}")
            time.sleep(10)
---------- END stock_analyzer.py ----------

--- END OF FILE project_structure_backend.txt ---
